Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Parquet format in Azure Data Factory and Azure Synapse Analytics
Article
2025-02-13
17 contributors
In this article
APPLIES TO:Azure Data FactoryAzure Synapse Analytics
Tip
Try outData Factory in Microsoft Fabric, an all-in-one analytics solution for enterprises.Microsoft Fabriccovers everything from data movement to data science, real-time analytics, business intelligence, and reporting. Learn how tostart a new trialfor free!
Follow this article when you want toparse the Parquet files or write the data into Parquet format.
Parquet format is supported for the following connectors:
Amazon S3
Amazon S3 Compatible Storage
Azure Blob
Azure Data Lake Storage Gen1
Azure Data Lake Storage Gen2
Azure Files
File System
FTP
Google Cloud Storage
HDFS
HTTP
Oracle Cloud Storage
SFTP
For a list of supported features for all available connectors, visit theConnectors Overviewarticle.
Using Self-hosted Integration Runtime
Important
For copy empowered by Self-hosted Integration Runtime e.g. between on-premises and cloud data stores, if you are not copying Parquet filesas-is, you need to install the64-bit JRE 8 (Java Runtime Environment), JDK 23 (Java Development Kit),  or OpenJDKon your IR machine. Check the following paragraph with more details.
For copy running on Self-hosted IR with Parquet file serialization/deserialization, the service locates the Java runtime by firstly checking the registry(SOFTWARE\JavaSoft\Java Runtime Environment\{Current Version}\JavaHome)for JRE, if not found, secondly checking system variableJAVA_HOMEfor OpenJDK.
(SOFTWARE\JavaSoft\Java Runtime Environment\{Current Version}\JavaHome)
JAVA_HOME
To use JRE: The 64-bit IR requires 64-bit JRE. You can find it fromhere.
To use JDK: The 64-but IR requires 64-bit JDK 23. You can find it fromhere. Be sure to update theJAVA_HOMEsystem variable to the root folder of the JDK 23 installation i.e.C:\Program Files\Java\jdk-23, and add the path to both theC:\Program Files\Java\jdk-23\binandC:\Program Files\Java\jdk-23\bin\serverfolders to thePathsystem variable.
JAVA_HOME
C:\Program Files\Java\jdk-23
C:\Program Files\Java\jdk-23\bin
C:\Program Files\Java\jdk-23\bin\server
Path
To use OpenJDK: It's supported since IR version 3.13. Package the jvm.dll with all other required assemblies of OpenJDK into Self-hosted IR machine, and set system environment variable JAVA_HOME accordingly, and then restart Self-hosted IR for taking effect immediately. To download the Microsoft Build of OpenJDK, seeMicrosoft Build of OpenJDKâ¢.
Tip
If you copy data to/from Parquet format using Self-hosted Integration Runtime and hit error saying "An error occurred when invoking java, message:java.lang.OutOfMemoryError:Java heap space", you can add an environment variable_JAVA_OPTIONSin the machine that hosts the Self-hosted IR to adjust the min/max heap size for JVM to empower such copy, then rerun the pipeline.
_JAVA_OPTIONS

Example: set variable_JAVA_OPTIONSwith value-Xms256m -Xmx16g. The flagXmsspecifies the initial memory allocation pool for a Java Virtual Machine (JVM), whileXmxspecifies the maximum memory allocation pool. This means that JVM will be started withXmsamount of memory and will be able to use a maximum ofXmxamount of memory. By default, the service uses min 64 MB and max 1G.
_JAVA_OPTIONS
-Xms256m -Xmx16g
Xms
Xmx
Xms
Xmx
Dataset properties
For a full list of sections and properties available for defining datasets, see theDatasetsarticle. This section provides a list of properties supported by the Parquet dataset.
location
Note
White space in column name is not supported for Parquet files.
Below is an example of Parquet dataset on Azure Blob Storage:
{
    "name": "ParquetDataset",
    "properties": {
        "type": "Parquet",
        "linkedServiceName": {
            "referenceName": "<Azure Blob Storage linked service name>",
            "type": "LinkedServiceReference"
        },
        "schema": [ < physical schema, optional, retrievable during authoring > ],
        "typeProperties": {
            "location": {
                "type": "AzureBlobStorageLocation",
                "container": "containername",
                "folderPath": "folder/subfolder",
            },
            "compressionCodec": "snappy"
        }
    }
}
{
    "name": "ParquetDataset",
    "properties": {
        "type": "Parquet",
        "linkedServiceName": {
            "referenceName": "<Azure Blob Storage linked service name>",
            "type": "LinkedServiceReference"
        },
        "schema": [ < physical schema, optional, retrievable during authoring > ],
        "typeProperties": {
            "location": {
                "type": "AzureBlobStorageLocation",
                "container": "containername",
                "folderPath": "folder/subfolder",
            },
            "compressionCodec": "snappy"
        }
    }
}
Copy activity properties
For a full list of sections and properties available for defining activities, see thePipelinesarticle. This section provides a list of properties supported by the Parquet source and sink.
Parquet as source
The following properties are supported in the copy activity*source*section.
storeSettings
Parquet as sink
The following properties are supported in the copy activity*sink*section.
storeSettings
SupportedParquet write settingsunderformatSettings:
formatSettings
maxRowsPerFile
<fileNamePrefix>_00000.<fileExtension>
Mapping data flow properties
In mapping data flows, you can read and write to parquet format in the following data stores:Azure Blob Storage,Azure Data Lake Storage Gen1,Azure Data Lake Storage Gen2andSFTP, and you can read parquet format inAmazon S3.
Source properties
The below table lists the properties supported by a parquet source. You can edit these properties in theSource optionstab.
parquet
parquet
true
false
true
false
[<from>, <to>]
true
false
Source example
The below image is an example of a parquet source configuration in mapping data flows.

The associated data flow script is:
source(allowSchemaDrift: true,
    validateSchema: false,
    rowUrlColumn: 'fileName',
    format: 'parquet') ~> ParquetSource
source(allowSchemaDrift: true,
    validateSchema: false,
    rowUrlColumn: 'fileName',
    format: 'parquet') ~> ParquetSource
Sink properties
The below table lists the properties supported by a parquet sink. You can edit these properties in theSettingstab.
parquet
parquet
true
false
part-#####-tid-<guid>
['<fileName>']
Sink example
The below image is an example of a parquet sink configuration in mapping data flows.

The associated data flow script is:
ParquetSource sink(
    format: 'parquet',
    filePattern:'output[n].parquet',
    truncate: true,
    allowSchemaDrift: true,
    validateSchema: false,
    skipDuplicateMapInputs: true,
    skipDuplicateMapOutputs: true) ~> ParquetSink
ParquetSource sink(
    format: 'parquet',
    filePattern:'output[n].parquet',
    truncate: true,
    allowSchemaDrift: true,
    validateSchema: false,
    skipDuplicateMapInputs: true,
    skipDuplicateMapOutputs: true) ~> ParquetSink
Data type support
Parquet complex data types (e.g. MAP, LIST, STRUCT) are currently supported only in Data Flows, not in Copy Activity. To use complex types in data flows, do not import the file schema in the dataset, leaving schema blank in the dataset. Then, in the Source transformation, import the projection.
Related content
Copy activity overview
Mapping data flow
Lookup activity
GetMetadata activity
Feedback
Was this page helpful?
Additional resources