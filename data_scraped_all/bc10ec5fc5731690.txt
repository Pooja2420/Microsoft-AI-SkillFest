Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Improve performance for NFS Azure file shares
Article
2025-03-31
3 contributors
In this article
This article explains how you can improve performance for network file system (NFS) Azure file shares.
Applies to
Increase read-ahead size to improve read throughput
Theread_ahead_kbkernel parameter in Linux represents the amount of data that should be "read ahead" or prefetched during a sequential read operation. Linux kernel versions before 5.4 set the read-ahead value to the equivalent of 15 times the mounted file system'srsize, which represents the client-side mount option for read buffer size. This sets the read-ahead value high enough to improve client sequential read throughput in most cases.
read_ahead_kb
rsize
However, beginning with Linux kernel version 5.4, the Linux NFS client uses a defaultread_ahead_kbvalue of 128 KiB. This small value might reduce the amount of read throughput for large files. Customers upgrading from Linux releases with the larger read-ahead value to releases with the 128 KiB default might experience a decrease in sequential read performance.
read_ahead_kb
For Linux kernels 5.4 or later, we recommend persistently setting theread_ahead_kbto 15 MiB for improved performance.
read_ahead_kb
To change this value, set the read-ahead size by adding a rule in udev, a Linux kernel device manager. Follow these steps:
In a text editor, create the/etc/udev/rules.d/99-nfs.rulesfile by entering and saving the following text:SUBSYSTEM=="bdi" \
, ACTION=="add" \
, PROGRAM="/usr/bin/awk -v bdi=$kernel 'BEGIN{ret=1} {if ($4 == bdi) {ret=0}} END{exit ret}' /proc/fs/nfsfs/volumes" \
, ATTR{read_ahead_kb}="15360"
In a text editor, create the/etc/udev/rules.d/99-nfs.rulesfile by entering and saving the following text:
SUBSYSTEM=="bdi" \
, ACTION=="add" \
, PROGRAM="/usr/bin/awk -v bdi=$kernel 'BEGIN{ret=1} {if ($4 == bdi) {ret=0}} END{exit ret}' /proc/fs/nfsfs/volumes" \
, ATTR{read_ahead_kb}="15360"
SUBSYSTEM=="bdi" \
, ACTION=="add" \
, PROGRAM="/usr/bin/awk -v bdi=$kernel 'BEGIN{ret=1} {if ($4 == bdi) {ret=0}} END{exit ret}' /proc/fs/nfsfs/volumes" \
, ATTR{read_ahead_kb}="15360"
In a console, apply the udev rule by running theudevadmcommand as a superuser and reloading the rules files and other databases. You only need to run this command once, to make udev aware of the new file.sudo udevadm control --reload
In a console, apply the udev rule by running theudevadmcommand as a superuser and reloading the rules files and other databases. You only need to run this command once, to make udev aware of the new file.
sudo udevadm control --reload
sudo udevadm control --reload
NFS nconnect
NFS nconnect is a client-side mount option for NFS file shares that allows you to use multiple TCP connections between the client and your NFS file share.
Benefits
With nconnect, you can increase performance at scale using fewer client machines to reduce total cost of ownership (TCO). The nconnect feature increases performance by using multiple TCP channels on one or more NICs, using single or multiple clients. Without nconnect, you'd need roughly 20 client machines in order to achieve the bandwidth scale limits (10 GiB / sec) offered by the largest SSD file share provisioning size. With nconnect, you can achieve those limits using only 6-7 clients, reducing compute costs by nearly 70% while providing significant improvements in I/O operations per second (IOPS) and throughput at scale. See the following table.
Prerequisites
The latest Linux distributions fully support nconnect. For older Linux distributions, ensure that the Linux kernel version is 5.3 or higher.
Per-mount configuration is only supported when a single file share is used per storage account over a private endpoint.
Performance impact
We achieved the following performance results when using the nconnect mount option with NFS Azure file shares on Linux clients at scale. For more information on how we achieved these results, seeperformance test configuration.


Recommendations
Follow these recommendations to get the best results fromnconnect.
nconnect
nconnect=4
While Azure Files supports setting nconnect up to the maximum setting of 16, we recommend configuring the mount options with the optimal setting of nconnect=4. Currently, there are no gains beyond four channels for the Azure Files implementation of nconnect. In fact, exceeding four channels to a single Azure file share from a single client might adversely affect performance due to TCP network saturation.
Depending on your workload requirements, it's important to correctly size the client virtual machines (VMs) to avoid being restricted by theirexpected network bandwidth. You don't need multiple network interface controllers (NICs) in order to achieve the expected network throughput. While it's common to usegeneral purpose VMswith Azure Files, various VM types are available depending on your workload needs and region availability. For more information, seeAzure VM Selector.
Queue depth is the number of pending I/O requests that a storage resource can service. We don't recommend exceeding the optimal queue depth of 64 because you won't see any more performance gains. For more information, seeQueue depth.
Per mount configuration
If a workload requires mounting multiple shares with one or more storage accounts with different nconnect settings from a single client, we can't guarantee that those settings persist when mounting over the public endpoint. Per mount configuration is only supported when a single Azure file share is used per storage account over the private endpoint as described in Scenario 1.
StorageAccount.file.core.windows.net = 10.10.10.10
StorageAccount2.file.core.windows.net = 10.10.10.11Mount StorageAccount.file.core.windows.net:/StorageAccount/FileShare1 nconnect=4Mount StorageAccount2.file.core.windows.net:/StorageAccount2/FileShare1
Mount StorageAccount.file.core.windows.net:/StorageAccount/FileShare1 nconnect=4
Mount StorageAccount.file.core.windows.net:/StorageAccount/FileShare1 nconnect=4
Mount StorageAccount2.file.core.windows.net:/StorageAccount2/FileShare1
Mount StorageAccount2.file.core.windows.net:/StorageAccount2/FileShare1
StorageAccount.file.core.windows.net = 52.239.238.8
StorageAccount2.file.core.windows.net = 52.239.238.7Mount StorageAccount.file.core.windows.net:/StorageAccount/FileShare1 nconnect=4Mount StorageAccount.file.core.windows.net:/StorageAccount/FileShare2Mount StorageAccount2.file.core.windows.net:/StorageAccount2/FileShare1
Mount StorageAccount.file.core.windows.net:/StorageAccount/FileShare1 nconnect=4
Mount StorageAccount.file.core.windows.net:/StorageAccount/FileShare1 nconnect=4
Mount StorageAccount.file.core.windows.net:/StorageAccount/FileShare2
Mount StorageAccount.file.core.windows.net:/StorageAccount/FileShare2
Mount StorageAccount2.file.core.windows.net:/StorageAccount2/FileShare1
Mount StorageAccount2.file.core.windows.net:/StorageAccount2/FileShare1
Note
Even if the storage account resolves to a different IP address, we can't guarantee that address persist because public endpoints aren't static addresses.
StorageAccount.file.core.windows.net = 10.10.10.10Mount StorageAccount.file.core.windows.net:/StorageAccount/FileShare1 nconnect=4Mount StorageAccount.file.core.windows.net:/StorageAccount/FileShare2Mount StorageAccount.file.core.windows.net:/StorageAccount/FileShare3
Mount StorageAccount.file.core.windows.net:/StorageAccount/FileShare1 nconnect=4
Mount StorageAccount.file.core.windows.net:/StorageAccount/FileShare1 nconnect=4
Mount StorageAccount.file.core.windows.net:/StorageAccount/FileShare2
Mount StorageAccount.file.core.windows.net:/StorageAccount/FileShare2
Mount StorageAccount.file.core.windows.net:/StorageAccount/FileShare3
Mount StorageAccount.file.core.windows.net:/StorageAccount/FileShare3
Performance test configuration
We used the following resources and benchmarking tools to achieve and measure the results outlined in this article.
Single client:Azure VM (DSv4-Series) with single NIC
OS:Linux (Ubuntu 20.40)
NFS storage:SSD file share (provisioned 30 TiB, setnconnect=4)
nconnect=4
Benchmarking tools and tests
We used Flexible I/O Tester (FIO), a free, open-source disk I/O tool used both for benchmark and stress/hardware verification. To install FIO, follow the Binary Packages section in theâ¯FIO README fileâ¯to install for the platform of your choice.
While these tests focus on random I/O access patterns, you get similar results when using sequential I/O.
4k I/O size - random read - 64 queue depth
fio --ioengine=libaio --direct=1 --nrfiles=4 --numjobs=1 --runtime=1800 --time_based --bs=4k --iodepth=64 --filesize=4G --rw=randread --group_reporting --ramp_time=300
fio --ioengine=libaio --direct=1 --nrfiles=4 --numjobs=1 --runtime=1800 --time_based --bs=4k --iodepth=64 --filesize=4G --rw=randread --group_reporting --ramp_time=300
8k I/O size - random read - 64 queue depth
fio --ioengine=libaio --direct=1 --nrfiles=4 --numjobs=1 --runtime=1800 --time_based --bs=8k --iodepth=64 --filesize=4G --rw=randread --group_reporting --ramp_time=300
fio --ioengine=libaio --direct=1 --nrfiles=4 --numjobs=1 --runtime=1800 --time_based --bs=8k --iodepth=64 --filesize=4G --rw=randread --group_reporting --ramp_time=300
64 KiB I/O size - random read - 64 queue depth
fio --ioengine=libaio --direct=1 --nrfiles=4 --numjobs=1 --runtime=1800 --time_based --bs=64k --iodepth=64 --filesize=4G --rw=randread --group_reporting --ramp_time=300
fio --ioengine=libaio --direct=1 --nrfiles=4 --numjobs=1 --runtime=1800 --time_based --bs=64k --iodepth=64 --filesize=4G --rw=randread --group_reporting --ramp_time=300
1,024 KiB I/O size - 100% random read - 64 queue depth
fio --ioengine=libaio --direct=1 --nrfiles=4 --numjobs=1 --runtime=1800 --time_based --bs=1024k --iodepth=64 --filesize=4G --rw=randread --group_reporting --ramp_time=300
fio --ioengine=libaio --direct=1 --nrfiles=4 --numjobs=1 --runtime=1800 --time_based --bs=1024k --iodepth=64 --filesize=4G --rw=randread --group_reporting --ramp_time=300
4 KiB I/O size - 100% random write - 64 queue depth
fio --ioengine=libaio --direct=1 --nrfiles=4 --numjobs=1 --runtime=1800 --time_based --bs=4k --iodepth=64 --filesize=4G --rw=randwrite --group_reporting --ramp_time=300
fio --ioengine=libaio --direct=1 --nrfiles=4 --numjobs=1 --runtime=1800 --time_based --bs=4k --iodepth=64 --filesize=4G --rw=randwrite --group_reporting --ramp_time=300
8 KiB I/O size - 100% random write - 64 queue depth
fio --ioengine=libaio --direct=1 --nrfiles=4 --numjobs=1 --runtime=1800 --time_based --bs=8k --iodepth=64 --filesize=4G --rw=randwrite --group_reporting --ramp_time=300
fio --ioengine=libaio --direct=1 --nrfiles=4 --numjobs=1 --runtime=1800 --time_based --bs=8k --iodepth=64 --filesize=4G --rw=randwrite --group_reporting --ramp_time=300
64 KiB I/O size  - 100% random write - 64 queue depth
fio --ioengine=libaio --direct=1 --nrfiles=4 --numjobs=1 --runtime=1800 --time_based --bs=64k --iodepth=64 --filesize=4G --rw=randwrite --group_reporting --ramp_time=300
fio --ioengine=libaio --direct=1 --nrfiles=4 --numjobs=1 --runtime=1800 --time_based --bs=64k --iodepth=64 --filesize=4G --rw=randwrite --group_reporting --ramp_time=300
1024 KiB I/O size  - 100% random write - 64 queue depth
fio --ioengine=libaio --direct=1 --nrfiles=4 --numjobs=1 --runtime=1800 --time_based --bs=1024k --iodepth=64 --filesize=4G --rw=randwrite --group_reporting --ramp_time=300
fio --ioengine=libaio --direct=1 --nrfiles=4 --numjobs=1 --runtime=1800 --time_based --bs=1024k --iodepth=64 --filesize=4G --rw=randwrite --group_reporting --ramp_time=300
Performance considerations fornconnect
nconnect
When using thenconnectmount option, you should closely evaluate workloads that have the following characteristics:
nconnect
Latency sensitive write workloads that are single threaded and/or use a low queue depth (less than 16)
Latency sensitive read workloads that are single threaded and/or use a low queue depth in combination with smaller I/O sizes
Not all workloads require high-scale IOPS or throughout performance. For smaller scale workloads,nconnectmight not make sense. Use the following table to decide whethernconnectis advantageous for your workload. Scenarios highlighted in green are recommended, while scenarios highlighted in red aren't. Scenarios highlighted in yellow are neutral.
nconnect
nconnect

See also
Mount NFS file Share to Linux
List of mount options
Understand Azure Files performance
Feedback
Was this page helpful?
Additional resources