Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Use Apache Zeppelin notebooks with Apache Spark cluster on Azure HDInsight
Article
2025-02-03
16 contributors
In this article
HDInsight Spark clusters includeApache Zeppelinnotebooks. Use the notebooks to run Apache Spark jobs. In this article, you learn how to use the Zeppelin notebook on an HDInsight cluster.
Prerequisites
An Apache Spark cluster on HDInsight. For instructions, seeCreate Apache Spark clusters in Azure HDInsight.
The URI scheme for your clusters primary storage. The scheme would bewasb://for Azure Blob Storage,abfs://for Azure Data Lake Storage Gen2 oradl://for Azure Data Lake Storage Gen1. If secure transfer is enabled for Blob Storage, the URI would bewasbs://.  For more information, seeRequire secure transfer in Azure Storage.
wasb://
abfs://
adl://
wasbs://
Launch an Apache Zeppelin notebook
From the Spark clusterOverview, selectZeppelin notebookfromCluster dashboards. Enter the admin credentials for the cluster.NoteYou may also reach the Zeppelin Notebook for your cluster by opening the following URL in your browser. ReplaceCLUSTERNAMEwith the name of your cluster:https://CLUSTERNAME.azurehdinsight.net/zeppelin
From the Spark clusterOverview, selectZeppelin notebookfromCluster dashboards. Enter the admin credentials for the cluster.
Note
You may also reach the Zeppelin Notebook for your cluster by opening the following URL in your browser. ReplaceCLUSTERNAMEwith the name of your cluster:
https://CLUSTERNAME.azurehdinsight.net/zeppelin
https://CLUSTERNAME.azurehdinsight.net/zeppelin
Create a new notebook. From the header pane, navigate toNotebook>Create new note.Enter a name for the notebook, then selectCreate Note.
Create a new notebook. From the header pane, navigate toNotebook>Create new note.

Enter a name for the notebook, then selectCreate Note.
Ensure the notebook header shows a connected status. It's denoted by a green dot in the top-right corner.
Ensure the notebook header shows a connected status. It's denoted by a green dot in the top-right corner.

Load sample data into a temporary table. When you create a Spark cluster in HDInsight, the sample data file,hvac.csv, is copied to the associated storage account under\HdiSamples\SensorSampleData\hvac.In the empty paragraph that is created by default in the new notebook, paste the following snippet.%livy2.spark
//The above magic instructs Zeppelin to use the Livy Scala interpreter

// Create an RDD using the default Spark context, sc
val hvacText = sc.textFile("wasbs:///HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv")

// Define a schema
case class Hvac(date: String, time: String, targettemp: Integer, actualtemp: Integer, buildingID: String)

// Map the values in the .csv file to the schema
val hvac = hvacText.map(s => s.split(",")).filter(s => s(0) != "Date").map(
    s => Hvac(s(0),
            s(1),
            s(2).toInt,
            s(3).toInt,
            s(6)
    )
).toDF()

// Register as a temporary table called "hvac"
hvac.registerTempTable("hvac")PressSHIFT + ENTERor select thePlaybutton for the paragraph to run the snippet. The status on the right-corner of the paragraph should progress from READY, PENDING, RUNNING to FINISHED. The output shows up at the bottom of the same paragraph. The screenshot looks like the following image:You can also provide a title to each paragraph. From the right-hand corner of the paragraph, select theSettingsicon (sprocket), and then selectShow title.Note%spark2 interpreter isn't supported in Zeppelin notebooks across all HDInsight versions, and %sh interpreter not supported from HDInsight 4.0 onwards.
Load sample data into a temporary table. When you create a Spark cluster in HDInsight, the sample data file,hvac.csv, is copied to the associated storage account under\HdiSamples\SensorSampleData\hvac.
hvac.csv
\HdiSamples\SensorSampleData\hvac
In the empty paragraph that is created by default in the new notebook, paste the following snippet.
%livy2.spark
//The above magic instructs Zeppelin to use the Livy Scala interpreter

// Create an RDD using the default Spark context, sc
val hvacText = sc.textFile("wasbs:///HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv")

// Define a schema
case class Hvac(date: String, time: String, targettemp: Integer, actualtemp: Integer, buildingID: String)

// Map the values in the .csv file to the schema
val hvac = hvacText.map(s => s.split(",")).filter(s => s(0) != "Date").map(
    s => Hvac(s(0),
            s(1),
            s(2).toInt,
            s(3).toInt,
            s(6)
    )
).toDF()

// Register as a temporary table called "hvac"
hvac.registerTempTable("hvac")
%livy2.spark
//The above magic instructs Zeppelin to use the Livy Scala interpreter

// Create an RDD using the default Spark context, sc
val hvacText = sc.textFile("wasbs:///HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv")

// Define a schema
case class Hvac(date: String, time: String, targettemp: Integer, actualtemp: Integer, buildingID: String)

// Map the values in the .csv file to the schema
val hvac = hvacText.map(s => s.split(",")).filter(s => s(0) != "Date").map(
    s => Hvac(s(0),
            s(1),
            s(2).toInt,
            s(3).toInt,
            s(6)
    )
).toDF()

// Register as a temporary table called "hvac"
hvac.registerTempTable("hvac")
PressSHIFT + ENTERor select thePlaybutton for the paragraph to run the snippet. The status on the right-corner of the paragraph should progress from READY, PENDING, RUNNING to FINISHED. The output shows up at the bottom of the same paragraph. The screenshot looks like the following image:

You can also provide a title to each paragraph. From the right-hand corner of the paragraph, select theSettingsicon (sprocket), and then selectShow title.
Note
%spark2 interpreter isn't supported in Zeppelin notebooks across all HDInsight versions, and %sh interpreter not supported from HDInsight 4.0 onwards.
You can now run Spark SQL statements on thehvactable. Paste the following query in a new paragraph. The query retrieves the building ID. Also the difference between the target and actual temperatures for each building on a given date. PressSHIFT + ENTER.%sql
select buildingID, (targettemp - actualtemp) as temp_diff, date from hvac where date = "6/1/13"The%sqlstatement at the beginning tells the notebook to use the Livy Scala interpreter.
You can now run Spark SQL statements on thehvactable. Paste the following query in a new paragraph. The query retrieves the building ID. Also the difference between the target and actual temperatures for each building on a given date. PressSHIFT + ENTER.
hvac
%sql
select buildingID, (targettemp - actualtemp) as temp_diff, date from hvac where date = "6/1/13"
%sql
select buildingID, (targettemp - actualtemp) as temp_diff, date from hvac where date = "6/1/13"
The%sqlstatement at the beginning tells the notebook to use the Livy Scala interpreter.
Select theBar Charticon to change the display.settingsappear after you selectedBar Chart, allows you to chooseKeys, andValues.  The following screenshot shows the output.
Select theBar Charticon to change the display.settingsappear after you selectedBar Chart, allows you to chooseKeys, andValues.  The following screenshot shows the output.

You can also run Spark SQL statements using variables in the query. The next snippet shows how to define a variable,Temp, in the query with the possible values you want to query with. When you first run the query, a drop-down is automatically populated with the values you specified for the variable.%sql  
select buildingID, date, targettemp, (targettemp - actualtemp) as temp_diff from hvac where targettemp > "${Temp = 65,65|75|85}"Paste this snippet in a new paragraph and pressSHIFT + ENTER. Then select65from theTempdrop-down list.
You can also run Spark SQL statements using variables in the query. The next snippet shows how to define a variable,Temp, in the query with the possible values you want to query with. When you first run the query, a drop-down is automatically populated with the values you specified for the variable.
Temp
%sql  
select buildingID, date, targettemp, (targettemp - actualtemp) as temp_diff from hvac where targettemp > "${Temp = 65,65|75|85}"
%sql  
select buildingID, date, targettemp, (targettemp - actualtemp) as temp_diff from hvac where targettemp > "${Temp = 65,65|75|85}"
Paste this snippet in a new paragraph and pressSHIFT + ENTER. Then select65from theTempdrop-down list.
Select theBar Charticon to change the display.  Then selectsettingsand make the following changes:Groups:Addtargettemp.Values:1. Removedate.  2. Addtemp_diff.  3.  Change the aggregator fromSUMtoAVG.The following screenshot shows the output.
Select theBar Charticon to change the display.  Then selectsettingsand make the following changes:
Groups:Addtargettemp.
Groups:Addtargettemp.
Values:1. Removedate.  2. Addtemp_diff.  3.  Change the aggregator fromSUMtoAVG.The following screenshot shows the output.
Values:1. Removedate.  2. Addtemp_diff.  3.  Change the aggregator fromSUMtoAVG.
The following screenshot shows the output.

How do I use external packages with the notebook?
Zeppelin notebook in Apache Spark cluster on HDInsight can use external, community-contributed packages that aren't included in the cluster. Search theMaven repositoryfor the complete list of packages that are available. You can also get a list of available packages from other sources. For example, a complete list of community-contributed packages is available atSpark Packages.
In this article, you see how to use thespark-csvpackage with the Jupyter Notebook.
Open interpreter settings. From the top-right corner, select the logged in user name, then selectInterpreter.
Open interpreter settings. From the top-right corner, select the logged in user name, then selectInterpreter.

Scroll tolivy2, then selectedit.
Scroll tolivy2, then selectedit.

Navigate to keylivy.spark.jars.packages, and set its value in the formatgroup:id:version. So, if you want to use thespark-csvpackage, you must set the value of the key tocom.databricks:spark-csv_2.10:1.4.0.SelectSaveand thenOKto restart the Livy interpreter.
Navigate to keylivy.spark.jars.packages, and set its value in the formatgroup:id:version. So, if you want to use thespark-csvpackage, you must set the value of the key tocom.databricks:spark-csv_2.10:1.4.0.
livy.spark.jars.packages
group:id:version
com.databricks:spark-csv_2.10:1.4.0

SelectSaveand thenOKto restart the Livy interpreter.
If you want to understand how to arrive at the value of the key entered, here's how.a. Locate the package in the Maven Repository. For this article, we usedspark-csv.b. From the repository, gather the values forGroupId,ArtifactId, andVersion.c. Concatenate the three values, separated by a colon (:).com.databricks:spark-csv_2.10:1.4.0
If you want to understand how to arrive at the value of the key entered, here's how.
a. Locate the package in the Maven Repository. For this article, we usedspark-csv.
b. From the repository, gather the values forGroupId,ArtifactId, andVersion.

c. Concatenate the three values, separated by a colon (:).
com.databricks:spark-csv_2.10:1.4.0
com.databricks:spark-csv_2.10:1.4.0
Where are the Zeppelin notebooks saved?
The Zeppelin notebooks saved to the cluster headnodes. So, if you delete the cluster, the notebooks will be deleted as well. If you want to preserve your notebooks for later use on other clusters, you must export them after you finished running the jobs. To export a notebook, select theExporticon as shown in the image as follows.

This action saves the notebook as a JSON file in your download location.
Note
In HDI 4.0, the zeppelin notebook directory path is,/usr/hdp/<version>/zeppelin/notebook/<notebook_session_id>/Eg. /usr/hdp/4.1.17.10/zeppelin/2JMC9BZ8X/Whereas in HDI 5.0 and this path is different/usr/hdp/<version>/zeppelin/notebook/<Kernel_name>/Eg. /usr/hdp/5.1.4.5/zeppelin/notebook/Scala/
In HDI 4.0, the zeppelin notebook directory path is,/usr/hdp/<version>/zeppelin/notebook/<notebook_session_id>/
/usr/hdp/<version>/zeppelin/notebook/<notebook_session_id>/
Eg. /usr/hdp/4.1.17.10/zeppelin/2JMC9BZ8X/
Whereas in HDI 5.0 and this path is different/usr/hdp/<version>/zeppelin/notebook/<Kernel_name>/
/usr/hdp/<version>/zeppelin/notebook/<Kernel_name>/
Eg. /usr/hdp/5.1.4.5/zeppelin/notebook/Scala/
The file name stored is different in HDI 5.0.
It's stored as<notebook_name>_<sessionid>.zplnEg. testzeppelin_2JJK53XQA.zplnIn HDI 4.0, the file name is just note.json stored under session_id directory.Eg. /2JMC9BZ8X/note.json
The file name stored is different in HDI 5.0.
It's stored as<notebook_name>_<sessionid>.zpln
<notebook_name>_<sessionid>.zpln
Eg. testzeppelin_2JJK53XQA.zpln
In HDI 4.0, the file name is just note.json stored under session_id directory.
Eg. /2JMC9BZ8X/note.json
HDI Zeppelin always saves the notebook in the path/usr/hdp/<version>/zeppelin/notebook/in hn0 local disk.If you want the notebook to be available even after cluster deletion, you can try to use Azure file storage (Using SMB protocol) and link it to local path. For more information, seeMount SMB Azure file share on LinuxAfter mounting it, you can modify the zeppelin configuration zeppelin.notebook.dir to the mounted path in Ambari UI.
HDI Zeppelin always saves the notebook in the path/usr/hdp/<version>/zeppelin/notebook/in hn0 local disk.
/usr/hdp/<version>/zeppelin/notebook/
If you want the notebook to be available even after cluster deletion, you can try to use Azure file storage (Using SMB protocol) and link it to local path. For more information, seeMount SMB Azure file share on Linux
After mounting it, you can modify the zeppelin configuration zeppelin.notebook.dir to the mounted path in Ambari UI.
The SMB fileshare as GitNotebookRepo storage isn't recommended for zeppelin version 0.10.1
UseShiroto Configure Access to Zeppelin Interpreters in Enterprise Security Package (ESP) Clusters
Shiro
As noted above, the%shinterpreter isn't supported from HDInsight 4.0 onwards. Furthermore, since%shinterpreter introduces potential security issues, such as access keytabs using shell commands, it has been removed from HDInsight 3.6 ESP clusters as well. It means%shinterpreter isn't available when clickingCreate new noteor in the Interpreter UI by default.
%sh
%sh
%sh
Privileged domain users can use theShiro.inifile to control access to the Interpreter UI. Only these users can create new%shinterpreters and set permissions on each new%shinterpreter. To control access using theshiro.inifile, use the following steps:
Shiro.ini
%sh
%sh
shiro.ini
Define a new role using an existing domain group name. In the following example,adminGroupNameis a group of privileged users in Microsoft Entra ID. Don't use special characters or white spaces in the group name. The characters after=give the permissions for this role.*means the group has full permissions.[roles]
adminGroupName = *
Define a new role using an existing domain group name. In the following example,adminGroupNameis a group of privileged users in Microsoft Entra ID. Don't use special characters or white spaces in the group name. The characters after=give the permissions for this role.*means the group has full permissions.
adminGroupName
=
*
[roles]
adminGroupName = *
[roles]
adminGroupName = *
Add the new role for access to Zeppelin interpreters. In the following example, all users inadminGroupNameare given access to Zeppelin interpreters and can create new interpreters. You can put multiple roles between the brackets inroles[], separated by commas. Then, users that have the necessary permissions, can access Zeppelin interpreters.[urls]
/api/interpreter/** = authc, roles[adminGroupName]
Add the new role for access to Zeppelin interpreters. In the following example, all users inadminGroupNameare given access to Zeppelin interpreters and can create new interpreters. You can put multiple roles between the brackets inroles[], separated by commas. Then, users that have the necessary permissions, can access Zeppelin interpreters.
adminGroupName
roles[]
[urls]
/api/interpreter/** = authc, roles[adminGroupName]
[urls]
/api/interpreter/** = authc, roles[adminGroupName]
Example shiro.ini for multiple domain groups:
[main]
anyofrolesuser = org.apache.zeppelin.utils.AnyOfRolesUserAuthorizationFilter

[roles]
group1 = *
group2 = *
group3 = *

[urls]
/api/interpreter/** = authc, anyofrolesuser[group1, group2, group3]
[main]
anyofrolesuser = org.apache.zeppelin.utils.AnyOfRolesUserAuthorizationFilter

[roles]
group1 = *
group2 = *
group3 = *

[urls]
/api/interpreter/** = authc, anyofrolesuser[group1, group2, group3]
Livy session management
The first code paragraph in your Zeppelin notebook creates a new Livy session in your cluster. This session is shared across all Zeppelin notebooks that you later create. If the Livy session is killed for any reason, jobs won't run from the Zeppelin notebook.
In such a case, you must do the following steps before you can start running jobs from a Zeppelin notebook.
Restart the Livy interpreter from the Zeppelin notebook. To do so, open interpreter settings by selecting the logged in user name from the top-right corner, then selectInterpreter.
Restart the Livy interpreter from the Zeppelin notebook. To do so, open interpreter settings by selecting the logged in user name from the top-right corner, then selectInterpreter.

Scroll tolivy2, then selectrestart.
Scroll tolivy2, then selectrestart.

Run a code cell from an existing Zeppelin notebook. This  code creates a new Livy session in the HDInsight cluster.
Run a code cell from an existing Zeppelin notebook. This  code creates a new Livy session in the HDInsight cluster.
General information
Validate service
To validate the service from Ambari, navigate tohttps://CLUSTERNAME.azurehdinsight.net/#/main/services/ZEPPELIN/summarywhere CLUSTERNAME is the name of your cluster.
https://CLUSTERNAME.azurehdinsight.net/#/main/services/ZEPPELIN/summary
To validate the service from a command line, SSH to the head node. Switch user to zeppelin using commandsudo su zeppelin. Status commands:
sudo su zeppelin
/usr/hdp/current/zeppelin-server/bin/zeppelin-daemon.sh status
/usr/hdp/current/zeppelin-server/bin/zeppelin-daemon.sh --version
ps -aux | grep zeppelin
Log locations
Shiro
log4j
Enable debug logging
Navigate tohttps://CLUSTERNAME.azurehdinsight.net/#/main/services/ZEPPELIN/summarywhere CLUSTERNAME is the name of your cluster.
Navigate tohttps://CLUSTERNAME.azurehdinsight.net/#/main/services/ZEPPELIN/summarywhere CLUSTERNAME is the name of your cluster.
https://CLUSTERNAME.azurehdinsight.net/#/main/services/ZEPPELIN/summary
Navigate toCONFIGS>Advanced zeppelin-log4j-properties>log4j_properties_content.
Navigate toCONFIGS>Advanced zeppelin-log4j-properties>log4j_properties_content.
Modifylog4j.appender.dailyfile.Threshold = INFOtolog4j.appender.dailyfile.Threshold = DEBUG.
Modifylog4j.appender.dailyfile.Threshold = INFOtolog4j.appender.dailyfile.Threshold = DEBUG.
log4j.appender.dailyfile.Threshold = INFO
log4j.appender.dailyfile.Threshold = DEBUG
Addlog4j.logger.org.apache.zeppelin.realm=DEBUG.
Addlog4j.logger.org.apache.zeppelin.realm=DEBUG.
log4j.logger.org.apache.zeppelin.realm=DEBUG
Save changes and restart service.
Save changes and restart service.
Next steps
Overview: Apache Spark on Azure HDInsight
Kernels available for Jupyter Notebook in Apache Spark cluster for HDInsight
Install Jupyter on your computer and connect to an HDInsight Spark cluster
Feedback
Was this page helpful?
Additional resources