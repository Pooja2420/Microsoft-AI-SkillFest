Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Deploy a flow as a managed online endpoint for real-time inference
Article
2024-08-28
10 contributors
In this article
After you build a flow and test it properly, you might want to deploy it as an endpoint so that you can invoke the endpoint for real-time inference.
In this article, you'll learn how to deploy a flow as a managed online endpoint for real-time inference. The steps you'll take are:
Test your flow and get it ready for deployment
Create an online deployment
Grant permissions to the endpoint
Test the endpoint
Consume the endpoint
Important
Items marked (preview) in this article are currently in public preview.
The preview version is provided without a service level agreement, and it's not recommended for production workloads. Certain features might not be supported or might have constrained capabilities.
For more information, seeSupplemental Terms of Use for Microsoft Azure Previews.
Prerequisites
Learnhow to build and test a flow in the prompt flow.
Learnhow to build and test a flow in the prompt flow.
Have basic understanding on managed online endpoints. Managed online endpoints work with powerful CPU and GPU machines in Azure in a scalable, fully managed way that frees you from the overhead of setting up and managing the underlying deployment infrastructure. For more information on managed online endpoints, seeOnline endpoints and deployments for real-time inference.
Have basic understanding on managed online endpoints. Managed online endpoints work with powerful CPU and GPU machines in Azure in a scalable, fully managed way that frees you from the overhead of setting up and managing the underlying deployment infrastructure. For more information on managed online endpoints, seeOnline endpoints and deployments for real-time inference.
Azure role-based access controls (Azure RBAC) are used to grant access to operations in Azure Machine Learning. To be able to deploy an endpoint in prompt flow, your user account must be assigned theAzureML Data scientistor role with more privileges for theAzure Machine Learning workspace.
Azure role-based access controls (Azure RBAC) are used to grant access to operations in Azure Machine Learning. To be able to deploy an endpoint in prompt flow, your user account must be assigned theAzureML Data scientistor role with more privileges for theAzure Machine Learning workspace.
Have basic understanding on managed identities.Learn more about managed identities.
Have basic understanding on managed identities.Learn more about managed identities.
Note
Managed online endpoint only supports managed virtual network. If your workspace is in custom vnet, you need to try other deployment options, such as deploy toKubernetes online endpoint using CLI/SDK, ordeploy to other platforms such as Docker.
Build the flow and get it ready for deployment
If you already completed theget started tutorial, you've already tested the flow properly by submitting batch run and evaluating the results.
If you didn't complete the tutorial, you need to build a flow. Testing the flow properly by batch run and evaluation before deployment is a recommended best practice.
We'll use the sample flowWeb Classificationas example to show how to deploy the flow. This sample flow is a standard flow. Deploying chat flows is similar. Evaluation flow doesn't support deployment.
Define the environment used by deployment
When you deploy prompt flow to managed online endpoint in UI, by default the deployment will use the environment created based on the latest prompt flow image and dependencies specified in therequirements.txtof the flow. You can specify extra packages you needed inrequirements.txt. You can findrequirements.txtin the root folder of your flow folder.
requirements.txt
requirements.txt
requirements.txt

Note
If you are using private feeds in Azure devops, you need build the image with private feeds first and select custom environment to deploy in UI.
Create an online deployment
Now that you have built a flow and tested it properly, it's time to create your online endpoint for real-time inference.
The prompt flow supports you to deploy endpoints from a flow, or a batch run. Testing your flow before deployment is recommended best practice.
In the flow authoring page or run detail page, selectDeploy.
Flow authoring page:

Run detail page:

A wizard for you to configure the endpoint occurs and include following steps.
Basic settings

This step allows you to configure the basic settings of the deployment.
After you finish the basic settings, you can directlyReview+Createto finish the creation, or you can selectNextto configureAdvanced settings.
Advanced settings - Endpoint
You can specify the following settings for the endpoint.

The authentication method for the endpoint. Key-based authentication provides a primary and secondary key that doesn't expire. Azure Machine Learning token-based authentication provides a token that periodically refreshes automatically. For more information on authenticating, seeAuthenticate to an online endpoint.
The endpoint needs to access Azure resources such as the Azure Container Registry or your workspace connections for inferencing. You can allow the endpoint permission to access Azure resources via giving permission to its managed identity.
System-assigned identity will be autocreated after your endpoint is created, while user-assigned identity is created by user.Learn more about managed identities.
You'll notice there is an option whetherEnforce access to connection secrets (preview). If your flow uses connections, the endpoint needs to access connections to perform inference. The option is by default enabled, the endpoint will be grantedAzure Machine Learning Workspace Connection Secrets Readerrole to access connections automatically if you have connection secrets reader permission. If you disable this option, you need to grant this role to the system-assigned identity manually by yourself or ask help from your admin.Learn more about how to grant permission to the endpoint identity.
When creating the deployment, Azure tries to pull the user container image from the workspace Azure Container Registry (ACR) and mount the user model and code artifacts into the user container from the workspace storage account.
If you created the associated endpoint withUser Assigned Identity, user-assigned identity must be granted following rolesbefore the deployment creation; otherwise, the deployment creation will fail.
See detailed guidance about how to grant permissions to the endpoint identity inGrant permissions to the endpoint.
Important
If your flow uses Microsoft Entra ID based authentication connections, no matter you use system-assigned identity or user-assigned identity, you always need to grant the managed identity appropriate roles of the corresponding resources so that it can make API calls to that resource. For example, if your Azure OpenAI connection uses Microsoft Entra ID based authentication, you need to grant your endpoint managed identityCognitive Services OpenAI User or Cognitive Services OpenAI Contributor roleof the corresponding Azure OpenAI resources.
Advanced settings - Deployment
In this step, except tags, you can also specify the environment used by the deployment.

By default the deployment will use the environment created based on the base image specified in theflow.dag.yamland dependencies specified in therequirements.txt.
flow.dag.yaml
requirements.txt
You can specify the base image in theflow.dag.yamlby selectingRaw file modeof the flow. If there is no image specified, the default base image is the latest prompt flow base image.
You can specify the base image in theflow.dag.yamlby selectingRaw file modeof the flow. If there is no image specified, the default base image is the latest prompt flow base image.
flow.dag.yaml
Raw file mode

You can findrequirements.txtin the root folder of your flow folder, and add dependencies within it.
You can findrequirements.txtin the root folder of your flow folder, and add dependencies within it.
requirements.txt

You can also create customized environment and use it for the deployment.
Note
Your custom environment must satisfy following requirements:
the docker image must be created based on prompt flow base image,mcr.microsoft.com/azureml/promptflow/promptflow-runtime-stable:<newest_version>. You can find the newest versionhere.
mcr.microsoft.com/azureml/promptflow/promptflow-runtime-stable:<newest_version>
the environment definition must include theinference_config.
inference_config
Following is an example of customized environment definition.
$schema: https://azuremlschemas.azureedge.net/latest/environment.schema.json
name: pf-customized-test
build:
  path: ./image_build
  dockerfile_path: Dockerfile
description: promptflow customized runtime
inference_config:
  liveness_route:
    port: 8080
    path: /health
  readiness_route:
    port: 8080
    path: /health
  scoring_route:
    port: 8080
    path: /score
$schema: https://azuremlschemas.azureedge.net/latest/environment.schema.json
name: pf-customized-test
build:
  path: ./image_build
  dockerfile_path: Dockerfile
description: promptflow customized runtime
inference_config:
  liveness_route:
    port: 8080
    path: /health
  readiness_route:
    port: 8080
    path: /health
  scoring_route:
    port: 8080
    path: /score
If you enable this, tracing data and system metrics during inference time (such as token count, flow latency, flow request, and etc.) will be collected into workspace linked Application Insights. To learn more, seeprompt flow serving tracing data and metrics.
If you want to specify a different Application Insights other than the workspace linked one,you can configure by CLI.
Advanced settings - Outputs & Connections
In this step, you can view all flow outputs, and specify which outputs will be included in the response of the endpoint you deploy. By default all flow outputs are selected.
You can also specify the connections used by the endpoint when it performs inference. By default they're inherited from the flow.
Once you configured and reviewed all the steps above, you can selectReview+Createto finish the creation.
Note
Expect the endpoint creation to take approximately more than 15 minutes, as it contains several stages including creating endpoint, registering model, creating deployment, etc.
You can understand the deployment creation progress via the notification starts byPrompt flow deployment.
Grant permissions to the endpoint
Important
Granting permissions (adding role assignment) is only enabled to theOwnerof the specific Azure resources. You might need to ask your IT admin for help.
It's recommended to grant roles to theuser-assignedidentitybefore the deployment creation.
It maight take more than 15 minutes for the granted permission to take effect.
You can grant all permissions in Azure portal UI by following steps.
Go to the Azure Machine Learning workspace overview page inAzure portal.
Go to the Azure Machine Learning workspace overview page inAzure portal.
SelectAccess control, and selectAdd role assignment.
SelectAccess control, and selectAdd role assignment.
SelectAzure Machine Learning Workspace Connection Secrets Reader, go toNext.NoteAzure Machine Learning Workspace Connection Secrets Reader is a built-in role which has permission to get workspace connections.If you want to use a customized role, make sure the customized role has the permission of "Microsoft.MachineLearningServices/workspaces/connections/listsecrets/action". Learn more abouthow to create custom roles.
SelectAzure Machine Learning Workspace Connection Secrets Reader, go toNext.
Note
Azure Machine Learning Workspace Connection Secrets Reader is a built-in role which has permission to get workspace connections.
If you want to use a customized role, make sure the customized role has the permission of "Microsoft.MachineLearningServices/workspaces/connections/listsecrets/action". Learn more abouthow to create custom roles.
SelectManaged identityand select members.Forsystem-assigned identity, selectMachine learning online endpointunderSystem-assigned managed identity, and search by endpoint name.Foruser-assigned identity, selectUser-assigned managed identity, and search by identity name.
SelectManaged identityand select members.
Forsystem-assigned identity, selectMachine learning online endpointunderSystem-assigned managed identity, and search by endpoint name.
Foruser-assigned identity, selectUser-assigned managed identity, and search by identity name.
Foruser-assignedidentity, you need to grant permissions to the workspace container registry and storage account as well. You can find the container registry and storage account in the workspace overview page in Azure portal.Go to the workspace container registry overview page, selectAccess control, and selectAdd role assignment, and assignACR pull |Pull container imageto the endpoint identity.Go to the workspace default storage overview page, selectAccess control, and selectAdd role assignment, and assignStorage Blob Data Readerto the endpoint identity.
Foruser-assignedidentity, you need to grant permissions to the workspace container registry and storage account as well. You can find the container registry and storage account in the workspace overview page in Azure portal.

Go to the workspace container registry overview page, selectAccess control, and selectAdd role assignment, and assignACR pull |Pull container imageto the endpoint identity.
Go to the workspace default storage overview page, selectAccess control, and selectAdd role assignment, and assignStorage Blob Data Readerto the endpoint identity.
(optional) Foruser-assignedidentity, if you want to monitor the endpoint related metrics like CPU/GPU/Disk/Memory utilization, you need to grantWorkspace metrics writerrole of workspace to the identity as well.
(optional) Foruser-assignedidentity, if you want to monitor the endpoint related metrics like CPU/GPU/Disk/Memory utilization, you need to grantWorkspace metrics writerrole of workspace to the identity as well.
Check the status of the endpoint
There will be notifications after you finish the deploy wizard. After the endpoint and deployment are created successfully, you can selectDeploy detailsin the notification to endpoint detail page.
You can also directly go to theEndpointspage in the studio, and check the status of the endpoint you deployed.

Test the endpoint with sample data
In the endpoint detail page, switch to theTesttab.
You can input the values and selectTestbutton.
TheTest resultshows as following:

Test the endpoint deployed from a chat flow
For endpoints deployed from chat flow, you can test it in an immersive chat window.

Thechat_inputwas set during development of the chat flow. You can input thechat_inputmessage in the input box. TheInputspanel on the right side is for you to specify the values for other inputs besides thechat_input. Learn more abouthow to develop a chat flow.
chat_input
chat_input
chat_input
Consume the endpoint
In the endpoint detail page, switch to theConsumetab. You can find the REST endpoint and key/token to consume your endpoint. There is also sample code for you to consume the endpoint in different languages.
Note that you need to fill the data values according to your flow inputs. Take the sample flow used in this articleWeb Classificationas example, you need to specifydata = {"url": "<the_url_to_be_classified>"}and fill the key or token in the sample consumption code.
data = {"url": "<the_url_to_be_classified>"}

Monitor endpoints
View managed online endpoints common metrics using Azure Monitor (optional)
You can view various metrics (request numbers, request latency, network bytes, CPU/GPU/Disk/Memory utilization, and more) for an online endpoint and its deployments by following links from the endpoint'sDetailspage in the studio. Following these links take you to the exact metrics page in the Azure portal for the endpoint or deployment.
Note
If you specify user-assigned identity for your endpoint, make sure that you have assignedWorkspace metrics writerofAzure Machine Learning Workspaceto your user-assigned identity. Otherwise, the endpoint will not be able to log the metrics.

For more information on how to view online endpoint metrics, seeMonitor online endpoints.
View prompt flow endpoints specific metrics and tracing data (optional)
If you enableApplication Insights diagnosticsin the UI deploy wizard, tracing data and prompt flow specific metrics will be collect to workspace linked Application Insights. See details aboutenabling tracing for your deployment.
Troubleshoot endpoints deployed from prompt flow
Lack authorization to perform action "Microsoft.MachineLearningService/workspaces/datastores/read"
If your flow contains Index Look Up tool, after deploying the flow, the endpoint needs to access workspace datastore to read MLIndex yaml file or FAISS folder containing chunks and embeddings. Hence, you need to manually grant the endpoint identity permission to do so.
You can either grant the endpoint identityAzureML Data Scientiston workspace scope, or a custom role which contains "MachineLearningService/workspace/datastore/reader" action.
MissingDriverProgram Error
If you deploy your flow with custom environment and encounter the following error, it might be because you didn't specify theinference_configin your custom environment definition.
inference_config
'error': 
{
    'code': 'BadRequest', 
    'message': 'The request is invalid.', 
    'details': 
         {'code': 'MissingDriverProgram', 
          'message': 'Could not find driver program in the request.', 
          'details': [], 
          'additionalInfo': []
         }
}
'error': 
{
    'code': 'BadRequest', 
    'message': 'The request is invalid.', 
    'details': 
         {'code': 'MissingDriverProgram', 
          'message': 'Could not find driver program in the request.', 
          'details': [], 
          'additionalInfo': []
         }
}
There are 2 ways to fix this error.
(Recommended) You can find the container image uri in your custom environment detail page, and set it as the flow base image in the flow.dag.yaml file. When you deploy the flow in UI, you just selectUse environment of current flow definition, and the backend service will create the customized environment based on this base image andrequirement.txtfor your deployment. Learn more aboutthe environment specified in the flow definition.
(Recommended) You can find the container image uri in your custom environment detail page, and set it as the flow base image in the flow.dag.yaml file. When you deploy the flow in UI, you just selectUse environment of current flow definition, and the backend service will create the customized environment based on this base image andrequirement.txtfor your deployment. Learn more aboutthe environment specified in the flow definition.
requirement.txt


You can fix this error by addinginference_configin your custom environment definition. Learn more abouthow to use customized environment.Following is an example of customized environment definition.
You can fix this error by addinginference_configin your custom environment definition. Learn more abouthow to use customized environment.
inference_config
Following is an example of customized environment definition.
$schema: https://azuremlschemas.azureedge.net/latest/environment.schema.json
name: pf-customized-test
build:
  path: ./image_build
  dockerfile_path: Dockerfile
description: promptflow customized runtime
inference_config:
  liveness_route:
    port: 8080
    path: /health
  readiness_route:
    port: 8080
    path: /health
  scoring_route:
    port: 8080
    path: /score
$schema: https://azuremlschemas.azureedge.net/latest/environment.schema.json
name: pf-customized-test
build:
  path: ./image_build
  dockerfile_path: Dockerfile
description: promptflow customized runtime
inference_config:
  liveness_route:
    port: 8080
    path: /health
  readiness_route:
    port: 8080
    path: /health
  scoring_route:
    port: 8080
    path: /score
Model response taking too long
Sometimes, you might notice that the deployment is taking too long to respond. There are several potential factors for this to occur.
Model is not powerful enough (ex. use gpt over text-ada)
Index query is not optimized and taking too long
Flow has many steps to process
Consider optimizing the endpoint with above considerations to improve the performance of the model.
Unable to fetch deployment schema
After you deploy the endpoint and want to test it in theTest tabin the endpoint detail page, if theTest tabshowsUnable to fetch deployment schemalike following, you can try the following 2 methods to mitigate this issue:

Make sure you have granted the correct permission to the endpoint identity. Learn more abouthow to grant permission to the endpoint identity.
Access denied to list workspace secret
If you encounter an error like "Access denied to list workspace secret", check whether you have granted the correct permission to the endpoint identity. Learn more abouthow to grant permission to the endpoint identity.
Clean up resources
If you aren't going use the endpoint after completing this tutorial, you should delete the endpoint.
Note
The complete deletion can take approximately 20 minutes.
Next Steps
Iterate and optimize your flow by tuning prompts using variants
Enable trace and collect feedback for your deployment
View costs for an Azure Machine Learning managed online endpoint
Troubleshoot prompt flow deployments.
Feedback
Was this page helpful?
Additional resources