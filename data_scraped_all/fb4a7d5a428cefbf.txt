Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Ingestion batching policy
Article
2024-08-21
9 contributors
In this article
Overview
Applies to: 창혵Microsoft Fabric창혵Azure Data Explorer
During the queued ingestion process, the service optimizes for throughput by batching small ingress data chunks together before ingestion. Batching reduces the resources consumed by the queued ingestion process and doesn't require post-ingestion resources to optimize the small data shards produced by non-batched ingestion.
The downside to doing batching before ingestion is the forced delay. Therefore, the end-to-end time from requesting the data ingestion until the data ready for query is larger.
When you define theIngestionBatchingpolicy, you'll need to find a balance between optimizing for throughput and time delay. This policy applies to queued ingestion. It defines the maximum forced delay allowed when batching small blobs together. To learn more about using batching policy commands, and optimizing for throughput, see:
IngestionBatching
Ingestion batching policy command reference
Ingestion best practices - optimizing for throughput
Sealing a batch
There's an optimal size of about 1 GB of uncompressed data for bulk ingestion. Ingestion of blobs with much less data is suboptimal, so in queued ingestion the service will batch small blobs together.
The following list shows the basic batching policy triggers to seal a batch. A batch is sealed and ingested when the first condition is met:
Size: Batch size limit reached or exceeded
Size
Count: Batch file number limit reached
Count
Time: Batching time has expired
Time
TheIngestionBatchingpolicy can be set on databases or tables. Default values are as follows:5 minutesmaximum delay time,500items, total size of1 GB.
IngestionBatching
Important
The impact of setting this policy to very small values is
an increase in the COGS (cost of goods sold) and reduced performance. Additionally,
reducing batching policy values might actually result inincreasedeffective
end-to-end ingestion latency, due to the overhead of managing multiple ingestion
processes in parallel.
The following list shows conditions to seal batches related to single blob ingestion. A batch is sealed and ingested when the conditions are met:
SingleBlob_FlushImmediately: Ingest a single blob because'FlushImmediately'was set
SingleBlob_FlushImmediately
SingleBlob_IngestIfNotExists: Ingest a single blob because'IngestIfNotExists'was set
SingleBlob_IngestIfNotExists
SingleBlob_IngestByTag: Ingest a single blob because'ingest-by'was set
SingleBlob_IngestByTag
SingleBlob_SizeUnknown: Ingest a single blob because blob size is unknown
SingleBlob_SizeUnknown
If theSystemFlushcondition is set, a batch will be sealed when a system flush is triggered. With theSystemFlushparameter set, the system flushes the data, for example due to database scaling or internal reset of system components.
SystemFlush
SystemFlush
Defaults and limits
The most effective way of controlling the end-to-end latency using ingestion batching policy is to alter its time boundary attableordatabaselevel, according to the higher bound of latency requirements.
A database level policy affects all tables in that database that don't have the table-level policy defined, and any newly created table.
Important
If you set the time boundary of the Ingestion Batching policy too low on low-ingress tables, you may incur additional compute and storage work as the database attempts to optimize the newly created data shards. For more information about data shards, seeextents.
Batch data size
The batching policy data size is set for uncompressed data. For Parquet, AVRO, and ORC files, an estimation is calculated based on file size. For compressed data, the uncompressed data size is evaluated as follows in descending order of accuracy:
If the uncompressed size is provided in the ingestion source options, that value is used.
When ingesting local files using SDKs, zip archives and gzip streams are inspected to assess their raw size.
If previous options don't provide a data size, a factor is applied to the compressed data size to estimate the uncompressed data size.
Batching latencies
Latencies can result from many causes that can be addressed using batching policy settings.
time
size
count
time
count
size
Feedback
Was this page helpful?
Additional resources