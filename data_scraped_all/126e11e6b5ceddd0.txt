Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Deploy models using Mosaic AI Model Serving
Article
2025-04-04
5 contributors
In this article
This article describes Mosaic AI Model Serving, the Databricks solution for deploying AI and ML models for real-time serving and batch inference.
What is Mosaic AI Model Serving?
Mosaic AI Model Serving provides a unified interface to deploy, govern, and query AI models for real-time and batch inference. Each model you serve is available as a REST API that you can integrate into your web or client application.
Model Serving provides a highly available and low-latency service for deploying models. The service automatically scales up or down to meet demand changes, saving infrastructure costs while optimizing latency performance. This functionality usesserverless compute. See theModel Serving pricing pagefor more details.
Model Serving offers a unified REST API and MLflow Deployment API for CRUD and querying tasks. In addition, it provides a single UI to manage all your models and their respective serving endpoints. You can also access models directly from SQL usingAI Functionsfor easy integration into analytics workflows.
AI Functions and Model Serving are tightly integrated for batch inference scenarios. You can use any of the task-specific AI Functions orai-queryin your batch inference pipelines. If you choose to use a pre-provisioned model that is hosted and managed by Databricks, you don't need to configure a model serving endpoint yourself.
ai-query
See the following guides to get started:
For performing batch inference, seePerform batch LLM inference using AI Functions.
For an introductory tutorial on how to serve custom models on Azure Databricks for real-time inference, seeTutorial: Deploy and query a custom model.
For a getting started tutorial on how to query a foundation model on Databricks for real-time inference, seeGet started querying LLMs on Databricks.
Models you can deploy
Model serving supports real time and batch inference for the following model types:
Custom models. These are Python models packaged in the MLflow format. They can be registered either in Unity Catalog or in the workspace model registry. Examples include scikit-learn, XGBoost, PyTorch, and Hugging Face transformer models.Agent serving is supported as a custom model. SeeDeploy an agent for generative AI applications
Agent serving is supported as a custom model. SeeDeploy an agent for generative AI applications
Foundation models.Databricks-hosted foundation modelslike Meta Llama. These models are available usingFoundation Model APIs. These models are curated foundation model architectures that support optimized inference. Base models, like Meta-Llama-3.3-70B-Instruct, GTE-Large, and Mistral-7B are available for immediate use withpay-per-tokenpricing, and workloads that require performance guarantees and fine-tuned model variants can be deployed withprovisioned throughput.Foundation models hosted outside of Databrickslike GPT-4 from OpenAI. These models are accessible usingExternal models. The endpoints that serve these models can be centrally governed from Azure Databricks, so you can streamline the use and management of various LLM providers, such as OpenAI and Anthropic, within your organization.
Databricks-hosted foundation modelslike Meta Llama. These models are available usingFoundation Model APIs. These models are curated foundation model architectures that support optimized inference. Base models, like Meta-Llama-3.3-70B-Instruct, GTE-Large, and Mistral-7B are available for immediate use withpay-per-tokenpricing, and workloads that require performance guarantees and fine-tuned model variants can be deployed withprovisioned throughput.
Foundation models hosted outside of Databrickslike GPT-4 from OpenAI. These models are accessible usingExternal models. The endpoints that serve these models can be centrally governed from Azure Databricks, so you can streamline the use and management of various LLM providers, such as OpenAI and Anthropic, within your organization.
Note
You can interact with supported large language models using theAI Playground. The AI Playground is a chat-like environment where you can test, prompt, and compare LLMs. This functionality is available in your Azure Databricks workspace.
Why use Model Serving?
Deploy and query any models: Model Serving provides a unified interface that so you can manage all models in one location and query them with a single API, regardless of whether they are hosted on Databricks or externally. This approach simplifies the process of experimenting with, customizing, and deploying models in production across various clouds and providers.
Securely customize models with your private data: Built on a Data Intelligence Platform, Model Serving simplifies the integration of features and embeddings into models through native integration with theDatabricks Feature StoreandMosaic AI Vector Search. For even more improved accuracy and contextual understanding, models can be fine-tuned with proprietary data and deployed effortlessly on Model Serving.
Govern and monitor models: The Serving UI allows you to centrally manage all model endpoints in one place, including those that are externally hosted. You can manage permissions, track and set usage limits and monitor the quality of all types of models usingAI Gateway. This enables you to democratize access to SaaS and open LLMs within your organization while ensuring appropriate guardrails are in place.
Reduce cost with optimized inference and fast scaling: Databricks has implemented a range of optimizations to ensure you get the best throughput and latency for large models. The endpoints automatically scale up or down to meet demand changes, saving infrastructure costs while optimizing latency performance.Monitor model serving costs.
Note
For workloads that are latency sensitive or involve a high number of queries per second, Databricks recommends usingroute optimizationon custom model serving endpoints. Reach out to your Databricks account team to ensure your workspace is enabled for high scalability.
Bring reliability and security to Model Serving: Model Serving is designed for high-availability, low-latency production use and can support over 25K queries per second with an overhead latency of less than 50 ms. The serving workloads are protected by multiple layers of security, ensuring a secure and reliable environment for even the most sensitive tasks.
Note
Model Serving does not provide security patches to existing model images because of the risk of destabilization to production deployments. A new model image created from a new model version will contain the latest patches. Reach out to your Databricks account team for more information.
Requirements
Registered model inUnity Catalogor theWorkspace Model Registry.
Permissions on the registered models as described inServing endpoint ACLs.MLflow 1.29 or higher.
MLflow 1.29 or higher.
If you are using Azure Private Link to respect networking-related ingress rules configured on the workspace, Azure Private Link is only supported for model serving endpoints that use provisioned throughput or endpoints that serve custom models. SeeConfigure private connectivity from serverless compute.
Enable Model Serving for your workspace
No additional steps are required to enable Model Serving in your workspace.
Limitations and region availability
Mosaic AI Model Serving imposes default limits to ensure reliable performance. SeeModel Serving limits and regions. If you have feedback on these limits or an endpoint in an unsupported region, reach out to your Databricks account team.
Data protection in Model Serving
Databricks takes data security seriously. Databricks understands the importance of the data you analyze using Mosaic AI Model Serving, and implements the following security controls to protect your data.
Every customer request to Model Serving is logically isolated, authenticated, and authorized.
Mosaic AI Model Serving encrypts all data at rest (AES-256) and in transit (TLS 1.2+).
For all paid accounts, Mosaic AI Model Serving does not use user inputs submitted to the service or outputs from the service to train any models or improve any Databricks services.
For Databricks Foundation Model APIs, as part of providing the service, Databricks may temporarily process and store inputs and outputs for the purposes of preventing, detecting, and mitigating abuse or harmful uses. Your inputs and outputs are isolated from those of other customers, stored in the same region as your workspace for up to thirty (30) days, and only accessible for detecting and responding to security or abuse concerns. Foundation Model APIs is aDatabricks Designated Service, meaning it adheres to data residency boundaries as implemented byDatabricks Geos.
Additional resources
Get started querying LLMs on Databricks.
Tutorial: Deploy and query a custom model
Tutorial: Create external model endpoints to query OpenAI models
Introduction to building gen AI apps on Databricks
Perform batch LLM inference using AI Functions
Migrate to Model Serving
Feedback
Was this page helpful?
Additional resources