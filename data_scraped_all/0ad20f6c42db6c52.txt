Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Monitor the performance of models deployed to production
Article
2025-03-31
16 contributors
In this article
APPLIES TO:Azure CLI ml extension v2 (current)Python SDK azure-ai-ml v2 (current)
In Azure Machine Learning, you can use model monitoring to continuously track the performance of machine learning models in production. Model monitoring provides you with a broad view of monitoring signals. It also alerts you to potential issues. When you monitor signals and performance metrics of models in production, you can critically evaluate the inherent risks of your models. You can also identify blind spots that might adversely affect your business.
In this article, you see how to perform the following tasks:
Set up out-of-box and advanced monitoring for models that are deployed to Azure Machine Learning online endpoints
Monitor performance metrics for models in production
Monitor models that are deployed outside Azure Machine Learning or deployed to Azure Machine Learning batch endpoints
Set up custom signals and metrics to use in model monitoring
Interpret monitoring results
Integrate Azure Machine Learning model monitoring with Azure Event Grid
Prerequisites
Azure CLI
Python SDK
Studio
TheAzure CLIand themlextension to the Azure CLI, installed and configured. For more information, seeInstall and set up the CLI (v2).
TheAzure CLIand themlextension to the Azure CLI, installed and configured. For more information, seeInstall and set up the CLI (v2).
ml
A Bash shell or a compatible shell, for example, a shell on a Linux system orWindows Subsystem for Linux. The Azure CLI examples in this article assume that you use this type of shell.
A Bash shell or a compatible shell, for example, a shell on a Linux system orWindows Subsystem for Linux. The Azure CLI examples in this article assume that you use this type of shell.
An Azure Machine Learning workspace. For instructions to create a workspace, seeSet up.
An Azure Machine Learning workspace. For instructions to create a workspace, seeSet up.
An Azure Machine Learning workspace. For steps for creating a workspace, seeCreate the workspace.
An Azure Machine Learning workspace. For steps for creating a workspace, seeCreate the workspace.
The Azure Machine Learning SDK for Python v2. To install the SDK, use the following command:pip install azure-ai-ml azure-identityTo update an existing installation of the SDK to the latest version, use the following command:pip install --upgrade azure-ai-ml azure-identityFor more information, seeAzure Machine Learning Package client library for Python.
The Azure Machine Learning SDK for Python v2. To install the SDK, use the following command:
pip install azure-ai-ml azure-identity
pip install azure-ai-ml azure-identity
To update an existing installation of the SDK to the latest version, use the following command:
pip install --upgrade azure-ai-ml azure-identity
pip install --upgrade azure-ai-ml azure-identity
For more information, seeAzure Machine Learning Package client library for Python.
An Azure subscription. If you don't have an Azure subscription, create afree accountbefore you begin.
An Azure subscription. If you don't have an Azure subscription, create afree accountbefore you begin.
An Azure Machine Learning workspace. For steps for creating a workspace, seeCreate the workspace.
An Azure Machine Learning workspace. For steps for creating a workspace, seeCreate the workspace.
An Azure Machine Learning compute instance. For steps for creating a compute instance, seeCreate a compute instance.
An Azure Machine Learning compute instance. For steps for creating a compute instance, seeCreate a compute instance.
A user account that has at least one of the following Azure role-based access control (Azure RBAC) roles:An Owner role for the Azure Machine Learning workspaceA Contributor role for the Azure Machine Learning workspaceA custom role that hasMicrosoft.MachineLearningServices/workspaces/onlineEndpoints/*permissionsFor more information, seeManage access to Azure Machine Learning workspaces.
A user account that has at least one of the following Azure role-based access control (Azure RBAC) roles:
An Owner role for the Azure Machine Learning workspace
A Contributor role for the Azure Machine Learning workspace
A custom role that hasMicrosoft.MachineLearningServices/workspaces/onlineEndpoints/*permissions
Microsoft.MachineLearningServices/workspaces/onlineEndpoints/*
For more information, seeManage access to Azure Machine Learning workspaces.
For monitoring an Azure Machine Learning managed online endpoint or Kubernetes online endpoint:A model that's deployed to the Azure Machine Learning online endpoint. Managed online endpoints and Kubernetes online endpoints are supported. For instructions for deploying a model to an Azure Machine Learning online endpoint, seeDeploy and score a machine learning model by using an online endpoint.Data collection enabled for your model deployment. You can enable data collection during the deployment step for Azure Machine Learning online endpoints. For more information, seeCollect production data from models deployed for real-time inferencing.
For monitoring an Azure Machine Learning managed online endpoint or Kubernetes online endpoint:
A model that's deployed to the Azure Machine Learning online endpoint. Managed online endpoints and Kubernetes online endpoints are supported. For instructions for deploying a model to an Azure Machine Learning online endpoint, seeDeploy and score a machine learning model by using an online endpoint.
A model that's deployed to the Azure Machine Learning online endpoint. Managed online endpoints and Kubernetes online endpoints are supported. For instructions for deploying a model to an Azure Machine Learning online endpoint, seeDeploy and score a machine learning model by using an online endpoint.
Data collection enabled for your model deployment. You can enable data collection during the deployment step for Azure Machine Learning online endpoints. For more information, seeCollect production data from models deployed for real-time inferencing.
Data collection enabled for your model deployment. You can enable data collection during the deployment step for Azure Machine Learning online endpoints. For more information, seeCollect production data from models deployed for real-time inferencing.
For monitoring a model that's deployed to an Azure Machine Learning batch endpoint or deployed outside Azure Machine Learning:A means to collect production data and register it as an Azure Machine Learning data assetA means to update the registered data asset continuously for model monitoring(Recommended) Registration of the model in an Azure Machine Learning workspace, for lineage tracking
For monitoring a model that's deployed to an Azure Machine Learning batch endpoint or deployed outside Azure Machine Learning:
A means to collect production data and register it as an Azure Machine Learning data asset
A means to update the registered data asset continuously for model monitoring
(Recommended) Registration of the model in an Azure Machine Learning workspace, for lineage tracking
Configure a serverless Spark compute pool
Model monitoring jobs are scheduled to run on serverless Spark compute pools. The following Azure Virtual Machines instance types are supported:
Standard_E4s_v3
Standard_E8s_v3
Standard_E16s_v3
Standard_E32s_v3
Standard_E64s_v3
To specify a virtual machine instance type when you follow the procedures in this article, take the following steps:
Azure CLI
Python SDK
Studio
When you use the Azure CLI to create a monitor, you use a YAML configuration file. In that file, set thecreate_monitor.compute.instance_typevalue to the type that you want to use.
create_monitor.compute.instance_type
When you use the Python SDK to create a monitor, you useazure.ai.ml.entities.ServerlessSparkComputeto create a compute instance. When you call that method, set theinstance_typeparameter to the type that you want to use.
azure.ai.ml.entities.ServerlessSparkCompute
instance_type
When you use Azure Machine Learning studio to add a monitor, the Basic settings page opens. UnderVirtual machine size, select the type that you want to use.
Set up out-of-box model monitoring
Consider a scenario in which you deploy your model to production in an Azure Machine Learning online endpoint and enabledata collectionat deployment time. In this case, Azure Machine Learning collects production inference data and automatically stores it in Azure Blob Storage. You can use Azure Machine Learning model monitoring to continuously monitor this production inference data.
You can use the Azure CLI, the Python SDK, or the studio for an out-of-box setup of model monitoring. The out-of-box model monitoring configuration provides the following monitoring capabilities:
Azure Machine Learning automatically detects the production inference data asset that's associated with an Azure Machine Learning online deployment and uses the data asset for model monitoring.
The comparison reference data asset is set as the recent, past production inference data asset.
Monitoring setup automatically includes and tracks the following built-in monitoring signals: data drift, prediction drift, and data quality. For each monitoring signal, Azure Machine Learning uses:The recent, past production inference data asset as the comparison reference data asset.Smart default values for metrics and thresholds.
The recent, past production inference data asset as the comparison reference data asset.
Smart default values for metrics and thresholds.
A monitoring job is configured to run on a regular schedule. That job acquires monitoring signals and evaluates each metric result against its corresponding threshold. By default, when any threshold is exceeded, Azure Machine Learning sends an alert email to the user who set up the monitor.
To set up out-of-box model monitoring, take the following steps.
Azure CLI
Python SDK
Studio
In the Azure CLI, you useaz ml scheduleto schedule a monitoring job.
az ml schedule
Create a monitoring definition in a YAML file. For a sample out-of-box definition, see the following YAML code, which is also available in theazureml-examples repository.Before you use this definition, adjust the values to fit your environment. Forendpoint_deployment_id, use a value in the formatazureml:<endpoint-name>:<deployment-name>.# out-of-box-monitoring.yaml
$schema:  http://azureml/sdk-2-0/Schedule.json
name: credit_default_model_monitoring
display_name: Credit default model monitoring
description: Credit default model monitoring setup with minimal configurations

trigger:
  # perform model monitoring activity daily at 3:15am
  type: recurrence
  frequency: day #can be minute, hour, day, week, month
  interval: 1 # #every day
  schedule: 
    hours: 3 # at 3am
    minutes: 15 # at 15 mins after 3am

create_monitor:

  compute: # specify a spark compute for monitoring job
    instance_type: standard_e4s_v3
    runtime_version: "3.3"

  monitoring_target: 
    ml_task: classification # model task type: [classification, regression, question_answering]
    endpoint_deployment_id: azureml:credit-default:main # azureml endpoint deployment id

  alert_notification: # emails to get alerts
    emails:
      - abc@example.com
      - def@example.com
Create a monitoring definition in a YAML file. For a sample out-of-box definition, see the following YAML code, which is also available in theazureml-examples repository.
Before you use this definition, adjust the values to fit your environment. Forendpoint_deployment_id, use a value in the formatazureml:<endpoint-name>:<deployment-name>.
endpoint_deployment_id
azureml:<endpoint-name>:<deployment-name>
# out-of-box-monitoring.yaml
$schema:  http://azureml/sdk-2-0/Schedule.json
name: credit_default_model_monitoring
display_name: Credit default model monitoring
description: Credit default model monitoring setup with minimal configurations

trigger:
  # perform model monitoring activity daily at 3:15am
  type: recurrence
  frequency: day #can be minute, hour, day, week, month
  interval: 1 # #every day
  schedule: 
    hours: 3 # at 3am
    minutes: 15 # at 15 mins after 3am

create_monitor:

  compute: # specify a spark compute for monitoring job
    instance_type: standard_e4s_v3
    runtime_version: "3.3"

  monitoring_target: 
    ml_task: classification # model task type: [classification, regression, question_answering]
    endpoint_deployment_id: azureml:credit-default:main # azureml endpoint deployment id

  alert_notification: # emails to get alerts
    emails:
      - abc@example.com
      - def@example.com
# out-of-box-monitoring.yaml
$schema:  http://azureml/sdk-2-0/Schedule.json
name: credit_default_model_monitoring
display_name: Credit default model monitoring
description: Credit default model monitoring setup with minimal configurations

trigger:
  # perform model monitoring activity daily at 3:15am
  type: recurrence
  frequency: day #can be minute, hour, day, week, month
  interval: 1 # #every day
  schedule: 
    hours: 3 # at 3am
    minutes: 15 # at 15 mins after 3am

create_monitor:

  compute: # specify a spark compute for monitoring job
    instance_type: standard_e4s_v3
    runtime_version: "3.3"

  monitoring_target: 
    ml_task: classification # model task type: [classification, regression, question_answering]
    endpoint_deployment_id: azureml:credit-default:main # azureml endpoint deployment id

  alert_notification: # emails to get alerts
    emails:
      - abc@example.com
      - def@example.com
Run the following command to create the model:az ml schedule create -f ./out-of-box-monitoring.yaml
Run the following command to create the model:
az ml schedule create -f ./out-of-box-monitoring.yaml
az ml schedule create -f ./out-of-box-monitoring.yaml
Use code that's similar to the following sample. Replace the following placeholders with appropriate values:
abc@example.com
from azure.identity import DefaultAzureCredential
from azure.ai.ml import MLClient
from azure.ai.ml.entities import (
    AlertNotification,
    MonitoringTarget,
    MonitorDefinition,
    MonitorSchedule,
    RecurrencePattern,
    RecurrenceTrigger,
    ServerlessSparkCompute
)

# Get a handle to the workspace.
ml_client = MLClient(
    DefaultAzureCredential(),
    subscription_id="<subscription-ID>",
    resource_group_name="<resource-group-name>",
    workspace_name="<workspace-name>",
)

# Create the compute instance.
spark_compute = ServerlessSparkCompute(
    instance_type="standard_e4s_v3",
    runtime_version="3.3"
)

# Specify your online endpoint deployment.
monitoring_target = MonitoringTarget(
    ml_task="classification",
    endpoint_deployment_id="azureml:<endpoint-name>:<deployment-name>"
)

# Create an alert notification object.
alert_notification = AlertNotification(
    emails=['<email-address-1>', '<email-address-2>']
)

# Create the monitor definition.
monitor_definition = MonitorDefinition(
    compute=spark_compute,
    monitoring_target=monitoring_target,
    alert_notification=alert_notification
)

# Specify the schedule frequency.
recurrence_trigger = RecurrenceTrigger(
    frequency="<frequency-unit>",
    interval=<interval>,
    schedule=RecurrencePattern(hours=<start-hour>, minutes=<start-minutes>)
)

# Create the monitoring schedule.
model_monitor = MonitorSchedule(
    name="credit_default_monitor_basic",
    trigger=recurrence_trigger,
    create_monitor=monitor_definition
)

# Schedule the monitoring job.
poller = ml_client.schedules.begin_create_or_update(model_monitor)
created_monitor = poller.result()
from azure.identity import DefaultAzureCredential
from azure.ai.ml import MLClient
from azure.ai.ml.entities import (
    AlertNotification,
    MonitoringTarget,
    MonitorDefinition,
    MonitorSchedule,
    RecurrencePattern,
    RecurrenceTrigger,
    ServerlessSparkCompute
)

# Get a handle to the workspace.
ml_client = MLClient(
    DefaultAzureCredential(),
    subscription_id="<subscription-ID>",
    resource_group_name="<resource-group-name>",
    workspace_name="<workspace-name>",
)

# Create the compute instance.
spark_compute = ServerlessSparkCompute(
    instance_type="standard_e4s_v3",
    runtime_version="3.3"
)

# Specify your online endpoint deployment.
monitoring_target = MonitoringTarget(
    ml_task="classification",
    endpoint_deployment_id="azureml:<endpoint-name>:<deployment-name>"
)

# Create an alert notification object.
alert_notification = AlertNotification(
    emails=['<email-address-1>', '<email-address-2>']
)

# Create the monitor definition.
monitor_definition = MonitorDefinition(
    compute=spark_compute,
    monitoring_target=monitoring_target,
    alert_notification=alert_notification
)

# Specify the schedule frequency.
recurrence_trigger = RecurrenceTrigger(
    frequency="<frequency-unit>",
    interval=<interval>,
    schedule=RecurrencePattern(hours=<start-hour>, minutes=<start-minutes>)
)

# Create the monitoring schedule.
model_monitor = MonitorSchedule(
    name="credit_default_monitor_basic",
    trigger=recurrence_trigger,
    create_monitor=monitor_definition
)

# Schedule the monitoring job.
poller = ml_client.schedules.begin_create_or_update(model_monitor)
created_monitor = poller.result()
InAzure Machine Learning studio, go to your workspace.
InAzure Machine Learning studio, go to your workspace.
UnderManage, selectMonitoring, and then selectAdd.
UnderManage, selectMonitoring, and then selectAdd.

On the Basic settings page, enter the following information:Under(Optional) Select model, select the model that you want to monitor.Under(Optional) Select deployment with data collection enabled, select the deployment that you want to monitor. This list should be automatically populated if the model is deployed to an Azure Machine Learning online endpoint.Under(Optional) Select training data, select the training data to use as the comparison reference.UnderMonitor name, enter a name for the monitoring, or keep the default name.UnderVirtual machine size, use the default size.UnderTime zone, select your time zone.For scheduling, selectRecurrenceorCron expression.For recurrence scheduling, specify the repeat frequency, day, and time. For cron expression scheduling, enter a cron expression for a monitoring run.
On the Basic settings page, enter the following information:
Under(Optional) Select model, select the model that you want to monitor.
Under(Optional) Select deployment with data collection enabled, select the deployment that you want to monitor. This list should be automatically populated if the model is deployed to an Azure Machine Learning online endpoint.
Under(Optional) Select training data, select the training data to use as the comparison reference.
UnderMonitor name, enter a name for the monitoring, or keep the default name.
UnderVirtual machine size, use the default size.
UnderTime zone, select your time zone.
For scheduling, selectRecurrenceorCron expression.
For recurrence scheduling, specify the repeat frequency, day, and time. For cron expression scheduling, enter a cron expression for a monitoring run.

SelectNext.
SelectNext.
On the following pages, selectNext:Configure data assetSelect monitoring signals
On the following pages, selectNext:
Configure data asset
Select monitoring signals
On the Notifications page, enter the email address that you want to use to receive notifications, and then selectNext.
On the Notifications page, enter the email address that you want to use to receive notifications, and then selectNext.
On the Review monitoring details page, review the settings, and then selectCreate.
On the Review monitoring details page, review the settings, and then selectCreate.
Set up advanced model monitoring
Azure Machine Learning provides many capabilities for continuous model monitoring. For a comprehensive list of this functionality, seeCapabilities of model monitoring. In many cases, you need to set up model monitoring that supports advanced monitoring tasks. The following section provides a few examples of advanced monitoring:
The use of multiple monitoring signals for a broad view
The use of historical model training data or validation data as the comparison reference data asset
Monitoring of theNmost important features and individual features
Configure feature importance
Feature importance represents the relative importance of each input feature to a model's output. For example, temperature might be more important to a model's prediction than elevation. When you turn on feature importance, you can provide visibility into which features you don't want drifting or having data quality issues in production.
To turn on feature importance with any of your signals, such as data drift or data quality, you need to provide:
Your training data asset as thereference_datadata asset.
reference_data
Thereference_data.data_column_names.target_columnproperty, which is the name of your model's output column, or prediction column.
reference_data.data_column_names.target_column
After you turn on feature importance, you see a feature importance for each feature that you monitor in Azure Machine Learning studio.
You can turn alerts on or off for each signal by setting thealert_enabledproperty when you use the Python SDK or the Azure CLI.
alert_enabled
You can use the Azure CLI, the Python SDK, or the studio to set up advanced model monitoring.
Azure CLI
Python SDK
Studio
Create a monitoring definition in a YAML file. For a sample advanced definition, see the following YAML code, which is also available in theazureml-examples repository.Before you use this definition, adjust the following settings and any others to meet the needs of your environment:Forendpoint_deployment_id, use a value in the formatazureml:<endpoint-name>:<deployment-name>.Forpathin reference input data sections, use a value in the formatazureml:<reference-data-asset-name>:<version>.Fortarget_column, use the name of the output column that contains values that the model predicts, such asDEFAULT_NEXT_MONTH.Forfeatures, list the features likeSEX,EDUCATION, andAGEthat you want to use in an advanced data quality signal.Underemails, list the email addresses that you want to use for notifications.# advanced-model-monitoring.yaml
$schema:  http://azureml/sdk-2-0/Schedule.json
name: fraud_detection_model_monitoring
display_name: Fraud detection model monitoring
description: Fraud detection model monitoring with advanced configurations

trigger:
  # perform model monitoring activity daily at 3:15am
  type: recurrence
  frequency: day #can be minute, hour, day, week, month
  interval: 1 # #every day
  schedule: 
    hours: 3 # at 3am
    minutes: 15 # at 15 mins after 3am

create_monitor:

  compute: 
    instance_type: standard_e4s_v3
    runtime_version: "3.3"

  monitoring_target:
    ml_task: classification
    endpoint_deployment_id: azureml:credit-default:main
  
  monitoring_signals:
    advanced_data_drift: # monitoring signal name, any user defined name works
      type: data_drift
      # reference_dataset is optional. By default referece_dataset is the production inference data associated with Azure Machine Learning online endpoint
      reference_data:
        input_data:
          path: azureml:credit-reference:1 # use training data as comparison reference dataset
          type: mltable
        data_context: training
        data_column_names:
          target_column: DEFAULT_NEXT_MONTH
      features: 
        top_n_feature_importance: 10 # monitor drift for top 10 features
      alert_enabled: true
      metric_thresholds:
        numerical:
          jensen_shannon_distance: 0.01
        categorical:
          pearsons_chi_squared_test: 0.02
    advanced_data_quality:
      type: data_quality
      # reference_dataset is optional. By default reference_dataset is the production inference data associated with Azure Machine Learning online endpoint
      reference_data:
        input_data:
          path: azureml:credit-reference:1
          type: mltable
        data_context: training
      features: # monitor data quality for 3 individual features only
        - SEX
        - EDUCATION
      alert_enabled: true
      metric_thresholds:
        numerical:
          null_value_rate: 0.05
        categorical:
          out_of_bounds_rate: 0.03

    feature_attribution_drift_signal:
      type: feature_attribution_drift
      # production_data: is not required input here
      # Please ensure Azure Machine Learning online endpoint is enabled to collected both model_inputs and model_outputs data
      # Azure Machine Learning model monitoring will automatically join both model_inputs and model_outputs data and used it for computation
      reference_data:
        input_data:
          path: azureml:credit-reference:1
          type: mltable
        data_context: training
        data_column_names:
          target_column: DEFAULT_NEXT_MONTH
      alert_enabled: true
      metric_thresholds:
        normalized_discounted_cumulative_gain: 0.9
  
  alert_notification:
    emails:
      - abc@example.com
      - def@example.com
Create a monitoring definition in a YAML file. For a sample advanced definition, see the following YAML code, which is also available in theazureml-examples repository.
Before you use this definition, adjust the following settings and any others to meet the needs of your environment:
Forendpoint_deployment_id, use a value in the formatazureml:<endpoint-name>:<deployment-name>.
endpoint_deployment_id
azureml:<endpoint-name>:<deployment-name>
Forpathin reference input data sections, use a value in the formatazureml:<reference-data-asset-name>:<version>.
path
azureml:<reference-data-asset-name>:<version>
Fortarget_column, use the name of the output column that contains values that the model predicts, such asDEFAULT_NEXT_MONTH.
target_column
DEFAULT_NEXT_MONTH
Forfeatures, list the features likeSEX,EDUCATION, andAGEthat you want to use in an advanced data quality signal.
features
SEX
EDUCATION
AGE
Underemails, list the email addresses that you want to use for notifications.
emails
# advanced-model-monitoring.yaml
$schema:  http://azureml/sdk-2-0/Schedule.json
name: fraud_detection_model_monitoring
display_name: Fraud detection model monitoring
description: Fraud detection model monitoring with advanced configurations

trigger:
  # perform model monitoring activity daily at 3:15am
  type: recurrence
  frequency: day #can be minute, hour, day, week, month
  interval: 1 # #every day
  schedule: 
    hours: 3 # at 3am
    minutes: 15 # at 15 mins after 3am

create_monitor:

  compute: 
    instance_type: standard_e4s_v3
    runtime_version: "3.3"

  monitoring_target:
    ml_task: classification
    endpoint_deployment_id: azureml:credit-default:main
  
  monitoring_signals:
    advanced_data_drift: # monitoring signal name, any user defined name works
      type: data_drift
      # reference_dataset is optional. By default referece_dataset is the production inference data associated with Azure Machine Learning online endpoint
      reference_data:
        input_data:
          path: azureml:credit-reference:1 # use training data as comparison reference dataset
          type: mltable
        data_context: training
        data_column_names:
          target_column: DEFAULT_NEXT_MONTH
      features: 
        top_n_feature_importance: 10 # monitor drift for top 10 features
      alert_enabled: true
      metric_thresholds:
        numerical:
          jensen_shannon_distance: 0.01
        categorical:
          pearsons_chi_squared_test: 0.02
    advanced_data_quality:
      type: data_quality
      # reference_dataset is optional. By default reference_dataset is the production inference data associated with Azure Machine Learning online endpoint
      reference_data:
        input_data:
          path: azureml:credit-reference:1
          type: mltable
        data_context: training
      features: # monitor data quality for 3 individual features only
        - SEX
        - EDUCATION
      alert_enabled: true
      metric_thresholds:
        numerical:
          null_value_rate: 0.05
        categorical:
          out_of_bounds_rate: 0.03

    feature_attribution_drift_signal:
      type: feature_attribution_drift
      # production_data: is not required input here
      # Please ensure Azure Machine Learning online endpoint is enabled to collected both model_inputs and model_outputs data
      # Azure Machine Learning model monitoring will automatically join both model_inputs and model_outputs data and used it for computation
      reference_data:
        input_data:
          path: azureml:credit-reference:1
          type: mltable
        data_context: training
        data_column_names:
          target_column: DEFAULT_NEXT_MONTH
      alert_enabled: true
      metric_thresholds:
        normalized_discounted_cumulative_gain: 0.9
  
  alert_notification:
    emails:
      - abc@example.com
      - def@example.com
# advanced-model-monitoring.yaml
$schema:  http://azureml/sdk-2-0/Schedule.json
name: fraud_detection_model_monitoring
display_name: Fraud detection model monitoring
description: Fraud detection model monitoring with advanced configurations

trigger:
  # perform model monitoring activity daily at 3:15am
  type: recurrence
  frequency: day #can be minute, hour, day, week, month
  interval: 1 # #every day
  schedule: 
    hours: 3 # at 3am
    minutes: 15 # at 15 mins after 3am

create_monitor:

  compute: 
    instance_type: standard_e4s_v3
    runtime_version: "3.3"

  monitoring_target:
    ml_task: classification
    endpoint_deployment_id: azureml:credit-default:main
  
  monitoring_signals:
    advanced_data_drift: # monitoring signal name, any user defined name works
      type: data_drift
      # reference_dataset is optional. By default referece_dataset is the production inference data associated with Azure Machine Learning online endpoint
      reference_data:
        input_data:
          path: azureml:credit-reference:1 # use training data as comparison reference dataset
          type: mltable
        data_context: training
        data_column_names:
          target_column: DEFAULT_NEXT_MONTH
      features: 
        top_n_feature_importance: 10 # monitor drift for top 10 features
      alert_enabled: true
      metric_thresholds:
        numerical:
          jensen_shannon_distance: 0.01
        categorical:
          pearsons_chi_squared_test: 0.02
    advanced_data_quality:
      type: data_quality
      # reference_dataset is optional. By default reference_dataset is the production inference data associated with Azure Machine Learning online endpoint
      reference_data:
        input_data:
          path: azureml:credit-reference:1
          type: mltable
        data_context: training
      features: # monitor data quality for 3 individual features only
        - SEX
        - EDUCATION
      alert_enabled: true
      metric_thresholds:
        numerical:
          null_value_rate: 0.05
        categorical:
          out_of_bounds_rate: 0.03

    feature_attribution_drift_signal:
      type: feature_attribution_drift
      # production_data: is not required input here
      # Please ensure Azure Machine Learning online endpoint is enabled to collected both model_inputs and model_outputs data
      # Azure Machine Learning model monitoring will automatically join both model_inputs and model_outputs data and used it for computation
      reference_data:
        input_data:
          path: azureml:credit-reference:1
          type: mltable
        data_context: training
        data_column_names:
          target_column: DEFAULT_NEXT_MONTH
      alert_enabled: true
      metric_thresholds:
        normalized_discounted_cumulative_gain: 0.9
  
  alert_notification:
    emails:
      - abc@example.com
      - def@example.com
Run the following command to create the model:az ml schedule create -f ./advanced-model-monitoring.yaml
Run the following command to create the model:
az ml schedule create -f ./advanced-model-monitoring.yaml
az ml schedule create -f ./advanced-model-monitoring.yaml
To set up advanced model monitoring, use code that's similar to the following sample. Replace the following placeholders with appropriate values:
abc@example.com
from azure.identity import DefaultAzureCredential
from azure.ai.ml import Input, MLClient
from azure.ai.ml.constants import (
    MonitorDatasetContext,
)
from azure.ai.ml.entities import (
    AlertNotification,
    BaselineDataRange,
    DataDriftSignal,
    DataQualitySignal,
    PredictionDriftSignal,
    DataDriftMetricThreshold,
    DataQualityMetricThreshold,
    FeatureAttributionDriftMetricThreshold,
    FeatureAttributionDriftSignal,
    PredictionDriftMetricThreshold,
    NumericalDriftMetrics,
    CategoricalDriftMetrics,
    DataQualityMetricsNumerical,
    DataQualityMetricsCategorical,
    MonitorFeatureFilter,
    MonitoringTarget,
    MonitorDefinition,
    MonitorSchedule,
    RecurrencePattern,
    RecurrenceTrigger,
    ServerlessSparkCompute,
    ReferenceData,
    ProductionData
)

# Get a handle to the workspace.
ml_client = MLClient(
    DefaultAzureCredential(),
    subscription_id="<subscription-ID>",
    resource_group_name="<resource-group-name>",
    workspace_name="<workspace-name>",
)

# Create a compute instance.
spark_compute = ServerlessSparkCompute(
    instance_type="standard_e4s_v3",
    runtime_version="3.3"
)

# Specify the online deployment if you have one.
monitoring_target = MonitoringTarget(
    ml_task="classification",
    endpoint_deployment_id="azureml:<endpoint-name>:<deployment-name>"
)

# Specify a look-back window size and offset to use. Omit this line to use the default values, which are listed in the documentation.
data_window = BaselineDataRange(lookback_window_size="P1D", lookback_window_offset="P0D")

# Set up the production data.
production_data = ProductionData(
    input_data=Input(
        type="uri_folder",
        path="azureml:<production-data-asset-name>:1"
    ),
    data_window=data_window,
    data_context=MonitorDatasetContext.MODEL_INPUTS,
)

# Set up the training data to use as a reference data asset.
reference_data_training = ReferenceData(
    input_data=Input(
        type="mltable",
        path="azureml:<reference-data-asset-name>:1"
    ),
    data_column_names={
        "target_column":"<target-column>"
    },
    data_context=MonitorDatasetContext.TRAINING,
)

# Create an advanced data drift signal.
features = MonitorFeatureFilter(top_n_feature_importance=10)

metric_thresholds = DataDriftMetricThreshold(
    numerical=NumericalDriftMetrics(
        jensen_shannon_distance=0.01
    ),
    categorical=CategoricalDriftMetrics(
        pearsons_chi_squared_test=0.02
    )
)

advanced_data_drift = DataDriftSignal(
    reference_data=reference_data_training,
    features=features,
    metric_thresholds=metric_thresholds,
    alert_enabled=True
)

# Create an advanced prediction drift signal.
metric_thresholds = PredictionDriftMetricThreshold(
    categorical=CategoricalDriftMetrics(
        jensen_shannon_distance=0.01
    )
)

advanced_prediction_drift = PredictionDriftSignal(
    reference_data=reference_data_training,
    metric_thresholds=metric_thresholds,
    alert_enabled=True
)

# Create an advanced data quality signal.
features = ['<feature-1>', '<feature-2>', '<feature-3>']

metric_thresholds = DataQualityMetricThreshold(
    numerical=DataQualityMetricsNumerical(
        null_value_rate=0.01
    ),
    categorical=DataQualityMetricsCategorical(
        out_of_bounds_rate=0.02
    )
)

advanced_data_quality = DataQualitySignal(
    reference_data=reference_data_training,
    features=features,
    metric_thresholds=metric_thresholds,
    alert_enabled=True
)

# Create a feature attribution drift signal.
metric_thresholds = FeatureAttributionDriftMetricThreshold(normalized_discounted_cumulative_gain=0.9)

feature_attribution_drift = FeatureAttributionDriftSignal(
    reference_data=reference_data_training,
    metric_thresholds=metric_thresholds,
    alert_enabled=True
)

# Put all monitoring signals in a dictionary.
monitoring_signals = {
    'data_drift_advanced':advanced_data_drift,
    'data_quality_advanced':advanced_data_quality,
    'feature_attribution_drift':feature_attribution_drift,
}

# Create an alert notification object.
alert_notification = AlertNotification(
    emails=['<email-address-1>', '<email-address-2>']
)

# Create the monitor definition.
monitor_definition = MonitorDefinition(
    compute=spark_compute,
    monitoring_target=monitoring_target,
    monitoring_signals=monitoring_signals,
    alert_notification=alert_notification
)

# Specify the schedule frequency.
recurrence_trigger = RecurrenceTrigger(
    frequency="<frequency-unit>",
    interval=<interval>,
    schedule=RecurrencePattern(hours=<start-hour>, minutes=<start-minutes>)
)

# Create the monitoring schedule.
model_monitor = MonitorSchedule(
    name="credit_default_monitor_advanced",
    trigger=recurrence_trigger,
    create_monitor=monitor_definition
)

# Schedule the monitoring job.
poller = ml_client.schedules.begin_create_or_update(model_monitor)
created_monitor = poller.result()
from azure.identity import DefaultAzureCredential
from azure.ai.ml import Input, MLClient
from azure.ai.ml.constants import (
    MonitorDatasetContext,
)
from azure.ai.ml.entities import (
    AlertNotification,
    BaselineDataRange,
    DataDriftSignal,
    DataQualitySignal,
    PredictionDriftSignal,
    DataDriftMetricThreshold,
    DataQualityMetricThreshold,
    FeatureAttributionDriftMetricThreshold,
    FeatureAttributionDriftSignal,
    PredictionDriftMetricThreshold,
    NumericalDriftMetrics,
    CategoricalDriftMetrics,
    DataQualityMetricsNumerical,
    DataQualityMetricsCategorical,
    MonitorFeatureFilter,
    MonitoringTarget,
    MonitorDefinition,
    MonitorSchedule,
    RecurrencePattern,
    RecurrenceTrigger,
    ServerlessSparkCompute,
    ReferenceData,
    ProductionData
)

# Get a handle to the workspace.
ml_client = MLClient(
    DefaultAzureCredential(),
    subscription_id="<subscription-ID>",
    resource_group_name="<resource-group-name>",
    workspace_name="<workspace-name>",
)

# Create a compute instance.
spark_compute = ServerlessSparkCompute(
    instance_type="standard_e4s_v3",
    runtime_version="3.3"
)

# Specify the online deployment if you have one.
monitoring_target = MonitoringTarget(
    ml_task="classification",
    endpoint_deployment_id="azureml:<endpoint-name>:<deployment-name>"
)

# Specify a look-back window size and offset to use. Omit this line to use the default values, which are listed in the documentation.
data_window = BaselineDataRange(lookback_window_size="P1D", lookback_window_offset="P0D")

# Set up the production data.
production_data = ProductionData(
    input_data=Input(
        type="uri_folder",
        path="azureml:<production-data-asset-name>:1"
    ),
    data_window=data_window,
    data_context=MonitorDatasetContext.MODEL_INPUTS,
)

# Set up the training data to use as a reference data asset.
reference_data_training = ReferenceData(
    input_data=Input(
        type="mltable",
        path="azureml:<reference-data-asset-name>:1"
    ),
    data_column_names={
        "target_column":"<target-column>"
    },
    data_context=MonitorDatasetContext.TRAINING,
)

# Create an advanced data drift signal.
features = MonitorFeatureFilter(top_n_feature_importance=10)

metric_thresholds = DataDriftMetricThreshold(
    numerical=NumericalDriftMetrics(
        jensen_shannon_distance=0.01
    ),
    categorical=CategoricalDriftMetrics(
        pearsons_chi_squared_test=0.02
    )
)

advanced_data_drift = DataDriftSignal(
    reference_data=reference_data_training,
    features=features,
    metric_thresholds=metric_thresholds,
    alert_enabled=True
)

# Create an advanced prediction drift signal.
metric_thresholds = PredictionDriftMetricThreshold(
    categorical=CategoricalDriftMetrics(
        jensen_shannon_distance=0.01
    )
)

advanced_prediction_drift = PredictionDriftSignal(
    reference_data=reference_data_training,
    metric_thresholds=metric_thresholds,
    alert_enabled=True
)

# Create an advanced data quality signal.
features = ['<feature-1>', '<feature-2>', '<feature-3>']

metric_thresholds = DataQualityMetricThreshold(
    numerical=DataQualityMetricsNumerical(
        null_value_rate=0.01
    ),
    categorical=DataQualityMetricsCategorical(
        out_of_bounds_rate=0.02
    )
)

advanced_data_quality = DataQualitySignal(
    reference_data=reference_data_training,
    features=features,
    metric_thresholds=metric_thresholds,
    alert_enabled=True
)

# Create a feature attribution drift signal.
metric_thresholds = FeatureAttributionDriftMetricThreshold(normalized_discounted_cumulative_gain=0.9)

feature_attribution_drift = FeatureAttributionDriftSignal(
    reference_data=reference_data_training,
    metric_thresholds=metric_thresholds,
    alert_enabled=True
)

# Put all monitoring signals in a dictionary.
monitoring_signals = {
    'data_drift_advanced':advanced_data_drift,
    'data_quality_advanced':advanced_data_quality,
    'feature_attribution_drift':feature_attribution_drift,
}

# Create an alert notification object.
alert_notification = AlertNotification(
    emails=['<email-address-1>', '<email-address-2>']
)

# Create the monitor definition.
monitor_definition = MonitorDefinition(
    compute=spark_compute,
    monitoring_target=monitoring_target,
    monitoring_signals=monitoring_signals,
    alert_notification=alert_notification
)

# Specify the schedule frequency.
recurrence_trigger = RecurrenceTrigger(
    frequency="<frequency-unit>",
    interval=<interval>,
    schedule=RecurrencePattern(hours=<start-hour>, minutes=<start-minutes>)
)

# Create the monitoring schedule.
model_monitor = MonitorSchedule(
    name="credit_default_monitor_advanced",
    trigger=recurrence_trigger,
    create_monitor=monitor_definition
)

# Schedule the monitoring job.
poller = ml_client.schedules.begin_create_or_update(model_monitor)
created_monitor = poller.result()
To set up advanced monitoring, take the steps in the following sections.
Configure basic settings
InAzure Machine Learning studio, go to your workspace.
InAzure Machine Learning studio, go to your workspace.
UnderManage, selectMonitoring, and then selectAdd.
UnderManage, selectMonitoring, and then selectAdd.
On the Basic settings page, enter information as described earlier inSet up out-of-box model monitoring.
On the Basic settings page, enter information as described earlier inSet up out-of-box model monitoring.
Add data assets
On the Basic settings page, after you configure the settings, selectNext. The Configure data asset page of theAdvanced settingssection opens.
On the Basic settings page, after you configure the settings, selectNext. The Configure data asset page of theAdvanced settingssection opens.
If you don't see the data asset that you want to use as a reference data asset, selectAdd. Then enter the settings for your data asset. We recommend that you use the model training data as the comparison reference data asset for data drift and data quality. Also, use the model validation data as the comparison reference data asset for prediction drift.
If you don't see the data asset that you want to use as a reference data asset, selectAdd. Then enter the settings for your data asset. We recommend that you use the model training data as the comparison reference data asset for data drift and data quality. Also, use the model validation data as the comparison reference data asset for prediction drift.

Edit data drift settings
On the Configure data asset page, after you add data assets, selectNext. TheSelect monitoring signalspage opens. If you're using an Azure Machine Learning online deployment, you see some monitoring signals. The data drift, data quality, and prediction drift signals use recent, past production data as the comparison reference data asset and use smart default values for metrics and thresholds.
On the Configure data asset page, after you add data assets, selectNext. TheSelect monitoring signalspage opens. If you're using an Azure Machine Learning online deployment, you see some monitoring signals. The data drift, data quality, and prediction drift signals use recent, past production data as the comparison reference data asset and use smart default values for metrics and thresholds.

Next to the data drift signal, selectEdit.
Next to the data drift signal, selectEdit.
In theEdit Signalwindow, take the following steps to configure the data drift signal:In step 1:For the production data asset, select your model input data asset.Select the look-back window size that you want to use.In step 2:For the reference data asset, select your training data asset.Select the target, or output, column.In step 3, selectTop N featuresto monitor drift for theNmost important features. Or select specific features if you want to monitor drift for a specific set.In step 4, select the metric and threshold that you want to use for numerical features.In step 5, select the metric and threshold that you want to use for categorical features.SelectSave.
In theEdit Signalwindow, take the following steps to configure the data drift signal:
In step 1:For the production data asset, select your model input data asset.Select the look-back window size that you want to use.
For the production data asset, select your model input data asset.
Select the look-back window size that you want to use.
In step 2:For the reference data asset, select your training data asset.Select the target, or output, column.
For the reference data asset, select your training data asset.
Select the target, or output, column.
In step 3, selectTop N featuresto monitor drift for theNmost important features. Or select specific features if you want to monitor drift for a specific set.
In step 4, select the metric and threshold that you want to use for numerical features.
In step 5, select the metric and threshold that you want to use for categorical features.
SelectSave.

Add a feature attribution drift signal
On the Select monitoring signals page, selectAdd.
On the Select monitoring signals page, selectAdd.
In the Edit Signal window, selectFeature attribution drift (PREVIEW), and then take the following steps to configure the feature attribution drift signal:In step 1:Select the production data asset that has your model input data.Select the look-back window size that you want to use.In step 2:Select the production data asset that has your model output data.Select the common column to use to join the production data and the output data. If you use thedata collectorto collect data, selectcorrelationid.(Optional) If you use the data collector to collect your input and output data in a format that joins them together, take the following steps:In step 1, for the production data asset, select the joined data asset.In step 2, selectRemoveto remove step 2 from the configuration panel.In step 3:For the reference data asset, select your training data asset.Select the target, or output, column for your training data asset.In step 4, select the metric and threshold that you want to use.SelectSave.
In the Edit Signal window, selectFeature attribution drift (PREVIEW), and then take the following steps to configure the feature attribution drift signal:
In step 1:Select the production data asset that has your model input data.Select the look-back window size that you want to use.
Select the production data asset that has your model input data.
Select the look-back window size that you want to use.
In step 2:Select the production data asset that has your model output data.Select the common column to use to join the production data and the output data. If you use thedata collectorto collect data, selectcorrelationid.
Select the production data asset that has your model output data.
Select the common column to use to join the production data and the output data. If you use thedata collectorto collect data, selectcorrelationid.
(Optional) If you use the data collector to collect your input and output data in a format that joins them together, take the following steps:In step 1, for the production data asset, select the joined data asset.In step 2, selectRemoveto remove step 2 from the configuration panel.
In step 1, for the production data asset, select the joined data asset.
In step 2, selectRemoveto remove step 2 from the configuration panel.
In step 3:For the reference data asset, select your training data asset.Select the target, or output, column for your training data asset.
For the reference data asset, select your training data asset.
Select the target, or output, column for your training data asset.
In step 4, select the metric and threshold that you want to use.
SelectSave.

Finish the configuration
On the Select monitoring signals page, finish configuring your monitoring signals, and then selectNext.
On the Select monitoring signals page, finish configuring your monitoring signals, and then selectNext.

On the Notifications page, turn on notifications for each signal, and then selectNext.
On the Notifications page, turn on notifications for each signal, and then selectNext.
On the Review monitoring details page, review your settings.
On the Review monitoring details page, review your settings.

SelectCreateto create your advanced model monitor.
SelectCreateto create your advanced model monitor.
Set up model performance monitoring
When you use Azure Machine Learning model monitoring, you can track the performance of your models in production by calculating their performance metrics. The following model performance metrics are currently supported:
For classification models:PrecisionAccuracyRecall
Precision
Accuracy
Recall
For regression models:Mean absolute error (MAE)Mean squared error (MSE)Root mean squared error (RMSE)
Mean absolute error (MAE)
Mean squared error (MSE)
Root mean squared error (RMSE)
Prerequisites for model performance monitoring
Output data for the production model (the model's predictions) with a unique ID for each row. If you use theAzure Machine Learning data collectorto collect production data, a correlation ID is provided for each inference request for you. The data collector also offers the option of logging your own unique ID from your application.NoteFor Azure Machine Learning model performance monitoring, we recommend that you use theAzure Machine Learning data collectorto log your unique ID in its own column.
Output data for the production model (the model's predictions) with a unique ID for each row. If you use theAzure Machine Learning data collectorto collect production data, a correlation ID is provided for each inference request for you. The data collector also offers the option of logging your own unique ID from your application.
Note
For Azure Machine Learning model performance monitoring, we recommend that you use theAzure Machine Learning data collectorto log your unique ID in its own column.
Ground truth data (actuals) with a unique ID for each row. The unique ID for a given row should match the unique ID for the model output data for that particular inference request. This unique ID is used to join your ground truth data asset with the model output data.If you don't have ground truth data, you can't perform model performance monitoring. Ground truth data is encountered at the application level, so it's your responsibility to collect it as it becomes available. You should also maintain a data asset in Azure Machine Learning that contains this ground truth data.
Ground truth data (actuals) with a unique ID for each row. The unique ID for a given row should match the unique ID for the model output data for that particular inference request. This unique ID is used to join your ground truth data asset with the model output data.
If you don't have ground truth data, you can't perform model performance monitoring. Ground truth data is encountered at the application level, so it's your responsibility to collect it as it becomes available. You should also maintain a data asset in Azure Machine Learning that contains this ground truth data.
(Optional) A prejoined tabular data asset with model output data and ground truth data already joined together.
(Optional) A prejoined tabular data asset with model output data and ground truth data already joined together.
Requirements for model performance monitoring when you use the data collector
Azure Machine Learning generates a correlation ID for you when you meet the following criteria:
You use theAzure Machine Learning data collectorto collect production inference data.
You don't supply your own unique ID for each row as a separate column.
The generated correlation ID is included in the logged JSON object. However, the data collectorbatches rowsthat are sent within short time intervals of each other. Batched rows fall within the same JSON object. Within each object, all rows have the same correlation ID.
To differentiate between the rows in a JSON object, Azure Machine Learning model performance monitoring uses indexing to determine the order of the rows within the object. For example, if a batch contains three rows and the correlation ID istest, the first row has an ID oftest_0, the second row has an ID oftest_1, and the third row has an ID oftest_2. To match your ground truth data asset unique IDs with the IDs of your collected production inference model output data, apply an index to each correlation ID appropriately. If your logged JSON object only has one row, usecorrelationid_0as thecorrelationidvalue.
test
test_0
test_1
test_2
correlationid_0
correlationid
To avoid using this indexing, we recommend that you log your unique ID in its own column. Put that column within the pandas data frame that theAzure Machine Learning data collectorlogs. In your model monitoring configuration, you can then specify the name of this column to join your model output data with your ground truth data. As long as the IDs for each row in both data assets are the same, Azure Machine Learning model monitoring can perform model performance monitoring.
Example workflow for monitoring model performance
To understand the concepts that are associated with model performance monitoring, consider the following example workflow. It applies to a scenario in which you deploy a model to predict whether credit card transactions are fraudulent:
Configure your deployment to use the data collector to collect the model's production inference data (input and output data). Store the output data in a column calledis_fraud.
is_fraud
For each row of the collected inference data, log a unique ID. The unique ID can come from your application, or you can use thecorrelationidvalue that Azure Machine Learning uniquely generates for each logged JSON object.
correlationid
When the ground truth (or actual)is_frauddata is available, log and map each row to the same unique ID that's logged for the corresponding row in the model's output data.
is_fraud
Register a data asset in Azure Machine Learning, and use it to collect and maintain the ground truthis_frauddata.
is_fraud
Create a model performance monitoring signal that uses the unique ID columns to join the model's production inference and ground truth data assets.
Compute the model performance metrics.
Azure CLI
Python SDK
Studio
After you satisfy theprerequisites for model performance monitoring, take the following steps to set up model monitoring:
Create a monitoring definition in a YAML file. The following sample specification defines model monitoring with production inference data. Before you use this definition, adjust the following settings and any others to meet the needs of your environment:Forendpoint_deployment_id, use a value in the formatazureml:<endpoint-name>:<deployment-name>.For eachpathvalue in an input data section, use a value in the formatazureml:<data-asset-name>:<version>.For thepredictionvalue, use the name of the output column that contains values that the model predicts.For theactualvalue, use the name of the ground truth column that contains the actual values that the model tries to predict.For thecorrelation_idvalues, use the names of the columns that are used to join the output data and the ground truth data.Underemails, list the email addresses that you want to use for notifications.# model-performance-monitoring.yaml
$schema:  http://azureml/sdk-2-0/Schedule.json
name: model_performance_monitoring
display_name: Credit card fraud model performance
description: Credit card fraud model performance

trigger:
  type: recurrence
  frequency: day
  interval: 7 
  schedule: 
    hours: 10
    minutes: 15

create_monitor:
  compute: 
    instance_type: standard_e8s_v3
    runtime_version: "3.3"
  monitoring_target:
    ml_task: classification
    endpoint_deployment_id: azureml:loan-approval-endpoint:loan-approval-deployment

  monitoring_signals:
    fraud_detection_model_performance: 
      type: model_performance 
      production_data:
        input_data:
          path: azureml:credit-default-main-model_outputs:1
          type: mltable
        data_column_names:
          prediction: is_fraud
          correlation_id: correlation_id
      reference_data:
        input_data:
          path: azureml:my_model_ground_truth_data:1
          type: mltable
        data_column_names:
          actual: is_fraud
          correlation_id: correlation_id
        data_context: ground_truth
      alert_enabled: true
      metric_thresholds: 
        tabular_classification:
          accuracy: 0.95
          precision: 0.8
  alert_notification: 
      emails: 
        - abc@example.com
Create a monitoring definition in a YAML file. The following sample specification defines model monitoring with production inference data. Before you use this definition, adjust the following settings and any others to meet the needs of your environment:
Forendpoint_deployment_id, use a value in the formatazureml:<endpoint-name>:<deployment-name>.
endpoint_deployment_id
azureml:<endpoint-name>:<deployment-name>
For eachpathvalue in an input data section, use a value in the formatazureml:<data-asset-name>:<version>.
path
azureml:<data-asset-name>:<version>
For thepredictionvalue, use the name of the output column that contains values that the model predicts.
prediction
For theactualvalue, use the name of the ground truth column that contains the actual values that the model tries to predict.
actual
For thecorrelation_idvalues, use the names of the columns that are used to join the output data and the ground truth data.
correlation_id
Underemails, list the email addresses that you want to use for notifications.
emails
# model-performance-monitoring.yaml
$schema:  http://azureml/sdk-2-0/Schedule.json
name: model_performance_monitoring
display_name: Credit card fraud model performance
description: Credit card fraud model performance

trigger:
  type: recurrence
  frequency: day
  interval: 7 
  schedule: 
    hours: 10
    minutes: 15

create_monitor:
  compute: 
    instance_type: standard_e8s_v3
    runtime_version: "3.3"
  monitoring_target:
    ml_task: classification
    endpoint_deployment_id: azureml:loan-approval-endpoint:loan-approval-deployment

  monitoring_signals:
    fraud_detection_model_performance: 
      type: model_performance 
      production_data:
        input_data:
          path: azureml:credit-default-main-model_outputs:1
          type: mltable
        data_column_names:
          prediction: is_fraud
          correlation_id: correlation_id
      reference_data:
        input_data:
          path: azureml:my_model_ground_truth_data:1
          type: mltable
        data_column_names:
          actual: is_fraud
          correlation_id: correlation_id
        data_context: ground_truth
      alert_enabled: true
      metric_thresholds: 
        tabular_classification:
          accuracy: 0.95
          precision: 0.8
  alert_notification: 
      emails: 
        - abc@example.com
# model-performance-monitoring.yaml
$schema:  http://azureml/sdk-2-0/Schedule.json
name: model_performance_monitoring
display_name: Credit card fraud model performance
description: Credit card fraud model performance

trigger:
  type: recurrence
  frequency: day
  interval: 7 
  schedule: 
    hours: 10
    minutes: 15

create_monitor:
  compute: 
    instance_type: standard_e8s_v3
    runtime_version: "3.3"
  monitoring_target:
    ml_task: classification
    endpoint_deployment_id: azureml:loan-approval-endpoint:loan-approval-deployment

  monitoring_signals:
    fraud_detection_model_performance: 
      type: model_performance 
      production_data:
        input_data:
          path: azureml:credit-default-main-model_outputs:1
          type: mltable
        data_column_names:
          prediction: is_fraud
          correlation_id: correlation_id
      reference_data:
        input_data:
          path: azureml:my_model_ground_truth_data:1
          type: mltable
        data_column_names:
          actual: is_fraud
          correlation_id: correlation_id
        data_context: ground_truth
      alert_enabled: true
      metric_thresholds: 
        tabular_classification:
          accuracy: 0.95
          precision: 0.8
  alert_notification: 
      emails: 
        - abc@example.com
Run the following command to create the model:az ml schedule create -f ./model-performance-monitoring.yaml
Run the following command to create the model:
az ml schedule create -f ./model-performance-monitoring.yaml
az ml schedule create -f ./model-performance-monitoring.yaml
After you satisfy theprerequisites for model performance monitoring, use the following Python code to set up model monitoring. First replace the following placeholders with appropriate values:
abc@example.com
from azure.identity import DefaultAzureCredential
from azure.ai.ml import Input, MLClient
from azure.ai.ml.constants import (
    MonitorDatasetContext,
)
from azure.ai.ml.entities import (
    AlertNotification,
    BaselineDataRange,
    ModelPerformanceMetricThreshold,
    ModelPerformanceSignal,
    ModelPerformanceClassificationThresholds,
    MonitoringTarget,
    MonitorDefinition,
    MonitorSchedule,
    RecurrencePattern,
    RecurrenceTrigger,
    ServerlessSparkCompute,
    ReferenceData,
    ProductionData
)

# Get a handle to the workspace.
ml_client = MLClient(
    DefaultAzureCredential(),
    subscription_id="<subscription-ID>",
    resource_group_name="<resource-group-name>",
    workspace_name="<workspace-name>",
)

# Create a compute instance.
spark_compute = ServerlessSparkCompute(
    instance_type="standard_e4s_v3",
    runtime_version="3.3"
)

# Specify the type of the model task.
monitoring_target = MonitoringTarget(
    ml_task="classification",
)

# Specify production data that the model data collector generates. 
production_data = ProductionData(
    input_data=Input(
        type="uri_folder",
        path="azureml:<production-data-asset-name>:1"
    ),
    data_column_names={
        "target_column": "<production-target-column>",
        "join_column": "<production-join-column>"
    },
    data_window=BaselineDataRange(
        lookback_window_offset="P0D",
        lookback_window_size="P10D",
    )
)

# Specify the ground truth reference data.
reference_data_ground_truth = ReferenceData(
    input_data=Input(
        type="mltable",
        path="azureml:<ground-truth-data-asset-name>:1"
    ),
    data_column_names={
        "target_column": "<ground-truth-target-column>",
        "join_column": "<ground-truth-join-column>"
    },
    data_context=MonitorDatasetContext.GROUND_TRUTH_DATA,
)

# Create the model performance signal.
metric_thresholds = ModelPerformanceMetricThreshold(
    classification=ModelPerformanceClassificationThresholds(
        accuracy=0.50,
        precision=0.50,
        recall=0.50
    ),
)

model_performance = ModelPerformanceSignal(
    production_data=production_data,
    reference_data=reference_data_ground_truth,
    metric_thresholds=metric_thresholds,
    alert_enabled=True
)

# Put all monitoring signals in a dictionary.
monitoring_signals = {
    'model_performance':model_performance,
}

# Create an alert notification object.
alert_notification = AlertNotification(
    emails=['<email-address-1>', '<email-address-2>']
)

# Set up the monitor definition.
monitor_definition = MonitorDefinition(
    compute=spark_compute,
    monitoring_target=monitoring_target,
    monitoring_signals=monitoring_signals,
    alert_notification=alert_notification
)

# Specify the schedule frequency.
recurrence_trigger = RecurrenceTrigger(
    frequency="<frequency-unit>",
    interval=<interval>,
    schedule=RecurrencePattern(hours=<start-hour>, minutes=<start-minutes>)
)

# Create the monitoring schedule.
model_monitor = MonitorSchedule(
    name="credit_default_model_performance",
    trigger=recurrence_trigger,
    create_monitor=monitor_definition
)

# Schedule the monitoring job.
poller = ml_client.schedules.begin_create_or_update(model_monitor)
created_monitor = poller.result()
from azure.identity import DefaultAzureCredential
from azure.ai.ml import Input, MLClient
from azure.ai.ml.constants import (
    MonitorDatasetContext,
)
from azure.ai.ml.entities import (
    AlertNotification,
    BaselineDataRange,
    ModelPerformanceMetricThreshold,
    ModelPerformanceSignal,
    ModelPerformanceClassificationThresholds,
    MonitoringTarget,
    MonitorDefinition,
    MonitorSchedule,
    RecurrencePattern,
    RecurrenceTrigger,
    ServerlessSparkCompute,
    ReferenceData,
    ProductionData
)

# Get a handle to the workspace.
ml_client = MLClient(
    DefaultAzureCredential(),
    subscription_id="<subscription-ID>",
    resource_group_name="<resource-group-name>",
    workspace_name="<workspace-name>",
)

# Create a compute instance.
spark_compute = ServerlessSparkCompute(
    instance_type="standard_e4s_v3",
    runtime_version="3.3"
)

# Specify the type of the model task.
monitoring_target = MonitoringTarget(
    ml_task="classification",
)

# Specify production data that the model data collector generates. 
production_data = ProductionData(
    input_data=Input(
        type="uri_folder",
        path="azureml:<production-data-asset-name>:1"
    ),
    data_column_names={
        "target_column": "<production-target-column>",
        "join_column": "<production-join-column>"
    },
    data_window=BaselineDataRange(
        lookback_window_offset="P0D",
        lookback_window_size="P10D",
    )
)

# Specify the ground truth reference data.
reference_data_ground_truth = ReferenceData(
    input_data=Input(
        type="mltable",
        path="azureml:<ground-truth-data-asset-name>:1"
    ),
    data_column_names={
        "target_column": "<ground-truth-target-column>",
        "join_column": "<ground-truth-join-column>"
    },
    data_context=MonitorDatasetContext.GROUND_TRUTH_DATA,
)

# Create the model performance signal.
metric_thresholds = ModelPerformanceMetricThreshold(
    classification=ModelPerformanceClassificationThresholds(
        accuracy=0.50,
        precision=0.50,
        recall=0.50
    ),
)

model_performance = ModelPerformanceSignal(
    production_data=production_data,
    reference_data=reference_data_ground_truth,
    metric_thresholds=metric_thresholds,
    alert_enabled=True
)

# Put all monitoring signals in a dictionary.
monitoring_signals = {
    'model_performance':model_performance,
}

# Create an alert notification object.
alert_notification = AlertNotification(
    emails=['<email-address-1>', '<email-address-2>']
)

# Set up the monitor definition.
monitor_definition = MonitorDefinition(
    compute=spark_compute,
    monitoring_target=monitoring_target,
    monitoring_signals=monitoring_signals,
    alert_notification=alert_notification
)

# Specify the schedule frequency.
recurrence_trigger = RecurrenceTrigger(
    frequency="<frequency-unit>",
    interval=<interval>,
    schedule=RecurrencePattern(hours=<start-hour>, minutes=<start-minutes>)
)

# Create the monitoring schedule.
model_monitor = MonitorSchedule(
    name="credit_default_model_performance",
    trigger=recurrence_trigger,
    create_monitor=monitor_definition
)

# Schedule the monitoring job.
poller = ml_client.schedules.begin_create_or_update(model_monitor)
created_monitor = poller.result()
To set up model performance monitoring, take the steps in the following sections.
Configure basic settings
InAzure Machine Learning studio, go to your workspace.
InAzure Machine Learning studio, go to your workspace.
UnderManage, selectMonitoring, and then selectAdd.
UnderManage, selectMonitoring, and then selectAdd.
On the Basic settings page, enter information as described earlier inSet up out-of-box model monitoring.
On the Basic settings page, enter information as described earlier inSet up out-of-box model monitoring.
Add data assets
On the Basic settings page, selectNextto open the Configure data asset page of theAdvanced settingssection.
On the Basic settings page, selectNextto open the Configure data asset page of theAdvanced settingssection.
SelectAdd, and then add the data asset that you want to use as the ground truth data asset. The ground truth data asset must have a unique ID column. Also, the values in the unique ID column of the ground truth data asset and the model output data asset must match. These data assets can then be joined together before metric computation occurs.
SelectAdd, and then add the data asset that you want to use as the ground truth data asset. The ground truth data asset must have a unique ID column. Also, the values in the unique ID column of the ground truth data asset and the model output data asset must match. These data assets can then be joined together before metric computation occurs.

If you don't see your model output data asset in the list of added data assets, selectAdd, and then add it.
If you don't see your model output data asset in the list of added data assets, selectAdd, and then add it.

Add a performance monitoring signal
On the Configure data asset page, selectNext. The Select monitoring signals page opens. If you're using an Azure Machine Learning online deployment, you see a list of monitoring signals.
On the Configure data asset page, selectNext. The Select monitoring signals page opens. If you're using an Azure Machine Learning online deployment, you see a list of monitoring signals.
Delete any monitoring signals that you see on the page. The focus of this section is to create a model performance monitoring signal.
Delete any monitoring signals that you see on the page. The focus of this section is to create a model performance monitoring signal.
SelectAdd.
SelectAdd.
In the Edit Signal window, selectModel performance (PREVIEW), and then take the following steps to configure the model performance signal:In step 1:For the production data asset, select your model output data asset.Select an appropriate target column, for example,DEFAULT_NEXT_MONTH.Select the look-back window size and offset that you want to use.In step 2:For the reference data asset, select your ground truth data asset.Select the target column, for example,ground_truth.Select the column to use for the join with the model output data asset, for example,correlationid. Both data assets should contain that column, and it should contain a unique ID for each row in the data asset.In step 3, select the performance metrics that you want to use, and specify their respective thresholds.
In the Edit Signal window, selectModel performance (PREVIEW), and then take the following steps to configure the model performance signal:
In step 1:For the production data asset, select your model output data asset.Select an appropriate target column, for example,DEFAULT_NEXT_MONTH.Select the look-back window size and offset that you want to use.
For the production data asset, select your model output data asset.
Select an appropriate target column, for example,DEFAULT_NEXT_MONTH.
DEFAULT_NEXT_MONTH
Select the look-back window size and offset that you want to use.
In step 2:For the reference data asset, select your ground truth data asset.Select the target column, for example,ground_truth.Select the column to use for the join with the model output data asset, for example,correlationid. Both data assets should contain that column, and it should contain a unique ID for each row in the data asset.
For the reference data asset, select your ground truth data asset.
Select the target column, for example,ground_truth.
ground_truth
Select the column to use for the join with the model output data asset, for example,correlationid. Both data assets should contain that column, and it should contain a unique ID for each row in the data asset.
correlationid
In step 3, select the performance metrics that you want to use, and specify their respective thresholds.

SelectSave. On the Select monitoring signals page, the model performance signal is visible.
SelectSave. On the Select monitoring signals page, the model performance signal is visible.

Finish the configuration
On the Select monitoring signals page, selectNext.
On the Select monitoring signals page, selectNext.
On the Notifications page, turn on notifications for the model performance signal, and then selectNext.
On the Notifications page, turn on notifications for the model performance signal, and then selectNext.
On the Review monitoring settings page, review your settings.
On the Review monitoring settings page, review your settings.

SelectCreateto create your model performance monitor.
SelectCreateto create your model performance monitor.
Set up model monitoring of production data
You can also monitor models that you deploy to Azure Machine Learning batch endpoints or that you deploy outside Azure Machine Learning. If you don't have a deployment but you have production data, you can use the data to perform continuous model monitoring. To monitor these models, you must be able to:
Collect production inference data from models deployed in production.
Register the production inference data as an Azure Machine Learning data asset, and ensure continuous updates of the data.
Provide a custom data preprocessing component and register it as an Azure Machine Learning component if you don't use thedata collectorto collect data. Without this custom data preprocessing component, the Azure Machine Learning model monitoring system can't process your data into a tabular form that supports time windowing.
Your custom preprocessing component must have the following input and output signatures:
data_window_start
data_window_end
input_data
preprocessed_data
For an example of a custom data preprocessing component, seecustom_preprocessing in the azuremml-examples GitHub repo.
For instructions for registering an Azure Machine Learning component, seeRegister component in your workspace.
After you register your production data and preprocessing component, you can set up model monitoring.
Azure CLI
Python SDK
Studio
Create a monitoring definition YAML file that's similar to the following one. Before you use this definition, adjust the following settings and any others to meet the needs of your environment:Forendpoint_deployment_id, use a value in the formatazureml:<endpoint-name>:<deployment-name>.Forpre_processing_component, use a value in the formatazureml:<component-name>:<component-version>. Specify the exact version, such as1.0.0, not1.For eachpath, use a value in the formatazureml:<data-asset-name>:<version>.For thetarget_columnvalue, use the name of the output column that contains values that the model predicts.Underemails, list the email addresses that you want to use for notifications.# model-monitoring-with-collected-data.yaml
$schema:  http://azureml/sdk-2-0/Schedule.json
name: fraud_detection_model_monitoring
display_name: Fraud detection model monitoring
description: Fraud detection model monitoring with your own production data

trigger:
  # perform model monitoring activity daily at 3:15am
  type: recurrence
  frequency: day #can be minute, hour, day, week, month
  interval: 1 # #every day
  schedule: 
    hours: 3 # at 3am
    minutes: 15 # at 15 mins after 3am

create_monitor:
  compute: 
    instance_type: standard_e4s_v3
    runtime_version: "3.3"
  monitoring_target:
    ml_task: classification
    endpoint_deployment_id: azureml:fraud-detection-endpoint:fraud-detection-deployment
  
  monitoring_signals:

    advanced_data_drift: # monitoring signal name, any user defined name works
      type: data_drift
      # define production dataset with your collected data
      production_data:
        input_data:
          path: azureml:my_production_inference_data_model_inputs:1  # your collected data is registered as Azure Machine Learning asset
          type: uri_folder
        data_context: model_inputs
        pre_processing_component: azureml:production_data_preprocessing:1.0.0
      reference_data:
        input_data:
          path: azureml:my_model_training_data:1 # use training data as comparison baseline
          type: mltable
        data_context: training
        data_column_names:
          target_column: is_fraud
      features: 
        top_n_feature_importance: 20 # monitor drift for top 20 features
      alert_enabled: true
      metric_thresholds:
        numerical:
          jensen_shannon_distance: 0.01
        categorical:
          pearsons_chi_squared_test: 0.02

    advanced_prediction_drift: # monitoring signal name, any user defined name works
      type: prediction_drift
      # define production dataset with your collected data
      production_data:
        input_data:
          path: azureml:my_production_inference_data_model_outputs:1  # your collected data is registered as Azure Machine Learning asset
          type: uri_folder
        data_context: model_outputs
        pre_processing_component: azureml:production_data_preprocessing:1.0.0
      reference_data:
        input_data:
          path: azureml:my_model_validation_data:1 # use training data as comparison reference dataset
          type: mltable
        data_context: validation
      alert_enabled: true
      metric_thresholds:
        categorical:
          pearsons_chi_squared_test: 0.02
  
  alert_notification:
    emails:
      - abc@example.com
      - def@example.com
Create a monitoring definition YAML file that's similar to the following one. Before you use this definition, adjust the following settings and any others to meet the needs of your environment:
Forendpoint_deployment_id, use a value in the formatazureml:<endpoint-name>:<deployment-name>.
endpoint_deployment_id
azureml:<endpoint-name>:<deployment-name>
Forpre_processing_component, use a value in the formatazureml:<component-name>:<component-version>. Specify the exact version, such as1.0.0, not1.
pre_processing_component
azureml:<component-name>:<component-version>
1.0.0
1
For eachpath, use a value in the formatazureml:<data-asset-name>:<version>.
path
azureml:<data-asset-name>:<version>
For thetarget_columnvalue, use the name of the output column that contains values that the model predicts.
target_column
Underemails, list the email addresses that you want to use for notifications.
emails
# model-monitoring-with-collected-data.yaml
$schema:  http://azureml/sdk-2-0/Schedule.json
name: fraud_detection_model_monitoring
display_name: Fraud detection model monitoring
description: Fraud detection model monitoring with your own production data

trigger:
  # perform model monitoring activity daily at 3:15am
  type: recurrence
  frequency: day #can be minute, hour, day, week, month
  interval: 1 # #every day
  schedule: 
    hours: 3 # at 3am
    minutes: 15 # at 15 mins after 3am

create_monitor:
  compute: 
    instance_type: standard_e4s_v3
    runtime_version: "3.3"
  monitoring_target:
    ml_task: classification
    endpoint_deployment_id: azureml:fraud-detection-endpoint:fraud-detection-deployment
  
  monitoring_signals:

    advanced_data_drift: # monitoring signal name, any user defined name works
      type: data_drift
      # define production dataset with your collected data
      production_data:
        input_data:
          path: azureml:my_production_inference_data_model_inputs:1  # your collected data is registered as Azure Machine Learning asset
          type: uri_folder
        data_context: model_inputs
        pre_processing_component: azureml:production_data_preprocessing:1.0.0
      reference_data:
        input_data:
          path: azureml:my_model_training_data:1 # use training data as comparison baseline
          type: mltable
        data_context: training
        data_column_names:
          target_column: is_fraud
      features: 
        top_n_feature_importance: 20 # monitor drift for top 20 features
      alert_enabled: true
      metric_thresholds:
        numerical:
          jensen_shannon_distance: 0.01
        categorical:
          pearsons_chi_squared_test: 0.02

    advanced_prediction_drift: # monitoring signal name, any user defined name works
      type: prediction_drift
      # define production dataset with your collected data
      production_data:
        input_data:
          path: azureml:my_production_inference_data_model_outputs:1  # your collected data is registered as Azure Machine Learning asset
          type: uri_folder
        data_context: model_outputs
        pre_processing_component: azureml:production_data_preprocessing:1.0.0
      reference_data:
        input_data:
          path: azureml:my_model_validation_data:1 # use training data as comparison reference dataset
          type: mltable
        data_context: validation
      alert_enabled: true
      metric_thresholds:
        categorical:
          pearsons_chi_squared_test: 0.02
  
  alert_notification:
    emails:
      - abc@example.com
      - def@example.com
# model-monitoring-with-collected-data.yaml
$schema:  http://azureml/sdk-2-0/Schedule.json
name: fraud_detection_model_monitoring
display_name: Fraud detection model monitoring
description: Fraud detection model monitoring with your own production data

trigger:
  # perform model monitoring activity daily at 3:15am
  type: recurrence
  frequency: day #can be minute, hour, day, week, month
  interval: 1 # #every day
  schedule: 
    hours: 3 # at 3am
    minutes: 15 # at 15 mins after 3am

create_monitor:
  compute: 
    instance_type: standard_e4s_v3
    runtime_version: "3.3"
  monitoring_target:
    ml_task: classification
    endpoint_deployment_id: azureml:fraud-detection-endpoint:fraud-detection-deployment
  
  monitoring_signals:

    advanced_data_drift: # monitoring signal name, any user defined name works
      type: data_drift
      # define production dataset with your collected data
      production_data:
        input_data:
          path: azureml:my_production_inference_data_model_inputs:1  # your collected data is registered as Azure Machine Learning asset
          type: uri_folder
        data_context: model_inputs
        pre_processing_component: azureml:production_data_preprocessing:1.0.0
      reference_data:
        input_data:
          path: azureml:my_model_training_data:1 # use training data as comparison baseline
          type: mltable
        data_context: training
        data_column_names:
          target_column: is_fraud
      features: 
        top_n_feature_importance: 20 # monitor drift for top 20 features
      alert_enabled: true
      metric_thresholds:
        numerical:
          jensen_shannon_distance: 0.01
        categorical:
          pearsons_chi_squared_test: 0.02

    advanced_prediction_drift: # monitoring signal name, any user defined name works
      type: prediction_drift
      # define production dataset with your collected data
      production_data:
        input_data:
          path: azureml:my_production_inference_data_model_outputs:1  # your collected data is registered as Azure Machine Learning asset
          type: uri_folder
        data_context: model_outputs
        pre_processing_component: azureml:production_data_preprocessing:1.0.0
      reference_data:
        input_data:
          path: azureml:my_model_validation_data:1 # use training data as comparison reference dataset
          type: mltable
        data_context: validation
      alert_enabled: true
      metric_thresholds:
        categorical:
          pearsons_chi_squared_test: 0.02
  
  alert_notification:
    emails:
      - abc@example.com
      - def@example.com
Run the following command to create the model.az ml schedule create -f ./model-monitoring-with-collected-data.yaml
Run the following command to create the model.
az ml schedule create -f ./model-monitoring-with-collected-data.yaml
az ml schedule create -f ./model-monitoring-with-collected-data.yaml
Use a script that's similar to the following Python code to set up model monitoring. First replace the following placeholders with appropriate values:
abc@example.com
from azure.identity import InteractiveBrowserCredential
from azure.ai.ml import Input, MLClient
from azure.ai.ml.constants import (
    MonitorFeatureType,
    MonitorMetricName,
    MonitorDatasetContext
)
from azure.ai.ml.entities import (
    AlertNotification,
    DataDriftSignal,
    DataQualitySignal,
    DataDriftMetricThreshold,
    DataQualityMetricThreshold,
    NumericalDriftMetrics,
    CategoricalDriftMetrics,
    DataQualityMetricsNumerical,
    DataQualityMetricsCategorical,
    MonitorFeatureFilter,
    MonitorInputData,
    MonitoringTarget,
    MonitorDefinition,
    MonitorSchedule,
    RecurrencePattern,
    RecurrenceTrigger,
    ServerlessSparkCompute,
    ReferenceData,
    ProductionData
)

# Get a handle to the workspace.
subscription_id = "<subscription-ID>"
resource_group = "<resource-group-name>"
workspace = "<workspace-name>"
ml_client = MLClient(
   InteractiveBrowserCredential(),
   subscription_id,
   resource_group,
   workspace
)

# Specify the compute instance.
spark_compute = ServerlessSparkCompute(
    instance_type="standard_e4s_v3",
    runtime_version="3.3"
)

# Specify the target data asset (the production data asset).
production_data = ProductionData(
    input_data=Input(
        type="uri_folder",
        path="azureml:<production-data-asset-name>:1"
    ),
    data_context=MonitorDatasetContext.MODEL_INPUTS,
    pre_processing_component="azureml:<preprocessing-component-name>:1.0.0"
)

# Specify the training data to use as a reference data asset.
reference_data_training = ReferenceData(
    input_data=Input(
        type="mltable",
        path="azureml:<training-data-asset-name>:1"
    ),
    data_context=MonitorDatasetContext.TRAINING
)

# Create an advanced data drift signal.
features = MonitorFeatureFilter(top_n_feature_importance=20)
metric_thresholds = DataDriftMetricThreshold(
    numerical=NumericalDriftMetrics(
        jensen_shannon_distance=0.01
    ),
    categorical=CategoricalDriftMetrics(
        pearsons_chi_squared_test=0.02
    )
)

advanced_data_drift = DataDriftSignal(
    production_data=production_data,
    reference_data=reference_data_training,
    features=features,
    metric_thresholds=metric_thresholds,
    alert_enabled=True
)

# Create an advanced data quality signal.
features = ['feature_A', 'feature_B', 'feature_C']
metric_thresholds = DataQualityMetricThreshold(
    numerical=DataQualityMetricsNumerical(
        null_value_rate=0.01
    ),
    categorical=DataQualityMetricsCategorical(
        out_of_bounds_rate=0.02
    )
)

advanced_data_quality = DataQualitySignal(
    production_data=production_data,
    reference_data=reference_data_training,
    features=features,
    metric_thresholds=metric_thresholds,
    alert_enabled=True
)

# Put all monitoring signals in a dictionary.
monitoring_signals = {
    'data_drift_advanced': advanced_data_drift,
    'data_quality_advanced': advanced_data_quality
}

# Create an alert notification object.
alert_notification = AlertNotification(
    emails=['<email-address-1>', '<email-address-2>']
)

# Set up the monitor definition.
monitor_definition = MonitorDefinition(
    compute=spark_compute,
    monitoring_signals=monitoring_signals,
    alert_notification=alert_notification
)

# Specify the schedule frequency.
recurrence_trigger = RecurrenceTrigger(
    frequency="<frequency-unit>",
    interval=<interval>,
    schedule=RecurrencePattern(hours=<start-hour>, minutes=<start-minutes>)
)

# Create the monitoring schedule.
model_monitor = MonitorSchedule(
    name="fraud_detection_model_monitoring_advanced",
    trigger=recurrence_trigger,
    create_monitor=monitor_definition
)

# Schedule the monitoring job.
poller = ml_client.schedules.begin_create_or_update(model_monitor)
created_monitor = poller.result()
from azure.identity import InteractiveBrowserCredential
from azure.ai.ml import Input, MLClient
from azure.ai.ml.constants import (
    MonitorFeatureType,
    MonitorMetricName,
    MonitorDatasetContext
)
from azure.ai.ml.entities import (
    AlertNotification,
    DataDriftSignal,
    DataQualitySignal,
    DataDriftMetricThreshold,
    DataQualityMetricThreshold,
    NumericalDriftMetrics,
    CategoricalDriftMetrics,
    DataQualityMetricsNumerical,
    DataQualityMetricsCategorical,
    MonitorFeatureFilter,
    MonitorInputData,
    MonitoringTarget,
    MonitorDefinition,
    MonitorSchedule,
    RecurrencePattern,
    RecurrenceTrigger,
    ServerlessSparkCompute,
    ReferenceData,
    ProductionData
)

# Get a handle to the workspace.
subscription_id = "<subscription-ID>"
resource_group = "<resource-group-name>"
workspace = "<workspace-name>"
ml_client = MLClient(
   InteractiveBrowserCredential(),
   subscription_id,
   resource_group,
   workspace
)

# Specify the compute instance.
spark_compute = ServerlessSparkCompute(
    instance_type="standard_e4s_v3",
    runtime_version="3.3"
)

# Specify the target data asset (the production data asset).
production_data = ProductionData(
    input_data=Input(
        type="uri_folder",
        path="azureml:<production-data-asset-name>:1"
    ),
    data_context=MonitorDatasetContext.MODEL_INPUTS,
    pre_processing_component="azureml:<preprocessing-component-name>:1.0.0"
)

# Specify the training data to use as a reference data asset.
reference_data_training = ReferenceData(
    input_data=Input(
        type="mltable",
        path="azureml:<training-data-asset-name>:1"
    ),
    data_context=MonitorDatasetContext.TRAINING
)

# Create an advanced data drift signal.
features = MonitorFeatureFilter(top_n_feature_importance=20)
metric_thresholds = DataDriftMetricThreshold(
    numerical=NumericalDriftMetrics(
        jensen_shannon_distance=0.01
    ),
    categorical=CategoricalDriftMetrics(
        pearsons_chi_squared_test=0.02
    )
)

advanced_data_drift = DataDriftSignal(
    production_data=production_data,
    reference_data=reference_data_training,
    features=features,
    metric_thresholds=metric_thresholds,
    alert_enabled=True
)

# Create an advanced data quality signal.
features = ['feature_A', 'feature_B', 'feature_C']
metric_thresholds = DataQualityMetricThreshold(
    numerical=DataQualityMetricsNumerical(
        null_value_rate=0.01
    ),
    categorical=DataQualityMetricsCategorical(
        out_of_bounds_rate=0.02
    )
)

advanced_data_quality = DataQualitySignal(
    production_data=production_data,
    reference_data=reference_data_training,
    features=features,
    metric_thresholds=metric_thresholds,
    alert_enabled=True
)

# Put all monitoring signals in a dictionary.
monitoring_signals = {
    'data_drift_advanced': advanced_data_drift,
    'data_quality_advanced': advanced_data_quality
}

# Create an alert notification object.
alert_notification = AlertNotification(
    emails=['<email-address-1>', '<email-address-2>']
)

# Set up the monitor definition.
monitor_definition = MonitorDefinition(
    compute=spark_compute,
    monitoring_signals=monitoring_signals,
    alert_notification=alert_notification
)

# Specify the schedule frequency.
recurrence_trigger = RecurrenceTrigger(
    frequency="<frequency-unit>",
    interval=<interval>,
    schedule=RecurrencePattern(hours=<start-hour>, minutes=<start-minutes>)
)

# Create the monitoring schedule.
model_monitor = MonitorSchedule(
    name="fraud_detection_model_monitoring_advanced",
    trigger=recurrence_trigger,
    create_monitor=monitor_definition
)

# Schedule the monitoring job.
poller = ml_client.schedules.begin_create_or_update(model_monitor)
created_monitor = poller.result()
The studio currently doesn't support configuring monitoring for models that are deployed outside Azure Machine Learning. See the Azure CLI or Python SDK tabs instead.
After you use the Azure CLI or the Python SDK to configure monitoring, you can view the monitoring results in the studio. For more information about interpreting monitoring results, seeInterpret monitoring results.
Set up model monitoring with custom signals and metrics
When you use Azure Machine Learning model monitoring, you can define a custom signal and implement any metric of your choice to monitor your model. You can register your custom signal as an Azure Machine Learning component. When your model monitoring job runs on its specified schedule, it computes the metrics that are defined within your custom signal, just as it does for the data drift, prediction drift, and data quality prebuilt signals.
To set up a custom signal to use for model monitoring, you must first define the custom signal and register it as an Azure Machine Learning component. The Azure Machine Learning component must have the following input and output signatures.
Component input signature
The component input data frame should contain the following items:
Anmltablestructure that contains the processed data from the preprocessing component.
mltable
Any number of literals, each representing an implemented metric as part of the custom signal component. For example, if you implement thestd_deviationmetric, you need an input forstd_deviation_threshold. Generally, there should be one input with the name<metric-name>_thresholdper metric.
std_deviation
std_deviation_threshold
<metric-name>_threshold
production_data
std_deviation_threshold
Component output signature
The component output port should have the following signature:
signal_metrics
The component output data frame should contain four columns:group,metric_name,metric_value, andthreshold_value.
group
metric_name
metric_value
threshold_value
group
metric_name
metric_value
threshold_value
The following table shows example output from a custom signal component that computes thestd_deviationmetric:
std_deviation
To see an example of a custom signal component definition and metric computation code, seecustom_signal in the azureml-examples repo.
For instructions for registering an Azure Machine Learning component, seeRegister component in your workspace.
Azure CLI
Python SDK
Studio
After you create and register your custom signal component in Azure Machine Learning, take the following steps to set up model monitoring:
Create a monitoring definition in a YAML file that's similar to the following one. Before you use this definition, adjust the following settings and any others to meet the needs of your environment:Forcomponent_id, use a value in the formatazureml:<custom-signal-name>:1.0.0.Forpathin the input data section, use a value in the formatazureml:<production-data-asset-name>:<version>.Forpre_processing_component:If you use thedata collectorto collect your data, you can omit thepre_processing_componentproperty.If you don't use the data collector and want to use a component to preprocess production data, use a value in the formatazureml:<custom-preprocessor-name>:<custom-preprocessor-version>.Underemails, list the email addresses that you want to use for notifications.# custom-monitoring.yaml
$schema:  http://azureml/sdk-2-0/Schedule.json
name: my-custom-signal
trigger:
  type: recurrence
  frequency: day # Possible frequency values include "minute," "hour," "day," "week," and "month."
  interval: 7 # Monitoring runs every day when you use the value 1.
create_monitor:
  compute:
    instance_type: "standard_e4s_v3"
    runtime_version: "3.3"
  monitoring_signals:
    customSignal:
      type: custom
      component_id: azureml:my_custom_signal:1.0.0
      input_data:
        production_data:
          input_data:
            type: uri_folder
            path: azureml:my_production_data:1
          data_context: test
          data_window:
            lookback_window_size: P30D
            lookback_window_offset: P7D
          pre_processing_component: azureml:custom_preprocessor:1.0.0
      metric_thresholds:
        - metric_name: std_deviation
          threshold: 2
  alert_notification:
    emails:
      - abc@example.com
Create a monitoring definition in a YAML file that's similar to the following one. Before you use this definition, adjust the following settings and any others to meet the needs of your environment:
Forcomponent_id, use a value in the formatazureml:<custom-signal-name>:1.0.0.
component_id
azureml:<custom-signal-name>:1.0.0
Forpathin the input data section, use a value in the formatazureml:<production-data-asset-name>:<version>.
path
azureml:<production-data-asset-name>:<version>
Forpre_processing_component:If you use thedata collectorto collect your data, you can omit thepre_processing_componentproperty.If you don't use the data collector and want to use a component to preprocess production data, use a value in the formatazureml:<custom-preprocessor-name>:<custom-preprocessor-version>.
pre_processing_component
If you use thedata collectorto collect your data, you can omit thepre_processing_componentproperty.
pre_processing_component
If you don't use the data collector and want to use a component to preprocess production data, use a value in the formatazureml:<custom-preprocessor-name>:<custom-preprocessor-version>.
azureml:<custom-preprocessor-name>:<custom-preprocessor-version>
Underemails, list the email addresses that you want to use for notifications.
emails
# custom-monitoring.yaml
$schema:  http://azureml/sdk-2-0/Schedule.json
name: my-custom-signal
trigger:
  type: recurrence
  frequency: day # Possible frequency values include "minute," "hour," "day," "week," and "month."
  interval: 7 # Monitoring runs every day when you use the value 1.
create_monitor:
  compute:
    instance_type: "standard_e4s_v3"
    runtime_version: "3.3"
  monitoring_signals:
    customSignal:
      type: custom
      component_id: azureml:my_custom_signal:1.0.0
      input_data:
        production_data:
          input_data:
            type: uri_folder
            path: azureml:my_production_data:1
          data_context: test
          data_window:
            lookback_window_size: P30D
            lookback_window_offset: P7D
          pre_processing_component: azureml:custom_preprocessor:1.0.0
      metric_thresholds:
        - metric_name: std_deviation
          threshold: 2
  alert_notification:
    emails:
      - abc@example.com
# custom-monitoring.yaml
$schema:  http://azureml/sdk-2-0/Schedule.json
name: my-custom-signal
trigger:
  type: recurrence
  frequency: day # Possible frequency values include "minute," "hour," "day," "week," and "month."
  interval: 7 # Monitoring runs every day when you use the value 1.
create_monitor:
  compute:
    instance_type: "standard_e4s_v3"
    runtime_version: "3.3"
  monitoring_signals:
    customSignal:
      type: custom
      component_id: azureml:my_custom_signal:1.0.0
      input_data:
        production_data:
          input_data:
            type: uri_folder
            path: azureml:my_production_data:1
          data_context: test
          data_window:
            lookback_window_size: P30D
            lookback_window_offset: P7D
          pre_processing_component: azureml:custom_preprocessor:1.0.0
      metric_thresholds:
        - metric_name: std_deviation
          threshold: 2
  alert_notification:
    emails:
      - abc@example.com
Run the following command to create the model:az ml schedule create -f ./custom-monitoring.yaml
Run the following command to create the model:
az ml schedule create -f ./custom-monitoring.yaml
az ml schedule create -f ./custom-monitoring.yaml
The Python SDK currently doesn't support monitoring for custom signals. See the Azure CLI tab instead.
The studio currently doesn't support monitoring for custom signals. See the Azure CLI tab instead.
Interpret monitoring results
After you configure your model monitor and the first run finishes, you can view the results in Azure Machine Learning studio.
In the studio, underManage, selectMonitoring. In the Monitoring page, select the name of your model monitor to see its overview page. This page shows the monitoring model, endpoint, and deployment. It also provides detailed information about configured signals. The following image shows a monitoring overview page that includes data drift and data quality signals.
In the studio, underManage, selectMonitoring. In the Monitoring page, select the name of your model monitor to see its overview page. This page shows the monitoring model, endpoint, and deployment. It also provides detailed information about configured signals. The following image shows a monitoring overview page that includes data drift and data quality signals.

Look in theNotificationssection of the overview page. In this section, you can see the feature for each signal that breaches the configured threshold for its respective metric.
Look in theNotificationssection of the overview page. In this section, you can see the feature for each signal that breaches the configured threshold for its respective metric.
In theSignalssection, selectdata_driftto see detailed information about the data drift signal. On the details page, you can see the data drift metric value for each numerical and categorical feature that your monitoring configuration includes. If your monitor has more than one run, you see a trend line for each feature.
In theSignalssection, selectdata_driftto see detailed information about the data drift signal. On the details page, you can see the data drift metric value for each numerical and categorical feature that your monitoring configuration includes. If your monitor has more than one run, you see a trend line for each feature.

On the details page, select the name of an individual feature. A detailed view opens that shows the production distribution compared to the reference distribution. You can also use this view to track drift over time for the feature.
On the details page, select the name of an individual feature. A detailed view opens that shows the production distribution compared to the reference distribution. You can also use this view to track drift over time for the feature.

Return to the monitoring overview page. In theSignalssection, selectdata_qualityto view detailed information about this signal. On this page, you can see the null value rates, out-of-bounds rates, and data type error rates for each feature that you monitor.
Return to the monitoring overview page. In theSignalssection, selectdata_qualityto view detailed information about this signal. On this page, you can see the null value rates, out-of-bounds rates, and data type error rates for each feature that you monitor.

Model monitoring is a continuous process. When you use Azure Machine Learning model monitoring, you can configure multiple monitoring signals to obtain a broad view into the performance of your models in production.
Integrate Azure Machine Learning model monitoring with Event Grid
When you useEvent Grid, you can configure events that are generated by Azure Machine Learning model monitoring to trigger applications, processes, and CI/CD workflows. You can consume events through various event handlers, such as Azure Event Hubs, Azure Functions, and Azure Logic Apps. When your monitors detect drift, you can take action programmatically, such as by running a machine learning pipeline to retrain a model and redeploy it.
To integrate Azure Machine Learning model monitoring with Event Grid, take the steps in the following sections.
Create a system topic
If you don't have an Event Grid system topic to use for monitoring, create one. For instructions, seeCreate, view, and manage Event Grid system topics in the Azure portal.
Create an event subscription
In the Azure portal, go to your Azure Machine Learning workspace.
In the Azure portal, go to your Azure Machine Learning workspace.
SelectEvents, and then selectEvent Subscription.
SelectEvents, and then selectEvent Subscription.

Next toName, enter a name for your event subscription, such asMonitoringEvent.
Next toName, enter a name for your event subscription, such asMonitoringEvent.
UnderEvent Types, select onlyRun status changed.WarningSelect onlyRun status changedfor the event type. Don't selectDataset drift detected, which applies to data drift v1, not Azure Machine Learning model monitoring.
UnderEvent Types, select onlyRun status changed.
Warning
Select onlyRun status changedfor the event type. Don't selectDataset drift detected, which applies to data drift v1, not Azure Machine Learning model monitoring.
Select theFilterstab. UnderAdvanced Filters, selectAdd new filter, and then enter the following values:UnderKey, enterdata.RunTags.azureml_modelmonitor_threshold_breached.UnderOperator, selectString contains.UnderValue, enterhas failed due to one or more features violating metric thresholds.When you use this filter, events are generated when the run status of any monitor in your Azure Machine Learning workspace changes. The run status can change from completed to failed or from failed to completed.To filter at the monitoring level, selectAdd new filteragain, and then enter the following values:UnderKey, enterdata.RunTags.azureml_modelmonitor_threshold_breached.UnderOperator, selectString contains.UnderValue, enter the name of a monitor signal that you want to filter events for, such ascredit_card_fraud_monitor_data_drift. The name that you enter must match the name of your monitoring signal. Any signal that you use in filtering should have a name in the format<monitor-name>_<signal-description>that includes the monitor name and a description of the signal.
Select theFilterstab. UnderAdvanced Filters, selectAdd new filter, and then enter the following values:
UnderKey, enterdata.RunTags.azureml_modelmonitor_threshold_breached.
UnderOperator, selectString contains.
UnderValue, enterhas failed due to one or more features violating metric thresholds.

When you use this filter, events are generated when the run status of any monitor in your Azure Machine Learning workspace changes. The run status can change from completed to failed or from failed to completed.
To filter at the monitoring level, selectAdd new filteragain, and then enter the following values:
UnderKey, enterdata.RunTags.azureml_modelmonitor_threshold_breached.
UnderOperator, selectString contains.
UnderValue, enter the name of a monitor signal that you want to filter events for, such ascredit_card_fraud_monitor_data_drift. The name that you enter must match the name of your monitoring signal. Any signal that you use in filtering should have a name in the format<monitor-name>_<signal-description>that includes the monitor name and a description of the signal.
<monitor-name>_<signal-description>
Select theBasicstab. Configure the endpoint that you want to serve as your event handler, such as Event Hubs.
Select theBasicstab. Configure the endpoint that you want to serve as your event handler, such as Event Hubs.
SelectCreateto create the event subscription.
SelectCreateto create the event subscription.
View events
After you capture events, you can view them on the event handler endpoint page:

You can also view events in the Azure MonitorMetricstab:

Related content
Data collection from models in production
CLI (v2) schedule YAML schema for model monitoring (preview)
Model monitoring for generative AI applications (preview)
Feedback
Was this page helpful?
Additional resources