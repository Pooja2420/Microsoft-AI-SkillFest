Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
QuickStart: Analyze text content
Article
2025-01-23
5 contributors
In this article
Get started with the Content Safety Studio, REST API, or client SDKs to do basic text moderation. The Azure AI Content Safety service provides you with AI algorithms for flagging objectionable content. Follow these steps to try it out.
For more information on text moderation, see theHarm categories concept page. For API input limits, see theInput requirementssection of the Overview.
Caution
The sample data and code may contain offensive content. User discretion is advised.
Prerequisites
An Azure account. If you don't have one, you cancreate one for free.
AnAzure AI resource.
Setup
Follow these steps to use the Content Safetytry it outpage:
Go toAzure AI Foundryand navigate to your project/hub. Then select theSafety+ Securitytab on the left nav and select theTry it outtab.
On theTry it outpage, you can experiment with various content safety features such as text and image content, using adjustable thresholds to filter for inappropriate or harmful content.

Analyze text
Select theModerate text contentpanel.
Add text to the input field, or select sample text from the panels on the page.
SelectRun test.
The service returns all the categories that were detected, with the severity level for each: 0-Safe, 2-Low, 4-Medium, 6-High. It also returns a binaryAccepted/Rejectedresult, based on the filters you configure. Use the matrix in theConfigure filterstab to set your allowed/prohibited severity levels for each category. Then you can run the text again to see how the filter works.
View and export code
You can use theView Codefeature in either theAnalyze text contentorAnalyze image contentpages to view and copy the sample code, which includes configuration for severity filtering, blocklists, and moderation functions. You can then deploy the code on your end.

Prerequisites
An Azure subscription -Create one for free
Once you have your Azure subscription,create a Content Safety resourcein the Azure portal to get your key and endpoint. Enter a unique name for your resource, select your subscription, and select a resource group, supported region (seeRegion availability), and supported pricing tier. Then selectCreate.The resource takes a few minutes to deploy. After it finishes, Selectgo to resource. In the left pane, underResource Management, selectSubscription Key and Endpoint. The endpoint and either of the keys are used to call APIs.
The resource takes a few minutes to deploy. After it finishes, Selectgo to resource. In the left pane, underResource Management, selectSubscription Key and Endpoint. The endpoint and either of the keys are used to call APIs.
cURLinstalled
Analyze text content
The following section walks through a sample request with cURL. Paste the command below into a text editor, and make the following changes.
Replace<endpoint>with the endpoint URL associated with your resource.
<endpoint>
Replace<your_subscription_key>with one of the keys that come with your resource.
<your_subscription_key>
Optionally, replace the"text"field in the body with your own text you'd like to analyze.TipText size and granularitySeeInput requirementsfor maximum text length limitations.
"text"
Tip
Text size and granularity
SeeInput requirementsfor maximum text length limitations.
curl --location --request POST '<endpoint>/contentsafety/text:analyze?api-version=2024-09-01' \
--header 'Ocp-Apim-Subscription-Key: <your_subscription_key>' \
--header 'Content-Type: application/json' \
--data-raw '{
  "text": "I hate you",
  "categories": ["Hate", "Sexual", "SelfHarm", "Violence"],
  "blocklistNames": ["string"],
  "haltOnBlocklistHit": true,
  "outputType": "FourSeverityLevels"
}'
curl --location --request POST '<endpoint>/contentsafety/text:analyze?api-version=2024-09-01' \
--header 'Ocp-Apim-Subscription-Key: <your_subscription_key>' \
--header 'Content-Type: application/json' \
--data-raw '{
  "text": "I hate you",
  "categories": ["Hate", "Sexual", "SelfHarm", "Violence"],
  "blocklistNames": ["string"],
  "haltOnBlocklistHit": true,
  "outputType": "FourSeverityLevels"
}'
The below fields must be included in the url:
<endpoint>/contentsafety/text:analyze?api-version=2024-09-01
The parameters in the request body are defined in this table:
0-9 A-Z a-z - . _ ~
true
false
"FourSeverityLevels"
"EightSeverityLevels"
0,2,4,6
0,1,2,3,4,5,6,7
See the following sample request body:
{
  "text": "I hate you",
  "categories": ["Hate", "Sexual", "SelfHarm", "Violence"],
  "blocklistNames": ["array"],
  "haltOnBlocklistHit": false,
  "outputType": "FourSeverityLevels"
}
{
  "text": "I hate you",
  "categories": ["Hate", "Sexual", "SelfHarm", "Violence"],
  "blocklistNames": ["array"],
  "haltOnBlocklistHit": false,
  "outputType": "FourSeverityLevels"
}
Open a command prompt window, paste in the edited cURL command, and run it.
Output
You should see the text moderation results displayed as JSON data in the console output. For example:
{
  "blocklistsMatch": [
    {
      "blocklistName": "string",
      "blocklistItemId": "string",
      "blocklistItemText": "string"
    }
  ],
  "categoriesAnalysis": [
    {
      "category": "Hate",
      "severity": 2
    },
    {
      "category": "SelfHarm",
      "severity": 0
    },
    {
      "category": "Sexual",
      "severity": 0
    },
    {
      "category": "Violence",
      "severity": 0
    }
  ]
}
{
  "blocklistsMatch": [
    {
      "blocklistName": "string",
      "blocklistItemId": "string",
      "blocklistItemText": "string"
    }
  ],
  "categoriesAnalysis": [
    {
      "category": "Hate",
      "severity": 2
    },
    {
      "category": "SelfHarm",
      "severity": 0
    },
    {
      "category": "Sexual",
      "severity": 0
    },
    {
      "category": "Violence",
      "severity": 0
    }
  ]
}
The JSON fields in the output are defined here:
Reference documentation|Library source code|Package (NuGet)|Samples
Prerequisites
An Azure subscription -Create one for free
TheVisual Studio IDEwith workload .NET desktop development enabled. Or if you don't plan on using Visual Studio IDE, you need the current version of.NET Core.
Once you have your Azure subscription,create a Content Safety resourcein the Azure portal to get your key and endpoint. Enter a unique name for your resource, select your subscription, and select a resource group, supported region (seeRegion availability), and supported pricing tier. Then selectCreate.The resource takes a few minutes to deploy. After it finishes, Selectgo to resource. In the left pane, underResource Management, selectSubscription Key and Endpoint. The endpoint and either of the keys are used to call APIs.
The resource takes a few minutes to deploy. After it finishes, Selectgo to resource. In the left pane, underResource Management, selectSubscription Key and Endpoint. The endpoint and either of the keys are used to call APIs.
Set up application
Create a new C# application.
Visual Studio IDE
CLI
Open Visual Studio, and underGet startedselectCreate a new project. Set the template filters toC#/All Platforms/Console. SelectConsole App(command-line application that can run on .NET on Windows, Linux and macOS) and chooseNext. Update the project name toContentSafetyQuickstartand chooseNext. Select.NET 6.0or above, and chooseCreateto create the project.
Install the client SDK
Once you've created a new project, install the client SDK by right-clicking on the project solution in theSolution Explorerand selectingManage NuGet Packages. In the package manager that opens selectBrowse, and search forAzure.AI.ContentSafety. SelectInstall.
Azure.AI.ContentSafety
In a console window (such as cmd, PowerShell, or Bash), use thedotnet newcommand to create a new console app with the namecontent-safety-quickstart. This command creates a simple "Hello World" C# project with a single source file:Program.cs.
dotnet new
content-safety-quickstart
dotnet new console -n content-safety-quickstart
dotnet new console -n content-safety-quickstart
Change your directory to the newly created app folder. You can build the application with:
dotnet build
dotnet build
The build output should contain no warnings or errors.
...
Build succeeded.
  0 Warning(s)
  0 Error(s)
...
...
Build succeeded.
  0 Warning(s)
  0 Error(s)
...
Install the client SDK
Within the application directory, install the Computer Vision client SDK for .NET with the following command:
dotnet add package Azure.AI.ContentSafety
dotnet add package Azure.AI.ContentSafety
Create environment variables
In this example, you'll write your credentials to environment variables on the local machine running the application.
To set the environment variable for your key and endpoint, open a console window and follow the instructions for your operating system and development environment.
To set theCONTENT_SAFETY_KEYenvironment variable, replaceYOUR_CONTENT_SAFETY_KEYwith one of the keys for your resource.
CONTENT_SAFETY_KEY
YOUR_CONTENT_SAFETY_KEY
To set theCONTENT_SAFETY_ENDPOINTenvironment variable, replaceYOUR_CONTENT_SAFETY_ENDPOINTwith the endpoint for your resource.
CONTENT_SAFETY_ENDPOINT
YOUR_CONTENT_SAFETY_ENDPOINT
Important
Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, seeAPI keys with Azure Key Vault.
For more information about AI services security, seeAuthenticate requests to Azure AI services.
Windows
Linux
setx CONTENT_SAFETY_KEY 'YOUR_CONTENT_SAFETY_KEY'
setx CONTENT_SAFETY_KEY 'YOUR_CONTENT_SAFETY_KEY'
setx CONTENT_SAFETY_ENDPOINT 'YOUR_CONTENT_SAFETY_ENDPOINT'
setx CONTENT_SAFETY_ENDPOINT 'YOUR_CONTENT_SAFETY_ENDPOINT'
After you add the environment variables, you might need to restart any running programs that will read the environment variables, including the console window.
export CONTENT_SAFETY_KEY='YOUR_CONTENT_SAFETY_KEY'
export CONTENT_SAFETY_KEY='YOUR_CONTENT_SAFETY_KEY'
export CONTENT_SAFETY_ENDPOINT='YOUR_CONTENT_SAFETY_ENDPOINT'
export CONTENT_SAFETY_ENDPOINT='YOUR_CONTENT_SAFETY_ENDPOINT'
After you add the environment variables, runsource ~/.bashrcfrom your console window to make the changes effective.
source ~/.bashrc
Analyze text content
From the project directory, open theProgram.csfile that was created previously. Paste in the following code:
using System;
using Azure.AI.ContentSafety;

namespace Azure.AI.ContentSafety.Dotnet.Sample
{
  class ContentSafetySampleAnalyzeText
  {
    public static void AnalyzeText()
    {
      // retrieve the endpoint and key from the environment variables created earlier
      string endpoint = Environment.GetEnvironmentVariable("CONTENT_SAFETY_ENDPOINT");
      string key = Environment.GetEnvironmentVariable("CONTENT_SAFETY_KEY");

      ContentSafetyClient client = new ContentSafetyClient(new Uri(endpoint), new AzureKeyCredential(key));

      string text = "Your input text";

      var request = new AnalyzeTextOptions(text);

      Response<AnalyzeTextResult> response;
      try
      {
          response = client.AnalyzeText(request);
      }
      catch (RequestFailedException ex)
      {
          Console.WriteLine("Analyze text failed.\nStatus code: {0}, Error code: {1}, Error message: {2}", ex.Status, ex.ErrorCode, ex.Message);
          throw;
      }

      Console.WriteLine("\nAnalyze text succeeded:");
      Console.WriteLine("Hate severity: {0}", response.Value.CategoriesAnalysis.FirstOrDefault(a => a.Category == TextCategory.Hate)?.Severity ?? 0);
      Console.WriteLine("SelfHarm severity: {0}", response.Value.CategoriesAnalysis.FirstOrDefault(a => a.Category == TextCategory.SelfHarm)?.Severity ?? 0);
      Console.WriteLine("Sexual severity: {0}", response.Value.CategoriesAnalysis.FirstOrDefault(a => a.Category == TextCategory.Sexual)?.Severity ?? 0);
      Console.WriteLine("Violence severity: {0}", response.Value.CategoriesAnalysis.FirstOrDefault(a => a.Category == TextCategory.Violence)?.Severity ?? 0);

    }
    static void Main()
    {
        AnalyzeText();
    }
  }
}
using System;
using Azure.AI.ContentSafety;

namespace Azure.AI.ContentSafety.Dotnet.Sample
{
  class ContentSafetySampleAnalyzeText
  {
    public static void AnalyzeText()
    {
      // retrieve the endpoint and key from the environment variables created earlier
      string endpoint = Environment.GetEnvironmentVariable("CONTENT_SAFETY_ENDPOINT");
      string key = Environment.GetEnvironmentVariable("CONTENT_SAFETY_KEY");

      ContentSafetyClient client = new ContentSafetyClient(new Uri(endpoint), new AzureKeyCredential(key));

      string text = "Your input text";

      var request = new AnalyzeTextOptions(text);

      Response<AnalyzeTextResult> response;
      try
      {
          response = client.AnalyzeText(request);
      }
      catch (RequestFailedException ex)
      {
          Console.WriteLine("Analyze text failed.\nStatus code: {0}, Error code: {1}, Error message: {2}", ex.Status, ex.ErrorCode, ex.Message);
          throw;
      }

      Console.WriteLine("\nAnalyze text succeeded:");
      Console.WriteLine("Hate severity: {0}", response.Value.CategoriesAnalysis.FirstOrDefault(a => a.Category == TextCategory.Hate)?.Severity ?? 0);
      Console.WriteLine("SelfHarm severity: {0}", response.Value.CategoriesAnalysis.FirstOrDefault(a => a.Category == TextCategory.SelfHarm)?.Severity ?? 0);
      Console.WriteLine("Sexual severity: {0}", response.Value.CategoriesAnalysis.FirstOrDefault(a => a.Category == TextCategory.Sexual)?.Severity ?? 0);
      Console.WriteLine("Violence severity: {0}", response.Value.CategoriesAnalysis.FirstOrDefault(a => a.Category == TextCategory.Violence)?.Severity ?? 0);

    }
    static void Main()
    {
        AnalyzeText();
    }
  }
}
Replace"Your input text"with the text content you'd like to use.
"Your input text"
Tip
Text size and granularity
SeeInput requirementsfor maximum text length limitations.
Visual Studio IDE
CLI
Build and run the application by selectingStart Debuggingfrom theDebugmenu at the top of the IDE window (or pressF5).
Build and run the application from your application directory with these commands:
dotnet build
dotnet run
dotnet build
dotnet run
Reference documentation|Library source code|Package (PyPI)|Samples|
Prerequisites
An Azure subscription -Create one for free
Once you have your Azure subscription,create a Content Safety resourcein the Azure portal to get your key and endpoint. Enter a unique name for your resource, select your subscription, and select a resource group, supported region (seeRegion availability), and supported pricing tier. Then selectCreate.The resource takes a few minutes to deploy. After it finishes, Selectgo to resource. In the left pane, underResource Management, selectSubscription Key and Endpoint. The endpoint and either of the keys are used to call APIs.
The resource takes a few minutes to deploy. After it finishes, Selectgo to resource. In the left pane, underResource Management, selectSubscription Key and Endpoint. The endpoint and either of the keys are used to call APIs.
Python 3.xYour Python installation should includepip. You can check if you have pip installed by runningpip --versionon the command line. Get pip by installing the latest version of Python.
Your Python installation should includepip. You can check if you have pip installed by runningpip --versionon the command line. Get pip by installing the latest version of Python.
pip --version
Create environment variables
In this example, you'll write your credentials to environment variables on the local machine running the application.
To set the environment variable for your key and endpoint, open a console window and follow the instructions for your operating system and development environment.
To set theCONTENT_SAFETY_KEYenvironment variable, replaceYOUR_CONTENT_SAFETY_KEYwith one of the keys for your resource.
CONTENT_SAFETY_KEY
YOUR_CONTENT_SAFETY_KEY
To set theCONTENT_SAFETY_ENDPOINTenvironment variable, replaceYOUR_CONTENT_SAFETY_ENDPOINTwith the endpoint for your resource.
CONTENT_SAFETY_ENDPOINT
YOUR_CONTENT_SAFETY_ENDPOINT
Important
Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, seeAPI keys with Azure Key Vault.
For more information about AI services security, seeAuthenticate requests to Azure AI services.
Windows
Linux
setx CONTENT_SAFETY_KEY 'YOUR_CONTENT_SAFETY_KEY'
setx CONTENT_SAFETY_KEY 'YOUR_CONTENT_SAFETY_KEY'
setx CONTENT_SAFETY_ENDPOINT 'YOUR_CONTENT_SAFETY_ENDPOINT'
setx CONTENT_SAFETY_ENDPOINT 'YOUR_CONTENT_SAFETY_ENDPOINT'
After you add the environment variables, you might need to restart any running programs that will read the environment variables, including the console window.
export CONTENT_SAFETY_KEY='YOUR_CONTENT_SAFETY_KEY'
export CONTENT_SAFETY_KEY='YOUR_CONTENT_SAFETY_KEY'
export CONTENT_SAFETY_ENDPOINT='YOUR_CONTENT_SAFETY_ENDPOINT'
export CONTENT_SAFETY_ENDPOINT='YOUR_CONTENT_SAFETY_ENDPOINT'
After you add the environment variables, runsource ~/.bashrcfrom your console window to make the changes effective.
source ~/.bashrc
Analyze text content
The following section walks through a sample request with the Python SDK.
Open a command prompt, navigate to your project folder, and create a new file namedquickstart.py.
Open a command prompt, navigate to your project folder, and create a new file namedquickstart.py.
Run this command to install the Azure AI Content Safety library:pip install azure-ai-contentsafety
Run this command to install the Azure AI Content Safety library:
pip install azure-ai-contentsafety
pip install azure-ai-contentsafety
Copy the following code intoquickstart.py:import os
from azure.ai.contentsafety import ContentSafetyClient
from azure.core.credentials import AzureKeyCredential
from azure.core.exceptions import HttpResponseError
from azure.ai.contentsafety.models import AnalyzeTextOptions, TextCategory

def analyze_text():
    # analyze text
    key = os.environ["CONTENT_SAFETY_KEY"]
    endpoint = os.environ["CONTENT_SAFETY_ENDPOINT"]

    # Create an Azure AI Content Safety client
    client = ContentSafetyClient(endpoint, AzureKeyCredential(key))

    # Contruct request
    request = AnalyzeTextOptions(text="Your input text")

    # Analyze text
    try:
        response = client.analyze_text(request)
    except HttpResponseError as e:
        print("Analyze text failed.")
        if e.error:
            print(f"Error code: {e.error.code}")
            print(f"Error message: {e.error.message}")
            raise
        print(e)
        raise

    hate_result = next(item for item in response.categories_analysis if item.category == TextCategory.HATE)
    self_harm_result = next(item for item in response.categories_analysis if item.category == TextCategory.SELF_HARM)
    sexual_result = next(item for item in response.categories_analysis if item.category == TextCategory.SEXUAL)
    violence_result = next(item for item in response.categories_analysis if item.category == TextCategory.VIOLENCE)

    if hate_result:
        print(f"Hate severity: {hate_result.severity}")
    if self_harm_result:
        print(f"SelfHarm severity: {self_harm_result.severity}")
    if sexual_result:
        print(f"Sexual severity: {sexual_result.severity}")
    if violence_result:
        print(f"Violence severity: {violence_result.severity}")

if __name__ == "__main__":
    analyze_text()
Copy the following code intoquickstart.py:
import os
from azure.ai.contentsafety import ContentSafetyClient
from azure.core.credentials import AzureKeyCredential
from azure.core.exceptions import HttpResponseError
from azure.ai.contentsafety.models import AnalyzeTextOptions, TextCategory

def analyze_text():
    # analyze text
    key = os.environ["CONTENT_SAFETY_KEY"]
    endpoint = os.environ["CONTENT_SAFETY_ENDPOINT"]

    # Create an Azure AI Content Safety client
    client = ContentSafetyClient(endpoint, AzureKeyCredential(key))

    # Contruct request
    request = AnalyzeTextOptions(text="Your input text")

    # Analyze text
    try:
        response = client.analyze_text(request)
    except HttpResponseError as e:
        print("Analyze text failed.")
        if e.error:
            print(f"Error code: {e.error.code}")
            print(f"Error message: {e.error.message}")
            raise
        print(e)
        raise

    hate_result = next(item for item in response.categories_analysis if item.category == TextCategory.HATE)
    self_harm_result = next(item for item in response.categories_analysis if item.category == TextCategory.SELF_HARM)
    sexual_result = next(item for item in response.categories_analysis if item.category == TextCategory.SEXUAL)
    violence_result = next(item for item in response.categories_analysis if item.category == TextCategory.VIOLENCE)

    if hate_result:
        print(f"Hate severity: {hate_result.severity}")
    if self_harm_result:
        print(f"SelfHarm severity: {self_harm_result.severity}")
    if sexual_result:
        print(f"Sexual severity: {sexual_result.severity}")
    if violence_result:
        print(f"Violence severity: {violence_result.severity}")

if __name__ == "__main__":
    analyze_text()
import os
from azure.ai.contentsafety import ContentSafetyClient
from azure.core.credentials import AzureKeyCredential
from azure.core.exceptions import HttpResponseError
from azure.ai.contentsafety.models import AnalyzeTextOptions, TextCategory

def analyze_text():
    # analyze text
    key = os.environ["CONTENT_SAFETY_KEY"]
    endpoint = os.environ["CONTENT_SAFETY_ENDPOINT"]

    # Create an Azure AI Content Safety client
    client = ContentSafetyClient(endpoint, AzureKeyCredential(key))

    # Contruct request
    request = AnalyzeTextOptions(text="Your input text")

    # Analyze text
    try:
        response = client.analyze_text(request)
    except HttpResponseError as e:
        print("Analyze text failed.")
        if e.error:
            print(f"Error code: {e.error.code}")
            print(f"Error message: {e.error.message}")
            raise
        print(e)
        raise

    hate_result = next(item for item in response.categories_analysis if item.category == TextCategory.HATE)
    self_harm_result = next(item for item in response.categories_analysis if item.category == TextCategory.SELF_HARM)
    sexual_result = next(item for item in response.categories_analysis if item.category == TextCategory.SEXUAL)
    violence_result = next(item for item in response.categories_analysis if item.category == TextCategory.VIOLENCE)

    if hate_result:
        print(f"Hate severity: {hate_result.severity}")
    if self_harm_result:
        print(f"SelfHarm severity: {self_harm_result.severity}")
    if sexual_result:
        print(f"Sexual severity: {sexual_result.severity}")
    if violence_result:
        print(f"Violence severity: {violence_result.severity}")

if __name__ == "__main__":
    analyze_text()
Replace"Your input text"with the text content you'd like to use.TipText size and granularitySeeInput requirementsfor maximum text length limitations.
Replace"Your input text"with the text content you'd like to use.
"Your input text"
Tip
Text size and granularity
SeeInput requirementsfor maximum text length limitations.
Then run the application with thepythoncommand on your quickstart file.python quickstart.py
Then run the application with thepythoncommand on your quickstart file.
python
python quickstart.py
python quickstart.py
Reference documentation|Library source code|Artifact (Maven)|Samples
Prerequisites
An Azure subscription -Create one for free
The current version of theJava Development Kit (JDK)
TheGradle build tool, or another dependency manager.
Once you have your Azure subscription,create a Content Safety resourcein the Azure portal to get your key and endpoint. Enter a unique name for your resource, select your subscription, and select a resource group, supported region (seeRegion availability), and supported pricing tier. Then selectCreate.The resource takes a few minutes to deploy. After it finishes, Selectgo to resource. In the left pane, underResource Management, selectSubscription Key and Endpoint. The endpoint and either of the keys are used to call APIs.
The resource takes a few minutes to deploy. After it finishes, Selectgo to resource. In the left pane, underResource Management, selectSubscription Key and Endpoint. The endpoint and either of the keys are used to call APIs.
Set up application
Create a new Gradle project.
In a console window (such as cmd, PowerShell, or Bash), create a new directory for your app, and navigate to it.
mkdir myapp && cd myapp
mkdir myapp && cd myapp
Run thegradle initcommand from your working directory. This command will create essential build files for Gradle, includingbuild.gradle.kts, which is used at runtime to create and configure your application.
gradle init
gradle init --type basic
gradle init --type basic
When prompted to choose aDSL, selectKotlin.
From your working directory, run the following command to create a project source folder:
mkdir -p src/main/java
mkdir -p src/main/java
Navigate to the new folder and create a file calledContentSafetyQuickstart.java.
Install the client SDK
This quickstart uses the Gradle dependency manager. You can find the client library and information for other dependency managers on theMaven Central Repository.
Locatebuild.gradle.ktsand open it with your preferred IDE or text editor. Then copy in the following build configuration. This configuration defines the project as a Java application whose entry point is the classContentSafetyQuickstart. It imports the Azure AI Vision library.
plugins {
    java
    application
}
application { 
    mainClass.set("ContentSafetyQuickstart")
}
repositories {
    mavenCentral()
}
dependencies {
    implementation(group = "com.azure", name = "azure-ai-contentsafety", version = "1.0.0")
}
plugins {
    java
    application
}
application { 
    mainClass.set("ContentSafetyQuickstart")
}
repositories {
    mavenCentral()
}
dependencies {
    implementation(group = "com.azure", name = "azure-ai-contentsafety", version = "1.0.0")
}
Create environment variables
In this example, you'll write your credentials to environment variables on the local machine running the application.
To set the environment variable for your key and endpoint, open a console window and follow the instructions for your operating system and development environment.
To set theCONTENT_SAFETY_KEYenvironment variable, replaceYOUR_CONTENT_SAFETY_KEYwith one of the keys for your resource.
CONTENT_SAFETY_KEY
YOUR_CONTENT_SAFETY_KEY
To set theCONTENT_SAFETY_ENDPOINTenvironment variable, replaceYOUR_CONTENT_SAFETY_ENDPOINTwith the endpoint for your resource.
CONTENT_SAFETY_ENDPOINT
YOUR_CONTENT_SAFETY_ENDPOINT
Important
Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, seeAPI keys with Azure Key Vault.
For more information about AI services security, seeAuthenticate requests to Azure AI services.
Windows
Linux
setx CONTENT_SAFETY_KEY 'YOUR_CONTENT_SAFETY_KEY'
setx CONTENT_SAFETY_KEY 'YOUR_CONTENT_SAFETY_KEY'
setx CONTENT_SAFETY_ENDPOINT 'YOUR_CONTENT_SAFETY_ENDPOINT'
setx CONTENT_SAFETY_ENDPOINT 'YOUR_CONTENT_SAFETY_ENDPOINT'
After you add the environment variables, you might need to restart any running programs that will read the environment variables, including the console window.
export CONTENT_SAFETY_KEY='YOUR_CONTENT_SAFETY_KEY'
export CONTENT_SAFETY_KEY='YOUR_CONTENT_SAFETY_KEY'
export CONTENT_SAFETY_ENDPOINT='YOUR_CONTENT_SAFETY_ENDPOINT'
export CONTENT_SAFETY_ENDPOINT='YOUR_CONTENT_SAFETY_ENDPOINT'
After you add the environment variables, runsource ~/.bashrcfrom your console window to make the changes effective.
source ~/.bashrc
Analyze text content
OpenContentSafetyQuickstart.javain your preferred editor or IDE and paste in the following code. Replace<your text sample>with the text content you'd like to use.
<your text sample>
Tip
Text size and granularity
SeeInput requirementsfor maximum text length limitations.
import com.azure.ai.contentsafety.ContentSafetyClient;
import com.azure.ai.contentsafety.ContentSafetyClientBuilder;
import com.azure.ai.contentsafety.models.AnalyzeTextOptions;
import com.azure.ai.contentsafety.models.AnalyzeTextResult;
import com.azure.ai.contentsafety.models.TextCategoriesAnalysis;
import com.azure.core.credential.KeyCredential;
import com.azure.core.util.Configuration;


public class ContentSafetyQuickstart {
    public static void main(String[] args) {

        // get endpoint and key from environment variables
        String endpoint = System.getenv("CONTENT_SAFETY_ENDPOINT");
        String key = System.getenv("CONTENT_SAFETY_KEY");
        
        ContentSafetyClient contentSafetyClient = new ContentSafetyClientBuilder()
            .credential(new KeyCredential(key))
            .endpoint(endpoint).buildClient();

        AnalyzeTextResult response = contentSafetyClient.analyzeText(new AnalyzeTextOptions("<your text sample>"));

        for (TextCategoriesAnalysis result : response.getCategoriesAnalysis()) {
            System.out.println(result.getCategory() + " severity: " + result.getSeverity());
        }
    }
}
import com.azure.ai.contentsafety.ContentSafetyClient;
import com.azure.ai.contentsafety.ContentSafetyClientBuilder;
import com.azure.ai.contentsafety.models.AnalyzeTextOptions;
import com.azure.ai.contentsafety.models.AnalyzeTextResult;
import com.azure.ai.contentsafety.models.TextCategoriesAnalysis;
import com.azure.core.credential.KeyCredential;
import com.azure.core.util.Configuration;


public class ContentSafetyQuickstart {
    public static void main(String[] args) {

        // get endpoint and key from environment variables
        String endpoint = System.getenv("CONTENT_SAFETY_ENDPOINT");
        String key = System.getenv("CONTENT_SAFETY_KEY");
        
        ContentSafetyClient contentSafetyClient = new ContentSafetyClientBuilder()
            .credential(new KeyCredential(key))
            .endpoint(endpoint).buildClient();

        AnalyzeTextResult response = contentSafetyClient.analyzeText(new AnalyzeTextOptions("<your text sample>"));

        for (TextCategoriesAnalysis result : response.getCategoriesAnalysis()) {
            System.out.println(result.getCategory() + " severity: " + result.getSeverity());
        }
    }
}
Navigate back to the project root folder, and build the app with:
gradle build
gradle build
Then, run it with thegradle runcommand:
gradle run
gradle run
gradle run
Output
Hate severity: 0
SelfHarm severity: 0
Sexual severity: 0
Violence severity: 0
Hate severity: 0
SelfHarm severity: 0
Sexual severity: 0
Violence severity: 0
Reference documentation|Library source code|Package (npm)|Samples|
Prerequisites
An Azure subscription -Create one for free
The current version ofNode.js
Once you have your Azure subscription,create a Content Safety resourcein the Azure portal to get your key and endpoint. Enter a unique name for your resource, select your subscription, and select a resource group, supported region (seeRegion availability), and supported pricing tier. Then selectCreate.The resource takes a few minutes to deploy. After it finishes, Selectgo to resource. In the left pane, underResource Management, selectSubscription Key and Endpoint. The endpoint and either of the keys are used to call APIs.
The resource takes a few minutes to deploy. After it finishes, Selectgo to resource. In the left pane, underResource Management, selectSubscription Key and Endpoint. The endpoint and either of the keys are used to call APIs.
Set up application
Create a new Node.js application. In a console window (such as cmd, PowerShell, or Bash), create a new directory for your app, and navigate to it.
mkdir myapp && cd myapp
mkdir myapp && cd myapp
Run thenpm initcommand to create a node application with apackage.jsonfile.
npm init
package.json
npm init
npm init
Install the client SDK
Install the@azure-rest/ai-content-safetynpm package:
@azure-rest/ai-content-safety
npm install @azure-rest/ai-content-safety
npm install @azure-rest/ai-content-safety
Also install thedotenvmodule to use environment variables:
dotenv
npm install dotenv
npm install dotenv
Your app'spackage.jsonfile will be updated with the dependencies.
package.json
Create environment variables
In this example, you'll write your credentials to environment variables on the local machine running the application.
To set the environment variable for your key and endpoint, open a console window and follow the instructions for your operating system and development environment.
To set theCONTENT_SAFETY_KEYenvironment variable, replaceYOUR_CONTENT_SAFETY_KEYwith one of the keys for your resource.
CONTENT_SAFETY_KEY
YOUR_CONTENT_SAFETY_KEY
To set theCONTENT_SAFETY_ENDPOINTenvironment variable, replaceYOUR_CONTENT_SAFETY_ENDPOINTwith the endpoint for your resource.
CONTENT_SAFETY_ENDPOINT
YOUR_CONTENT_SAFETY_ENDPOINT
Important
Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, seeAPI keys with Azure Key Vault.
For more information about AI services security, seeAuthenticate requests to Azure AI services.
Windows
Linux
setx CONTENT_SAFETY_KEY 'YOUR_CONTENT_SAFETY_KEY'
setx CONTENT_SAFETY_KEY 'YOUR_CONTENT_SAFETY_KEY'
setx CONTENT_SAFETY_ENDPOINT 'YOUR_CONTENT_SAFETY_ENDPOINT'
setx CONTENT_SAFETY_ENDPOINT 'YOUR_CONTENT_SAFETY_ENDPOINT'
After you add the environment variables, you might need to restart any running programs that will read the environment variables, including the console window.
export CONTENT_SAFETY_KEY='YOUR_CONTENT_SAFETY_KEY'
export CONTENT_SAFETY_KEY='YOUR_CONTENT_SAFETY_KEY'
export CONTENT_SAFETY_ENDPOINT='YOUR_CONTENT_SAFETY_ENDPOINT'
export CONTENT_SAFETY_ENDPOINT='YOUR_CONTENT_SAFETY_ENDPOINT'
After you add the environment variables, runsource ~/.bashrcfrom your console window to make the changes effective.
source ~/.bashrc
Analyze text content
Create a new file in your directory,index.js. Open it in your preferred editor or IDE and paste in the following code. Replace<your text sample>with the text content you'd like to use.
<your text sample>
Tip
Text size and granularity
SeeInput requirementsfor maximum text length limitations.
const ContentSafetyClient = require("@azure-rest/ai-content-safety").default,
  { isUnexpected } = require("@azure-rest/ai-content-safety");
const { AzureKeyCredential } = require("@azure/core-auth");

// Load the .env file if it exists
require("dotenv").config();

async function main() {
    // get endpoint and key from environment variables
    const endpoint = process.env["CONTENT_SAFETY_ENDPOINT"];
    const key = process.env["CONTENT_SAFETY_KEY"];
    
    const credential = new AzureKeyCredential(key);
    const client = ContentSafetyClient(endpoint, credential);
    
    // replace with your own sample text string 
    const text = "<your sample text>";
    const analyzeTextOption = { text: text };
    const analyzeTextParameters = { body: analyzeTextOption };
    
    const result = await client.path("/text:analyze").post(analyzeTextParameters);
    
    if (isUnexpected(result)) {
        throw result;
    }
    
    for (let i = 0; i < result.body.categoriesAnalysis.length; i++) {
    const textCategoriesAnalysisOutput = result.body.categoriesAnalysis[i];
    console.log(
      textCategoriesAnalysisOutput.category,
      " severity: ",
      textCategoriesAnalysisOutput.severity
    );
  }
}

main().catch((err) => {
    console.error("The sample encountered an error:", err);
});
const ContentSafetyClient = require("@azure-rest/ai-content-safety").default,
  { isUnexpected } = require("@azure-rest/ai-content-safety");
const { AzureKeyCredential } = require("@azure/core-auth");

// Load the .env file if it exists
require("dotenv").config();

async function main() {
    // get endpoint and key from environment variables
    const endpoint = process.env["CONTENT_SAFETY_ENDPOINT"];
    const key = process.env["CONTENT_SAFETY_KEY"];
    
    const credential = new AzureKeyCredential(key);
    const client = ContentSafetyClient(endpoint, credential);
    
    // replace with your own sample text string 
    const text = "<your sample text>";
    const analyzeTextOption = { text: text };
    const analyzeTextParameters = { body: analyzeTextOption };
    
    const result = await client.path("/text:analyze").post(analyzeTextParameters);
    
    if (isUnexpected(result)) {
        throw result;
    }
    
    for (let i = 0; i < result.body.categoriesAnalysis.length; i++) {
    const textCategoriesAnalysisOutput = result.body.categoriesAnalysis[i];
    console.log(
      textCategoriesAnalysisOutput.category,
      " severity: ",
      textCategoriesAnalysisOutput.severity
    );
  }
}

main().catch((err) => {
    console.error("The sample encountered an error:", err);
});
Run the application with thenodecommand on your quickstart file.
node
node index.js
node index.js
Output
Hate severity:  0
SelfHarm severity:  0
Sexual severity:  0
Violence severity:  0
Hate severity:  0
SelfHarm severity:  0
Sexual severity:  0
Violence severity:  0
Clean up resources
If you want to clean up and remove an Azure AI services subscription, you can delete the resource or resource group. Deleting the resource group also deletes any other resources associated with it.
Azure portal
Azure CLI
Related content
Harm categories
Configure filters for each category and test on datasets usingContent Safety Studio, export the code and deploy.
Feedback
Was this page helpful?
Additional resources