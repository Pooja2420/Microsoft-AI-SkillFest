Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
What are compute targets in Azure Machine Learning?
Article
2025-01-16
27 contributors
In this article
Acompute targetis a designated compute resource or environment where you run your training script or host your service deployment. This location might be your local machine or a cloud-based compute resource. Using compute targets makes it easy for you to later change your compute environment without having to change your code.
Azure Machine Learning has varying support across different compute targets. In a typical model development lifecycle, you might:
Start by developing and experimenting on a small amount of data. At this stage, use your local environment, such as a local computer or cloud-based virtual machine (VM), as your compute target.
Scale up to larger data, or dodistributed trainingby using one of thesetraining compute targets.
After your model is ready, deploy it to a web hosting environment with one of thesedeployment compute targets.
The compute resources you use for your compute targets are attached to aworkspace. Compute resources other than the local machine are shared by users of the workspace.
Training compute targets
As you scale up your training on larger datasets or performdistributed training, use Azure Machine Learning compute to create a single- or multi-node cluster that autoscales each time you submit a job. You can also attach your own compute resource, although support for different scenarios might vary.
Compute targets can be reused from one training job to the next.For example, after you attach a remote VM to your workspace, you can reuse it for multiple jobs.
For machine learning pipelines, use the appropriatepipeline stepfor each compute target.
You can use any of the following resources for a training compute target for most jobs. Not all resources can be used for automated machine learning, machine learning pipelines, or designer. Azure Databricks can be used as a training resource for local runs and machine learning pipelines, but not as a remote target for other training.
Tip
The compute instance has 120GB OS disk. If you run out of disk space,use the terminalto clear at least 1-2 GB before youstop or restartthe compute instance.
Compute targets for inference
When performing inference, Azure Machine Learning creates a Docker container that hosts the model and associated resources needed to use it. This container is then used in a compute target.
The compute target you use to host your model affects the cost and availability of your deployed endpoint. Use this table to choose an appropriate compute target.
Note
When choosing a cluster SKU, first scale up and then scale out. Start with a machine that has 150% of the RAM your model requires, profile the result and find a machine that has the performance you need. Once you've learned that, increase the number of machines to fit your need for concurrent inference.
Deploy and score a machine learning model by using an online endpoint.
Deploy machine learning models to Azure.
Azure Machine Learning compute (managed)
Azure Machine Learning creates and manages the managed compute resources. This type of compute is optimized for machine learning workloads. Azure Machine Learning compute clusters,serverless compute, andcompute instancesare the only managed computes.
There's no need to create serverless compute. You can create Azure Machine Learning compute instances or compute clusters from:
Azure Machine Learning studio
The Python SDK and the Azure CLI:Compute instanceCompute cluster
Compute instance
Compute cluster
An Azure Resource Manager template. For an example template, seeCreate an Azure Machine Learning compute cluster.
Note
Instead of creating a compute cluster, useserverless computeto offload compute lifecycle management to Azure Machine Learning.
When created, these compute resources are automatically part of your workspace, unlike other kinds of compute targets.
Note
To avoid charges when the compute is idle:
For a computecluster, make sure the minimum number of nodes is set to 0, or useserverless compute.
For a computeinstance,enable idle shutdown. While stopping the compute instance stops the billing for compute hours, you'll still be billed for disk, public IP, and standard load balancer.
Supported VM series and sizes
Important
If your compute instance or compute clusters are based on any of these series, recreate with another VM size.
These series retired on August 31, 2023:
Azure NC-series
Azure NCv2-series
Azure ND-series
Azure NV- and NV_Promo series
These series retired on August 31, 2024:
Azure Av1-series
Azure HB-series
When you select a node size for a managed compute resource in Azure Machine Learning, you can choose from among select VM sizes available in Azure. Azure offers a range of sizes for Linux and Windows for different workloads. To learn more, seeVM types and sizes.
There are a few exceptions and limitations to choosing a VM size:
Some VM series aren't supported in Azure Machine Learning.
Some VM series, such as GPUs and other special SKUs, might not initially appear in your list of available VMs.  But you can still use them, once you request a quota change. For more information about requesting quotas, seeRequest quota and limit increases.
See the following table to learn more about supported series.
While Azure Machine Learning supports these VM series, they might not be available in all Azure regions. To check whether VM series are available, seeProducts available by region.
Note
Azure Machine Learning doesn't support all VM sizes that Azure Compute supports. To list the available VM sizes, use the following method:
REST API
Note
Azure Machine Learning doesn't support all VM sizes that Azure Compute supports. To list the available VM sizes supported by specific compute VM types, use one of the following methods:
REST API
TheAzure CLI extension 2.0 for machine learningcommand,az ml compute list-sizes.
If you use the GPU-enabled compute targets, it's important to ensure that the correct CUDA drivers are installed in the training environment. Use the following table to determine the correct CUDA version to use:
In addition to ensuring the CUDA version and hardware are compatible, also ensure that the CUDA version is compatible with the version of the machine learning framework you're using:
For PyTorch, you can check the compatibility by visitingPyTorch's previous versions page.
For TensorFlow, you can check the compatibility by visitingTensorFlow's build from source page.
Compute isolation
Azure Machine Learning compute offers VM sizes that are isolated to a specific hardware type and dedicated to a single customer. Isolated VM sizes are best suited for workloads that require a high degree of isolation from other customers' workloads for reasons that include meeting compliance and regulatory requirements. Utilizing an isolated size guarantees that your VM is the only one running on that specific server instance.
The current isolated VM offerings include:
Standard_M128ms
Standard_F72s_v2
Standard_NC24s_v3
Standard_NC24rs_v3 (RDMA capable)
To learn more about isolation, seeIsolation in the Azure public cloud.
Unmanaged compute
Azure Machine Learning doesn't manage anunmanagedcompute target. You create this type of compute target outside Azure Machine Learning and then attach it to your workspace. Unmanaged compute resources can require extra steps for you to maintain or to improve performance for machine learning workloads.
Azure Machine Learning supports the following unmanaged compute types:
Remote virtual machines
Azure HDInsight
Azure Databricks
Azure Data Lake Analytics
Azure Kubernetes Service
Azure Synapse Spark pool(deprecated)
Kubernetes
For more information, seeManage compute resources.
Related content
Deploy and score a machine learning model by using an online endpoint
Deploy machine learning models to Azure
Feedback
Was this page helpful?
Additional resources