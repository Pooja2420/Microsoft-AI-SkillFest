Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Create and manage data assets
Article
2024-08-28
18 contributors
In this article
APPLIES TO:Azure CLI ml extension v2 (current)Python SDK azure-ai-ml v2 (current)
This article shows how to create and manage data assets in Azure Machine Learning.
Data assets can help when you need:
Versioning:Data assets support data versioning.
Reproducibility:Once you create a data asset version, it isimmutable. It cannot be modified or deleted. Therefore, training jobs or pipelines that consume the data asset can be reproduced.
Auditability:Because the data asset version is immutable, you can track the asset versions, who updated a version, and when the version updates occurred.
Lineage:For any given data asset, you can view which jobs or pipelines consume the data.
Ease-of-use:An Azure machine learning data asset resembles web browser bookmarks (favorites). Instead of remembering long storage paths (URIs) thatreferenceyour frequently-used data on Azure Storage, you can create a data assetversionand then access that version of the asset with a friendly name (for example:azureml:<my_data_asset_name>:<version>).
azureml:<my_data_asset_name>:<version>
Tip
To access your data in an interactive session (for example, a notebook) or a job, you arenotrequired to first create a data asset. You can use Datastore URIs to access the data. Datastore URIs offer a simple way to access data to get started with Azure machine learning.
Prerequisites
To create and work with data assets, you need:
An Azure subscription. If you don't have one, create a free account before you begin. Try thefree or paid version of Azure Machine Learning.
An Azure subscription. If you don't have one, create a free account before you begin. Try thefree or paid version of Azure Machine Learning.
An Azure Machine Learning workspace.Create workspace resources.
An Azure Machine Learning workspace.Create workspace resources.
TheAzure Machine Learning CLI/SDK installed.
TheAzure Machine Learning CLI/SDK installed.
Create data assets
When you create your data asset, you need to set the data asset type. Azure Machine Learning supports three data asset types:
uri_file
uri_folder
mltable
Note
Only use embedded newlines in csv files if you register the data as an MLTable. Embedded newlines in csv files might cause misaligned field values when you read the data. MLTable has thesupport_multi_lineparameteravailable in theread_delimitedtransformation, to interpret quoted line breaks as one record.
support_multi_line
read_delimited
When you consume the data asset in an Azure Machine Learning job, you can eithermountordownloadthe asset to the compute node(s). For more information, please visitModes.
Also, you must specify apathparameter that points to the data asset location. Supported paths include:
path
./home/username/data/my_data
azureml://datastores/<data_store_name>/paths/<path>
https://raw.githubusercontent.com/pandas-dev/pandas/main/doc/data/titanic.csv
wasbs://<containername>@<accountname>.blob.core.windows.net/<path_to_data>/
abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>
adl://<accountname>.azuredatalakestore.net/<path_to_data>/
Note
When you create a data asset from a local path, it will automatically upload to the default Azure Machine Learning cloud datastore.
Create a data asset: File type
A data asset of a File (uri_file) type points to asingle fileon storage (for example, a CSV file). You can create a file typed data asset with:
uri_file
Azure CLI
Python SDK
Studio
Create a YAML file, and copy-and-paste the following code snippet. Be sure to update the<>placeholders with the
<>
name of your data asset
the version
description
path to a single file on a supported location
$schema: https://azuremlschemas.azureedge.net/latest/data.schema.json

# Supported paths include:
# local: './<path>/<file>' (this will be automatically uploaded to cloud storage)
# blob:  'wasbs://<container_name>@<account_name>.blob.core.windows.net/<path>/<file>'
# ADLS gen2: 'abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>/<file>'
# Datastore: 'azureml://datastores/<data_store_name>/paths/<path>/<file>'

type: uri_file
name: <NAME OF DATA ASSET>
version: <VERSION>
description: <DESCRIPTION>
path: <SUPPORTED PATH>
$schema: https://azuremlschemas.azureedge.net/latest/data.schema.json

# Supported paths include:
# local: './<path>/<file>' (this will be automatically uploaded to cloud storage)
# blob:  'wasbs://<container_name>@<account_name>.blob.core.windows.net/<path>/<file>'
# ADLS gen2: 'abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>/<file>'
# Datastore: 'azureml://datastores/<data_store_name>/paths/<path>/<file>'

type: uri_file
name: <NAME OF DATA ASSET>
version: <VERSION>
description: <DESCRIPTION>
path: <SUPPORTED PATH>
Next, execute the following command in the CLI. Be sure to update the<filename>placeholder to the YAML filename.
<filename>
az ml data create -f <filename>.yml
az ml data create -f <filename>.yml
To create a File type data asset, use this code snippet, and update the<>placeholders with your information.
<>
from azure.ai.ml import MLClient
from azure.ai.ml.entities import Data
from azure.ai.ml.constants import AssetTypes
from azure.identity import DefaultAzureCredential

# Connect to the AzureML workspace
subscription_id = "<SUBSCRIPTION_ID>"
resource_group = "<RESOURCE_GROUP>"
workspace = "<AML_WORKSPACE_NAME>"

ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)

# Set the version number of the data asset (for example: '1')
VERSION = "<VERSION>"

# Set the path, supported paths include:
# local: './<path>/<file>' (this will be automatically uploaded to cloud storage)
# blob:  'wasbs://<container_name>@<account_name>.blob.core.windows.net/<path>/<file>'
# ADLS gen2: 'abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>/<file>'
# Datastore: 'azureml://datastores/<data_store_name>/paths/<path>/<file>'
path = "<SUPPORTED PATH>"

# Define the Data asset object
my_data = Data(
    path=path,
    type=AssetTypes.URI_FILE,
    description="<ADD A DESCRIPTION HERE>",
    name="<NAME OF DATA ASSET>",
    version=VERSION,
)

# Create the data asset in the workspace
ml_client.data.create_or_update(my_data)
from azure.ai.ml import MLClient
from azure.ai.ml.entities import Data
from azure.ai.ml.constants import AssetTypes
from azure.identity import DefaultAzureCredential

# Connect to the AzureML workspace
subscription_id = "<SUBSCRIPTION_ID>"
resource_group = "<RESOURCE_GROUP>"
workspace = "<AML_WORKSPACE_NAME>"

ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)

# Set the version number of the data asset (for example: '1')
VERSION = "<VERSION>"

# Set the path, supported paths include:
# local: './<path>/<file>' (this will be automatically uploaded to cloud storage)
# blob:  'wasbs://<container_name>@<account_name>.blob.core.windows.net/<path>/<file>'
# ADLS gen2: 'abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>/<file>'
# Datastore: 'azureml://datastores/<data_store_name>/paths/<path>/<file>'
path = "<SUPPORTED PATH>"

# Define the Data asset object
my_data = Data(
    path=path,
    type=AssetTypes.URI_FILE,
    description="<ADD A DESCRIPTION HERE>",
    name="<NAME OF DATA ASSET>",
    version=VERSION,
)

# Create the data asset in the workspace
ml_client.data.create_or_update(my_data)
To create a File type data asset in the Azure Machine Learning studio:
Navigate toAzure Machine Learning studio
Navigate toAzure Machine Learning studio
UnderAssetsin the left navigation, selectData. On the Data assets tab, selectCreate
UnderAssetsin the left navigation, selectData. On the Data assets tab, selectCreate
Give your data asset a name and an optional description. Then, select theFile (uri_file)option under Type.
Give your data asset a name and an optional description. Then, select theFile (uri_file)option under Type.
You have multiple options for your data source. If you already have the path to the file you want to upload, chooseFrom a URI. For a file already stored in Azure, chooseFrom Azure storage. To upload your file from your local drive, chooseFrom local files.
You have multiple options for your data source. If you already have the path to the file you want to upload, chooseFrom a URI. For a file already stored in Azure, chooseFrom Azure storage. To upload your file from your local drive, chooseFrom local files.
Follow the steps; once you reach the Review step, selectCreateon the last page
Follow the steps; once you reach the Review step, selectCreateon the last page
Create a data asset: Folder type
A Folder (uri_folder) type data asset points to afolderin a storage resource - for example, a folder containing several subfolders of images. You can create a folder typed data asset with:
uri_folder
Azure CLI
Python SDK
Studio
Copy-and-paste the following code into a new YAML file. Be sure to update the<>placeholders with the
<>
Name of your data asset
The version
Description
Path to a folder on a supported location
$schema: https://azuremlschemas.azureedge.net/latest/data.schema.json

# Supported paths include:
# local: './<path>/<folder>' (this will be automatically uploaded to cloud storage)
# blob:  'wasbs://<container_name>@<account_name>.blob.core.windows.net/<path>/<folder>'
# ADLS gen2: 'abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>/<folder>'
# Datastore: 'azureml://datastores/<data_store_name>/paths/<path>/<folder>'

type: uri_folder
name: <NAME OF DATA ASSET>
version: <VERSION>
description: <DESCRIPTION>
path: <SUPPORTED PATH>
$schema: https://azuremlschemas.azureedge.net/latest/data.schema.json

# Supported paths include:
# local: './<path>/<folder>' (this will be automatically uploaded to cloud storage)
# blob:  'wasbs://<container_name>@<account_name>.blob.core.windows.net/<path>/<folder>'
# ADLS gen2: 'abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>/<folder>'
# Datastore: 'azureml://datastores/<data_store_name>/paths/<path>/<folder>'

type: uri_folder
name: <NAME OF DATA ASSET>
version: <VERSION>
description: <DESCRIPTION>
path: <SUPPORTED PATH>
Next, execute the following command in the CLI. Be sure to update the<filename>placeholder to the YAML filename.
<filename>
az ml data create -f <filename>.yml
az ml data create -f <filename>.yml
To create a Folder type data asset, use the following code and update the<>placeholders with your information.
<>
from azure.ai.ml import MLClient
from azure.ai.ml.entities import Data
from azure.ai.ml.constants import AssetTypes
from azure.identity import DefaultAzureCredential

# Connect to the AzureML workspace
subscription_id = "<SUBSCRIPTION_ID>"
resource_group = "<RESOURCE_GROUP>"
workspace = "<AML_WORKSPACE_NAME>"

ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)

# Set the version number of the data asset (for example: '1')
VERSION = "<VERSION>"

# Set the path, supported paths include:
# local: './<path>/<folder>' (this will be automatically uploaded to cloud storage)
# blob:  'wasbs://<container_name>@<account_name>.blob.core.windows.net/<path>/<folder>'
# ADLS gen2: 'abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>/<folder>'
# Datastore: 'azureml://datastores/<data_store_name>/paths/<path>/<folder>'
path = "<SUPPORTED PATH>"

# Define the Data asset object
my_data = Data(
    path=path,
    type=AssetTypes.URI_FOLDER,
    description="<ADD A DESCRIPTION HERE>",
    name="<NAME OF DATA ASSET>",
    version=VERSION,
)

# Create the data asset in the workspace
ml_client.data.create_or_update(my_data)
from azure.ai.ml import MLClient
from azure.ai.ml.entities import Data
from azure.ai.ml.constants import AssetTypes
from azure.identity import DefaultAzureCredential

# Connect to the AzureML workspace
subscription_id = "<SUBSCRIPTION_ID>"
resource_group = "<RESOURCE_GROUP>"
workspace = "<AML_WORKSPACE_NAME>"

ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)

# Set the version number of the data asset (for example: '1')
VERSION = "<VERSION>"

# Set the path, supported paths include:
# local: './<path>/<folder>' (this will be automatically uploaded to cloud storage)
# blob:  'wasbs://<container_name>@<account_name>.blob.core.windows.net/<path>/<folder>'
# ADLS gen2: 'abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>/<folder>'
# Datastore: 'azureml://datastores/<data_store_name>/paths/<path>/<folder>'
path = "<SUPPORTED PATH>"

# Define the Data asset object
my_data = Data(
    path=path,
    type=AssetTypes.URI_FOLDER,
    description="<ADD A DESCRIPTION HERE>",
    name="<NAME OF DATA ASSET>",
    version=VERSION,
)

# Create the data asset in the workspace
ml_client.data.create_or_update(my_data)
To create a Folder typed data asset in the Azure Machine Learning studio:
Navigate toAzure Machine Learning studio
Navigate toAzure Machine Learning studio
UnderAssetsin the left navigation, selectData. On the Data assets tab, selectCreate
UnderAssetsin the left navigation, selectData. On the Data assets tab, selectCreate
Give your data asset a name and optional description. Next, select theFolder (uri_folder)option under Type, if it isn't already selected.
Give your data asset a name and optional description. Next, select theFolder (uri_folder)option under Type, if it isn't already selected.
You have multiple options for your data source. If you already have the path to the folder you want to upload, chooseFrom a URI. For a folder already stored in Azure, chooseFrom Azure storage. To upload a folder from your local drive, chooseFrom local files.
You have multiple options for your data source. If you already have the path to the folder you want to upload, chooseFrom a URI. For a folder already stored in Azure, chooseFrom Azure storage. To upload a folder from your local drive, chooseFrom local files.
Follow the steps, and once you reach the Review step, selectCreateon the last page.
Follow the steps, and once you reach the Review step, selectCreateon the last page.
Create a data asset: Table type
Azure Machine Learning Tables (MLTable) have rich functionality, described in more detail atWorking with tables in Azure Machine Learning. Instead of repeating that documentation here, read this example that describes how to create a Table-typed data asset, with Titanic data located on a publicly available Azure Blob Storage account.
MLTable
Azure CLI
Python SDK
Studio
First, create a new directory called data, and create a file calledMLTable:
mkdir data
touch MLTable
mkdir data
touch MLTable
Next, copy-and-paste the following YAML into theMLTablefile you created in the previous step:
Caution
Donotrename theMLTablefile toMLTable.yamlorMLTable.yml. Azure machine learning expects anMLTablefile.
MLTable
MLTable.yaml
MLTable.yml
MLTable
paths:
- file: wasbs://data@azuremlexampledata.blob.core.windows.net/titanic.csv
transformations:
- read_delimited:
    delimiter: ','
    empty_as_string: false
    encoding: utf8
    header: all_files_same_headers
    include_path_column: false
    infer_column_types: true
    partition_size: 20971520
    path_column: Path
    support_multi_line: false
- filter: col('Age') > 0
- drop_columns:
  - PassengerId
- convert_column_types:
  - column_type:
      boolean:
        false_values:
        - 'False'
        - 'false'
        - '0'
        mismatch_as: error
        true_values:
        - 'True'
        - 'true'
        - '1'
    columns: Survived
type: mltable
paths:
- file: wasbs://data@azuremlexampledata.blob.core.windows.net/titanic.csv
transformations:
- read_delimited:
    delimiter: ','
    empty_as_string: false
    encoding: utf8
    header: all_files_same_headers
    include_path_column: false
    infer_column_types: true
    partition_size: 20971520
    path_column: Path
    support_multi_line: false
- filter: col('Age') > 0
- drop_columns:
  - PassengerId
- convert_column_types:
  - column_type:
      boolean:
        false_values:
        - 'False'
        - 'false'
        - '0'
        mismatch_as: error
        true_values:
        - 'True'
        - 'true'
        - '1'
    columns: Survived
type: mltable
Execute the following command in the CLI. Be sure to update the<>placeholders with the data asset name and version values.
<>
az ml data create --path ./data --name <DATA ASSET NAME> --version <VERSION> --type mltable
az ml data create --path ./data --name <DATA ASSET NAME> --version <VERSION> --type mltable
Important
Thepathshould be afolderthat contains a validMLTablefile.
path
MLTable
Use this code snippet to create a Table (mltable) data asset type. Be sure to update the<>placeholders with your information.
mltable
<>
import mltable
from mltable import MLTableHeaders, MLTableFileEncoding, DataType
from azure.ai.ml import MLClient
from azure.ai.ml.entities import Data
from azure.ai.ml.constants import AssetTypes
from azure.identity import DefaultAzureCredential

# create paths to the data files
paths = [{"file": "wasbs://data@azuremlexampledata.blob.core.windows.net/titanic.csv"}]

# create an MLTable from the data files
tbl = mltable.from_delimited_files(
    paths=paths,
    delimiter=",",
    header=MLTableHeaders.all_files_same_headers,
    infer_column_types=True,
    include_path_column=False,
    encoding=MLTableFileEncoding.utf8,
)

# filter out rows undefined ages
tbl = tbl.filter("col('Age') > 0")

# drop PassengerId
tbl = tbl.drop_columns(["PassengerId"])

# ensure survived column is treated as boolean
data_types = {
    "Survived": DataType.to_bool(
        true_values=["True", "true", "1"], false_values=["False", "false", "0"]
    )
}
tbl = tbl.convert_column_types(data_types)

# show the first few records
print(tbl.show())

# save the data loading steps in an MLTable file
mltable_folder = "./titanic"
tbl.save(mltable_folder)

# Connect to the AzureML workspace
subscription_id = "<SUBSCRIPTION_ID>"
resource_group = "<RESOURCE_GROUP>"
workspace = "<AML_WORKSPACE_NAME>"

ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)

# Define the Data asset object
my_data = Data(
    path=mltable_folder,
    type=AssetTypes.MLTABLE,
    description="<ADD A DESCRIPTION HERE>",
    name="<NAME OF DATA ASSET>",
    version="<SET VERSION HERE>",
)

# Create the data asset in the workspace
ml_client.data.create_or_update(my_data)
import mltable
from mltable import MLTableHeaders, MLTableFileEncoding, DataType
from azure.ai.ml import MLClient
from azure.ai.ml.entities import Data
from azure.ai.ml.constants import AssetTypes
from azure.identity import DefaultAzureCredential

# create paths to the data files
paths = [{"file": "wasbs://data@azuremlexampledata.blob.core.windows.net/titanic.csv"}]

# create an MLTable from the data files
tbl = mltable.from_delimited_files(
    paths=paths,
    delimiter=",",
    header=MLTableHeaders.all_files_same_headers,
    infer_column_types=True,
    include_path_column=False,
    encoding=MLTableFileEncoding.utf8,
)

# filter out rows undefined ages
tbl = tbl.filter("col('Age') > 0")

# drop PassengerId
tbl = tbl.drop_columns(["PassengerId"])

# ensure survived column is treated as boolean
data_types = {
    "Survived": DataType.to_bool(
        true_values=["True", "true", "1"], false_values=["False", "false", "0"]
    )
}
tbl = tbl.convert_column_types(data_types)

# show the first few records
print(tbl.show())

# save the data loading steps in an MLTable file
mltable_folder = "./titanic"
tbl.save(mltable_folder)

# Connect to the AzureML workspace
subscription_id = "<SUBSCRIPTION_ID>"
resource_group = "<RESOURCE_GROUP>"
workspace = "<AML_WORKSPACE_NAME>"

ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)

# Define the Data asset object
my_data = Data(
    path=mltable_folder,
    type=AssetTypes.MLTABLE,
    description="<ADD A DESCRIPTION HERE>",
    name="<NAME OF DATA ASSET>",
    version="<SET VERSION HERE>",
)

# Create the data asset in the workspace
ml_client.data.create_or_update(my_data)
Important
At this time, the Studio UI has limited functionality for the creation of Table (MLTable) typed assets. We recommend that you use the Python SDK to author and create Table (MLTable) typed data assets.
MLTable
MLTable
Creating data assets from job outputs
You can create a data asset from an Azure Machine Learning job. To do this, set thenameparameter in the output. In this example, you submit a job that copies data from a public blob store to your default Azure Machine Learning Datastore and creates a data asset calledjob_output_titanic_asset.
name
job_output_titanic_asset
Azure CLI
Python SDK
Studio
Create a job specification YAML file (<file-name>.yml):
<file-name>.yml
$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json

# path: Set the URI path for the data. Supported paths include
# local: `./<path>
# Blob: wasbs://<container_name>@<account_name>.blob.core.windows.net/<path>
# ADLS: abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>
# Datastore: azureml://datastores/<data_store_name>/paths/<path>
# Data Asset: azureml:<my_data>:<version>

# type: What type of data are you pointing to?
# uri_file (a specific file)
# uri_folder (a folder)
# mltable (a table)

# mode: Set INPUT mode:
# ro_mount (read-only mount)
# download (download from storage to node)
# mode: Set the OUTPUT mode
# rw_mount (read-write mount)
# upload (upload data from node to storage)

type: command
command: cp ${{inputs.input_data}} ${{outputs.output_data}}
compute: azureml:cpu-cluster
environment: azureml://registries/azureml/environments/sklearn-1.1/versions/4
inputs:
  input_data:
    mode: ro_mount
    path: wasbs://data@azuremlexampledata.blob.core.windows.net/titanic.csv
    type: uri_file
outputs:
  output_data:
    mode: rw_mount
    path: azureml://datastores/workspaceblobstore/paths/quickstart-output/titanic.csv
    type: uri_file
    name: job_output_titanic_asset
$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json

# path: Set the URI path for the data. Supported paths include
# local: `./<path>
# Blob: wasbs://<container_name>@<account_name>.blob.core.windows.net/<path>
# ADLS: abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>
# Datastore: azureml://datastores/<data_store_name>/paths/<path>
# Data Asset: azureml:<my_data>:<version>

# type: What type of data are you pointing to?
# uri_file (a specific file)
# uri_folder (a folder)
# mltable (a table)

# mode: Set INPUT mode:
# ro_mount (read-only mount)
# download (download from storage to node)
# mode: Set the OUTPUT mode
# rw_mount (read-write mount)
# upload (upload data from node to storage)

type: command
command: cp ${{inputs.input_data}} ${{outputs.output_data}}
compute: azureml:cpu-cluster
environment: azureml://registries/azureml/environments/sklearn-1.1/versions/4
inputs:
  input_data:
    mode: ro_mount
    path: wasbs://data@azuremlexampledata.blob.core.windows.net/titanic.csv
    type: uri_file
outputs:
  output_data:
    mode: rw_mount
    path: azureml://datastores/workspaceblobstore/paths/quickstart-output/titanic.csv
    type: uri_file
    name: job_output_titanic_asset
Next, submit the job using the CLI:
az ml job create --file <file-name>.yml
az ml job create --file <file-name>.yml
from azure.ai.ml import command, Input, Output, MLClient
from azure.ai.ml.constants import AssetTypes, InputOutputModes
from azure.identity import DefaultAzureCredential

# Set your subscription, resource group and workspace name:
subscription_id = "<SUBSCRIPTION_ID>"
resource_group = "<RESOURCE_GROUP>"
workspace = "<AML_WORKSPACE_NAME>"

# connect to the AzureML workspace
ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)

# ==============================================================
# Set the input and output URI paths for the data. Supported paths include:
# local: `./<path>
# Blob: wasbs://<container_name>@<account_name>.blob.core.windows.net/<path>
# ADLS: abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>
# Datastore: azureml://datastores/<data_store_name>/paths/<path>
# Data Asset: azureml:<my_data>:<version>
# As an example, we set the input path to a file on a public blob container
# As an example, we set the output path to a folder in the default datastore
# ==============================================================
input_path = "wasbs://data@azuremlexampledata.blob.core.windows.net/titanic.csv"
output_path = "azureml://datastores/workspaceblobstore/paths/quickstart-output/titanic.csv"

# ==============================================================
# What type of data are you pointing to?
# AssetTypes.URI_FILE (a specific file)
# AssetTypes.URI_FOLDER (a folder)
# AssetTypes.MLTABLE (a table)
# The path we set above is a specific file
# ==============================================================
data_type = AssetTypes.URI_FILE

# ==============================================================
# Set the input mode. The most commonly-used modes:
# InputOutputModes.RO_MOUNT
# InputOutputModes.DOWNLOAD
# Set the mode to Read Only (RO) to mount the data
# ==============================================================
input_mode = InputOutputModes.RO_MOUNT

# ==============================================================
# Set the output mode. The most commonly-used modes:
# InputOutputModes.RW_MOUNT
# InputOutputModes.UPLOAD
# Set the mode to Read Write (RW) to mount the data
# ==============================================================
output_mode = InputOutputModes.RW_MOUNT

# ==============================================================
# Set a data asset name for the output
# ==============================================================
data_asset_name = "job_output_titanic_asset"

# Set the input and output for the job:
inputs = {
    "input_data": Input(type=data_type, path=input_path, mode=input_mode)
}

outputs = {
    "output_data": Output(type=data_type, path=output_path, mode=output_mode, name = data_asset_name)
}

# This command job copies the data to your default Datastore
job = command(
    command="cp ${{inputs.input_data}} ${{outputs.output_data}}",
    inputs=inputs,
    outputs=outputs,
    environment="azureml://registries/azureml/environments/sklearn-1.1/versions/4",
    compute="cpu-cluster",
)

# Submit the command
ml_client.jobs.create_or_update(job)
from azure.ai.ml import command, Input, Output, MLClient
from azure.ai.ml.constants import AssetTypes, InputOutputModes
from azure.identity import DefaultAzureCredential

# Set your subscription, resource group and workspace name:
subscription_id = "<SUBSCRIPTION_ID>"
resource_group = "<RESOURCE_GROUP>"
workspace = "<AML_WORKSPACE_NAME>"

# connect to the AzureML workspace
ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)

# ==============================================================
# Set the input and output URI paths for the data. Supported paths include:
# local: `./<path>
# Blob: wasbs://<container_name>@<account_name>.blob.core.windows.net/<path>
# ADLS: abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>
# Datastore: azureml://datastores/<data_store_name>/paths/<path>
# Data Asset: azureml:<my_data>:<version>
# As an example, we set the input path to a file on a public blob container
# As an example, we set the output path to a folder in the default datastore
# ==============================================================
input_path = "wasbs://data@azuremlexampledata.blob.core.windows.net/titanic.csv"
output_path = "azureml://datastores/workspaceblobstore/paths/quickstart-output/titanic.csv"

# ==============================================================
# What type of data are you pointing to?
# AssetTypes.URI_FILE (a specific file)
# AssetTypes.URI_FOLDER (a folder)
# AssetTypes.MLTABLE (a table)
# The path we set above is a specific file
# ==============================================================
data_type = AssetTypes.URI_FILE

# ==============================================================
# Set the input mode. The most commonly-used modes:
# InputOutputModes.RO_MOUNT
# InputOutputModes.DOWNLOAD
# Set the mode to Read Only (RO) to mount the data
# ==============================================================
input_mode = InputOutputModes.RO_MOUNT

# ==============================================================
# Set the output mode. The most commonly-used modes:
# InputOutputModes.RW_MOUNT
# InputOutputModes.UPLOAD
# Set the mode to Read Write (RW) to mount the data
# ==============================================================
output_mode = InputOutputModes.RW_MOUNT

# ==============================================================
# Set a data asset name for the output
# ==============================================================
data_asset_name = "job_output_titanic_asset"

# Set the input and output for the job:
inputs = {
    "input_data": Input(type=data_type, path=input_path, mode=input_mode)
}

outputs = {
    "output_data": Output(type=data_type, path=output_path, mode=output_mode, name = data_asset_name)
}

# This command job copies the data to your default Datastore
job = command(
    command="cp ${{inputs.input_data}} ${{outputs.output_data}}",
    inputs=inputs,
    outputs=outputs,
    environment="azureml://registries/azureml/environments/sklearn-1.1/versions/4",
    compute="cpu-cluster",
)

# Submit the command
ml_client.jobs.create_or_update(job)
Not available.
Manage data assets
Delete a data asset
Important
By design, data asset deletion is not supported.
If Azure machine learning allowed data asset deletion, it would have the following adverse and negative effects:
Production jobsthat consume data assets that were later deleted would fail.
It would become more difficult toreproducean ML experiment.
Joblineagewould break, because it would become impossible to view the deleted data asset version.
You would not be able totrack and auditcorrectly, since versions could be missing.
Therefore, theimmutabilityof data assets provides a level of protection when working in a team creating production workloads.
For a mistakenly created data asset - for example, with an incorrect name, type or path - Azure Machine Learning offers solutions to handle the situation without the negative consequences of deletion:
Archive a data asset
Archiving a data asset hides it by default from both list queries (for example, in the CLIaz ml data list) and the data asset listing in the Studio UI. You can still continue to reference and use an archived data asset in your workflows. You can archive either:
az ml data list
All versionsof the data asset under a given name
or
A specific data asset version
To archiveall versionsof the data asset under a given name, use:
Azure CLI
Python SDK
Studio
Execute the following command. Be sure to update the<>placeholders with your information.
<>
az ml data archive --name <NAME OF DATA ASSET>
az ml data archive --name <NAME OF DATA ASSET>
from azure.ai.ml import MLClient
from azure.identity import DefaultAzureCredential

# Connect to the AzureML workspace
subscription_id = "<SUBSCRIPTION_ID>"
resource_group = "<RESOURCE_GROUP>"
workspace = "<AML_WORKSPACE_NAME>"

ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)

# Create the data asset in the workspace
ml_client.data.archive(name="<DATA ASSET NAME>")
from azure.ai.ml import MLClient
from azure.identity import DefaultAzureCredential

# Connect to the AzureML workspace
subscription_id = "<SUBSCRIPTION_ID>"
resource_group = "<RESOURCE_GROUP>"
workspace = "<AML_WORKSPACE_NAME>"

ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)

# Create the data asset in the workspace
ml_client.data.archive(name="<DATA ASSET NAME>")
In the Studio UI, selectDatafrom the left-hand menu.
On theData assetstab, select the data asset you want to archive.
SelectArchive, followed byArchivein the confirmation dialog box.

To archive a specific data asset version, use:
Azure CLI
Python SDK
Studio
Execute the following command. Be sure to update the<>placeholders with the name of your data asset and version.
<>
az ml data archive --name <NAME OF DATA ASSET> --version <VERSION TO ARCHIVE>
az ml data archive --name <NAME OF DATA ASSET> --version <VERSION TO ARCHIVE>
from azure.ai.ml import MLClient
from azure.identity import DefaultAzureCredential

# Connect to the AzureML workspace
subscription_id = "<SUBSCRIPTION_ID>"
resource_group = "<RESOURCE_GROUP>"
workspace = "<AML_WORKSPACE_NAME>"

ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)

# Create the data asset in the workspace
ml_client.data.archive(name="<DATA ASSET NAME>", version="<VERSION TO ARCHIVE>")
from azure.ai.ml import MLClient
from azure.identity import DefaultAzureCredential

# Connect to the AzureML workspace
subscription_id = "<SUBSCRIPTION_ID>"
resource_group = "<RESOURCE_GROUP>"
workspace = "<AML_WORKSPACE_NAME>"

ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)

# Create the data asset in the workspace
ml_client.data.archive(name="<DATA ASSET NAME>", version="<VERSION TO ARCHIVE>")
Important
At this time, archiving a specific data asset version is not supported in the Studio UI.
Restore an archived data asset
You can restore an archived data asset. If all of versions of the data asset are archived, you can't restore individual versions of the data asset - you must restore all versions.
To restoreall versionsof the data asset under a given name, use:
Azure CLI
Python SDK
Studio
Execute the following command. Be sure to update the<>placeholders with the name of your data asset.
<>
az ml data restore --name <NAME OF DATA ASSET>
az ml data restore --name <NAME OF DATA ASSET>
from azure.ai.ml import MLClient
from azure.identity import DefaultAzureCredential

# Connect to the AzureML workspace
subscription_id = "<SUBSCRIPTION_ID>"
resource_group = "<RESOURCE_GROUP>"
workspace = "<AML_WORKSPACE_NAME>"

ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)

# Create the data asset in the workspace
ml_client.data.restore(name="<DATA ASSET NAME>")
from azure.ai.ml import MLClient
from azure.identity import DefaultAzureCredential

# Connect to the AzureML workspace
subscription_id = "<SUBSCRIPTION_ID>"
resource_group = "<RESOURCE_GROUP>"
workspace = "<AML_WORKSPACE_NAME>"

ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)

# Create the data asset in the workspace
ml_client.data.restore(name="<DATA ASSET NAME>")
In the Studio UI, selectDatafrom the left-hand menu.
On theData assetstab, enableInclude Archived.
Select the data asset name.
Next, on the data asset details page, selectRestore.
Important
If all data asset versions were archived, you cannot restore individual versions of the data asset - you must restore all versions.
To restore a specific data asset version, use:
Azure CLI
Python SDK
Studio
Execute the following command. Be sure to update the<>placeholders with the name of your data asset and version.
<>
az ml data restore --name <NAME OF DATA ASSET> --version <VERSION TO ARCHIVE>
az ml data restore --name <NAME OF DATA ASSET> --version <VERSION TO ARCHIVE>
from azure.ai.ml import MLClient
from azure.identity import DefaultAzureCredential

# Connect to the AzureML workspace
subscription_id = "<SUBSCRIPTION_ID>"
resource_group = "<RESOURCE_GROUP>"
workspace = "<AML_WORKSPACE_NAME>"

ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)

# Create the data asset in the workspace
ml_client.data.restore(name="<DATA ASSET NAME>", version="<VERSION TO ARCHIVE>")
from azure.ai.ml import MLClient
from azure.identity import DefaultAzureCredential

# Connect to the AzureML workspace
subscription_id = "<SUBSCRIPTION_ID>"
resource_group = "<RESOURCE_GROUP>"
workspace = "<AML_WORKSPACE_NAME>"

ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)

# Create the data asset in the workspace
ml_client.data.restore(name="<DATA ASSET NAME>", version="<VERSION TO ARCHIVE>")
Important
At this time, restoring a specific data asset version is not supported in the Studio UI.
Data lineage
Data lineage is broadly understood as the lifecycle that spans the origin of the data, and where it moves over time across storage. Different kinds of backwards-looking scenarios use it, for example
Troubleshooting
Tracing root causes in ML pipelines
Debugging
Data quality analysis, compliance and âwhat ifâ scenarios also use lineage. Lineage is represented visually to show data moving from source to destination, and additionally covers data transformations. Given the complexity of most enterprise data environments, these views can become hard to understand without consolidation or masking of peripheral data points.
In an Azure Machine Learning Pipeline, data assets show the origin of the data and how the data was processed, for example:

You can view the jobs that consume the data asset in the Studio UI. First, selectDatafrom the left-hand menu, and then select the data asset name. Note the jobs consuming the data asset:

The jobs view in Data assets makes it easier to find job failures and do root-cause analysis in your ML pipelines and debugging.
Data asset tagging
Data assets support tagging, which is extra metadata applied to the data asset as a key-value pair. Data tagging provides many benefits:
Data quality description. For example, if your organization uses amedallion lakehouse architecture, you can tag assets withmedallion:bronze(raw),medallion:silver(validated) andmedallion:gold(enriched).
medallion:bronze
medallion:silver
medallion:gold
Efficient searching and filtering of data, to help data discovery.
Identification of sensitive personal data, to properly manage and govern data access. For example,sensitivity:PII/sensitivity:nonPII.
sensitivity:PII
sensitivity:nonPII
Determination of whether or not data is approved by a responsible AI (RAI) audit. For example,RAI_audit:approved/RAI_audit:todo.
RAI_audit:approved
RAI_audit:todo
You can add tags to data assets as part of their creation flow, or you can add tags to existing data assets. This section shows both:
Azure CLI
Python SDK
Studio
Create a YAML file, and copy-and-paste the following code into that YAML file. Be sure to update the<>placeholders with the
<>
name of your data asset
the version
description
tags (key-value pairs)
path to a single file on a supported location
$schema: https://azuremlschemas.azureedge.net/latest/data.schema.json

# Supported paths include:
# local: './<path>/<file>' (this will be automatically uploaded to cloud storage)
# blob:  'wasbs://<container_name>@<account_name>.blob.core.windows.net/<path>/<file>'
# ADLS gen2: 'abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>/<file>'
# Datastore: 'azureml://datastores/<data_store_name>/paths/<path>/<file>'

# Data asset types, use one of:
# uri_file, uri_folder, mltable

type: uri_file
name: <NAME OF DATA ASSET>
version: <VERSION>
description: <DESCRIPTION>
tags:
    <KEY1>: <VALUE>
    <KEY2>: <VALUE>
path: <SUPPORTED PATH>
$schema: https://azuremlschemas.azureedge.net/latest/data.schema.json

# Supported paths include:
# local: './<path>/<file>' (this will be automatically uploaded to cloud storage)
# blob:  'wasbs://<container_name>@<account_name>.blob.core.windows.net/<path>/<file>'
# ADLS gen2: 'abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>/<file>'
# Datastore: 'azureml://datastores/<data_store_name>/paths/<path>/<file>'

# Data asset types, use one of:
# uri_file, uri_folder, mltable

type: uri_file
name: <NAME OF DATA ASSET>
version: <VERSION>
description: <DESCRIPTION>
tags:
    <KEY1>: <VALUE>
    <KEY2>: <VALUE>
path: <SUPPORTED PATH>
Execute the following command in the CLI. Be sure to update the<filename>placeholder to the YAML filename.
<filename>
az ml data create -f <filename>.yml
az ml data create -f <filename>.yml
Use the following code to create a File type data asset, and update the<>placeholders with your information:
<>
from azure.ai.ml import MLClient
from azure.ai.ml.entities import Data
from azure.ai.ml.constants import AssetTypes
from azure.identity import DefaultAzureCredential

# Connect to the AzureML workspace
subscription_id = "<SUBSCRIPTION_ID>"
resource_group = "<RESOURCE_GROUP>"
workspace = "<AML_WORKSPACE_NAME>"

ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)

# Set the version number of the data asset (for example: '1')
VERSION = "<VERSION>"

# Set the path, supported paths include:
# local: './<path>/<file>' (this will be automatically uploaded to cloud storage)
# blob:  'wasbs://<container_name>@<account_name>.blob.core.windows.net/<path>/<file>'
# ADLS gen2: 'abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>/<file>'
# Datastore: 'azureml://datastores/<data_store_name>/paths/<path>/<file>'
path = "<SUPPORTED PATH>"

# Set the type, use on of the following asset type constants:
# AssetTypes.URI_FILE, AssetTypes.URI_FOLDER, AssetTypes.MLTABLE
data_asset_type = AssetTypes.<TYPE>

# Set the tags - update with your key-value pairs
tags = {
    "<KEY1>:" "<VALUE>"
    "<KEY2>:" "<VALUE>"
}

# Define the Data asset object
my_data = Data(
    path=path,
    type=data_asset_type,
    description="<ADD A DESCRIPTION HERE>",
    name="<NAME OF DATA ASSET>",
    version=VERSION,
    tags=tags,
)

# Create the data asset in the workspace
ml_client.data.create_or_update(my_data)
from azure.ai.ml import MLClient
from azure.ai.ml.entities import Data
from azure.ai.ml.constants import AssetTypes
from azure.identity import DefaultAzureCredential

# Connect to the AzureML workspace
subscription_id = "<SUBSCRIPTION_ID>"
resource_group = "<RESOURCE_GROUP>"
workspace = "<AML_WORKSPACE_NAME>"

ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)

# Set the version number of the data asset (for example: '1')
VERSION = "<VERSION>"

# Set the path, supported paths include:
# local: './<path>/<file>' (this will be automatically uploaded to cloud storage)
# blob:  'wasbs://<container_name>@<account_name>.blob.core.windows.net/<path>/<file>'
# ADLS gen2: 'abfss://<file_system>@<account_name>.dfs.core.windows.net/<path>/<file>'
# Datastore: 'azureml://datastores/<data_store_name>/paths/<path>/<file>'
path = "<SUPPORTED PATH>"

# Set the type, use on of the following asset type constants:
# AssetTypes.URI_FILE, AssetTypes.URI_FOLDER, AssetTypes.MLTABLE
data_asset_type = AssetTypes.<TYPE>

# Set the tags - update with your key-value pairs
tags = {
    "<KEY1>:" "<VALUE>"
    "<KEY2>:" "<VALUE>"
}

# Define the Data asset object
my_data = Data(
    path=path,
    type=data_asset_type,
    description="<ADD A DESCRIPTION HERE>",
    name="<NAME OF DATA ASSET>",
    version=VERSION,
    tags=tags,
)

# Create the data asset in the workspace
ml_client.data.create_or_update(my_data)
Important
At this time, the Studio UI does not support adding tags as part of the data asset creation flow. You can add tags in the Studio UI after creation of the data asset.
Azure CLI
Python SDK
Studio
Execute the following command in the Azure CLI. Be sure to update the<>placeholders with the
<>
Name of your data asset
The version
Key-value pair for the tag
az ml data update --name <DATA ASSET NAME> --version <VERSION> --set tags.<KEY>=<VALUE>
az ml data update --name <DATA ASSET NAME> --version <VERSION> --set tags.<KEY>=<VALUE>
from azure.ai.ml import MLClient
from azure.identity import DefaultAzureCredential

# Connect to the AzureML workspace
subscription_id = "<SUBSCRIPTION_ID>"
resource_group = "<RESOURCE_GROUP>"
workspace = "<AML_WORKSPACE_NAME>"

ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)

# Get the data asset in the workspace
data = ml_client.data.get(name="<DATA ASSET NAME>", version="<VERSION>")

# add a tag
tags = {
    "<KEY1>": "<VALUE>",
    "<KEY2>": "<VALUE>",
}

# add the tags to the data asset object
data.tags = tags

# update the data asset in your workspace
ml_client.data.create_or_update(data)
from azure.ai.ml import MLClient
from azure.identity import DefaultAzureCredential

# Connect to the AzureML workspace
subscription_id = "<SUBSCRIPTION_ID>"
resource_group = "<RESOURCE_GROUP>"
workspace = "<AML_WORKSPACE_NAME>"

ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)

# Get the data asset in the workspace
data = ml_client.data.get(name="<DATA ASSET NAME>", version="<VERSION>")

# add a tag
tags = {
    "<KEY1>": "<VALUE>",
    "<KEY2>": "<VALUE>",
}

# add the tags to the data asset object
data.tags = tags

# update the data asset in your workspace
ml_client.data.create_or_update(data)
SelectDataon the left-hand menu in the Studio UI.
Select theData Assetstab.
Select the data asset you would like to add tags to.
In the data asset details, select theEditbutton underTags:
Add your key-value pair
SelectSave.
Versioning best practices
Typically, your ETL processes organize your folder structure on Azure storage by time, for example:
/
âââ ð mydata
    âââ ð year=2022
    â   âââ ð month=11
    â   â   âââ ð file1
    â   â   âââ ð file2
    â   âââ ð month=12
    â       âââ ð file1
    â   â   âââ ð file2
    âââ ð year=2023
        âââ ð month=1
            âââ ð file1
    â   â   âââ ð file2
/
âââ ð mydata
    âââ ð year=2022
    â   âââ ð month=11
    â   â   âââ ð file1
    â   â   âââ ð file2
    â   âââ ð month=12
    â       âââ ð file1
    â   â   âââ ð file2
    âââ ð year=2023
        âââ ð month=1
            âââ ð file1
    â   â   âââ ð file2
The combination of time/version structured foldersandAzure Machine Learning Tables (MLTable) allows you to construct versioned datasets. Ahypothetical exampleshows how to achieve versioned data with Azure Machine Learning Tables. Suppose you have a process that uploads camera images to Azure Blob storage every week, in this structure:
MLTable
/myimages
âââ ð year=2022
    âââ ð week52
    â   âââ ð camera1
    â   â   âââ ð¼ï¸ file1.jpeg
    â   â   âââ ð¼ï¸ file2.jpeg
    â   âââ ð camera2
    â       âââ ð¼ï¸ file1.jpeg
    â   â   âââ ð¼ï¸ file2.jpeg
âââ ð year=2023
    âââ ð week1
    â   âââ ð camera1
    â   â   âââ ð¼ï¸ file1.jpeg
    â   â   âââ ð¼ï¸ file2.jpeg
    â   âââ ð camera2
    â       âââ ð¼ï¸ file1.jpeg
    â   â   âââ ð¼ï¸ file2.jpeg
/myimages
âââ ð year=2022
    âââ ð week52
    â   âââ ð camera1
    â   â   âââ ð¼ï¸ file1.jpeg
    â   â   âââ ð¼ï¸ file2.jpeg
    â   âââ ð camera2
    â       âââ ð¼ï¸ file1.jpeg
    â   â   âââ ð¼ï¸ file2.jpeg
âââ ð year=2023
    âââ ð week1
    â   âââ ð camera1
    â   â   âââ ð¼ï¸ file1.jpeg
    â   â   âââ ð¼ï¸ file2.jpeg
    â   âââ ð camera2
    â       âââ ð¼ï¸ file1.jpeg
    â   â   âââ ð¼ï¸ file2.jpeg
Note
While we show how to version image (jpeg) data, the same approach works for any file type (for example, Parquet, CSV).
jpeg
With Azure Machine Learning Tables (mltable), construct a Table of paths that include the data up to the end of the first week in 2023. Then create a data asset:
mltable
import mltable
from mltable import MLTableHeaders, MLTableFileEncoding, DataType
from azure.ai.ml import MLClient
from azure.ai.ml.entities import Data
from azure.ai.ml.constants import AssetTypes
from azure.identity import DefaultAzureCredential

# The ** in the pattern below will glob all sub-folders (camera1, ..., camera2)
paths = [
    {
        "pattern": "abfss://<file_system>@<account_name>.dfs.core.windows.net/myimages/year=2022/week=52/**/*.jpeg"
    },
    {
        "pattern": "abfss://<file_system>@<account_name>.dfs.core.windows.net/myimages/year=2023/week=1/**/*.jpeg"
    },
]

tbl = mltable.from_paths(paths)
tbl.save("./myimages")

# Connect to the AzureML workspace
subscription_id = "<SUBSCRIPTION_ID>"
resource_group = "<RESOURCE_GROUP>"
workspace = "<AML_WORKSPACE_NAME>"

ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)

# Define the Data asset object
my_data = Data(
    path=mltable_folder,
    type=AssetTypes.MLTABLE,
    description="My images. Version includes data through to 2023-Jan-08.",
    name="myimages",
    version="20230108",
)

# Create the data asset in the workspace
ml_client.data.create_or_update(my_data)
import mltable
from mltable import MLTableHeaders, MLTableFileEncoding, DataType
from azure.ai.ml import MLClient
from azure.ai.ml.entities import Data
from azure.ai.ml.constants import AssetTypes
from azure.identity import DefaultAzureCredential

# The ** in the pattern below will glob all sub-folders (camera1, ..., camera2)
paths = [
    {
        "pattern": "abfss://<file_system>@<account_name>.dfs.core.windows.net/myimages/year=2022/week=52/**/*.jpeg"
    },
    {
        "pattern": "abfss://<file_system>@<account_name>.dfs.core.windows.net/myimages/year=2023/week=1/**/*.jpeg"
    },
]

tbl = mltable.from_paths(paths)
tbl.save("./myimages")

# Connect to the AzureML workspace
subscription_id = "<SUBSCRIPTION_ID>"
resource_group = "<RESOURCE_GROUP>"
workspace = "<AML_WORKSPACE_NAME>"

ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)

# Define the Data asset object
my_data = Data(
    path=mltable_folder,
    type=AssetTypes.MLTABLE,
    description="My images. Version includes data through to 2023-Jan-08.",
    name="myimages",
    version="20230108",
)

# Create the data asset in the workspace
ml_client.data.create_or_update(my_data)
At the end of the following week, your ETL updated the data to include more data:
/myimages
âââ ð year=2022
    âââ ð week52
    â   âââ ð camera1
    â   â   âââ ð¼ï¸ file1.jpeg
    â   â   âââ ð¼ï¸ file2.jpeg
    â   âââ ð camera2
    â   â   âââ ð¼ï¸ file1.jpeg
    â   â   âââ ð¼ï¸ file2.jpeg
âââ ð year=2023
    âââ ð week1
    â   âââ ð camera1
    â   â   âââ ð¼ï¸ file1.jpeg
    â   â   âââ ð¼ï¸ file2.jpeg
    â   âââ ð camera2
    â   â   âââ ð¼ï¸ file1.jpeg
    â   â   âââ ð¼ï¸ file2.jpeg
    âââ ð week2
    â   âââ ð camera1
    â   â   âââ ð¼ï¸ file1.jpeg
    â   â   âââ ð¼ï¸ file2.jpeg
    â   âââ ð camera2
    â   â   âââ ð¼ï¸ file1.jpeg
    â   â   âââ ð¼ï¸ file2.jpeg
/myimages
âââ ð year=2022
    âââ ð week52
    â   âââ ð camera1
    â   â   âââ ð¼ï¸ file1.jpeg
    â   â   âââ ð¼ï¸ file2.jpeg
    â   âââ ð camera2
    â   â   âââ ð¼ï¸ file1.jpeg
    â   â   âââ ð¼ï¸ file2.jpeg
âââ ð year=2023
    âââ ð week1
    â   âââ ð camera1
    â   â   âââ ð¼ï¸ file1.jpeg
    â   â   âââ ð¼ï¸ file2.jpeg
    â   âââ ð camera2
    â   â   âââ ð¼ï¸ file1.jpeg
    â   â   âââ ð¼ï¸ file2.jpeg
    âââ ð week2
    â   âââ ð camera1
    â   â   âââ ð¼ï¸ file1.jpeg
    â   â   âââ ð¼ï¸ file2.jpeg
    â   âââ ð camera2
    â   â   âââ ð¼ï¸ file1.jpeg
    â   â   âââ ð¼ï¸ file2.jpeg
The first version (20230108) continues to only mount/download files fromyear=2022/week=52andyear=2023/week=1because the paths are declared in theMLTablefile. This ensuresreproducibilityfor your experiments. To create a new version of the data asset that includesyear=2023/week2, use:
20230108
year=2022/week=52
year=2023/week=1
MLTable
year=2023/week2
import mltable
from mltable import MLTableHeaders, MLTableFileEncoding, DataType
from azure.ai.ml import MLClient
from azure.ai.ml.entities import Data
from azure.ai.ml.constants import AssetTypes
from azure.identity import DefaultAzureCredential

# The ** in the pattern below will glob all sub-folders (camera1, ..., camera2)
paths = [
    {
        "pattern": "abfss://<file_system>@<account_name>.dfs.core.windows.net/myimages/year=2022/week=52/**/*.jpeg"
    },
    {
        "pattern": "abfss://<file_system>@<account_name>.dfs.core.windows.net/myimages/year=2023/week=1/**/*.jpeg"
    },
    {
        "pattern": "abfss://<file_system>@<account_name>.dfs.core.windows.net/myimages/year=2023/week=2/**/*.jpeg"
    },
]

# Save to an MLTable file on local storage
tbl = mltable.from_paths(paths)
tbl.save("./myimages")

# Next, you create a data asset - the MLTable file will automatically be uploaded

# Connect to the AzureML workspace
subscription_id = "<SUBSCRIPTION_ID>"
resource_group = "<RESOURCE_GROUP>"
workspace = "<AML_WORKSPACE_NAME>"

ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)

# Define the Data asset object
my_data = Data(
    path=mltable_folder,
    type=AssetTypes.MLTABLE,
    description="My images. Version includes data through to 2023-Jan-15.",
    name="myimages",
    version="20230115", # update version to the date
)

# Create the data asset in the workspace
ml_client.data.create_or_update(my_data)
import mltable
from mltable import MLTableHeaders, MLTableFileEncoding, DataType
from azure.ai.ml import MLClient
from azure.ai.ml.entities import Data
from azure.ai.ml.constants import AssetTypes
from azure.identity import DefaultAzureCredential

# The ** in the pattern below will glob all sub-folders (camera1, ..., camera2)
paths = [
    {
        "pattern": "abfss://<file_system>@<account_name>.dfs.core.windows.net/myimages/year=2022/week=52/**/*.jpeg"
    },
    {
        "pattern": "abfss://<file_system>@<account_name>.dfs.core.windows.net/myimages/year=2023/week=1/**/*.jpeg"
    },
    {
        "pattern": "abfss://<file_system>@<account_name>.dfs.core.windows.net/myimages/year=2023/week=2/**/*.jpeg"
    },
]

# Save to an MLTable file on local storage
tbl = mltable.from_paths(paths)
tbl.save("./myimages")

# Next, you create a data asset - the MLTable file will automatically be uploaded

# Connect to the AzureML workspace
subscription_id = "<SUBSCRIPTION_ID>"
resource_group = "<RESOURCE_GROUP>"
workspace = "<AML_WORKSPACE_NAME>"

ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)

# Define the Data asset object
my_data = Data(
    path=mltable_folder,
    type=AssetTypes.MLTABLE,
    description="My images. Version includes data through to 2023-Jan-15.",
    name="myimages",
    version="20230115", # update version to the date
)

# Create the data asset in the workspace
ml_client.data.create_or_update(my_data)
You now have two versions of the data, where the name of the version corresponds to the date the images were uploaded to storage:
20230108:The images up to 2023-Jan-08.
20230115:The images up to 2023-Jan-15.
In both cases, MLTable constructs a table of paths thatonly include the images up to those dates.
In an Azure Machine Learning job you can mount or download those paths in the versioned MLTable to your compute target using either theeval_downloadoreval_mountmodes:
eval_download
eval_mount
from azure.ai.ml import MLClient, command, Input
from azure.ai.ml.entities import Environment
from azure.identity import DefaultAzureCredential
from azure.ai.ml.constants import InputOutputModes

# connect to the AzureML workspace
ml_client = MLClient.from_config(
    DefaultAzureCredential()
)

# Get the 20230115 version of the data
data_asset = ml_client.data.get(name="myimages", version="20230115")

input = {
    "images": Input(type="mltable",
                   path=data_asset.id,
                   mode=InputOutputModes.EVAL_MOUNT
            )
}

cmd = """
ls ${{inputs.images}}/**
"""

job = command(
    command=cmd,
    inputs=input,
    compute="cpu-cluster",
    environment="azureml://registries/azureml/environments/sklearn-1.1/versions/4"
)

ml_client.jobs.create_or_update(job)
from azure.ai.ml import MLClient, command, Input
from azure.ai.ml.entities import Environment
from azure.identity import DefaultAzureCredential
from azure.ai.ml.constants import InputOutputModes

# connect to the AzureML workspace
ml_client = MLClient.from_config(
    DefaultAzureCredential()
)

# Get the 20230115 version of the data
data_asset = ml_client.data.get(name="myimages", version="20230115")

input = {
    "images": Input(type="mltable",
                   path=data_asset.id,
                   mode=InputOutputModes.EVAL_MOUNT
            )
}

cmd = """
ls ${{inputs.images}}/**
"""

job = command(
    command=cmd,
    inputs=input,
    compute="cpu-cluster",
    environment="azureml://registries/azureml/environments/sklearn-1.1/versions/4"
)

ml_client.jobs.create_or_update(job)
Note
Theeval_mountandeval_downloadmodes are unique to MLTable. In this case, the AzureML data runtime capability evaluates theMLTablefile and mounts the paths on the compute target.
eval_mount
eval_download
MLTable
Next steps
Access data in a job
Working with tables in Azure Machine Learning
Access data from Azure cloud storage during interactive development
Feedback
Was this page helpful?
Additional resources