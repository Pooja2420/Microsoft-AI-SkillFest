Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Work with Delta Lake table history
Article
2024-07-26
4 contributors
In this article
Each operation that modifies a Delta Lake table creates a new table version. You can use history information to audit operations, rollback a table, or query a table at a specific point in time using time travel.
Note
Databricks does not recommend using Delta Lake table history as a long-term backup solution for data archival. Databricks recommends using only the past 7 days for time travel operations unless you have set both data and log retention configurations to a larger value.
Retrieve Delta table history
You can retrieve information including the operations, user, and timestamp for each write to a Delta table by running thehistorycommand. The operations are returned in reverse chronological order.
history
Table history retention is determined by the table settingdelta.logRetentionDuration, which is 30 days by default.
delta.logRetentionDuration
Note
Time travel and table history are controlled by different retention thresholds. SeeWhat is Delta Lake time travel?.
DESCRIBE HISTORY table_name       -- get the full history of the table

DESCRIBE HISTORY table_name LIMIT 1  -- get the last operation only
DESCRIBE HISTORY table_name       -- get the full history of the table

DESCRIBE HISTORY table_name LIMIT 1  -- get the last operation only
For Spark SQL syntax details, seeDESCRIBE HISTORY.
See theDelta Lake API documentationfor Scala/Java/Python syntax details.
Catalog Explorerprovides a visual view of this detailed table information and history for Delta tables. In addition to the table schema and sample data, you can click theHistorytab to see the table history that displays withDESCRIBE HISTORY.
DESCRIBE HISTORY
History schema
The output of thehistoryoperation has the following columns.
history
+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+
|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|
+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+
|      5|2019-07-29 14:07:47|   ###|     ###|   DELETE|[predicate -> ["(...|null|     ###|      ###|          4|WriteSerializable|        false|[numTotalRows -> ...|
|      4|2019-07-29 14:07:41|   ###|     ###|   UPDATE|[predicate -> (id...|null|     ###|      ###|          3|WriteSerializable|        false|[numTotalRows -> ...|
|      3|2019-07-29 14:07:29|   ###|     ###|   DELETE|[predicate -> ["(...|null|     ###|      ###|          2|WriteSerializable|        false|[numTotalRows -> ...|
|      2|2019-07-29 14:06:56|   ###|     ###|   UPDATE|[predicate -> (id...|null|     ###|      ###|          1|WriteSerializable|        false|[numTotalRows -> ...|
|      1|2019-07-29 14:04:31|   ###|     ###|   DELETE|[predicate -> ["(...|null|     ###|      ###|          0|WriteSerializable|        false|[numTotalRows -> ...|
|      0|2019-07-29 14:01:40|   ###|     ###|    WRITE|[mode -> ErrorIfE...|null|     ###|      ###|       null|WriteSerializable|         true|[numFiles -> 2, n...|
+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+
+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+
|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|
+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+
|      5|2019-07-29 14:07:47|   ###|     ###|   DELETE|[predicate -> ["(...|null|     ###|      ###|          4|WriteSerializable|        false|[numTotalRows -> ...|
|      4|2019-07-29 14:07:41|   ###|     ###|   UPDATE|[predicate -> (id...|null|     ###|      ###|          3|WriteSerializable|        false|[numTotalRows -> ...|
|      3|2019-07-29 14:07:29|   ###|     ###|   DELETE|[predicate -> ["(...|null|     ###|      ###|          2|WriteSerializable|        false|[numTotalRows -> ...|
|      2|2019-07-29 14:06:56|   ###|     ###|   UPDATE|[predicate -> (id...|null|     ###|      ###|          1|WriteSerializable|        false|[numTotalRows -> ...|
|      1|2019-07-29 14:04:31|   ###|     ###|   DELETE|[predicate -> ["(...|null|     ###|      ###|          0|WriteSerializable|        false|[numTotalRows -> ...|
|      0|2019-07-29 14:01:40|   ###|     ###|    WRITE|[mode -> ErrorIfE...|null|     ###|      ###|       null|WriteSerializable|         true|[numFiles -> 2, n...|
+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+
Note
A few of the other columns are not available if you write into a Delta table using the following methods:JDBC or ODBCRun a command using the REST APISome task types for jobs
JDBC or ODBC
Run a command using the REST API
Some task types for jobs
Columns added in the future will always be added after the last column.
Operation metrics keys
Thehistoryoperation returns a collection of operations metrics in theoperationMetricscolumn map.
history
operationMetrics
The following tables list the map key definitions by operation.
What is Delta Lake time travel?
Delta Lake time travel supports querying previous table versions based on timestamp or table version (as recorded in the transaction log). You can use time travel for applications such as the following:
Re-creating analyses, reports, or outputs (for example, the output of a machine learning model). This could be useful for debugging or auditing, especially in regulated industries.
Writing complex temporal queries.
Fixing mistakes in your data.
Providing snapshot isolation for a set of queries for fast changing tables.
Important
Table versions accessible with time travel are determined by a combination of the retention threshold for transaction log files and the frequency and specified retention forVACUUMoperations. If you runVACUUMdaily with the default values, 7 days of data is available for time travel.
VACUUM
VACUUM
Delta time travel syntax
You query a Delta table with time travel by adding a clause after the table name specification.
timestamp_expressioncan be any one of:'2018-10-18T22:15:12.013Z', that is, a string that can be cast to a timestampcast('2018-10-18 13:36:32 CEST' as timestamp)'2018-10-18', that is, a date stringcurrent_timestamp() - interval 12 hoursdate_sub(current_date(), 1)Any other expression that is or can be cast to a timestamp
timestamp_expression
'2018-10-18T22:15:12.013Z', that is, a string that can be cast to a timestamp
'2018-10-18T22:15:12.013Z'
cast('2018-10-18 13:36:32 CEST' as timestamp)
cast('2018-10-18 13:36:32 CEST' as timestamp)
'2018-10-18', that is, a date string
'2018-10-18'
current_timestamp() - interval 12 hours
current_timestamp() - interval 12 hours
date_sub(current_date(), 1)
date_sub(current_date(), 1)
Any other expression that is or can be cast to a timestamp
versionis a long value that can be obtained from the output ofDESCRIBE HISTORY table_spec.
version
DESCRIBE HISTORY table_spec
Neithertimestamp_expressionnorversioncan be subqueries.
timestamp_expression
version
Only date or timestamp strings are accepted. For example,"2019-01-01"and"2019-01-01T00:00:00.000Z". See the following code for example syntax:
"2019-01-01"
"2019-01-01T00:00:00.000Z"
SQL
SELECT * FROM people10m TIMESTAMP AS OF '2018-10-18T22:15:12.013Z';
SELECT * FROM people10m VERSION AS OF 123;
SELECT * FROM people10m TIMESTAMP AS OF '2018-10-18T22:15:12.013Z';
SELECT * FROM people10m VERSION AS OF 123;
Python
df1 = spark.read.option("timestampAsOf", "2019-01-01").table("people10m")
df2 = spark.read.option("versionAsOf", 123).table("people10m")
df1 = spark.read.option("timestampAsOf", "2019-01-01").table("people10m")
df2 = spark.read.option("versionAsOf", 123).table("people10m")
You can also use the@syntax to specify the timestamp or version as part of the table name. The timestamp must be inyyyyMMddHHmmssSSSformat. You can specify a version after@by prepending avto the version. See the following code for example syntax:
@
yyyyMMddHHmmssSSS
@
v
SQL
SELECT * FROM people10m@20190101000000000
SELECT * FROM people10m@v123
SELECT * FROM people10m@20190101000000000
SELECT * FROM people10m@v123
Python
spark.read.table("people10m@20190101000000000")
spark.read.table("people10m@v123")
spark.read.table("people10m@20190101000000000")
spark.read.table("people10m@v123")
What are transaction log checkpoints?
Delta Lake records table versions as JSON files within the_delta_logdirectory, which is stored alongside table data. To optimize checkpoint querying, Delta Lake aggregates table versions to Parquet checkpoint files, preventing the need to read all JSON versions of table history. Azure Databricks optimizes checkpointing frequency for data size and workload. Users should not need to interact with checkpoints directly. The checkpoint frequency is subject to change without notice.
_delta_log
Configure data retention for time travel queries
To query a previous table version, you must retainboththe log and the data files for that version.
Data files are deleted whenVACUUMruns against a table. Delta Lake manages log file removal automatically after checkpointing table versions.
VACUUM
Because most Delta tables haveVACUUMrun against them regularly, point-in-time queries should respect the retention threshold forVACUUM, which is 7 days by default.
VACUUM
VACUUM
In order to increase the data retention threshold for Delta tables, you must configure the following table properties:
delta.logRetentionDuration = "interval <interval>": controls how long the history for a table is kept. The default isinterval 30 days.
delta.logRetentionDuration = "interval <interval>"
interval 30 days
delta.deletedFileRetentionDuration = "interval <interval>": determines the thresholdVACUUMuses to remove data files no longer referenced in the current table version. The default isinterval 7 days.
delta.deletedFileRetentionDuration = "interval <interval>"
VACUUM
interval 7 days
You can specify Delta properties during table creation or set them with anALTER TABLEstatement. SeeDelta table properties reference.
ALTER TABLE
Note
You must set both of these properties to ensure table history is retained for longer duration for tables with frequentVACUUMoperations. For example, to access 30 days of historical data, setdelta.deletedFileRetentionDuration = "interval 30 days"(which matches the default setting fordelta.logRetentionDuration).
VACUUM
delta.deletedFileRetentionDuration = "interval 30 days"
delta.logRetentionDuration
Increasing data retention threshold can cause your storage costs to go up, as more data files are maintained.
Restore a Delta table to an earlier state
You can restore a Delta table to its earlier state by using theRESTOREcommand. A Delta table internally maintains historic versions of the table that enable it to be restored to an earlier state.
A version corresponding to the earlier state or a timestamp of when the earlier state was created are supported as options by theRESTOREcommand.
RESTORE
RESTORE
Important
You can restore an already restored table.
You can restore aclonedtable.
You must haveMODIFYpermission on the table being restored.
MODIFY
You cannot restore a table to an older version where the data files were deleted manually or byvacuum. Restoring to this version partially is still possible ifspark.sql.files.ignoreMissingFilesis set totrue.
vacuum
spark.sql.files.ignoreMissingFiles
true
The timestamp format for restoring to an earlier state isyyyy-MM-dd HH:mm:ss. Providing only a date(yyyy-MM-dd) string is also supported.
yyyy-MM-dd HH:mm:ss
yyyy-MM-dd
RESTORE TABLE target_table TO VERSION AS OF <version>;
RESTORE TABLE target_table TO TIMESTAMP AS OF <timestamp>;
RESTORE TABLE target_table TO VERSION AS OF <version>;
RESTORE TABLE target_table TO TIMESTAMP AS OF <timestamp>;
For syntax details, seeRESTORE.
Important
Restore is considered a data-changing operation. Delta Lake log entries added by theRESTOREcommand containdataChangeset to true. If there is a downstream application, such as aStructured streamingjob that processes the updates to a Delta Lake table, the data change log entries added by the restore operation are considered as new data updates, and processing them may result in duplicate data.
RESTORE
For example:
In the preceding example, theRESTOREcommand results in updates that were already seen when reading the Delta table version 0 and 1. If a streaming query was reading this table, then these files will be considered as newly added data and will be processed again.
RESTORE
Restore metrics
RESTOREreports the following metrics as a single row DataFrame once the operation is complete:
RESTORE
table_size_after_restore: The size of the table after restoring.
table_size_after_restore: The size of the table after restoring.
table_size_after_restore
num_of_files_after_restore: The number of files in the table after restoring.
num_of_files_after_restore: The number of files in the table after restoring.
num_of_files_after_restore
num_removed_files: Number of files removed (logically deleted) from the table.
num_removed_files: Number of files removed (logically deleted) from the table.
num_removed_files
num_restored_files: Number of files restored due to rolling back.
num_restored_files: Number of files restored due to rolling back.
num_restored_files
removed_files_size: Total size in bytes of the files that are removed from the table.
removed_files_size: Total size in bytes of the files that are removed from the table.
removed_files_size
restored_files_size: Total size in bytes of the files that are restored.
restored_files_size: Total size in bytes of the files that are restored.
restored_files_size

Examples of using Delta Lake time travel
Fix accidental deletes to a table for the user111:INSERT INTO my_table
  SELECT * FROM my_table TIMESTAMP AS OF date_sub(current_date(), 1)
  WHERE userId = 111
Fix accidental deletes to a table for the user111:
111
INSERT INTO my_table
  SELECT * FROM my_table TIMESTAMP AS OF date_sub(current_date(), 1)
  WHERE userId = 111
INSERT INTO my_table
  SELECT * FROM my_table TIMESTAMP AS OF date_sub(current_date(), 1)
  WHERE userId = 111
Fix accidental incorrect updates to a table:MERGE INTO my_table target
  USING my_table TIMESTAMP AS OF date_sub(current_date(), 1) source
  ON source.userId = target.userId
  WHEN MATCHED THEN UPDATE SET *
Fix accidental incorrect updates to a table:
MERGE INTO my_table target
  USING my_table TIMESTAMP AS OF date_sub(current_date(), 1) source
  ON source.userId = target.userId
  WHEN MATCHED THEN UPDATE SET *
MERGE INTO my_table target
  USING my_table TIMESTAMP AS OF date_sub(current_date(), 1) source
  ON source.userId = target.userId
  WHEN MATCHED THEN UPDATE SET *
Query the number of new customers added over the last week.SELECT count(distinct userId)
FROM my_table  - (
  SELECT count(distinct userId)
  FROM my_table TIMESTAMP AS OF date_sub(current_date(), 7))
Query the number of new customers added over the last week.
SELECT count(distinct userId)
FROM my_table  - (
  SELECT count(distinct userId)
  FROM my_table TIMESTAMP AS OF date_sub(current_date(), 7))
SELECT count(distinct userId)
FROM my_table  - (
  SELECT count(distinct userId)
  FROM my_table TIMESTAMP AS OF date_sub(current_date(), 7))
How do I find the last commitâs version in the Spark session?
To get the version number of the last commit written by the currentSparkSessionacross all threads
and all tables, query the SQL configurationspark.databricks.delta.lastCommitVersionInSession.
SparkSession
spark.databricks.delta.lastCommitVersionInSession
SQL
SET spark.databricks.delta.lastCommitVersionInSession
SET spark.databricks.delta.lastCommitVersionInSession
Python
spark.conf.get("spark.databricks.delta.lastCommitVersionInSession")
spark.conf.get("spark.databricks.delta.lastCommitVersionInSession")
Scala
spark.conf.get("spark.databricks.delta.lastCommitVersionInSession")
spark.conf.get("spark.databricks.delta.lastCommitVersionInSession")
If no commits have been made by theSparkSession, querying the key returns an empty value.
SparkSession
Note
If you share the sameSparkSessionacross multiple threads, itâs similar to sharing a variable
across multiple threads; you may hit race conditions as the configuration value is updated
concurrently.
SparkSession
Feedback
Was this page helpful?
Additional resources