Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Data featurization in automated machine learning (AutoML)
Article
2025-04-14
5 contributors
In this article
APPLIES TO:Python SDK azuremlv1
Important
This article provides information on using the Azure Machine Learning SDK v1. The SDK v1 is deprecated as of March 31, 2025 and support for it will end on June 30, 2026. You're able to install and use the SDK v1 until that date.
We recommend that you transition to the SDK v2 before June 30, 2026. For more information on the SDK v2, seeWhat is the Azure Machine Learning Python SDK v2and theSDK v2 reference.
This article explains how to customize the data featurization settings in Azure Machine Learning for yourautomated machine learning (AutoML) experiments.
Feature engineering and featurization
Training data consists of rows and columns. Each row is an observation or record, and the columns of each row are the features that describe each record. Typically, the features that best characterize the patterns in the data are selected to create predictive models.
Although many of the raw data fields can be used directly to train a model, it's often necessary to create other (engineered) features that provide information to better differentiate patterns in the data. This process is calledfeature engineering, where the use of domain knowledge of the data is used to create features that, in turn, help machine learning algorithms to learn better.
In Azure Machine Learning, data-scaling and normalization techniques are applied to make feature engineering easier. Collectively, these techniques and this feature engineering are calledfeaturizationin AutoML experiments.
Prerequisites
This article assumes that you already know how to configure an AutoML experiment.
Important
The Python commands in this article require the latestazureml-train-automlpackage version.
azureml-train-automl
Install the latestazureml-train-automlpackage to your local environment.
azureml-train-automl
For details on the latestazureml-train-automlpackage, see therelease notes.
azureml-train-automl
For information about configuration, see the following articles:
For a code-first experience:Set up AutoML training with Python
For a no-code experience:Set up no-code AutoML training for tabular data with the studio UI
Configure featurization
In every AutoML experiment,automatic scaling and normalization techniquesare applied to your data by default. These techniques are types of featurization that helpcertainalgorithms that are sensitive to features on different scales. You can enable more featurization, such asmissing-values imputation,encoding, andtransforms.
Note
Steps for AutoML featurization (such as feature normalization, handling missing data,
or converting text to numeric) become part of the underlying model. When you use the model for
predictions, the same featurization steps that are applied during training are applied to
your input data automatically.
For experiments that you configure with the Python SDK, you can enable or disable the featurization setting and further specify the featurization steps to be used for your experiment. If you're using the Azure Machine Learning studio, see thesteps to enable featurization.
The following table shows the accepted settings forfeaturizationin theAutoMLConfig class:
featurization
"featurization": 'auto'
"featurization": 'off'
"featurization": 'FeaturizationConfig'

Automatic featurization
The following table summarizes techniques that are automatically applied to your data. These techniques are applied for experiments that are configured by using the SDK or the studio UI. To disable this behavior, set"featurization": 'off'in yourAutoMLConfigobject.
"featurization": 'off'
AutoMLConfig
Note
*If you plan to export your AutoML-created models to anONNX model, only the featurization options indicated with an asterisk are supported in the ONNX format. To learn more, seeMake predictions with an AutoML ONNX model.
In every AutoML experiment, your data is automatically scaled or normalized to help algorithms perform well. During model training, one of the following scaling or normalization techniques are applied to each model.
scipy.sparse
Data guardrails
Data guardrailshelp you identify potential issues with your data, such as missing values orimbalanced data. They also help you take corrective actions for improved results.
Data guardrails are applied:
For SDK experiments: When the parameters"featurization": 'auto'orvalidation=autoare specified in yourAutoMLConfigobject.
"featurization": 'auto'
validation=auto
AutoMLConfig
For studio experiments: When automatic featurization is enabled.
You can review the data guardrails for your experiment:
By settingshow_output=Truewhen you submit an experiment by using the SDK.
show_output=True
In the studio, on theData guardrailstab of your AutoML run.
Data guardrail states
Data guardrails display one of three states:
Supported data guardrails
The following table describes the data guardrails that are currently supported and the associated statuses that you might see when you submit your experiment:
'auto'
'auto'
Customize featurization
You can customize your featurization settings to ensure that the data and features that are used to train your machine learning model result in relevant predictions.
To customize featurizations, specify"featurization": FeaturizationConfigin yourAutoMLConfigobject. If you're using the Azure Machine Learning studio for your experiment, see theConfigure featurization settings. To customize featurization for forecastings task types, refer to theCustomize featurization.
"featurization": FeaturizationConfig
AutoMLConfig
Supported customizations include:
Note
*Thedrop columnsfunctionality is deprecated as of SDK version 1.19. Drop columns from your dataset as part of data cleansing, before consuming it in your AutoML experiment.
You can create theFeaturizationConfigobject by using API calls:
FeaturizationConfig
featurization_config = FeaturizationConfig()
featurization_config.blocked_transformers = ['LabelEncoder']
featurization_config.drop_columns = ['aspiration', 'stroke']
featurization_config.add_column_purpose('engine-size', 'Numeric')
featurization_config.add_column_purpose('body-style', 'CategoricalHash')
#default strategy mean, add transformer param for 3 columns
featurization_config.add_transformer_params('Imputer', ['engine-size'], {"strategy": "median"})
featurization_config.add_transformer_params('Imputer', ['city-mpg'], {"strategy": "median"})
featurization_config.add_transformer_params('Imputer', ['bore'], {"strategy": "most_frequent"})
featurization_config.add_transformer_params('HashOneHotEncoder', [], {"number_of_bits": 3})
featurization_config = FeaturizationConfig()
featurization_config.blocked_transformers = ['LabelEncoder']
featurization_config.drop_columns = ['aspiration', 'stroke']
featurization_config.add_column_purpose('engine-size', 'Numeric')
featurization_config.add_column_purpose('body-style', 'CategoricalHash')
#default strategy mean, add transformer param for 3 columns
featurization_config.add_transformer_params('Imputer', ['engine-size'], {"strategy": "median"})
featurization_config.add_transformer_params('Imputer', ['city-mpg'], {"strategy": "median"})
featurization_config.add_transformer_params('Imputer', ['bore'], {"strategy": "most_frequent"})
featurization_config.add_transformer_params('HashOneHotEncoder', [], {"number_of_bits": 3})
Featurization transparency
Every AutoML model has featurization automatically applied. Featurization includes automated feature engineering (when"featurization": 'auto') and scaling and normalization, which then impacts the selected algorithm and its hyperparameter values. AutoML supports different methods to ensure you have visibility into what was applied to your model.
"featurization": 'auto'
Consider this forecasting example:
There are four input features: A (Numeric), B (Numeric), C (Numeric), D (DateTime).
Numeric feature C is dropped because it's an ID column with all unique values.
Numeric features A and B have missing values and hence are imputed by the mean.
DateTime feature D is featurized into 11 different engineered features.
To get this information, use thefitted_modeloutput from your AutoML experiment run.
fitted_model
automl_config = AutoMLConfig(â¦)
automl_run = experiment.submit(automl_config â¦)
best_run, fitted_model = automl_run.get_output()
automl_config = AutoMLConfig(â¦)
automl_run = experiment.submit(automl_config â¦)
best_run, fitted_model = automl_run.get_output()
Automated feature engineering
Theget_engineered_feature_names()returns a list of engineered feature names.
get_engineered_feature_names()
Note
Use'timeseriestransformer'fortask='forecasting', else use'datatransformer'for'regression'or'classification'task.
'timeseriestransformer'
task='forecasting'
'datatransformer'
'regression'
'classification'
fitted_model.named_steps['timeseriestransformer']. get_engineered_feature_names ()
fitted_model.named_steps['timeseriestransformer']. get_engineered_feature_names ()
This list includes all engineered feature names.
['A', 'B', 'A_WASNULL', 'B_WASNULL', 'year', 'half', 'quarter', 'month', 'day', 'hour', 'am_pm', 'hour12', 'wday', 'qday', 'week']
['A', 'B', 'A_WASNULL', 'B_WASNULL', 'year', 'half', 'quarter', 'month', 'day', 'hour', 'am_pm', 'hour12', 'wday', 'qday', 'week']
Theget_featurization_summary()gets a featurization summary of all the input features.
get_featurization_summary()
fitted_model.named_steps['timeseriestransformer'].get_featurization_summary()
fitted_model.named_steps['timeseriestransformer'].get_featurization_summary()
Output
[{'RawFeatureName': 'A',
  'TypeDetected': 'Numeric',
  'Dropped': 'No',
  'EngineeredFeatureCount': 2,
  'Tranformations': ['MeanImputer', 'ImputationMarker']},
 {'RawFeatureName': 'B',
  'TypeDetected': 'Numeric',
  'Dropped': 'No',
  'EngineeredFeatureCount': 2,
  'Tranformations': ['MeanImputer', 'ImputationMarker']},
 {'RawFeatureName': 'C',
  'TypeDetected': 'Numeric',
  'Dropped': 'Yes',
  'EngineeredFeatureCount': 0,
  'Tranformations': []},
 {'RawFeatureName': 'D',
  'TypeDetected': 'DateTime',
  'Dropped': 'No',
  'EngineeredFeatureCount': 11,
  'Tranformations': ['DateTime','DateTime','DateTime','DateTime','DateTime','DateTime','DateTime','DateTime',ateTime','DateTime','DateTime']}]
[{'RawFeatureName': 'A',
  'TypeDetected': 'Numeric',
  'Dropped': 'No',
  'EngineeredFeatureCount': 2,
  'Tranformations': ['MeanImputer', 'ImputationMarker']},
 {'RawFeatureName': 'B',
  'TypeDetected': 'Numeric',
  'Dropped': 'No',
  'EngineeredFeatureCount': 2,
  'Tranformations': ['MeanImputer', 'ImputationMarker']},
 {'RawFeatureName': 'C',
  'TypeDetected': 'Numeric',
  'Dropped': 'Yes',
  'EngineeredFeatureCount': 0,
  'Tranformations': []},
 {'RawFeatureName': 'D',
  'TypeDetected': 'DateTime',
  'Dropped': 'No',
  'EngineeredFeatureCount': 11,
  'Tranformations': ['DateTime','DateTime','DateTime','DateTime','DateTime','DateTime','DateTime','DateTime',ateTime','DateTime','DateTime']}]
Scaling and normalization
To understand scaling/normalization and the selected algorithm with its hyperparameter values, usefitted_model.steps.
fitted_model.steps
The following sample output is from runningfitted_model.stepsfor a chosen run:
fitted_model.steps
[('RobustScaler', 
  RobustScaler(copy=True, 
  quantile_range=[10, 90], 
  with_centering=True, 
  with_scaling=True)), 

  ('LogisticRegression', 
  LogisticRegression(C=0.18420699693267145, class_weight='balanced', 
  dual=False, 
  fit_intercept=True, 
  intercept_scaling=1, 
  max_iter=100, 
  multi_class='multinomial', 
  n_jobs=1, penalty='l2', 
  random_state=None, 
  solver='newton-cg', 
  tol=0.0001, 
  verbose=0, 
  warm_start=False))]
[('RobustScaler', 
  RobustScaler(copy=True, 
  quantile_range=[10, 90], 
  with_centering=True, 
  with_scaling=True)), 

  ('LogisticRegression', 
  LogisticRegression(C=0.18420699693267145, class_weight='balanced', 
  dual=False, 
  fit_intercept=True, 
  intercept_scaling=1, 
  max_iter=100, 
  multi_class='multinomial', 
  n_jobs=1, penalty='l2', 
  random_state=None, 
  solver='newton-cg', 
  tol=0.0001, 
  verbose=0, 
  warm_start=False))]
To get more details, use this helper function:
from pprint import pprint

def print_model(model, prefix=""):
    for step in model.steps:
        print(prefix + step[0])
        if hasattr(step[1], 'estimators') and hasattr(step[1], 'weights'):
            pprint({'estimators': list(e[0] for e in step[1].estimators), 'weights': step[1].weights})
            print()
            for estimator in step[1].estimators:
                print_model(estimator[1], estimator[0]+ ' - ')
        elif hasattr(step[1], '_base_learners') and hasattr(step[1], '_meta_learner'):
            print("\nMeta Learner")
            pprint(step[1]._meta_learner)
            print()
            for estimator in step[1]._base_learners:
                print_model(estimator[1], estimator[0]+ ' - ')
        else:
            pprint(step[1].get_params())
            print()
from pprint import pprint

def print_model(model, prefix=""):
    for step in model.steps:
        print(prefix + step[0])
        if hasattr(step[1], 'estimators') and hasattr(step[1], 'weights'):
            pprint({'estimators': list(e[0] for e in step[1].estimators), 'weights': step[1].weights})
            print()
            for estimator in step[1].estimators:
                print_model(estimator[1], estimator[0]+ ' - ')
        elif hasattr(step[1], '_base_learners') and hasattr(step[1], '_meta_learner'):
            print("\nMeta Learner")
            pprint(step[1]._meta_learner)
            print()
            for estimator in step[1]._base_learners:
                print_model(estimator[1], estimator[0]+ ' - ')
        else:
            pprint(step[1].get_params())
            print()
This helper function returns the following output for a particular run usingLogisticRegression with RobustScalaras the specific algorithm.
LogisticRegression with RobustScalar
RobustScaler
{'copy': True,
'quantile_range': [10, 90],
'with_centering': True,
'with_scaling': True}

LogisticRegression
{'C': 0.18420699693267145,
'class_weight': 'balanced',
'dual': False,
'fit_intercept': True,
'intercept_scaling': 1,
'max_iter': 100,
'multi_class': 'multinomial',
'n_jobs': 1,
'penalty': 'l2',
'random_state': None,
'solver': 'newton-cg',
'tol': 0.0001,
'verbose': 0,
'warm_start': False}
RobustScaler
{'copy': True,
'quantile_range': [10, 90],
'with_centering': True,
'with_scaling': True}

LogisticRegression
{'C': 0.18420699693267145,
'class_weight': 'balanced',
'dual': False,
'fit_intercept': True,
'intercept_scaling': 1,
'max_iter': 100,
'multi_class': 'multinomial',
'n_jobs': 1,
'penalty': 'l2',
'random_state': None,
'solver': 'newton-cg',
'tol': 0.0001,
'verbose': 0,
'warm_start': False}
Predict class probability
Models produced using AutoML all have wrapper objects that mirror functionality from their open-source origin class. Most classification model wrapper objects returned by AutoML implement thepredict_proba()function, which accepts an array-like or sparse matrix data sample of your features (X values), and returns an n-dimensional array of each sample and its respective class probability.
predict_proba()
Assuming you retrieved the best run and fitted model using the same calls, you can callpredict_proba()directly from the fitted model, supplying anX_testsample in the appropriate format depending on the model type.
predict_proba()
X_test
best_run, fitted_model = automl_run.get_output()
class_prob = fitted_model.predict_proba(X_test)
best_run, fitted_model = automl_run.get_output()
class_prob = fitted_model.predict_proba(X_test)
If the underlying model doesn't support thepredict_proba()function or the format is incorrect, a model class-specific exception is thrown. See theRandomForestClassifierandXGBoostreference docs for examples of how this function is implemented for different model types.
predict_proba()

BERT integration in AutoML
Bidirectional Encoder Representations from Transformers (BERT)is used in the featurization layer of AutoML. In this layer, if a column contains free text or other types of data like timestamps or simple numbers, then featurization is applied accordingly.
For BERT, the model is fine-tuned and trained by utilizing the user-provided labels. From here, document embeddings are output as features alongside others, like timestamp-based features, day of week.
Learn how toSet up AutoML to train a natural language processing model with Python.
Steps to invoke BERT
In order to invoke BERT, setenable_dnn: Truein yourautoml_settingsand use a GPU compute (vm_size = "STANDARD_NC6"or higher GPU). If a CPU compute is used, then instead of BERT, AutoML enables the BiLSTM DNN featurizer.
enable_dnn: True
automl_settings
vm_size = "STANDARD_NC6"
AutoML takes the following steps for BERT.
Preprocesses and tokenizes all text columns. For example, theStringCasttransformer can be found in the final model's featurization summary. An example of how to produce the model's featurization summary can be found in thisJupyter notebook.
Preprocesses and tokenizes all text columns. For example, theStringCasttransformer can be found in the final model's featurization summary. An example of how to produce the model's featurization summary can be found in thisJupyter notebook.
StringCast
Concatenates all text columns into a single text column, hence theStringConcatTransformerin the final model.Our implementation of BERT limits total text length of a training sample to 128 tokens. That means, all text columns when concatenated, should ideally be at most 128 tokens in length. If multiple columns are present, each column should be pruned so this condition is satisfied. Otherwise, for concatenated columns of length >128 tokens BERT's tokenizer layer truncates this input to 128 tokens.
Concatenates all text columns into a single text column, hence theStringConcatTransformerin the final model.
StringConcatTransformer
Our implementation of BERT limits total text length of a training sample to 128 tokens. That means, all text columns when concatenated, should ideally be at most 128 tokens in length. If multiple columns are present, each column should be pruned so this condition is satisfied. Otherwise, for concatenated columns of length >128 tokens BERT's tokenizer layer truncates this input to 128 tokens.
As part of feature sweeping, AutoML compares BERT against the baseline (bag of words features) on a sample of the data.This comparison determines if BERT would give accuracy improvements. If BERT performs better than the baseline, AutoML then uses BERT for text featurization for the whole data. In that case, you see thePretrainedTextDNNTransformerin the final model.
As part of feature sweeping, AutoML compares BERT against the baseline (bag of words features) on a sample of the data.This comparison determines if BERT would give accuracy improvements. If BERT performs better than the baseline, AutoML then uses BERT for text featurization for the whole data. In that case, you see thePretrainedTextDNNTransformerin the final model.
PretrainedTextDNNTransformer
BERT generally runs longer than other featurizers. For better performance, we recommend usingSTANDARD_NC24rorSTANDARD_NC24rs_V3for their RDMA capabilities.
AutoML distributes BERT training across multiple nodes if they're available (up to a max of eight nodes). This can be done in yourAutoMLConfigobject by setting themax_concurrent_iterationsparameter to higher than 1.
AutoMLConfig
max_concurrent_iterations
Supported languages for BERT
AutoML currently supports around 100 languages. Depending on the dataset's language, AutoML chooses the appropriate BERT model. For German data, we use the German BERT model. For English, we use the English BERT model. For all other languages, we use the multilingual BERT model.
In the following code, the German BERT model is triggered because the dataset language is specified todeu, the three-letter language code for German according toISO classification:
deu
from azureml.automl.core.featurization import FeaturizationConfig

featurization_config = FeaturizationConfig(dataset_language='deu')

automl_settings = {
    "experiment_timeout_minutes": 120,
    "primary_metric": 'accuracy',
# All other settings you want to use
    "featurization": featurization_config,
    
    "enable_dnn": True, # This enables BERT DNN featurizer
    "enable_voting_ensemble": False,
    "enable_stack_ensemble": False
}
from azureml.automl.core.featurization import FeaturizationConfig

featurization_config = FeaturizationConfig(dataset_language='deu')

automl_settings = {
    "experiment_timeout_minutes": 120,
    "primary_metric": 'accuracy',
# All other settings you want to use
    "featurization": featurization_config,
    
    "enable_dnn": True, # This enables BERT DNN featurizer
    "enable_voting_ensemble": False,
    "enable_stack_ensemble": False
}
Related content
Deploy machine learning models to Azure
Train a regression model with AutoML and Python
Feedback
Was this page helpful?
Additional resources