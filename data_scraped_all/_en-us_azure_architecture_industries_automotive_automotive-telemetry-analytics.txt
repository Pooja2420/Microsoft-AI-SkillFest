Data analytics for automotive test fleets
Automotive original equipment manufacturers (OEMs) need solutions to minimize the time between test drives and delivering test drive diagnostic data to R&D engineers. As vehicles become more automated, the software development lifecycles become shorter, which requires faster digital feedback loops. New technology can democratize data access and provide R&D engineers with near real-time insights into test drive diagnostic data. Use Copilot for Data Science and Data Engineering for data analytics to further reduce the time to insight. Secure data sharing can enhance collaboration between OEMs and suppliers and reduce development cycle times.
The guidance in this article is for telemetry scenarios and batch test drive data ingestion scenarios. This architecture focuses on the data platform that processes diagnostic data and the connectors for data visualization and data reporting.
Architecture

Download aPowerPoint filewith all the diagrams in this article.
Dataflow
The following dataflow corresponds to the preceding diagram:
The data capture device is connected to the vehicle networks and collects high-resolution vehicle signal data and video. (1a) The device publishes real-time telemetry messages or (1b) requests the upload of recorded data files to the Azure Event Grid MQTT broker functionality by using an MQTT client. This functionality uses a Claim-Check pattern.
The data capture device is connected to the vehicle networks and collects high-resolution vehicle signal data and video. (1a) The device publishes real-time telemetry messages or (1b) requests the upload of recorded data files to the Azure Event Grid MQTT broker functionality by using an MQTT client. This functionality uses a Claim-Check pattern.
(2a) Event Grid routes live vehicle signal data to an Azure Functions app. This app decodes the vehicle signals to the JavaScript Object Notation (JSON) format and posts them to an eventstream.(2b) Event Grid coordinates the file upload from the device client to the lakehouse. A completed file upload triggers a pipeline that decodes the data and writes the decoded file to OneLine in a format that's suitable for ingestion, such as parquet or CSV.
(2a) Event Grid routes live vehicle signal data to an Azure Functions app. This app decodes the vehicle signals to the JavaScript Object Notation (JSON) format and posts them to an eventstream.
(2b) Event Grid coordinates the file upload from the device client to the lakehouse. A completed file upload triggers a pipeline that decodes the data and writes the decoded file to OneLine in a format that's suitable for ingestion, such as parquet or CSV.
(3a) The eventstream routes the decoded JSON vehicle signals for ingestion in the Eventhouse.(3b) A data pipeline triggers the ingestion of decoded files from the lakehouse.
(3a) The eventstream routes the decoded JSON vehicle signals for ingestion in the Eventhouse.
(3b) A data pipeline triggers the ingestion of decoded files from the lakehouse.
The Eventhouse usesupdate policiesto enrich the data and to expand the JSON data into a suitable row format, for example location data might be clustered to align with geospatial analytics. Every time a new row is ingested, the real-time analytics engine invokes an associatedUpdate()function.
The Eventhouse usesupdate policiesto enrich the data and to expand the JSON data into a suitable row format, for example location data might be clustered to align with geospatial analytics. Every time a new row is ingested, the real-time analytics engine invokes an associatedUpdate()function.
Update()
Data engineers and data scientists useKusto Query Language (KQL)to build analytics use cases. Users store frequently used cases as shareable user-defined functions. The engineers use built-in KQL functions such as aggregation, time-series analysis, geospatial clustering, windowing, and machine learning plugins with Copilot support.
Data engineers and data scientists useKusto Query Language (KQL)to build analytics use cases. Users store frequently used cases as shareable user-defined functions. The engineers use built-in KQL functions such as aggregation, time-series analysis, geospatial clustering, windowing, and machine learning plugins with Copilot support.
R&D engineers and data scientists use notebooks to analyze data and build test and validation use cases.R&D engineers useKQL query setsandCopilot for Real-Time Intelligenceto perform interactive data analysis.Data engineers and data scientists usenotebooksto store and share their analysis processes. With notebooks, engineers can use Azure Spark to run analytics and use Git tomanage the notebook code. Users can take advantage ofCopilot for Data Science and Data Engineeringto support their workflow with contextual code suggestions.
R&D engineers and data scientists use notebooks to analyze data and build test and validation use cases.
R&D engineers useKQL query setsandCopilot for Real-Time Intelligenceto perform interactive data analysis.
R&D engineers useKQL query setsandCopilot for Real-Time Intelligenceto perform interactive data analysis.
Data engineers and data scientists usenotebooksto store and share their analysis processes. With notebooks, engineers can use Azure Spark to run analytics and use Git tomanage the notebook code. Users can take advantage ofCopilot for Data Science and Data Engineeringto support their workflow with contextual code suggestions.
Data engineers and data scientists usenotebooksto store and share their analysis processes. With notebooks, engineers can use Azure Spark to run analytics and use Git tomanage the notebook code. Users can take advantage ofCopilot for Data Science and Data Engineeringto support their workflow with contextual code suggestions.
R&D engineers and data scientists can use Power BI with dynamic queries or real-time analytics dashboards to create visualizations to share with business users. These visualizations invoke user-defined functions for ease of maintenance.
R&D engineers and data scientists can use Power BI with dynamic queries or real-time analytics dashboards to create visualizations to share with business users. These visualizations invoke user-defined functions for ease of maintenance.
Engineers can also connect more tools to Microsoft Fabric. For instance, they can connect Azure Managed Grafana to the Eventhouse or create a web application that queries the Eventhouse directly.
Engineers can also connect more tools to Microsoft Fabric. For instance, they can connect Azure Managed Grafana to the Eventhouse or create a web application that queries the Eventhouse directly.
Data engineers and R&D engineers useData Activatorto create reflex items to monitor conditions and trigger actions, such as triggering Power Automate flows for business integration. For example, Data Activator can notify a Teams channel if the health of a device degrades.
Data engineers and R&D engineers useData Activatorto create reflex items to monitor conditions and trigger actions, such as triggering Power Automate flows for business integration. For example, Data Activator can notify a Teams channel if the health of a device degrades.
The data collector configuration enables engineers to change the data collection policies of the data capture device. Azure API Management abstracts and secures the partner configuration API and provides observability.
The data collector configuration enables engineers to change the data collection policies of the data capture device. Azure API Management abstracts and secures the partner configuration API and provides observability.
KQL database schema

Whenyou design the table schema, consider the difference betweenfacttables anddimensiontables. Telemetry is afacttable because vehicle signals are progressively appended in a streaming fashion or as part of a complete recording, and telemetry doesn't change. You can classify fleet metadata as afacttable that updates slowly.
fact
dimension
fact
fact
The vehicle telemetry lands in raw tables. You can use the following message processing concepts to organize the data for analysis and reporting:
Create update policies to expand the JSON telemetry files into individual vehicle signal records by using methods such as:mv-expand()expands complex values that are stored in JSON structures into rows with individual signals.geo_point_to_h3cell()orgeo_point_to_geohash()converts latitude and longitude to geohashes for geospatial analytics.todouble()andtostring()casts extracted values from dynamic JSON objects into the appropriate data types.lookupextends the records with values from a dimension table.
Create update policies to expand the JSON telemetry files into individual vehicle signal records by using methods such as:
mv-expand()expands complex values that are stored in JSON structures into rows with individual signals.
mv-expand()
geo_point_to_h3cell()orgeo_point_to_geohash()converts latitude and longitude to geohashes for geospatial analytics.
geo_point_to_h3cell()
geo_point_to_geohash()
todouble()andtostring()casts extracted values from dynamic JSON objects into the appropriate data types.
todouble()
tostring()
lookupextends the records with values from a dimension table.
lookup
Create aSignals Dedupedmaterialized view by using the aggregation functiontake_any()on the unique key and timestamp. This materialized view deduplicates signals.
Create aSignals Dedupedmaterialized view by using the aggregation functiontake_any()on the unique key and timestamp. This materialized view deduplicates signals.
take_any()
Create aSignals Last Known Valuesmaterialized view by using the aggregation functionarg_max()on the timestamp. This materialized view provides an up-to-date status of the vehicles.
Create aSignals Last Known Valuesmaterialized view by using the aggregation functionarg_max()on the timestamp. This materialized view provides an up-to-date status of the vehicles.
arg_max()
Create aSignals Downsampledmaterialized view by using thesummarize operatorwith time bins such ashourlyanddaily. This materialized view aggregates signals and simplifies reporting across the fleet.
Create aSignals Downsampledmaterialized view by using thesummarize operatorwith time bins such ashourlyanddaily. This materialized view aggregates signals and simplifies reporting across the fleet.
Create user-defined functions that provide anomaly detection or root cause analysis.Use time-series functions foranomaly detection and forecastingto detect potential problems and predict failures.Use thescan operatorto scan, match, and build sequences from the data. Engineers can use thescanoperator to detect sequences. For example, if a specific event occurs, then a subsequent event must occur within a certain amount of time.Use machine learning plugins likeautoclusterto find common patterns of discrete attributes.
Create user-defined functions that provide anomaly detection or root cause analysis.
Use time-series functions foranomaly detection and forecastingto detect potential problems and predict failures.
Use time-series functions foranomaly detection and forecastingto detect potential problems and predict failures.
Use thescan operatorto scan, match, and build sequences from the data. Engineers can use thescanoperator to detect sequences. For example, if a specific event occurs, then a subsequent event must occur within a certain amount of time.
Use thescan operatorto scan, match, and build sequences from the data. Engineers can use thescanoperator to detect sequences. For example, if a specific event occurs, then a subsequent event must occur within a certain amount of time.
scan
Use machine learning plugins likeautoclusterto find common patterns of discrete attributes.
Use machine learning plugins likeautoclusterto find common patterns of discrete attributes.
Perform geospatial analytics with user-defined functions. Use thegeospatial analyticsfunctions to convert coordinates to a suitable grid system and perform aggregations on the data.
Perform geospatial analytics with user-defined functions. Use thegeospatial analyticsfunctions to convert coordinates to a suitable grid system and perform aggregations on the data.
Create afleet metadata tableto store changes on the vehicle metadata and configuration. Create afleet metadata last known valuesmaterialized view to store the latest state of the vehicle fleet based on a last-time modified column.
Create afleet metadata tableto store changes on the vehicle metadata and configuration. Create afleet metadata last known valuesmaterialized view to store the latest state of the vehicle fleet based on a last-time modified column.
Components
The following key technologies implement this workload. For each component in the architecture, use the relevant service guide in the Well-Architected Framework where available. For more information, seeWell-Architected Framework service guides.
Fabric Real-Time Intelligenceenables extraction of insights and visualization of vehicle telemetry in motion. You can use eventstreams and time-series KQL databases to store and analyze data and use reflexes to react to events.
Fabric Real-Time Intelligenceenables extraction of insights and visualization of vehicle telemetry in motion. You can use eventstreams and time-series KQL databases to store and analyze data and use reflexes to react to events.
Data Activatoris a no-code tool that you can use to automate actions when patterns or conditions change in data.
Data Activatoris a no-code tool that you can use to automate actions when patterns or conditions change in data.
Event Gridis a highly scalable, fully managed Publish/Subscribe message distribution service that supports MQTT protocols. Vehicles can use Event Grid to publish and subscribe to topics, for example they can publish telemetry and subscribe to command and control messages.
Event Gridis a highly scalable, fully managed Publish/Subscribe message distribution service that supports MQTT protocols. Vehicles can use Event Grid to publish and subscribe to topics, for example they can publish telemetry and subscribe to command and control messages.
Azure Event Hubsis a real-time data streaming platform that's well-suited for streaming millions of vehicle events per second with low latency.
Azure Event Hubsis a real-time data streaming platform that's well-suited for streaming millions of vehicle events per second with low latency.
Functionsis a serverless solution that simplifies processing vehicle telemetry events at scale with event-driven triggers and bindings by using the language of your choice.
Functionsis a serverless solution that simplifies processing vehicle telemetry events at scale with event-driven triggers and bindings by using the language of your choice.
Azure Managed Grafanais a data visualization platform that's based on the software from Grafana Labs. Microsoft manages and supports Azure Managed Grafana.
Azure Managed Grafanais a data visualization platform that's based on the software from Grafana Labs. Microsoft manages and supports Azure Managed Grafana.
Azure App Serviceenables you to build and host web apps, mobile back ends, and RESTful APIs that provide access to the vehicle telemetry data that's stored in Fabric. This approach simplifies consumption.
Azure App Serviceenables you to build and host web apps, mobile back ends, and RESTful APIs that provide access to the vehicle telemetry data that's stored in Fabric. This approach simplifies consumption.
API Managementis a hybrid multicloud management platform for APIs.
API Managementis a hybrid multicloud management platform for APIs.
Alternatives
You can also use the following Azure services to implement this architecture:
Azure Blob Storagestores massive amounts of unstructured data, such as recordings, logs, and videos from the vehicles. It replaces OneLake storage.
Azure Blob Storagestores massive amounts of unstructured data, such as recordings, logs, and videos from the vehicles. It replaces OneLake storage.
Azure Data Exploreris a fast, fully managed data analytics service for real-time analysis. It replaces the Fabric Real-Time Intelligence KQL database.
Azure Data Exploreris a fast, fully managed data analytics service for real-time analysis. It replaces the Fabric Real-Time Intelligence KQL database.
Azure Batchis an alternative that you can use to decode complex files. This scenario involves a large number of files that are over 300 megabytes each. The files require different decoding algorithms based on the file version or the file type. You can use either Fabric or use Blob Storage and Azure Data Explorer to implement the following approach.
Azure Batchis an alternative that you can use to decode complex files. This scenario involves a large number of files that are over 300 megabytes each. The files require different decoding algorithms based on the file version or the file type. You can use either Fabric or use Blob Storage and Azure Data Explorer to implement the following approach.

The user or recording device uploads a recorded data file to the lakehouse. When the upload finishes, it triggers a Functions app that schedules decoding.
The user or recording device uploads a recorded data file to the lakehouse. When the upload finishes, it triggers a Functions app that schedules decoding.
The scheduler starts a Functions app that creates a batch job based on the file type, file size, and required decoding algorithm. The app selects a virtual machine with a suitable size from the pool and starts the job.
The scheduler starts a Functions app that creates a batch job based on the file type, file size, and required decoding algorithm. The app selects a virtual machine with a suitable size from the pool and starts the job.
Batch writes the resulting decoded file back to the lakehouse when the job finishes. This file must be suitable for direct ingestion in a format that the Eventhouse supports.
Batch writes the resulting decoded file back to the lakehouse when the job finishes. This file must be suitable for direct ingestion in a format that the Eventhouse supports.
The lakehouse triggers a function that ingests the data into the Eventhouse upon file write. This function creates the table and data mapping if necessary and starts the ingestion process.
The lakehouse triggers a function that ingests the data into the Eventhouse upon file write. This function creates the table and data mapping if necessary and starts the ingestion process.
The KQL database ingests the data files from the lakehouse.
The KQL database ingests the data files from the lakehouse.
This approach provides the following benefits:
Functions and Batch pools can handle scalable data processing tasks robustly and efficiently.
Functions and Batch pools can handle scalable data processing tasks robustly and efficiently.
Batch pools provide insight into processing statistics, task queues, and batch pool health. You can visualize status, detect problems, and rerun failed tasks.
Batch pools provide insight into processing statistics, task queues, and batch pool health. You can visualize status, detect problems, and rerun failed tasks.
The combination of Functions and Batch supports plug-and-play processing in Docker containers.
The combination of Functions and Batch supports plug-and-play processing in Docker containers.
You can usespot virtual machinesto process files during off-peak times. This approach saves money.
You can usespot virtual machinesto process files during off-peak times. This approach saves money.
Scenario details
Automotive OEMs use large fleets of prototype and test vehicles to test and verify several vehicle functions. Test procedures are expensive because they require real drivers and vehicles, and specific real-world road testing scenarios must pass multiple times. Integration testing is especially important to evaluate interactions between electrical, electronic, and mechanical components in complex systems.
To validate vehicle functions and analyze anomalies and failures, you must capture petabytes of diagnostic data from electronic control units (ECUs), computer nodes, vehicle communication buses like Controller Area Network (CAN) and Ethernet, and sensors.
In the past, small data logger servers in the vehicles stored diagnostic data locally as Measurement Data Format (MDF), multimedia fusion extension (MFX), CSV, or JSON files. After test drives were complete, the servers uploaded diagnostic data to datacenters, which processed it and sent it to R&D engineers for analytics. This process could take hours or sometimes days. More recent scenarios use telemetry ingestion patterns like Message Queuing Telemetry Transport (MQTT)-based synchronous data streams or near real-time file uploads.
Potential use cases
Vehicle management evaluates the performance and collected data per vehicle across multiple test scenarios.
Vehicle management evaluates the performance and collected data per vehicle across multiple test scenarios.
System and component validation uses collected vehicle data to verify that the behavior of vehicle components falls within operational boundaries across trips.
System and component validation uses collected vehicle data to verify that the behavior of vehicle components falls within operational boundaries across trips.
Anomaly detection locates deviation patterns of a sensor value relative to its typical baseline pattern in real time.
Anomaly detection locates deviation patterns of a sensor value relative to its typical baseline pattern in real time.
Root cause analysis uses machine learning plugins such as clustering algorithms to identify changes in the distribution of values on multiple dimensions.
Root cause analysis uses machine learning plugins such as clustering algorithms to identify changes in the distribution of values on multiple dimensions.
Predictive maintenance combines multiple data sources, enriched location data, and vehicle signals to predict component time to failure.
Predictive maintenance combines multiple data sources, enriched location data, and vehicle signals to predict component time to failure.
Sustainability evaluation uses driver behavior and energy consumption to evaluate the environmental impact of vehicle operations.
Sustainability evaluation uses driver behavior and energy consumption to evaluate the environmental impact of vehicle operations.
Automotive racing to understand and improve the performance of the vehicles before, during, and after a race.
Automotive racing to understand and improve the performance of the vehicles before, during, and after a race.
Considerations
These considerations implement the pillars of the Azure Well-Architected Framework, which is a set of guiding tenets that can be used to improve the quality of a workload. For more information, seeMicrosoft Azure Well-Architected Framework.
Reliability
Reliability ensures your application can meet the commitments you make to your customers. For more information, seeDesign review checklist for Reliability.
Azure availability zonesare unique physical locations within the same Azure region. Availability zones can protect Azure Data Explorer compute clusters and data from partial region failure.
Azure availability zonesare unique physical locations within the same Azure region. Availability zones can protect Azure Data Explorer compute clusters and data from partial region failure.
Business continuity and disaster recovery (BCDR)in Azure Data Explorer lets your business continue operating in the face of disruption.
Business continuity and disaster recovery (BCDR)in Azure Data Explorer lets your business continue operating in the face of disruption.
Follower databasesseparate compute resources between production and nonproduction use cases.
Follower databasesseparate compute resources between production and nonproduction use cases.
Security
Security provides assurances against deliberate attacks and the abuse of your valuable data and systems. For more information, seeDesign review checklist for Security.
It's important to understand the division of responsibility between the automotive OEM and Microsoft. In the vehicle, the OEM owns the whole stack, but as the data moves to the cloud, some responsibilities transfer to Microsoft. Azure platform as a service (PaaS) provides built-in security on the physical stack, including the operating system.
UseAzure Policyto apply security guardrails.
UseAzure Policyto apply security guardrails.
Review thegovernance overview and guidancefor Fabric.
Review thegovernance overview and guidancefor Fabric.
Use private endpoints to provide network security for all services.Useprivate endpoints for Azure Data Explorer.Allow access to Event Hubs namespaces through private endpoints.
Use private endpoints to provide network security for all services.
Useprivate endpoints for Azure Data Explorer.
Useprivate endpoints for Azure Data Explorer.
Allow access to Event Hubs namespaces through private endpoints.
Allow access to Event Hubs namespaces through private endpoints.
Encrypt data at rest and data in transit.
Encrypt data at rest and data in transit.
Use Microsoft Entra identities andMicrosoft Entra Conditional Accesspolicies.
Use Microsoft Entra identities andMicrosoft Entra Conditional Accesspolicies.
Userow level security (RLS)for KQL databases and Azure Data Explorer.
Userow level security (RLS)for KQL databases and Azure Data Explorer.
Use therestrict statementwhen you implement middleware applications with access to the KQL database. This configuration creates a logical model that restricts user access to the data.
Use therestrict statementwhen you implement middleware applications with access to the KQL database. This configuration creates a logical model that restricts user access to the data.
All these features help automotive OEMs create a secure environment for their vehicle telemetry data. For more information, seeSecurity in Fabric.
Cost Optimization
Cost Optimization is about looking at ways to reduce unnecessary expenses and improve operational efficiencies. For more information, seeDesign review checklist for Cost Optimization.
This solution uses the following practices to help optimize costs:
Correctly configure hot caches and cold storage for the raw and signals tables. The hot data cache is stored in RAM or SSD and provides improved performance. Cold data, however, is 45 times cheaper. Set a hot cache policy that's adequate for your use case, such as 30 days.
Correctly configure hot caches and cold storage for the raw and signals tables. The hot data cache is stored in RAM or SSD and provides improved performance. Cold data, however, is 45 times cheaper. Set a hot cache policy that's adequate for your use case, such as 30 days.
Set up a retention policy on the raw table and signals table. Determine when the signal data is no longer relevant, such as after 365 days, and set the retention policy accordingly.
Set up a retention policy on the raw table and signals table. Determine when the signal data is no longer relevant, such as after 365 days, and set the retention policy accordingly.
Consider which signals are relevant for analysis.
Consider which signals are relevant for analysis.
Use materialized views when you query the signals last-known values, signals deduped, and signals downsampled. Materialized views consume fewer resources than doing source table aggregations on each query.
Use materialized views when you query the signals last-known values, signals deduped, and signals downsampled. Materialized views consume fewer resources than doing source table aggregations on each query.
Consider your real-time data analytics needs. Set up streaming ingestion for the live telemetry table to provide latency of less than one second between ingestion and query. This approach increases CPU cycles and cost.
Consider your real-time data analytics needs. Set up streaming ingestion for the live telemetry table to provide latency of less than one second between ingestion and query. This approach increases CPU cycles and cost.
Performance Efficiency
Performance Efficiency is the ability of your workload to scale to meet the demands placed on it by users in an efficient manner. For more information, seeDesign review checklist for Performance Efficiency.
Consider using Batch to perform decoding if the number and size of recorded data files is more than 1,000 files or 300 MB per day.
Consider using Batch to perform decoding if the number and size of recorded data files is more than 1,000 files or 300 MB per day.
Consider performing common calculations and analysis after ingest and storing them in extra tables.
Consider performing common calculations and analysis after ingest and storing them in extra tables.
UseKQL query best practicesto make your query run faster.
UseKQL query best practicesto make your query run faster.
Use awhereclause to define a time window to reduce the amount of data that's queried. Consider changing the data partition policy for the signals table if your common search criteria aren't time-based, for instance if you filter by recording ID and signal name. When the KQL database expands to contain billions or trillions of records, proper data filtration becomes essential, especially considering the activepartition policy.
Use awhereclause to define a time window to reduce the amount of data that's queried. Consider changing the data partition policy for the signals table if your common search criteria aren't time-based, for instance if you filter by recording ID and signal name. When the KQL database expands to contain billions or trillions of records, proper data filtration becomes essential, especially considering the activepartition policy.
where
Warning
Consult with your support team before you alter a data partition policy.
Deploy this scenario
Use thestep-by-step tutorialto deploy this scenario. The guide shows how to deploy a free instance, parse MDF files, ingest data, and perform several basic queries.
Contributors
This article is maintained by Microsoft. It was originally written by the following contributors.
Principal authors:
Boris Scholl| Partner, Chief Architect
Frank Kaleck| Industry Advisor Automotive
Henning Rauch| Principal Program Manager
Mario Ortegon-Cabrera| Principal Program Manager
Other contributors:
Devang Shah| Principal Program Manager
Hans-Peter Bareiner| Cloud Solution Architect
Jason Bouska| Sr. Software Engineer
To see non-public LinkedIn profiles, sign in to LinkedIn.
Next steps
MQTT broker feature in Event Grid
Add a KQL database destination to an eventstream
Get data from OneLake
Materialized views
Create a real-time dashboard
Create Data Activator alerts from a real-time dashboard
Power BI report
Visualize data from Azure Data Explorer in Grafana
Automotive messaging, data, and analytics reference architecture
Related resources
Software-defined vehicle DevOps toolchain
Reference architecture for autonomous vehicle operations (AVOps)
Claim-Check pattern