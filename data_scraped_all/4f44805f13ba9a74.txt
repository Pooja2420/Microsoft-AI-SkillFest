Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Stream processing with Apache Kafka and Azure Databricks
Article
2025-02-14
2 contributors
In this article
This article describes how you can use Apache Kafka as either a source or a sink when running Structured Streaming workloads on Azure Databricks.
For more Kafka, see theKafka documentation.
Read data from Kafka
The following is an example for a streaming read from Kafka:
df = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "<server:ip>")
  .option("subscribe", "<topic>")
  .option("startingOffsets", "latest")
  .load()
)
df = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "<server:ip>")
  .option("subscribe", "<topic>")
  .option("startingOffsets", "latest")
  .load()
)
Azure Databricks also supports batch read semantics for Kafka data sources, as shown in the following example:
df = (spark
  .read
  .format("kafka")
  .option("kafka.bootstrap.servers", "<server:ip>")
  .option("subscribe", "<topic>")
  .option("startingOffsets", "earliest")
  .option("endingOffsets", "latest")
  .load()
)
df = (spark
  .read
  .format("kafka")
  .option("kafka.bootstrap.servers", "<server:ip>")
  .option("subscribe", "<topic>")
  .option("startingOffsets", "earliest")
  .option("endingOffsets", "latest")
  .load()
)
For incremental batch loading, Databricks recommends using Kafka withTrigger.AvailableNow. SeeConfiguring incremental batch processing.
Trigger.AvailableNow
In Databricks Runtime 13.3 LTS and above, Azure Databricks provides a SQL function for reading Kafka data. Streaming with SQL is supported only in DLT or with streaming tables in Databricks SQL. Seeread_kafkatable-valued function.
read_kafka
Configure Kafka Structured Streaming reader
Azure Databricks provides thekafkakeyword as a data format to configure connections to Kafka 0.10+.
kafka
The following are the most common configurations for Kafka:
There are multiple ways of specifying which topics to subscribe to. You should provide only one of these parameters:
{"topicA":[0,1],"topic":[2,4]}
Other notable configurations:
bootstrap.servers
true
false
true
false
minPartitions
minPartitions
Concurrently running queries (both, batch and streaming) with the same group ID are likely interfere with each other causing each query to read only part of the data.
This may also occur when queries are started/restarted in quick succession. To minimize such issues, set the Kafka consumer configurationsession.timeout.msto be very small.
session.timeout.ms
SeeStructured Streaming Kafka Integration Guidefor other optional configurations.
Schema for Kafka records
The schema of Kafka records is:
Thekeyand thevalueare always deserialized as byte arrays with theByteArrayDeserializer. Use DataFrame operations (such ascast("string")) to explicitly deserialize the keys and values.
key
value
ByteArrayDeserializer
cast("string")
Write data to Kafka
The following is an example for a streaming write to Kafka:
(df
  .writeStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "<server:ip>")
  .option("topic", "<topic>")
  .start()
)
(df
  .writeStream
  .format("kafka")
  .option("kafka.bootstrap.servers", "<server:ip>")
  .option("topic", "<topic>")
  .start()
)
Azure Databricks also supports batch write semantics to Kafka data sinks, as shown in the following example:
(df
  .write
  .format("kafka")
  .option("kafka.bootstrap.servers", "<server:ip>")
  .option("topic", "<topic>")
  .save()
)
(df
  .write
  .format("kafka")
  .option("kafka.bootstrap.servers", "<server:ip>")
  .option("topic", "<topic>")
  .save()
)
Configure the Kafka Structured Streaming writer
Important
Databricks Runtime 13.3 LTS and above includes a newer version of thekafka-clientslibrary that enables idempotent writes by default. If a Kafka sink uses version 2.8.0 or below with ACLs configured, but withoutIDEMPOTENT_WRITEenabled, the write fails with the error messageorg.apache.kafka.common.KafkaException:Cannot execute transactional method because we are in an error state.
kafka-clients
IDEMPOTENT_WRITE
org.apache.kafka.common.KafkaException:
Cannot execute transactional method because we are in an error state
Resolve this error by upgrading to Kafka version 2.8.0 or above, or by setting.option(âkafka.enable.idempotenceâ, âfalseâ)while configuring your Structured Streaming writer.
.option(âkafka.enable.idempotenceâ, âfalseâ)
The schema provided to the DataStreamWriter interacts with the Kafka sink. You can use the following fields:
key
STRING
BINARY
value
STRING
BINARY
headers
ARRAY
topic
topic
STRING
partition
INT
The following are common options set while writing to Kafka:
kafka.boostrap.servers
<host:port>
bootstrap.servers
topic
STRING
includeHeaders
BOOLEAN
false
SeeStructured Streaming Kafka Integration Guidefor other optional configurations.
Retrieve Kafka metrics
You can get the average, min, and max of the number of offsets that the streaming query is behind the latest available offset among all the subscribed topics with theavgOffsetsBehindLatest,maxOffsetsBehindLatest, andminOffsetsBehindLatestmetrics. SeeReading Metrics Interactively.
avgOffsetsBehindLatest
maxOffsetsBehindLatest
minOffsetsBehindLatest
Note
Available in Databricks Runtime 9.1 and above.
Get the estimated total number of bytes that the query process has not consumed from the subscribed topics by examining the value ofestimatedTotalBytesBehindLatest. This estimate is based on the batches that were processed in the last 300 seconds. The timeframe that the estimate is based on can be changed by setting the optionbytesEstimateWindowLengthto a different value. For example, to set it to 10 minutes:
estimatedTotalBytesBehindLatest
bytesEstimateWindowLength
df = (spark.readStream
  .format("kafka")
  .option("bytesEstimateWindowLength", "10m") # m for minutes, you can also use "600s" for 600 seconds
)
df = (spark.readStream
  .format("kafka")
  .option("bytesEstimateWindowLength", "10m") # m for minutes, you can also use "600s" for 600 seconds
)
If you are running the stream in a notebook, you can see these metrics under theRaw Datatab in the streaming query progress dashboard:
{
  "sources": [
    {
      "description": "KafkaV2[Subscribe[topic]]",
      "metrics": {
        "avgOffsetsBehindLatest": "4.0",
        "maxOffsetsBehindLatest": "4",
        "minOffsetsBehindLatest": "4",
        "estimatedTotalBytesBehindLatest": "80.0"
      }
    }
  ]
}
{
  "sources": [
    {
      "description": "KafkaV2[Subscribe[topic]]",
      "metrics": {
        "avgOffsetsBehindLatest": "4.0",
        "maxOffsetsBehindLatest": "4",
        "minOffsetsBehindLatest": "4",
        "estimatedTotalBytesBehindLatest": "80.0"
      }
    }
  ]
}
Use SSL to connect Azure Databricks to Kafka
To enable SSL connections to Kafka, follow the instructions in the Confluent documentationEncryption and Authentication with SSL. You can provide the configurations described there, prefixed withkafka., as options. For example, you specify the trust store location in the propertykafka.ssl.truststore.location.
kafka.
kafka.ssl.truststore.location
Databricks recommends that you:
Store your certificates in cloud object storage. You can restrict access to the certificates only to clusters that can access Kafka. SeeData governance with Unity Catalog.
Store your certificate passwords assecretsin asecret scope.
The following example uses object storage locations and Databricks secrets to enable an SSL connection:
df = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", ...)
  .option("kafka.security.protocol", "SASL_SSL")
  .option("kafka.ssl.truststore.location", <truststore-location>)
  .option("kafka.ssl.keystore.location", <keystore-location>)
  .option("kafka.ssl.keystore.password", dbutils.secrets.get(scope=<certificate-scope-name>,key=<keystore-password-key-name>))
  .option("kafka.ssl.truststore.password", dbutils.secrets.get(scope=<certificate-scope-name>,key=<truststore-password-key-name>))
)
df = (spark.readStream
  .format("kafka")
  .option("kafka.bootstrap.servers", ...)
  .option("kafka.security.protocol", "SASL_SSL")
  .option("kafka.ssl.truststore.location", <truststore-location>)
  .option("kafka.ssl.keystore.location", <keystore-location>)
  .option("kafka.ssl.keystore.password", dbutils.secrets.get(scope=<certificate-scope-name>,key=<keystore-password-key-name>))
  .option("kafka.ssl.truststore.password", dbutils.secrets.get(scope=<certificate-scope-name>,key=<truststore-password-key-name>))
)
Connect Kafka on HDInsight to Azure Databricks
Create an HDInsight Kafka cluster.SeeConnect to Kafka on HDInsight through an Azure Virtual Networkfor instructions.
Create an HDInsight Kafka cluster.
SeeConnect to Kafka on HDInsight through an Azure Virtual Networkfor instructions.
Configure the Kafka brokers to advertise the correct address.Follow the instructions inConfigure Kafka for IP advertising. If you manage Kafka yourself on Azure Virtual Machines, make sure that theadvertised.listenersconfiguration of the brokers is set to the internal IP of the hosts.
Configure the Kafka brokers to advertise the correct address.
Follow the instructions inConfigure Kafka for IP advertising. If you manage Kafka yourself on Azure Virtual Machines, make sure that theadvertised.listenersconfiguration of the brokers is set to the internal IP of the hosts.
advertised.listeners
Create an Azure Databricks cluster.
Create an Azure Databricks cluster.
Peer the Kafka cluster to the Azure Databricks cluster.Follow the instructions inPeer virtual networks.
Peer the Kafka cluster to the Azure Databricks cluster.
Follow the instructions inPeer virtual networks.
Service Principal authentication with Microsoft Entra ID and Azure Event Hubs
Azure Databricks supports the authentication of Spark jobs with Event Hubs services. This authentication is done via OAuth with Microsoft Entra ID.

Azure Databricks supports Microsoft Entra ID authentication with a client ID and secret in the following compute environments:
Databricks Runtime 12.2 LTS and above on compute configured with dedicated access mode (formerly single user access mode).
Databricks Runtime 14.3 LTS and above on compute configured with standard access mode (formerly shared access mode).
DLT pipelines configured without Unity Catalog.
Azure Databricks does not support Microsoft Entra ID authentication with a certificate in any compute environment, or in DLT pipelines configured with Unity Catalog.
This authentication does not work on compute with standard access mode or on Unity Catalog DLT.
Configuring the Structured Streaming Kafka Connector
To perform authentication with Microsoft Entra ID, youâll need the following values:
A tenant ID. You can find this in theMicrosoft Entra IDservices tab.
A tenant ID. You can find this in theMicrosoft Entra IDservices tab.
A clientID (also known as Application ID).
A clientID (also known as Application ID).
A client secret. Once you have this, you should add it as a secret to your Databricks Workspace. To add this secret, seeSecret management.
A client secret. Once you have this, you should add it as a secret to your Databricks Workspace. To add this secret, seeSecret management.
An EventHubs topic. You can find a list of topics in theEvent Hubssection under theEntitiessection on a specificEvent Hubs Namespacepage. To work with multiple topics, you can set the IAM role at the Event Hubs level.
An EventHubs topic. You can find a list of topics in theEvent Hubssection under theEntitiessection on a specificEvent Hubs Namespacepage. To work with multiple topics, you can set the IAM role at the Event Hubs level.
An EventHubs server. You can find this on the overview page of your specificEvent Hubs namespace:
An EventHubs server. You can find this on the overview page of your specificEvent Hubs namespace:

Additionally, to use Entra ID, we need to tell Kafka to use the OAuth SASL mechanism (SASL is a generic protocol, and OAuth is a type of SASL âmechanismâ):
kafka.security.protocolshould beSASL_SSL
kafka.security.protocol
SASL_SSL
kafka.sasl.mechanismshould beOAUTHBEARER
kafka.sasl.mechanism
OAUTHBEARER
kafka.sasl.login.callback.handler.classshould be a fully qualified name of the Java class with a value ofkafkashadedto the login callback handler of our shaded Kafka class. See the following example for the exact class.
kafka.sasl.login.callback.handler.class
kafkashaded
Example
Next, letâs look at a running example:
# This is the only section you need to modify for auth purposes!
# ------------------------------
tenant_id = "..."
client_id = "..."
client_secret = dbutils.secrets.get("your-scope", "your-secret-name")

event_hubs_server = "..."
event_hubs_topic = "..."
# -------------------------------

sasl_config = f'kafkashaded.org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required clientId="{client_id}" clientSecret="{client_secret}" scope="https://{event_hubs_server}/.default" ssl.protocol="SSL";'

kafka_options = {
# Port 9093 is the EventHubs Kafka port
"kafka.bootstrap.servers": f"{event_hubs_server}:9093",
"kafka.sasl.jaas.config": sasl_config,
"kafka.sasl.oauthbearer.token.endpoint.url": f"https://login.microsoft.com/{tenant_id}/oauth2/v2.0/token",
"subscribe": event_hubs_topic,

# You should not need to modify these
"kafka.security.protocol": "SASL_SSL",
"kafka.sasl.mechanism": "OAUTHBEARER",
"kafka.sasl.login.callback.handler.class": "kafkashaded.org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerLoginCallbackHandler"
}

df = spark.readStream.format("kafka").options(**kafka_options)

display(df)
# This is the only section you need to modify for auth purposes!
# ------------------------------
tenant_id = "..."
client_id = "..."
client_secret = dbutils.secrets.get("your-scope", "your-secret-name")

event_hubs_server = "..."
event_hubs_topic = "..."
# -------------------------------

sasl_config = f'kafkashaded.org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required clientId="{client_id}" clientSecret="{client_secret}" scope="https://{event_hubs_server}/.default" ssl.protocol="SSL";'

kafka_options = {
# Port 9093 is the EventHubs Kafka port
"kafka.bootstrap.servers": f"{event_hubs_server}:9093",
"kafka.sasl.jaas.config": sasl_config,
"kafka.sasl.oauthbearer.token.endpoint.url": f"https://login.microsoft.com/{tenant_id}/oauth2/v2.0/token",
"subscribe": event_hubs_topic,

# You should not need to modify these
"kafka.security.protocol": "SASL_SSL",
"kafka.sasl.mechanism": "OAUTHBEARER",
"kafka.sasl.login.callback.handler.class": "kafkashaded.org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerLoginCallbackHandler"
}

df = spark.readStream.format("kafka").options(**kafka_options)

display(df)
// This is the only section you need to modify for auth purposes!
// -------------------------------
val tenantId = "..."
val clientId = "..."
val clientSecret = dbutils.secrets.get("your-scope", "your-secret-name")

val eventHubsServer = "..."
val eventHubsTopic = "..."
// -------------------------------

val saslConfig = s"""kafkashaded.org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required clientId="$clientId" clientSecret="$clientSecret" scope="https://$eventHubsServer/.default" ssl.protocol="SSL";"""

val kafkaOptions = Map(
// Port 9093 is the EventHubs Kafka port
"kafka.bootstrap.servers" -> s"$eventHubsServer:9093",
"kafka.sasl.jaas.config" -> saslConfig,
"kafka.sasl.oauthbearer.token.endpoint.url" -> s"https://login.microsoft.com/$tenantId/oauth2/v2.0/token",
"subscribe" -> eventHubsTopic,

// You should not need to modify these
"kafka.security.protocol" -> "SASL_SSL",
"kafka.sasl.mechanism" -> "OAUTHBEARER",
"kafka.sasl.login.callback.handler.class" -> "kafkashaded.org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerLoginCallbackHandler"
)

val scalaDF = spark.readStream
  .format("kafka")
  .options(kafkaOptions)
  .load()

display(scalaDF)
// This is the only section you need to modify for auth purposes!
// -------------------------------
val tenantId = "..."
val clientId = "..."
val clientSecret = dbutils.secrets.get("your-scope", "your-secret-name")

val eventHubsServer = "..."
val eventHubsTopic = "..."
// -------------------------------

val saslConfig = s"""kafkashaded.org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required clientId="$clientId" clientSecret="$clientSecret" scope="https://$eventHubsServer/.default" ssl.protocol="SSL";"""

val kafkaOptions = Map(
// Port 9093 is the EventHubs Kafka port
"kafka.bootstrap.servers" -> s"$eventHubsServer:9093",
"kafka.sasl.jaas.config" -> saslConfig,
"kafka.sasl.oauthbearer.token.endpoint.url" -> s"https://login.microsoft.com/$tenantId/oauth2/v2.0/token",
"subscribe" -> eventHubsTopic,

// You should not need to modify these
"kafka.security.protocol" -> "SASL_SSL",
"kafka.sasl.mechanism" -> "OAUTHBEARER",
"kafka.sasl.login.callback.handler.class" -> "kafkashaded.org.apache.kafka.common.security.oauthbearer.secured.OAuthBearerLoginCallbackHandler"
)

val scalaDF = spark.readStream
  .format("kafka")
  .options(kafkaOptions)
  .load()

display(scalaDF)
Handling potential errors
Streaming options are not supported.If you try to use this authentication mechanism in a DLT pipeline configured with Unity Catalog you might receive the following error:To resolve this error, use a supported compute configuration. SeeService Principal authentication with Microsoft Entra ID and Azure Event Hubs.
Streaming options are not supported.
If you try to use this authentication mechanism in a DLT pipeline configured with Unity Catalog you might receive the following error:

To resolve this error, use a supported compute configuration. SeeService Principal authentication with Microsoft Entra ID and Azure Event Hubs.
Failed to create a newKafkaAdminClient.This is an internal error that Kafka throws if any of the following authentication options are incorrect:Client ID (also known as Application ID)Tenant IDEventHubs serverTo resolve the error, verify that the values are correct for these options.Additionally, you might see this error if you modify the configuration options provided by default in the example (that you were asked not to modify), such askafka.security.protocol.
Failed to create a newKafkaAdminClient.
KafkaAdminClient
This is an internal error that Kafka throws if any of the following authentication options are incorrect:
Client ID (also known as Application ID)
Tenant ID
EventHubs server
To resolve the error, verify that the values are correct for these options.
Additionally, you might see this error if you modify the configuration options provided by default in the example (that you were asked not to modify), such askafka.security.protocol.
kafka.security.protocol
There are no records being returnedIf you are trying to display or process your DataFrame but arenât getting results, you will see the following in the UI.This message means that authentication was successful, but EventHubs didnât return any data. Some possible (though by no means exhaustive) reasons are:You specified the wrongEventHubstopic.The default Kafka configuration option forstartingOffsetsislatest, and youâre not currently receiving any data through the topic yet. You can setstartingOffsetstoearliestto start reading data starting from Kafkaâs earliest offsets.
There are no records being returned
If you are trying to display or process your DataFrame but arenât getting results, you will see the following in the UI.

This message means that authentication was successful, but EventHubs didnât return any data. Some possible (though by no means exhaustive) reasons are:
You specified the wrongEventHubstopic.
The default Kafka configuration option forstartingOffsetsislatest, and youâre not currently receiving any data through the topic yet. You can setstartingOffsetstoearliestto start reading data starting from Kafkaâs earliest offsets.
startingOffsets
latest
startingOffsetstoearliest
Feedback
Was this page helpful?
Additional resources