Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Get data from Azure storage
Article
2025-01-06
6 contributors
In this article
Data ingestion is the process used to load data from one or more sources into a table in Azure Data Explorer. Once ingested, the data becomes available for query. In this article, you learn how to get data from Azure storage (ADLS Gen2 container, blob container, or individual blobs) into either a new or existing table.
Ingestion from an Azure storage account is a one-time operation. To ingest data continuously, seeConfigure streaming ingestion.
For general information on data ingestion, seeAzure Data Explorer data ingestion overview.
Prerequisites
A Microsoft account or a Microsoft Entra user identity. An Azure subscription isn't required.
Sign-in to theAzure Data Explorer web UI.
An Azure Data Explorer cluster and database.Create a cluster and database.
Astorage account.
Get data
From the left menu, selectQuery.
From the left menu, selectQuery.
Right-click on the database where you want to ingest the data. SelectGet data.
Right-click on the database where you want to ingest the data. SelectGet data.

Source
In theGet datawindow, theSourcetab is selected.
Select the data source from the available list. In this example, you're ingesting data fromAzure storage.

Configure
Select a target database and table. If you want to ingest data into a new table, select+ New tableand enter a table name.NoteTable names can be up to 1024 characters including spaces, alphanumeric, hyphens, and underscores. Special characters aren't supported.
Select a target database and table. If you want to ingest data into a new table, select+ New tableand enter a table name.
Note
Table names can be up to 1024 characters including spaces, alphanumeric, hyphens, and underscores. Special characters aren't supported.
To add your source, selectSelect containerorAdd URI.If you selectedSelect container, fill in the following fields:SettingField descriptionSubscriptionThe subscription ID where the storage account is located.Storage accountThe name that identifies your storage account.ContainerThe storage container you want to ingest.File filters (optional)Folder pathFilters data to ingest files with a specific folder path.File extensionFilters data to ingest files with a specific file extension only.If you selectedAdd URI, from the storage account, generate an SAS URL for the container or individual blobs you want to ingest. Set the permissions toReadandListfor containers orReadfor individual blobs. For more information, seeGenerate a SAS token.Paste the URL into theURIfield, and then select plus (+). You can add multiple URIs for individual blobs, or a single URI for a container.NoteYou can add up to 10 individual blobs. Each blob can be a max of 1 GB uncompressed.You can ingest up to 5000 blobs from a single container.You can't ingest individual blobs and containers in the same ingestion.
To add your source, selectSelect containerorAdd URI.
If you selectedSelect container, fill in the following fields:SettingField descriptionSubscriptionThe subscription ID where the storage account is located.Storage accountThe name that identifies your storage account.ContainerThe storage container you want to ingest.File filters (optional)Folder pathFilters data to ingest files with a specific folder path.File extensionFilters data to ingest files with a specific file extension only.
If you selectedSelect container, fill in the following fields:

If you selectedAdd URI, from the storage account, generate an SAS URL for the container or individual blobs you want to ingest. Set the permissions toReadandListfor containers orReadfor individual blobs. For more information, seeGenerate a SAS token.Paste the URL into theURIfield, and then select plus (+). You can add multiple URIs for individual blobs, or a single URI for a container.NoteYou can add up to 10 individual blobs. Each blob can be a max of 1 GB uncompressed.You can ingest up to 5000 blobs from a single container.You can't ingest individual blobs and containers in the same ingestion.
If you selectedAdd URI, from the storage account, generate an SAS URL for the container or individual blobs you want to ingest. Set the permissions toReadandListfor containers orReadfor individual blobs. For more information, seeGenerate a SAS token.
Paste the URL into theURIfield, and then select plus (+). You can add multiple URIs for individual blobs, or a single URI for a container.

Note
You can add up to 10 individual blobs. Each blob can be a max of 1 GB uncompressed.
You can ingest up to 5000 blobs from a single container.
You can't ingest individual blobs and containers in the same ingestion.
SelectNext
SelectNext
Inspect
TheInspecttab opens with a preview of the data.
To complete the ingestion process, selectFinish.

Optionally:
SelectCommand viewerto view and copy the automatic commands generated from your inputs.
Use theSchema definition filedropdown to change the file that the schema is inferred from.
Change the automatically inferred data format by selecting the desired format from the dropdown. SeeData formats supported by Azure Data Explorer for ingestion.
Edit columns.
ExploreAdvanced options based on data type.
Edit columns
Note
For tabular formats (CSV, TSV, PSV), you can't map a column twice. To map to an existing column, first delete the new column.
You can't change an existing column type. If you try to map to a column having a different format, you may end up with empty columns.
The changes you can make in a table depend on the following parameters:
Tabletype is new or existing
Mappingtype is new or existing

Mapping transformations
Some data format mappings (Parquet, JSON, and Avro) support simple ingest-time transformations. To apply mapping transformations, create or update a column in theEdit columnswindow.
Mapping transformations can be performed on a column of type string or datetime, with the source having data type int or long. Supported mapping transformations are:
DateTimeFromUnixSeconds
DateTimeFromUnixMilliseconds
DateTimeFromUnixMicroseconds
DateTimeFromUnixNanoseconds
Advanced options based on data type
Tabular (CSV, TSV, PSV):
If you're ingesting tabular formats in anexisting table, you can selectAdvanced>Keep current table schema. Tabular data doesn't necessarily include the column names that are used to map source data to the existing columns. When this option is checked, mapping is done by-order, and the table schema remains the same. If this option is unchecked, new columns are created for incoming data, regardless of data structure.
If you're ingesting tabular formats in anexisting table, you can selectAdvanced>Keep current table schema. Tabular data doesn't necessarily include the column names that are used to map source data to the existing columns. When this option is checked, mapping is done by-order, and the table schema remains the same. If this option is unchecked, new columns are created for incoming data, regardless of data structure.
To use the first row as column names, selectAdvanced>First row is column header.
To use the first row as column names, selectAdvanced>First row is column header.

JSON:
To determine column division of JSON data, selectAdvanced>Nested levels, from 1 to 100.
To determine column division of JSON data, selectAdvanced>Nested levels, from 1 to 100.
If you selectAdvanced>Ignore data format errors, the data is ingested in JSON format. If you leave this check box unselected, the data is ingested in multijson format.
If you selectAdvanced>Ignore data format errors, the data is ingested in JSON format. If you leave this check box unselected, the data is ingested in multijson format.

Summary
In theData preparationwindow, all three steps are marked with green check marks when data ingestion finishes successfully. You can view the commands that were used for each step, or select a card to query, visualize, or drop the ingested data.

Related content
Write Kusto Query Language queries in the web UI
Tutorial: Learn common Kusto Query Language operators
Visualize data with Azure Data Explorer dashboards
Feedback
Was this page helpful?
Additional resources