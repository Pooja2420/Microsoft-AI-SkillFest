Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Quickstart: Chat with Azure OpenAI models using your own data
Article
2025-01-10
17 contributors
In this article
In this quickstart, you can use your own data with Azure OpenAI models. Using Azure OpenAI's models on your data can provide you with a powerful conversational AI platform that enables faster and more accurate communication.
Prerequisites
The following resources:
Azure OpenAI
Azure Blob Storage
Azure AI Search
AnAzure OpenAI resourcedeployed in asupported region and with a supported model.Be sure that you're assigned at least theCognitive Services Contributorrole for the Azure OpenAI resource.
Be sure that you're assigned at least theCognitive Services Contributorrole for the Azure OpenAI resource.
Download the example data fromGitHubif you don't have your own data.
Add your data using Azure AI Foundry portal
Tip
You canuse the Azure Developer CLIto programmatically create the resources needed for Azure OpenAI On Your Data
Navigate toAzure AI Foundry portaland sign-in with credentials that have access to your Azure OpenAI resource.
You can eithercreate an Azure AI Foundry projectby clickingCreate project, or continue directly by clicking the button on theFocused on Azure OpenAI Servicetile.
You can eithercreate an Azure AI Foundry projectby clickingCreate project, or continue directly by clicking the button on theFocused on Azure OpenAI Servicetile.

SelectChatunderPlaygroundsin the left pane, and select your model deployment.
SelectChatunderPlaygroundsin the left pane, and select your model deployment.
In theChat playground, SelectAdd your dataand thenAdd a data source
In theChat playground, SelectAdd your dataand thenAdd a data source

In the pane that appears, selectUpload files (preview)underSelect data source. Azure OpenAI needs both a storage resource and a search resource to access and index your data.TipSee the following resource for more information:Data source optionssupported file types and formatsFor documents and datasets with long text, we recommend using the availabledata preparation script.For Azure OpenAI to access your storage account, you will need to turn onCross-origin resource sharing (CORS). If CORS isn't already turned on for the Azure Blob Storage resource, selectTurn on CORS.Select your Azure AI Search resource, and select the acknowledgment that connecting it will incur usage on your account. Then selectNext.
In the pane that appears, selectUpload files (preview)underSelect data source. Azure OpenAI needs both a storage resource and a search resource to access and index your data.
Tip
See the following resource for more information:Data source optionssupported file types and formats
Data source options
supported file types and formats
For documents and datasets with long text, we recommend using the availabledata preparation script.
For Azure OpenAI to access your storage account, you will need to turn onCross-origin resource sharing (CORS). If CORS isn't already turned on for the Azure Blob Storage resource, selectTurn on CORS.
For Azure OpenAI to access your storage account, you will need to turn onCross-origin resource sharing (CORS). If CORS isn't already turned on for the Azure Blob Storage resource, selectTurn on CORS.
Select your Azure AI Search resource, and select the acknowledgment that connecting it will incur usage on your account. Then selectNext.
Select your Azure AI Search resource, and select the acknowledgment that connecting it will incur usage on your account. Then selectNext.

On theUpload filespane, selectBrowse for a fileand select the files you downloaded from theprerequisitessection, or your own data. Then selectUpload files. Then selectNext.
On theUpload filespane, selectBrowse for a fileand select the files you downloaded from theprerequisitessection, or your own data. Then selectUpload files. Then selectNext.
On theData managementpane, you can choose whether to enablesemantic search or vector searchfor your index.ImportantSemantic searchandvector searchare subject to additional pricing. You need to chooseBasic or higher SKUto enable semantic search or vector search. Seepricing tier differenceandservice limitsfor more information.To help improve the quality of the information retrieval and model response, we recommend enablingsemantic searchfor the following data source languages: English, French, Spanish, Portuguese, Italian, Germany, Chinese(Zh), Japanese, Korean, Russian, and Arabic.
On theData managementpane, you can choose whether to enablesemantic search or vector searchfor your index.
Important
Semantic searchandvector searchare subject to additional pricing. You need to chooseBasic or higher SKUto enable semantic search or vector search. Seepricing tier differenceandservice limitsfor more information.
To help improve the quality of the information retrieval and model response, we recommend enablingsemantic searchfor the following data source languages: English, French, Spanish, Portuguese, Italian, Germany, Chinese(Zh), Japanese, Korean, Russian, and Arabic.
Review the details you entered, and selectSave and close. You can now chat with the model and it will use information from your data to construct the response.
Review the details you entered, and selectSave and close. You can now chat with the model and it will use information from your data to construct the response.
Chat playground
Start exploring Azure OpenAI capabilities with a no-code approach through the chat playground. It's simply a text box where you can submit a prompt to generate a completion. From this page, you can quickly iterate and experiment with the capabilities.

The playground gives you options to tailor your chat experience. On the top menu, you can selectDeployto determine which model generates a response using the search results from your index. You choose the number of past messages to include as conversation history for future generated responses.Conversation historygives context to generate related responses but also consumestoken usage. The input token progress indicator keeps track of the token count of the question you submit.
TheAdvanced settingson the left areruntime parameters, which give you control over retrieval and search relevant information from your data. A good use case is when you want to make sure responses are generated only based on your data or you find the model cannot generate a response based on existed information on your data.
Strictnessdetermines the system's aggressiveness in filtering search documents based on their similarity scores. Setting strictness to 5 indicates that the system will aggressively filter out documents, applying a very high similarity threshold.Semantic searchcan be helpful in this scenario because the ranking models do a better job of inferring the intent of the query. Lower levels of strictness produce more verbose answers, but might also include information that isn't in your index. This is set to 3 by default.
Strictnessdetermines the system's aggressiveness in filtering search documents based on their similarity scores. Setting strictness to 5 indicates that the system will aggressively filter out documents, applying a very high similarity threshold.Semantic searchcan be helpful in this scenario because the ranking models do a better job of inferring the intent of the query. Lower levels of strictness produce more verbose answers, but might also include information that isn't in your index. This is set to 3 by default.
Retrieved documentsis an integer that can be set to 3, 5, 10, or 20, and controls the number of document chunks provided to the large language model for formulating the final response. By default, this is set to 5.
Retrieved documentsis an integer that can be set to 3, 5, 10, or 20, and controls the number of document chunks provided to the large language model for formulating the final response. By default, this is set to 5.
WhenLimit responses to your datais enabled, the model attempts to only rely on your documents for responses. This is set to true by default.
WhenLimit responses to your datais enabled, the model attempts to only rely on your documents for responses. This is set to true by default.

Send your first query. The chat models perform best in question and answer exercises. For example, "What are my available health plans?" or "What is the health plus option?".
Queries that require data analysis would probably fail, such as "Which health plan is most popular?". Queries that require information about all of your data will also likely fail, such as "How many documents have I uploaded?". Remember that the search engine looks for chunks having exact or similar terms, phrases, or construction to the query. And while the model might understand the question, if search results are chunks from the data set, it's not the right information to answer that kind of question.
Chats are constrained by the number of documents (chunks) returned in the response (limited to 3-20 in Azure AI Foundry portal playground). As you can imagine, posing a question about "all of the titles" requires a full scan of the entire vector store.
Deploy your model
Once you're satisfied with the experience, you can deploy a web app directly from the portal by selecting theDeploy tobutton.

This gives you the option to either deploy to a standalone web application, or a copilot in Copilot Studio (preview) if you'reusing your own dataon the model.
As an example, if you choose to deploy a web app:
The first time you deploy a web app, you should selectCreate a new web app. Choose a name for the app, which will
become part of the app URL. For example,https://<appname>.azurewebsites.net.
https://<appname>.azurewebsites.net
Select your subscription, resource group, location, and pricing plan for the published app. To
update an existing app, selectPublish to an existing web appand choose the name of your previous
app from the dropdown menu.
If you choose to deploy a web app, see theimportant considerationsfor using it.
Prerequisites
The following resources:
Azure OpenAI
Azure Blob Storage
Azure AI Search
AnAzure OpenAI resourcedeployed in asupported region and with a supported model.Be sure that you're assigned at least theCognitive Services Contributorrole for the Azure OpenAI resource.
Be sure that you're assigned at least theCognitive Services Contributorrole for the Azure OpenAI resource.
Download the example data fromGitHubif you don't have your own data.
The.NET 8 SDK
Add your data using Azure AI Foundry portal
Tip
You canuse the Azure Developer CLIto programmatically create the resources needed for Azure OpenAI On Your Data
Navigate toAzure AI Foundry portaland sign-in with credentials that have access to your Azure OpenAI resource.
You can eithercreate an Azure AI Foundry projectby clickingCreate project, or continue directly by clicking the button on theFocused on Azure OpenAI Servicetile.
You can eithercreate an Azure AI Foundry projectby clickingCreate project, or continue directly by clicking the button on theFocused on Azure OpenAI Servicetile.

SelectChatunderPlaygroundsin the left pane, and select your model deployment.
SelectChatunderPlaygroundsin the left pane, and select your model deployment.
In theChat playground, SelectAdd your dataand thenAdd a data source
In theChat playground, SelectAdd your dataand thenAdd a data source

In the pane that appears, selectUpload files (preview)underSelect data source. Azure OpenAI needs both a storage resource and a search resource to access and index your data.TipSee the following resource for more information:Data source optionssupported file types and formatsFor documents and datasets with long text, we recommend using the availabledata preparation script.For Azure OpenAI to access your storage account, you will need to turn onCross-origin resource sharing (CORS). If CORS isn't already turned on for the Azure Blob Storage resource, selectTurn on CORS.Select your Azure AI Search resource, and select the acknowledgment that connecting it will incur usage on your account. Then selectNext.
In the pane that appears, selectUpload files (preview)underSelect data source. Azure OpenAI needs both a storage resource and a search resource to access and index your data.
Tip
See the following resource for more information:Data source optionssupported file types and formats
Data source options
supported file types and formats
For documents and datasets with long text, we recommend using the availabledata preparation script.
For Azure OpenAI to access your storage account, you will need to turn onCross-origin resource sharing (CORS). If CORS isn't already turned on for the Azure Blob Storage resource, selectTurn on CORS.
For Azure OpenAI to access your storage account, you will need to turn onCross-origin resource sharing (CORS). If CORS isn't already turned on for the Azure Blob Storage resource, selectTurn on CORS.
Select your Azure AI Search resource, and select the acknowledgment that connecting it will incur usage on your account. Then selectNext.
Select your Azure AI Search resource, and select the acknowledgment that connecting it will incur usage on your account. Then selectNext.

On theUpload filespane, selectBrowse for a fileand select the files you downloaded from theprerequisitessection, or your own data. Then selectUpload files. Then selectNext.
On theUpload filespane, selectBrowse for a fileand select the files you downloaded from theprerequisitessection, or your own data. Then selectUpload files. Then selectNext.
On theData managementpane, you can choose whether to enablesemantic search or vector searchfor your index.ImportantSemantic searchandvector searchare subject to additional pricing. You need to chooseBasic or higher SKUto enable semantic search or vector search. Seepricing tier differenceandservice limitsfor more information.To help improve the quality of the information retrieval and model response, we recommend enablingsemantic searchfor the following data source languages: English, French, Spanish, Portuguese, Italian, Germany, Chinese(Zh), Japanese, Korean, Russian, and Arabic.
On theData managementpane, you can choose whether to enablesemantic search or vector searchfor your index.
Important
Semantic searchandvector searchare subject to additional pricing. You need to chooseBasic or higher SKUto enable semantic search or vector search. Seepricing tier differenceandservice limitsfor more information.
To help improve the quality of the information retrieval and model response, we recommend enablingsemantic searchfor the following data source languages: English, French, Spanish, Portuguese, Italian, Germany, Chinese(Zh), Japanese, Korean, Russian, and Arabic.
Review the details you entered, and selectSave and close. You can now chat with the model and it will use information from your data to construct the response.
Review the details you entered, and selectSave and close. You can now chat with the model and it will use information from your data to construct the response.
Retrieve resource information
You need to retrieve the following information to authenticate your application with your Azure OpenAI resource. This quickstart assumes you've uploaded your data to an Azure blob storage account and have an Azure AI Search index created. SeeAdd your data using Azure AI Foundry portal.
Microsoft Entra ID
API key
AZURE_OPENAI_ENDPOINT
https://my-resoruce.openai.azure.com
AZURE_OPENAI_DEPLOYMENT_NAME
AZURE_AI_SEARCH_ENDPOINT
AZURE_AI_SEARCH_INDEX
Learn more aboutkeyless authenticationandsetting environment variables.
AZURE_OPENAI_ENDPOINT
https://my-resoruce.openai.azure.com
AZURE_OPENAI_API_KEY
KEY1
KEY2
AZURE_OPENAI_DEPLOYMENT_NAME
AZURE_AI_SEARCH_ENDPOINT
AZURE_AI_SEARCH_API_KEY
AZURE_AI_SEARCH_INDEX
Learn more aboutfinding API keysandsetting environment variables.
Important
Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, seeAPI keys with Azure Key Vault.
For more information about AI services security, seeAuthenticate requests to Azure AI services.
From the project directory, open theProgram.csfile and replace its contents with the following code:
using System;
using Azure.AI.OpenAI;
using System.ClientModel;
using Azure.AI.OpenAI.Chat;
using OpenAI.Chat;
using static System.Environment;

string azureOpenAIEndpoint = GetEnvironmentVariable("AZURE_OPENAI_ENDPOINT");
string azureOpenAIKey = GetEnvironmentVariable("AZURE_OPENAI_API_KEY");
string deploymentName = GetEnvironmentVariable("AZURE_OPENAI_DEPLOYMENT_NAME");
string searchEndpoint = GetEnvironmentVariable("AZURE_AI_SEARCH_ENDPOINT");
string searchKey = GetEnvironmentVariable("AZURE_AI_SEARCH_API_KEY");
string searchIndex = GetEnvironmentVariable("AZURE_AI_SEARCH_INDEX");

AzureOpenAIClient openAIClient = new(
			new Uri(azureOpenAIEndpoint),
			new ApiKeyCredential(azureOpenAIKey));
ChatClient chatClient = openAIClient.GetChatClient(deploymentName);

// Extension methods to use data sources with options are subject to SDK surface changes. Suppress the
// warning to acknowledge and this and use the subject-to-change AddDataSource method.
#pragma warning disable AOAI001

ChatCompletionOptions options = new();
options.AddDataSource(new AzureSearchChatDataSource()
{
	Endpoint = new Uri(searchEndpoint),
	IndexName = searchIndex,
	Authentication = DataSourceAuthentication.FromApiKey(searchKey),
});

ChatCompletion completion = chatClient.CompleteChat(
	[
		new UserChatMessage("What health plans are available?"),
			],
	options);

ChatMessageContext onYourDataContext = completion.GetMessageContext();

if (onYourDataContext?.Intent is not null)
{
	Console.WriteLine($"Intent: {onYourDataContext.Intent}");
}
foreach (ChatCitation citation in onYourDataContext?.Citations ?? [])
{
	Console.WriteLine($"Citation: {citation.Content}");
}
using System;
using Azure.AI.OpenAI;
using System.ClientModel;
using Azure.AI.OpenAI.Chat;
using OpenAI.Chat;
using static System.Environment;

string azureOpenAIEndpoint = GetEnvironmentVariable("AZURE_OPENAI_ENDPOINT");
string azureOpenAIKey = GetEnvironmentVariable("AZURE_OPENAI_API_KEY");
string deploymentName = GetEnvironmentVariable("AZURE_OPENAI_DEPLOYMENT_NAME");
string searchEndpoint = GetEnvironmentVariable("AZURE_AI_SEARCH_ENDPOINT");
string searchKey = GetEnvironmentVariable("AZURE_AI_SEARCH_API_KEY");
string searchIndex = GetEnvironmentVariable("AZURE_AI_SEARCH_INDEX");

AzureOpenAIClient openAIClient = new(
			new Uri(azureOpenAIEndpoint),
			new ApiKeyCredential(azureOpenAIKey));
ChatClient chatClient = openAIClient.GetChatClient(deploymentName);

// Extension methods to use data sources with options are subject to SDK surface changes. Suppress the
// warning to acknowledge and this and use the subject-to-change AddDataSource method.
#pragma warning disable AOAI001

ChatCompletionOptions options = new();
options.AddDataSource(new AzureSearchChatDataSource()
{
	Endpoint = new Uri(searchEndpoint),
	IndexName = searchIndex,
	Authentication = DataSourceAuthentication.FromApiKey(searchKey),
});

ChatCompletion completion = chatClient.CompleteChat(
	[
		new UserChatMessage("What health plans are available?"),
			],
	options);

ChatMessageContext onYourDataContext = completion.GetMessageContext();

if (onYourDataContext?.Intent is not null)
{
	Console.WriteLine($"Intent: {onYourDataContext.Intent}");
}
foreach (ChatCitation citation in onYourDataContext?.Citations ?? [])
{
	Console.WriteLine($"Citation: {citation.Content}");
}
Important
For production, use a secure way of storing and accessing your credentials likeAzure Key Vault. For more information about credential security, see the Azure AI servicessecurityarticle.
dotnet run Program.cs
dotnet run Program.cs
Output
Contoso Electronics offers two health plans: Northwind Health Plus and Northwind Standard [doc1]. Northwind Health Plus is a comprehensive plan that provides coverage for medical, vision, and dental services, prescription drug coverage, mental health and substance abuse coverage, and coverage for preventive care services. It also offers coverage for emergency services, both in-network and out-of-network. On the other hand, Northwind Standard is a basic plan that provides coverage for medical, vision, and dental services, prescription drug coverage, and coverage for preventive care services. However, it does not offer coverage for emergency services, mental health and substance abuse coverage, or out-of-network services [doc1].

Intent: ["What are the available health plans?", "List of health plans available", "Health insurance options", "Types of health plans offered"]

Citation:
Contoso Electronics plan and benefit packages

Thank you for your interest in the Contoso electronics plan and benefit packages. Use this document to

learn more about the various options available to you...// Omitted for brevity
Contoso Electronics offers two health plans: Northwind Health Plus and Northwind Standard [doc1]. Northwind Health Plus is a comprehensive plan that provides coverage for medical, vision, and dental services, prescription drug coverage, mental health and substance abuse coverage, and coverage for preventive care services. It also offers coverage for emergency services, both in-network and out-of-network. On the other hand, Northwind Standard is a basic plan that provides coverage for medical, vision, and dental services, prescription drug coverage, and coverage for preventive care services. However, it does not offer coverage for emergency services, mental health and substance abuse coverage, or out-of-network services [doc1].

Intent: ["What are the available health plans?", "List of health plans available", "Health insurance options", "Types of health plans offered"]

Citation:
Contoso Electronics plan and benefit packages

Thank you for your interest in the Contoso electronics plan and benefit packages. Use this document to

learn more about the various options available to you...// Omitted for brevity
This will wait until the model has generated its entire response before printing the results.
Source code|Source code|Sample
Prerequisites
The following resources:
Azure OpenAI
Azure Blob Storage
Azure AI Search
AnAzure OpenAI resourcedeployed in asupported region and with a supported model.Be sure that you're assigned at least theCognitive Services Contributorrole for the Azure OpenAI resource.
Be sure that you're assigned at least theCognitive Services Contributorrole for the Azure OpenAI resource.
Download the example data fromGitHubif you don't have your own data.
Add your data using Azure AI Foundry portal
Tip
You canuse the Azure Developer CLIto programmatically create the resources needed for Azure OpenAI On Your Data
Navigate toAzure AI Foundry portaland sign-in with credentials that have access to your Azure OpenAI resource.
You can eithercreate an Azure AI Foundry projectby clickingCreate project, or continue directly by clicking the button on theFocused on Azure OpenAI Servicetile.
You can eithercreate an Azure AI Foundry projectby clickingCreate project, or continue directly by clicking the button on theFocused on Azure OpenAI Servicetile.

SelectChatunderPlaygroundsin the left pane, and select your model deployment.
SelectChatunderPlaygroundsin the left pane, and select your model deployment.
In theChat playground, SelectAdd your dataand thenAdd a data source
In theChat playground, SelectAdd your dataand thenAdd a data source

In the pane that appears, selectUpload files (preview)underSelect data source. Azure OpenAI needs both a storage resource and a search resource to access and index your data.TipSee the following resource for more information:Data source optionssupported file types and formatsFor documents and datasets with long text, we recommend using the availabledata preparation script.For Azure OpenAI to access your storage account, you will need to turn onCross-origin resource sharing (CORS). If CORS isn't already turned on for the Azure Blob Storage resource, selectTurn on CORS.Select your Azure AI Search resource, and select the acknowledgment that connecting it will incur usage on your account. Then selectNext.
In the pane that appears, selectUpload files (preview)underSelect data source. Azure OpenAI needs both a storage resource and a search resource to access and index your data.
Tip
See the following resource for more information:Data source optionssupported file types and formats
Data source options
supported file types and formats
For documents and datasets with long text, we recommend using the availabledata preparation script.
For Azure OpenAI to access your storage account, you will need to turn onCross-origin resource sharing (CORS). If CORS isn't already turned on for the Azure Blob Storage resource, selectTurn on CORS.
For Azure OpenAI to access your storage account, you will need to turn onCross-origin resource sharing (CORS). If CORS isn't already turned on for the Azure Blob Storage resource, selectTurn on CORS.
Select your Azure AI Search resource, and select the acknowledgment that connecting it will incur usage on your account. Then selectNext.
Select your Azure AI Search resource, and select the acknowledgment that connecting it will incur usage on your account. Then selectNext.

On theUpload filespane, selectBrowse for a fileand select the files you downloaded from theprerequisitessection, or your own data. Then selectUpload files. Then selectNext.
On theUpload filespane, selectBrowse for a fileand select the files you downloaded from theprerequisitessection, or your own data. Then selectUpload files. Then selectNext.
On theData managementpane, you can choose whether to enablesemantic search or vector searchfor your index.ImportantSemantic searchandvector searchare subject to additional pricing. You need to chooseBasic or higher SKUto enable semantic search or vector search. Seepricing tier differenceandservice limitsfor more information.To help improve the quality of the information retrieval and model response, we recommend enablingsemantic searchfor the following data source languages: English, French, Spanish, Portuguese, Italian, Germany, Chinese(Zh), Japanese, Korean, Russian, and Arabic.
On theData managementpane, you can choose whether to enablesemantic search or vector searchfor your index.
Important
Semantic searchandvector searchare subject to additional pricing. You need to chooseBasic or higher SKUto enable semantic search or vector search. Seepricing tier differenceandservice limitsfor more information.
To help improve the quality of the information retrieval and model response, we recommend enablingsemantic searchfor the following data source languages: English, French, Spanish, Portuguese, Italian, Germany, Chinese(Zh), Japanese, Korean, Russian, and Arabic.
Review the details you entered, and selectSave and close. You can now chat with the model and it will use information from your data to construct the response.
Review the details you entered, and selectSave and close. You can now chat with the model and it will use information from your data to construct the response.
Retrieve required variables
To successfully make a call against Azure OpenAI, you need the following variables. This quickstart assumes you've uploaded your data to an Azure blob storage account and have an Azure AI Search index created. For more information, seeAdd your data using Azure AI Foundry.
AZURE_OPENAI_ENDPOINT
https://my-resource.openai.azure.com
AZURE_OPENAI_API_KEY
KEY1
KEY2
AZURE_OPEN_AI_DEPLOYMENT_ID
AZURE_AI_SEARCH_ENDPOINT
AZURE_AI_SEARCH_API_KEY
AZURE_AI_SEARCH_INDEX
Environment variables
Create and assign persistent environment variables for your key and endpoint.
Important
Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, seeAPI keys with Azure Key Vault.
For more information about AI services security, seeAuthenticate requests to Azure AI services.
Note
Spring AI defaults the model name togpt-35-turbo. It's only necessary to provide theSPRING_AI_AZURE_OPENAI_MODELvalue if you've deployed a model with a different name.
gpt-35-turbo
SPRING_AI_AZURE_OPENAI_MODEL
export SPRING_AI_AZURE_OPENAI_ENDPOINT=REPLACE_WITH_YOUR_AOAI_ENDPOINT_VALUE_HERE
export SPRING_AI_AZURE_OPENAI_API_KEY=REPLACE_WITH_YOUR_AOAI_KEY_VALUE_HERE
export SPRING_AI_AZURE_COGNITIVE_SEARCH_ENDPOINT=REPLACE_WITH_YOUR_AZURE_SEARCH_RESOURCE_VALUE_HERE
export SPRING_AI_AZURE_COGNITIVE_SEARCH_API_KEY=REPLACE_WITH_YOUR_AZURE_SEARCH_RESOURCE_KEY_VALUE_HERE
export SPRING_AI_AZURE_COGNITIVE_SEARCH_INDEX=REPLACE_WITH_YOUR_INDEX_NAME_HERE
export SPRING_AI_AZURE_OPENAI_MODEL=REPLACE_WITH_YOUR_MODEL_NAME_HERE
export SPRING_AI_AZURE_OPENAI_ENDPOINT=REPLACE_WITH_YOUR_AOAI_ENDPOINT_VALUE_HERE
export SPRING_AI_AZURE_OPENAI_API_KEY=REPLACE_WITH_YOUR_AOAI_KEY_VALUE_HERE
export SPRING_AI_AZURE_COGNITIVE_SEARCH_ENDPOINT=REPLACE_WITH_YOUR_AZURE_SEARCH_RESOURCE_VALUE_HERE
export SPRING_AI_AZURE_COGNITIVE_SEARCH_API_KEY=REPLACE_WITH_YOUR_AZURE_SEARCH_RESOURCE_KEY_VALUE_HERE
export SPRING_AI_AZURE_COGNITIVE_SEARCH_INDEX=REPLACE_WITH_YOUR_INDEX_NAME_HERE
export SPRING_AI_AZURE_OPENAI_MODEL=REPLACE_WITH_YOUR_MODEL_NAME_HERE
Create a new Spring application
Spring AI doesn't currently support theAzureCognitiveSearchChatExtensionConfigurationoptions that allow an Azure AI query to encapsulate theRetrieval Augmented Generation (RAG)method and hide the details from the user. As an alternative, you can still invoke the RAG method directly in your application to query data in your Azure AI Search index and use retrieved documents to augment your query.
AzureCognitiveSearchChatExtensionConfiguration
Spring AI supports a VectorStore abstraction, and you can wrap Azure AI Search can be wrapped in a Spring AI VectorStore implementation for querying your custom data. The following project implements a custom VectorStore backed by Azure AI Search and directly executes RAG operations.
In a Bash window, create a new directory for your app, and navigate to it.
mkdir ai-custom-data-demo && cd ai-custom-data-demo
mkdir ai-custom-data-demo && cd ai-custom-data-demo
Run thespring initcommand from your working directory. This command creates a standard directory structure for your Spring project including the main Java class source file and thepom.xmlfile used for managing Maven based projects.
spring init
spring init -a ai-custom-data-demo -n AICustomData --force --build maven -x
spring init -a ai-custom-data-demo -n AICustomData --force --build maven -x
The generated files and folders resemble the following structure:
ai-custom-data-demo/
|-- pom.xml
|-- mvn
|-- mvn.cmd
|-- HELP.md
|-- src/
    |-- main/
    |   |-- resources/
    |   |   |-- application.properties
    |   |-- java/
    |       |-- com/
    |           |-- example/
    |               |-- aicustomdatademo/
    |                   |-- AiCustomDataApplication.java
    |-- test/
        |-- java/
            |-- com/
                |-- example/
                    |-- aicustomdatademo/
                        |-- AiCustomDataApplicationTests.java
ai-custom-data-demo/
|-- pom.xml
|-- mvn
|-- mvn.cmd
|-- HELP.md
|-- src/
    |-- main/
    |   |-- resources/
    |   |   |-- application.properties
    |   |-- java/
    |       |-- com/
    |           |-- example/
    |               |-- aicustomdatademo/
    |                   |-- AiCustomDataApplication.java
    |-- test/
        |-- java/
            |-- com/
                |-- example/
                    |-- aicustomdatademo/
                        |-- AiCustomDataApplicationTests.java
Edit Spring application
Edit thepom.xmlfile.From the root of the project directory, open thepom.xmlfile in your preferred editor or IDE and overwrite the file with following content:<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-parent</artifactId>
        <version>3.2.0</version>
        <relativePath/> <!-- lookup parent from repository -->
    </parent>
    <groupId>com.example</groupId>
    <artifactId>ai-custom-data-demo</artifactId>
    <version>0.0.1-SNAPSHOT</version>
    <name>AICustomData</name>
    <description>Demo project for Spring Boot</description>
    <properties>
        <java.version>17</java.version>
    </properties>
    <dependencies>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.experimental.ai</groupId>
            <artifactId>spring-ai-azure-openai-spring-boot-starter</artifactId>
            <version>0.7.0-SNAPSHOT</version>
        </dependency>
        <dependency>
            <groupId>com.azure</groupId>
            <artifactId>azure-search-documents</artifactId>
            <version>11.6.0-beta.10</version>
            <exclusions>
                <!-- exclude this to avoid changing the default serializer and the null-value behavior -->
                <exclusion>
                    <groupId>com.azure</groupId>
                    <artifactId>azure-core-serializer-json-jackson</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.projectlombok</groupId>
            <artifactId>lombok</artifactId>
            <optional>true</optional>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-maven-plugin</artifactId>
            </plugin>
        </plugins>
    </build>
    <repositories>
        <repository>
            <id>spring-snapshots</id>
            <name>Spring Snapshots</name>
            <url>https://repo.spring.io/snapshot</url>
            <releases>
                <enabled>false</enabled>
            </releases>
        </repository>
    </repositories>
</project>
Edit thepom.xmlfile.
From the root of the project directory, open thepom.xmlfile in your preferred editor or IDE and overwrite the file with following content:
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-parent</artifactId>
        <version>3.2.0</version>
        <relativePath/> <!-- lookup parent from repository -->
    </parent>
    <groupId>com.example</groupId>
    <artifactId>ai-custom-data-demo</artifactId>
    <version>0.0.1-SNAPSHOT</version>
    <name>AICustomData</name>
    <description>Demo project for Spring Boot</description>
    <properties>
        <java.version>17</java.version>
    </properties>
    <dependencies>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.experimental.ai</groupId>
            <artifactId>spring-ai-azure-openai-spring-boot-starter</artifactId>
            <version>0.7.0-SNAPSHOT</version>
        </dependency>
        <dependency>
            <groupId>com.azure</groupId>
            <artifactId>azure-search-documents</artifactId>
            <version>11.6.0-beta.10</version>
            <exclusions>
                <!-- exclude this to avoid changing the default serializer and the null-value behavior -->
                <exclusion>
                    <groupId>com.azure</groupId>
                    <artifactId>azure-core-serializer-json-jackson</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.projectlombok</groupId>
            <artifactId>lombok</artifactId>
            <optional>true</optional>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-maven-plugin</artifactId>
            </plugin>
        </plugins>
    </build>
    <repositories>
        <repository>
            <id>spring-snapshots</id>
            <name>Spring Snapshots</name>
            <url>https://repo.spring.io/snapshot</url>
            <releases>
                <enabled>false</enabled>
            </releases>
        </repository>
    </repositories>
</project>
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-parent</artifactId>
        <version>3.2.0</version>
        <relativePath/> <!-- lookup parent from repository -->
    </parent>
    <groupId>com.example</groupId>
    <artifactId>ai-custom-data-demo</artifactId>
    <version>0.0.1-SNAPSHOT</version>
    <name>AICustomData</name>
    <description>Demo project for Spring Boot</description>
    <properties>
        <java.version>17</java.version>
    </properties>
    <dependencies>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.experimental.ai</groupId>
            <artifactId>spring-ai-azure-openai-spring-boot-starter</artifactId>
            <version>0.7.0-SNAPSHOT</version>
        </dependency>
        <dependency>
            <groupId>com.azure</groupId>
            <artifactId>azure-search-documents</artifactId>
            <version>11.6.0-beta.10</version>
            <exclusions>
                <!-- exclude this to avoid changing the default serializer and the null-value behavior -->
                <exclusion>
                    <groupId>com.azure</groupId>
                    <artifactId>azure-core-serializer-json-jackson</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
            <groupId>org.projectlombok</groupId>
            <artifactId>lombok</artifactId>
            <optional>true</optional>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-maven-plugin</artifactId>
            </plugin>
        </plugins>
    </build>
    <repositories>
        <repository>
            <id>spring-snapshots</id>
            <name>Spring Snapshots</name>
            <url>https://repo.spring.io/snapshot</url>
            <releases>
                <enabled>false</enabled>
            </releases>
        </repository>
    </repositories>
</project>
From thesrc/main/java/com/example/aicustomdatademofolder, openAiCustomDataApplication.javain your preferred editor or IDE and paste in the following code:package com.example.aicustomdatademo;

import java.util.Collections;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import java.util.stream.Collectors;

import org.springframework.ai.client.AiClient;
import org.springframework.ai.document.Document;
import org.springframework.ai.embedding.EmbeddingClient;
import org.springframework.ai.prompt.Prompt;
import org.springframework.ai.prompt.SystemPromptTemplate;
import org.springframework.ai.prompt.messages.MessageType;
import org.springframework.ai.prompt.messages.UserMessage;
import org.springframework.ai.vectorstore.VectorStore;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.boot.CommandLineRunner;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.annotation.Bean;

import com.azure.core.credential.AzureKeyCredential;
import com.azure.core.util.Context;
import com.azure.search.documents.SearchClient;
import com.azure.search.documents.SearchClientBuilder;
import com.azure.search.documents.models.IndexingResult;
import com.azure.search.documents.models.SearchOptions;
import com.azure.search.documents.models.RawVectorQuery;

import lombok.AllArgsConstructor;
import lombok.NoArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.extern.jackson.Jacksonized;

@SpringBootApplication
public class AiCustomDataApplication implements CommandLineRunner
{
    private static final String ROLE_INFO_KEY = "role";

    private static final String template = """
            You are a helpful assistant. Use the information from the DOCUMENTS section to augment answers.

            DOCUMENTS:
            {documents}
            """;

    @Value("${spring.ai.azure.cognitive-search.endpoint}")
    private String acsEndpoint;

    @Value("${spring.ai.azure.cognitive-search.api-key}")
    private String acsApiKey;

    @Value("${spring.ai.azure.cognitive-search.index}")
    private String acsIndexName;

    @Autowired
    private AiClient aiClient;

    @Autowired
    private EmbeddingClient embeddingClient;

    public static void main(String[] args) {
        SpringApplication.run(AiCustomDataApplication.class, args);
    }

    @Override
    public void run(String... args) throws Exception
    {
        System.out.println(String.format("Sending custom data prompt to AI service. One moment please...\r\n"));

        final var store = vectorStore(embeddingClient);

        final String question = "What are my available health plans?";

        final var candidateDocs = store.similaritySearch(question);

        final var userMessage = new UserMessage(question);

        final String docPrompts =
                candidateDocs.stream().map(entry -> entry.getContent()).collect(Collectors.joining("\n"));

        final SystemPromptTemplate promptTemplate = new SystemPromptTemplate(template);
        final var systemMessage = promptTemplate.createMessage(Map.of("documents", docPrompts));

        final var prompt = new Prompt(List.of(systemMessage, userMessage));

        final var resps = aiClient.generate(prompt);

        System.out.println(String.format("Prompt created %d generated response(s).", resps.getGenerations().size()));

        resps.getGenerations().stream()
          .forEach(gen -> {
              final var role = gen.getInfo().getOrDefault(ROLE_INFO_KEY, MessageType.ASSISTANT.getValue());

              System.out.println(String.format("Generated respose from \"%s\": %s", role, gen.getText()));
          });

    }

    @Bean
    public VectorStore vectorStore(EmbeddingClient embeddingClient)
    {
        final SearchClient searchClient = new SearchClientBuilder()
                .endpoint(acsEndpoint)
                .credential(new AzureKeyCredential(acsApiKey))
                .indexName(acsIndexName)
                .buildClient();
        return new AzureCognitiveSearchVectorStore(searchClient, embeddingClient);
    }

    public static class AzureCognitiveSearchVectorStore implements VectorStore
    {
        private static final int DEFAULT_TOP_K = 4;

        private static final Double DEFAULT_SIMILARITY_THRESHOLD = 0.0;

        private SearchClient searchClient;

        private final EmbeddingClient embeddingClient;

        public AzureCognitiveSearchVectorStore(SearchClient searchClient, EmbeddingClient embeddingClient)
        {
            this.searchClient = searchClient;
            this.embeddingClient = embeddingClient;
        }

        @Override
        public void add(List<Document> documents)
        {
            final var docs = documents.stream().map(document -> {

                final var embeddings = embeddingClient.embed(document);

                return new DocEntry(document.getId(), "", document.getContent(), embeddings);

            }).toList();

            searchClient.uploadDocuments(docs);
        }

        @Override
        public Optional<Boolean> delete(List<String> idList)
        {
            final List<DocEntry> docIds = idList.stream().map(id -> DocEntry.builder().id(id).build())
                .toList();

            var results = searchClient.deleteDocuments(docIds);

            boolean resSuccess = true;

            for (IndexingResult result : results.getResults())
                if (!result.isSucceeded()) {
                    resSuccess = false;
                    break;
                }

            return Optional.of(resSuccess);
        }

        @Override
        public List<Document> similaritySearch(String query)
        {
            return similaritySearch(query, DEFAULT_TOP_K);
        }

        @Override
        public List<Document> similaritySearch(String query, int k)
        {
            return similaritySearch(query, k, DEFAULT_SIMILARITY_THRESHOLD);
        }

        @Override
        public List<Document> similaritySearch(String query, int k, double threshold)
        {
            final var searchQueryVector = new RawVectorQuery()
                    .setVector(toFloatList(embeddingClient.embed(query)))
                    .setKNearestNeighborsCount(k)
                    .setFields("contentVector");

            final var searchResults = searchClient.search(null,
                    new SearchOptions().setVectorQueries(searchQueryVector), Context.NONE);

            return searchResults.stream()
                    .filter(r -> r.getScore() >= threshold)
                    .map(r -> {

                        final DocEntry entry = r.getDocument(DocEntry.class);

                        final Document doc = new Document(entry.getId(), entry.getContent(), Collections.emptyMap());
                        doc.setEmbedding(entry.getContentVector());

                        return doc;
                    })
                    .collect(Collectors.toList());
        }

        private List<Float> toFloatList(List<Double> doubleList)
        {
            return doubleList.stream().map(Double::floatValue).toList();
        }

    }

    @Data
    @Builder
    @Jacksonized
    @AllArgsConstructor
    @NoArgsConstructor
    static class DocEntry
    {
        private String id;

        private String hash;

        private String content;

        private List<Double> contentVector;
    }

}ImportantFor production, use a secure way of storing and accessing your credentials likeAzure Key Vault. For more information about credential security, see the Azure AI servicessecurityarticle.
From thesrc/main/java/com/example/aicustomdatademofolder, openAiCustomDataApplication.javain your preferred editor or IDE and paste in the following code:
package com.example.aicustomdatademo;

import java.util.Collections;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import java.util.stream.Collectors;

import org.springframework.ai.client.AiClient;
import org.springframework.ai.document.Document;
import org.springframework.ai.embedding.EmbeddingClient;
import org.springframework.ai.prompt.Prompt;
import org.springframework.ai.prompt.SystemPromptTemplate;
import org.springframework.ai.prompt.messages.MessageType;
import org.springframework.ai.prompt.messages.UserMessage;
import org.springframework.ai.vectorstore.VectorStore;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.boot.CommandLineRunner;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.annotation.Bean;

import com.azure.core.credential.AzureKeyCredential;
import com.azure.core.util.Context;
import com.azure.search.documents.SearchClient;
import com.azure.search.documents.SearchClientBuilder;
import com.azure.search.documents.models.IndexingResult;
import com.azure.search.documents.models.SearchOptions;
import com.azure.search.documents.models.RawVectorQuery;

import lombok.AllArgsConstructor;
import lombok.NoArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.extern.jackson.Jacksonized;

@SpringBootApplication
public class AiCustomDataApplication implements CommandLineRunner
{
    private static final String ROLE_INFO_KEY = "role";

    private static final String template = """
            You are a helpful assistant. Use the information from the DOCUMENTS section to augment answers.

            DOCUMENTS:
            {documents}
            """;

    @Value("${spring.ai.azure.cognitive-search.endpoint}")
    private String acsEndpoint;

    @Value("${spring.ai.azure.cognitive-search.api-key}")
    private String acsApiKey;

    @Value("${spring.ai.azure.cognitive-search.index}")
    private String acsIndexName;

    @Autowired
    private AiClient aiClient;

    @Autowired
    private EmbeddingClient embeddingClient;

    public static void main(String[] args) {
        SpringApplication.run(AiCustomDataApplication.class, args);
    }

    @Override
    public void run(String... args) throws Exception
    {
        System.out.println(String.format("Sending custom data prompt to AI service. One moment please...\r\n"));

        final var store = vectorStore(embeddingClient);

        final String question = "What are my available health plans?";

        final var candidateDocs = store.similaritySearch(question);

        final var userMessage = new UserMessage(question);

        final String docPrompts =
                candidateDocs.stream().map(entry -> entry.getContent()).collect(Collectors.joining("\n"));

        final SystemPromptTemplate promptTemplate = new SystemPromptTemplate(template);
        final var systemMessage = promptTemplate.createMessage(Map.of("documents", docPrompts));

        final var prompt = new Prompt(List.of(systemMessage, userMessage));

        final var resps = aiClient.generate(prompt);

        System.out.println(String.format("Prompt created %d generated response(s).", resps.getGenerations().size()));

        resps.getGenerations().stream()
          .forEach(gen -> {
              final var role = gen.getInfo().getOrDefault(ROLE_INFO_KEY, MessageType.ASSISTANT.getValue());

              System.out.println(String.format("Generated respose from \"%s\": %s", role, gen.getText()));
          });

    }

    @Bean
    public VectorStore vectorStore(EmbeddingClient embeddingClient)
    {
        final SearchClient searchClient = new SearchClientBuilder()
                .endpoint(acsEndpoint)
                .credential(new AzureKeyCredential(acsApiKey))
                .indexName(acsIndexName)
                .buildClient();
        return new AzureCognitiveSearchVectorStore(searchClient, embeddingClient);
    }

    public static class AzureCognitiveSearchVectorStore implements VectorStore
    {
        private static final int DEFAULT_TOP_K = 4;

        private static final Double DEFAULT_SIMILARITY_THRESHOLD = 0.0;

        private SearchClient searchClient;

        private final EmbeddingClient embeddingClient;

        public AzureCognitiveSearchVectorStore(SearchClient searchClient, EmbeddingClient embeddingClient)
        {
            this.searchClient = searchClient;
            this.embeddingClient = embeddingClient;
        }

        @Override
        public void add(List<Document> documents)
        {
            final var docs = documents.stream().map(document -> {

                final var embeddings = embeddingClient.embed(document);

                return new DocEntry(document.getId(), "", document.getContent(), embeddings);

            }).toList();

            searchClient.uploadDocuments(docs);
        }

        @Override
        public Optional<Boolean> delete(List<String> idList)
        {
            final List<DocEntry> docIds = idList.stream().map(id -> DocEntry.builder().id(id).build())
                .toList();

            var results = searchClient.deleteDocuments(docIds);

            boolean resSuccess = true;

            for (IndexingResult result : results.getResults())
                if (!result.isSucceeded()) {
                    resSuccess = false;
                    break;
                }

            return Optional.of(resSuccess);
        }

        @Override
        public List<Document> similaritySearch(String query)
        {
            return similaritySearch(query, DEFAULT_TOP_K);
        }

        @Override
        public List<Document> similaritySearch(String query, int k)
        {
            return similaritySearch(query, k, DEFAULT_SIMILARITY_THRESHOLD);
        }

        @Override
        public List<Document> similaritySearch(String query, int k, double threshold)
        {
            final var searchQueryVector = new RawVectorQuery()
                    .setVector(toFloatList(embeddingClient.embed(query)))
                    .setKNearestNeighborsCount(k)
                    .setFields("contentVector");

            final var searchResults = searchClient.search(null,
                    new SearchOptions().setVectorQueries(searchQueryVector), Context.NONE);

            return searchResults.stream()
                    .filter(r -> r.getScore() >= threshold)
                    .map(r -> {

                        final DocEntry entry = r.getDocument(DocEntry.class);

                        final Document doc = new Document(entry.getId(), entry.getContent(), Collections.emptyMap());
                        doc.setEmbedding(entry.getContentVector());

                        return doc;
                    })
                    .collect(Collectors.toList());
        }

        private List<Float> toFloatList(List<Double> doubleList)
        {
            return doubleList.stream().map(Double::floatValue).toList();
        }

    }

    @Data
    @Builder
    @Jacksonized
    @AllArgsConstructor
    @NoArgsConstructor
    static class DocEntry
    {
        private String id;

        private String hash;

        private String content;

        private List<Double> contentVector;
    }

}
package com.example.aicustomdatademo;

import java.util.Collections;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import java.util.stream.Collectors;

import org.springframework.ai.client.AiClient;
import org.springframework.ai.document.Document;
import org.springframework.ai.embedding.EmbeddingClient;
import org.springframework.ai.prompt.Prompt;
import org.springframework.ai.prompt.SystemPromptTemplate;
import org.springframework.ai.prompt.messages.MessageType;
import org.springframework.ai.prompt.messages.UserMessage;
import org.springframework.ai.vectorstore.VectorStore;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.boot.CommandLineRunner;
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.annotation.Bean;

import com.azure.core.credential.AzureKeyCredential;
import com.azure.core.util.Context;
import com.azure.search.documents.SearchClient;
import com.azure.search.documents.SearchClientBuilder;
import com.azure.search.documents.models.IndexingResult;
import com.azure.search.documents.models.SearchOptions;
import com.azure.search.documents.models.RawVectorQuery;

import lombok.AllArgsConstructor;
import lombok.NoArgsConstructor;
import lombok.Builder;
import lombok.Data;
import lombok.extern.jackson.Jacksonized;

@SpringBootApplication
public class AiCustomDataApplication implements CommandLineRunner
{
    private static final String ROLE_INFO_KEY = "role";

    private static final String template = """
            You are a helpful assistant. Use the information from the DOCUMENTS section to augment answers.

            DOCUMENTS:
            {documents}
            """;

    @Value("${spring.ai.azure.cognitive-search.endpoint}")
    private String acsEndpoint;

    @Value("${spring.ai.azure.cognitive-search.api-key}")
    private String acsApiKey;

    @Value("${spring.ai.azure.cognitive-search.index}")
    private String acsIndexName;

    @Autowired
    private AiClient aiClient;

    @Autowired
    private EmbeddingClient embeddingClient;

    public static void main(String[] args) {
        SpringApplication.run(AiCustomDataApplication.class, args);
    }

    @Override
    public void run(String... args) throws Exception
    {
        System.out.println(String.format("Sending custom data prompt to AI service. One moment please...\r\n"));

        final var store = vectorStore(embeddingClient);

        final String question = "What are my available health plans?";

        final var candidateDocs = store.similaritySearch(question);

        final var userMessage = new UserMessage(question);

        final String docPrompts =
                candidateDocs.stream().map(entry -> entry.getContent()).collect(Collectors.joining("\n"));

        final SystemPromptTemplate promptTemplate = new SystemPromptTemplate(template);
        final var systemMessage = promptTemplate.createMessage(Map.of("documents", docPrompts));

        final var prompt = new Prompt(List.of(systemMessage, userMessage));

        final var resps = aiClient.generate(prompt);

        System.out.println(String.format("Prompt created %d generated response(s).", resps.getGenerations().size()));

        resps.getGenerations().stream()
          .forEach(gen -> {
              final var role = gen.getInfo().getOrDefault(ROLE_INFO_KEY, MessageType.ASSISTANT.getValue());

              System.out.println(String.format("Generated respose from \"%s\": %s", role, gen.getText()));
          });

    }

    @Bean
    public VectorStore vectorStore(EmbeddingClient embeddingClient)
    {
        final SearchClient searchClient = new SearchClientBuilder()
                .endpoint(acsEndpoint)
                .credential(new AzureKeyCredential(acsApiKey))
                .indexName(acsIndexName)
                .buildClient();
        return new AzureCognitiveSearchVectorStore(searchClient, embeddingClient);
    }

    public static class AzureCognitiveSearchVectorStore implements VectorStore
    {
        private static final int DEFAULT_TOP_K = 4;

        private static final Double DEFAULT_SIMILARITY_THRESHOLD = 0.0;

        private SearchClient searchClient;

        private final EmbeddingClient embeddingClient;

        public AzureCognitiveSearchVectorStore(SearchClient searchClient, EmbeddingClient embeddingClient)
        {
            this.searchClient = searchClient;
            this.embeddingClient = embeddingClient;
        }

        @Override
        public void add(List<Document> documents)
        {
            final var docs = documents.stream().map(document -> {

                final var embeddings = embeddingClient.embed(document);

                return new DocEntry(document.getId(), "", document.getContent(), embeddings);

            }).toList();

            searchClient.uploadDocuments(docs);
        }

        @Override
        public Optional<Boolean> delete(List<String> idList)
        {
            final List<DocEntry> docIds = idList.stream().map(id -> DocEntry.builder().id(id).build())
                .toList();

            var results = searchClient.deleteDocuments(docIds);

            boolean resSuccess = true;

            for (IndexingResult result : results.getResults())
                if (!result.isSucceeded()) {
                    resSuccess = false;
                    break;
                }

            return Optional.of(resSuccess);
        }

        @Override
        public List<Document> similaritySearch(String query)
        {
            return similaritySearch(query, DEFAULT_TOP_K);
        }

        @Override
        public List<Document> similaritySearch(String query, int k)
        {
            return similaritySearch(query, k, DEFAULT_SIMILARITY_THRESHOLD);
        }

        @Override
        public List<Document> similaritySearch(String query, int k, double threshold)
        {
            final var searchQueryVector = new RawVectorQuery()
                    .setVector(toFloatList(embeddingClient.embed(query)))
                    .setKNearestNeighborsCount(k)
                    .setFields("contentVector");

            final var searchResults = searchClient.search(null,
                    new SearchOptions().setVectorQueries(searchQueryVector), Context.NONE);

            return searchResults.stream()
                    .filter(r -> r.getScore() >= threshold)
                    .map(r -> {

                        final DocEntry entry = r.getDocument(DocEntry.class);

                        final Document doc = new Document(entry.getId(), entry.getContent(), Collections.emptyMap());
                        doc.setEmbedding(entry.getContentVector());

                        return doc;
                    })
                    .collect(Collectors.toList());
        }

        private List<Float> toFloatList(List<Double> doubleList)
        {
            return doubleList.stream().map(Double::floatValue).toList();
        }

    }

    @Data
    @Builder
    @Jacksonized
    @AllArgsConstructor
    @NoArgsConstructor
    static class DocEntry
    {
        private String id;

        private String hash;

        private String content;

        private List<Double> contentVector;
    }

}
Important
For production, use a secure way of storing and accessing your credentials likeAzure Key Vault. For more information about credential security, see the Azure AI servicessecurityarticle.
Navigate back to the project root folder, and run the app by using the following command:./mvnw spring-boot:run
Navigate back to the project root folder, and run the app by using the following command:
./mvnw spring-boot:run
./mvnw spring-boot:run
Output
.   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::                (v3.1.5)

2023-11-07T14:40:45.250-06:00  INFO 18557 --- [           main] c.e.a.AiCustomDataApplication            : No active profile set, falling back to 1 default profile: "default"
2023-11-07T14:40:46.035-06:00  INFO 18557 --- [           main] c.e.a.AiCustomDataApplication            : Started AiCustomDataApplication in 1.095 seconds (process running for 1.397)
Sending custom data prompt to AI service. One moment please...

Prompt created 1 generated response(s).
Generated response from "assistant": The available health plans in the Contoso Electronics plan and benefit packages are the Northwind Health Plus and Northwind Standard plans.
.   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::                (v3.1.5)

2023-11-07T14:40:45.250-06:00  INFO 18557 --- [           main] c.e.a.AiCustomDataApplication            : No active profile set, falling back to 1 default profile: "default"
2023-11-07T14:40:46.035-06:00  INFO 18557 --- [           main] c.e.a.AiCustomDataApplication            : Started AiCustomDataApplication in 1.095 seconds (process running for 1.397)
Sending custom data prompt to AI service. One moment please...

Prompt created 1 generated response(s).
Generated response from "assistant": The available health plans in the Contoso Electronics plan and benefit packages are the Northwind Health Plus and Northwind Standard plans.
Reference documentation|Source code|Package (npm)|Samples
Prerequisites
An Azure subscription -Create one for free.
An Azure subscription -Create one for free.
LTS versions of Node.js
LTS versions of Node.js
Azure CLIused for passwordless authentication in a local development environment, create the necessary context by signing in with the Azure CLI.
Azure CLIused for passwordless authentication in a local development environment, create the necessary context by signing in with the Azure CLI.
An Azure OpenAI resource deployed in asupported region and with a supported model.
An Azure OpenAI resource deployed in asupported region and with a supported model.
Be sure that you're assigned at least theCognitive Services Contributorrole for the Azure OpenAI resource.
Be sure that you're assigned at least theCognitive Services Contributorrole for the Azure OpenAI resource.
Download the example data fromGitHubif you don't have your own data.
Download the example data fromGitHubif you don't have your own data.
Microsoft Entra ID prerequisites
For the recommended keyless authentication with Microsoft Entra ID, you need to:
Install theAzure CLIused for keyless authentication with Microsoft Entra ID.
Assign theCognitive Services Userrole to your user account. You can assign roles in the Azure portal underAccess control (IAM)>Add role assignment.
Cognitive Services User
Add your data using Azure AI Foundry portal
Tip
You canuse the Azure Developer CLIto programmatically create the resources needed for Azure OpenAI On Your Data
Navigate toAzure AI Foundry portaland sign-in with credentials that have access to your Azure OpenAI resource.
You can eithercreate an Azure AI Foundry projectby clickingCreate project, or continue directly by clicking the button on theFocused on Azure OpenAI Servicetile.
You can eithercreate an Azure AI Foundry projectby clickingCreate project, or continue directly by clicking the button on theFocused on Azure OpenAI Servicetile.

SelectChatunderPlaygroundsin the left pane, and select your model deployment.
SelectChatunderPlaygroundsin the left pane, and select your model deployment.
In theChat playground, SelectAdd your dataand thenAdd a data source
In theChat playground, SelectAdd your dataand thenAdd a data source

In the pane that appears, selectUpload files (preview)underSelect data source. Azure OpenAI needs both a storage resource and a search resource to access and index your data.TipSee the following resource for more information:Data source optionssupported file types and formatsFor documents and datasets with long text, we recommend using the availabledata preparation script.For Azure OpenAI to access your storage account, you will need to turn onCross-origin resource sharing (CORS). If CORS isn't already turned on for the Azure Blob Storage resource, selectTurn on CORS.Select your Azure AI Search resource, and select the acknowledgment that connecting it will incur usage on your account. Then selectNext.
In the pane that appears, selectUpload files (preview)underSelect data source. Azure OpenAI needs both a storage resource and a search resource to access and index your data.
Tip
See the following resource for more information:Data source optionssupported file types and formats
Data source options
supported file types and formats
For documents and datasets with long text, we recommend using the availabledata preparation script.
For Azure OpenAI to access your storage account, you will need to turn onCross-origin resource sharing (CORS). If CORS isn't already turned on for the Azure Blob Storage resource, selectTurn on CORS.
For Azure OpenAI to access your storage account, you will need to turn onCross-origin resource sharing (CORS). If CORS isn't already turned on for the Azure Blob Storage resource, selectTurn on CORS.
Select your Azure AI Search resource, and select the acknowledgment that connecting it will incur usage on your account. Then selectNext.
Select your Azure AI Search resource, and select the acknowledgment that connecting it will incur usage on your account. Then selectNext.

On theUpload filespane, selectBrowse for a fileand select the files you downloaded from theprerequisitessection, or your own data. Then selectUpload files. Then selectNext.
On theUpload filespane, selectBrowse for a fileand select the files you downloaded from theprerequisitessection, or your own data. Then selectUpload files. Then selectNext.
On theData managementpane, you can choose whether to enablesemantic search or vector searchfor your index.ImportantSemantic searchandvector searchare subject to additional pricing. You need to chooseBasic or higher SKUto enable semantic search or vector search. Seepricing tier differenceandservice limitsfor more information.To help improve the quality of the information retrieval and model response, we recommend enablingsemantic searchfor the following data source languages: English, French, Spanish, Portuguese, Italian, Germany, Chinese(Zh), Japanese, Korean, Russian, and Arabic.
On theData managementpane, you can choose whether to enablesemantic search or vector searchfor your index.
Important
Semantic searchandvector searchare subject to additional pricing. You need to chooseBasic or higher SKUto enable semantic search or vector search. Seepricing tier differenceandservice limitsfor more information.
To help improve the quality of the information retrieval and model response, we recommend enablingsemantic searchfor the following data source languages: English, French, Spanish, Portuguese, Italian, Germany, Chinese(Zh), Japanese, Korean, Russian, and Arabic.
Review the details you entered, and selectSave and close. You can now chat with the model and it will use information from your data to construct the response.
Review the details you entered, and selectSave and close. You can now chat with the model and it will use information from your data to construct the response.
Retrieve resource information
You need to retrieve the following information to authenticate your application with your Azure OpenAI resource. This quickstart assumes you've uploaded your data to an Azure blob storage account and have an Azure AI Search index created. SeeAdd your data using Azure AI Foundry portal.
Microsoft Entra ID
API key
AZURE_OPENAI_ENDPOINT
https://my-resoruce.openai.azure.com
AZURE_OPENAI_DEPLOYMENT_NAME
AZURE_AI_SEARCH_ENDPOINT
AZURE_AI_SEARCH_INDEX
Learn more aboutkeyless authenticationandsetting environment variables.
AZURE_OPENAI_ENDPOINT
https://my-resoruce.openai.azure.com
AZURE_OPENAI_API_KEY
KEY1
KEY2
AZURE_OPENAI_DEPLOYMENT_NAME
AZURE_AI_SEARCH_ENDPOINT
AZURE_AI_SEARCH_API_KEY
AZURE_AI_SEARCH_INDEX
Learn more aboutfinding API keysandsetting environment variables.
Important
Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, seeAPI keys with Azure Key Vault.
For more information about AI services security, seeAuthenticate requests to Azure AI services.
Set up
Create a new folderuse-data-quickstartand go to the quickstart folder with the following command:mkdir use-data-quickstart && cd use-data-quickstart
Create a new folderuse-data-quickstartand go to the quickstart folder with the following command:
use-data-quickstart
mkdir use-data-quickstart && cd use-data-quickstart
mkdir use-data-quickstart && cd use-data-quickstart
Create thepackage.jsonwith the following command:npm init -y
Create thepackage.jsonwith the following command:
package.json
npm init -y
npm init -y
Install the OpenAI client library for JavaScript with:npm install openai
Install the OpenAI client library for JavaScript with:
npm install openai
npm install openai
For therecommendedpasswordless authentication:npm install @azure/identity
For therecommendedpasswordless authentication:
npm install @azure/identity
npm install @azure/identity
Add the JavaScript code
Microsoft Entra ID
API key
Create theindex.jsfile with the following code:const { DefaultAzureCredential, getBearerTokenProvider } = require("@azure/identity");
const { AzureOpenAI } = require("openai");

// Set the Azure and AI Search values from environment variables
const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";
const searchEndpoint = process.env.AZURE_AI_SEARCH_ENDPOINT || "Your search endpoint";
const searchIndex = process.env.AZURE_AI_SEARCH_INDEX || "Your search index";

// keyless authentication    
const credential = new DefaultAzureCredential();
const scope = "https://cognitiveservices.azure.com/.default";
const azureADTokenProvider = getBearerTokenProvider(credential, scope);

// Required Azure OpenAI deployment name and API version
const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "gpt-4";
const apiVersion = process.env.OPENAI_API_VERSION || "2024-10-21";

function getClient() {
  return new AzureOpenAI({
    endpoint,
    azureADTokenProvider,
    deployment: deploymentName,
    apiVersion,
  });
}

async function main() {
  const client = getClient();

  const messages = [
    { role: "user", content: "What are my available health plans?" },
  ];

  console.log(`Message: ${messages.map((m) => m.content).join("\n")}`);

  const events = await client.chat.completions.create({
    stream: true,
    messages: [
      {
        role: "user",
        content:
          "What's the most common feedback we received from our customers about the product?",
      },
    ],
    max_tokens: 128,
    model: "",
    data_sources: [
      {
        type: "azure_search",
        parameters: {
          endpoint: searchEndpoint,
          index_name: searchIndex,
          authentication: {
            type: "api_key",
            key: searchKey,
          },
        },
      },
    ],
  });

  let response = "";
  for await (const event of events) {
    for (const choice of event.choices) {
      const newText = choice.delta?.content;
      if (newText) {
        response += newText;
        // To see streaming results as they arrive, uncomment line below
        // console.log(newText);
      }
    }
  }
  console.log(response);
}

main().catch((err) => {
  console.error("The sample encountered an error:", err);
});
Create theindex.jsfile with the following code:
index.js
const { DefaultAzureCredential, getBearerTokenProvider } = require("@azure/identity");
const { AzureOpenAI } = require("openai");

// Set the Azure and AI Search values from environment variables
const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";
const searchEndpoint = process.env.AZURE_AI_SEARCH_ENDPOINT || "Your search endpoint";
const searchIndex = process.env.AZURE_AI_SEARCH_INDEX || "Your search index";

// keyless authentication    
const credential = new DefaultAzureCredential();
const scope = "https://cognitiveservices.azure.com/.default";
const azureADTokenProvider = getBearerTokenProvider(credential, scope);

// Required Azure OpenAI deployment name and API version
const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "gpt-4";
const apiVersion = process.env.OPENAI_API_VERSION || "2024-10-21";

function getClient() {
  return new AzureOpenAI({
    endpoint,
    azureADTokenProvider,
    deployment: deploymentName,
    apiVersion,
  });
}

async function main() {
  const client = getClient();

  const messages = [
    { role: "user", content: "What are my available health plans?" },
  ];

  console.log(`Message: ${messages.map((m) => m.content).join("\n")}`);

  const events = await client.chat.completions.create({
    stream: true,
    messages: [
      {
        role: "user",
        content:
          "What's the most common feedback we received from our customers about the product?",
      },
    ],
    max_tokens: 128,
    model: "",
    data_sources: [
      {
        type: "azure_search",
        parameters: {
          endpoint: searchEndpoint,
          index_name: searchIndex,
          authentication: {
            type: "api_key",
            key: searchKey,
          },
        },
      },
    ],
  });

  let response = "";
  for await (const event of events) {
    for (const choice of event.choices) {
      const newText = choice.delta?.content;
      if (newText) {
        response += newText;
        // To see streaming results as they arrive, uncomment line below
        // console.log(newText);
      }
    }
  }
  console.log(response);
}

main().catch((err) => {
  console.error("The sample encountered an error:", err);
});
const { DefaultAzureCredential, getBearerTokenProvider } = require("@azure/identity");
const { AzureOpenAI } = require("openai");

// Set the Azure and AI Search values from environment variables
const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";
const searchEndpoint = process.env.AZURE_AI_SEARCH_ENDPOINT || "Your search endpoint";
const searchIndex = process.env.AZURE_AI_SEARCH_INDEX || "Your search index";

// keyless authentication    
const credential = new DefaultAzureCredential();
const scope = "https://cognitiveservices.azure.com/.default";
const azureADTokenProvider = getBearerTokenProvider(credential, scope);

// Required Azure OpenAI deployment name and API version
const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "gpt-4";
const apiVersion = process.env.OPENAI_API_VERSION || "2024-10-21";

function getClient() {
  return new AzureOpenAI({
    endpoint,
    azureADTokenProvider,
    deployment: deploymentName,
    apiVersion,
  });
}

async function main() {
  const client = getClient();

  const messages = [
    { role: "user", content: "What are my available health plans?" },
  ];

  console.log(`Message: ${messages.map((m) => m.content).join("\n")}`);

  const events = await client.chat.completions.create({
    stream: true,
    messages: [
      {
        role: "user",
        content:
          "What's the most common feedback we received from our customers about the product?",
      },
    ],
    max_tokens: 128,
    model: "",
    data_sources: [
      {
        type: "azure_search",
        parameters: {
          endpoint: searchEndpoint,
          index_name: searchIndex,
          authentication: {
            type: "api_key",
            key: searchKey,
          },
        },
      },
    ],
  });

  let response = "";
  for await (const event of events) {
    for (const choice of event.choices) {
      const newText = choice.delta?.content;
      if (newText) {
        response += newText;
        // To see streaming results as they arrive, uncomment line below
        // console.log(newText);
      }
    }
  }
  console.log(response);
}

main().catch((err) => {
  console.error("The sample encountered an error:", err);
});
Sign in to Azure with the following command:az login
Sign in to Azure with the following command:
az login
az login
Run the JavaScript file.node index.js
Run the JavaScript file.
node index.js
node index.js
Create theindex.jsfile with the following code:const { AzureOpenAI } = require("openai");

// Set the Azure and AI Search values from environment variables
const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";
const apiKey = process.env.AZURE_OPENAI_API_KEY || "Your API key";
const searchEndpoint = process.env.AZURE_AI_SEARCH_ENDPOINT || "Your search endpoint";
const searchKey = process.env.AZURE_AI_SEARCH_API_KEY || "Your search key";
const searchIndex = process.env.AZURE_AI_SEARCH_INDEX || "Your search index";

// Required Azure OpenAI deployment name and API version
const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "gpt-4";
const apiVersion = process.env.OPENAI_API_VERSION || "2024-10-21";

function getClient() {
  return new AzureOpenAI({
    endpoint,
    apiKey,
    deployment: deploymentName,
    apiVersion,
  });
}

async function main() {
  const client = getClient();

  const messages = [
    { role: "user", content: "What are my available health plans?" },
  ];

  console.log(`Message: ${messages.map((m) => m.content).join("\n")}`);

  const events = await client.chat.completions.create({
    stream: true,
    messages: [
      {
        role: "user",
        content:
          "What's the most common feedback we received from our customers about the product?",
      },
    ],
    max_tokens: 128,
    model: "",
    data_sources: [
      {
        type: "azure_search",
        parameters: {
          endpoint: searchEndpoint,
          index_name: searchIndex,
          authentication: {
            type: "api_key",
            key: searchKey,
          },
        },
      },
    ],
  });

  let response = "";
  for await (const event of events) {
    for (const choice of event.choices) {
      const newText = choice.delta?.content;
      if (newText) {
        response += newText;
        // To see streaming results as they arrive, uncomment line below
        // console.log(newText);
      }
    }
  }
  console.log(response);
}

main().catch((err) => {
  console.error("The sample encountered an error:", err);
});
Create theindex.jsfile with the following code:
index.js
const { AzureOpenAI } = require("openai");

// Set the Azure and AI Search values from environment variables
const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";
const apiKey = process.env.AZURE_OPENAI_API_KEY || "Your API key";
const searchEndpoint = process.env.AZURE_AI_SEARCH_ENDPOINT || "Your search endpoint";
const searchKey = process.env.AZURE_AI_SEARCH_API_KEY || "Your search key";
const searchIndex = process.env.AZURE_AI_SEARCH_INDEX || "Your search index";

// Required Azure OpenAI deployment name and API version
const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "gpt-4";
const apiVersion = process.env.OPENAI_API_VERSION || "2024-10-21";

function getClient() {
  return new AzureOpenAI({
    endpoint,
    apiKey,
    deployment: deploymentName,
    apiVersion,
  });
}

async function main() {
  const client = getClient();

  const messages = [
    { role: "user", content: "What are my available health plans?" },
  ];

  console.log(`Message: ${messages.map((m) => m.content).join("\n")}`);

  const events = await client.chat.completions.create({
    stream: true,
    messages: [
      {
        role: "user",
        content:
          "What's the most common feedback we received from our customers about the product?",
      },
    ],
    max_tokens: 128,
    model: "",
    data_sources: [
      {
        type: "azure_search",
        parameters: {
          endpoint: searchEndpoint,
          index_name: searchIndex,
          authentication: {
            type: "api_key",
            key: searchKey,
          },
        },
      },
    ],
  });

  let response = "";
  for await (const event of events) {
    for (const choice of event.choices) {
      const newText = choice.delta?.content;
      if (newText) {
        response += newText;
        // To see streaming results as they arrive, uncomment line below
        // console.log(newText);
      }
    }
  }
  console.log(response);
}

main().catch((err) => {
  console.error("The sample encountered an error:", err);
});
const { AzureOpenAI } = require("openai");

// Set the Azure and AI Search values from environment variables
const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";
const apiKey = process.env.AZURE_OPENAI_API_KEY || "Your API key";
const searchEndpoint = process.env.AZURE_AI_SEARCH_ENDPOINT || "Your search endpoint";
const searchKey = process.env.AZURE_AI_SEARCH_API_KEY || "Your search key";
const searchIndex = process.env.AZURE_AI_SEARCH_INDEX || "Your search index";

// Required Azure OpenAI deployment name and API version
const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "gpt-4";
const apiVersion = process.env.OPENAI_API_VERSION || "2024-10-21";

function getClient() {
  return new AzureOpenAI({
    endpoint,
    apiKey,
    deployment: deploymentName,
    apiVersion,
  });
}

async function main() {
  const client = getClient();

  const messages = [
    { role: "user", content: "What are my available health plans?" },
  ];

  console.log(`Message: ${messages.map((m) => m.content).join("\n")}`);

  const events = await client.chat.completions.create({
    stream: true,
    messages: [
      {
        role: "user",
        content:
          "What's the most common feedback we received from our customers about the product?",
      },
    ],
    max_tokens: 128,
    model: "",
    data_sources: [
      {
        type: "azure_search",
        parameters: {
          endpoint: searchEndpoint,
          index_name: searchIndex,
          authentication: {
            type: "api_key",
            key: searchKey,
          },
        },
      },
    ],
  });

  let response = "";
  for await (const event of events) {
    for (const choice of event.choices) {
      const newText = choice.delta?.content;
      if (newText) {
        response += newText;
        // To see streaming results as they arrive, uncomment line below
        // console.log(newText);
      }
    }
  }
  console.log(response);
}

main().catch((err) => {
  console.error("The sample encountered an error:", err);
});
Run the JavaScript file.node index.js
Run the JavaScript file.
node index.js
node index.js
Output
Message: What are my available health plans?
The available health plans in the Contoso Electronics plan and benefit packages are the Northwind Health Plus and Northwind Standard plans.
Message: What are my available health plans?
The available health plans in the Contoso Electronics plan and benefit packages are the Northwind Health Plus and Northwind Standard plans.
Reference documentation|Source code|Package (npm)|Samples
Prerequisites
An Azure subscription -Create one for free.
An Azure subscription -Create one for free.
LTS versions of Node.js
LTS versions of Node.js
TypeScript
TypeScript
Azure CLIused for passwordless authentication in a local development environment, create the necessary context by signing in with the Azure CLI.
Azure CLIused for passwordless authentication in a local development environment, create the necessary context by signing in with the Azure CLI.
An Azure OpenAI resource deployed in asupported region and with a supported model.
An Azure OpenAI resource deployed in asupported region and with a supported model.
Be sure that you're assigned at least theCognitive Services Contributorrole for the Azure OpenAI resource.
Be sure that you're assigned at least theCognitive Services Contributorrole for the Azure OpenAI resource.
Download the example data fromGitHubif you don't have your own data.
Download the example data fromGitHubif you don't have your own data.
Microsoft Entra ID prerequisites
For the recommended keyless authentication with Microsoft Entra ID, you need to:
Install theAzure CLIused for keyless authentication with Microsoft Entra ID.
Assign theCognitive Services Userrole to your user account. You can assign roles in the Azure portal underAccess control (IAM)>Add role assignment.
Cognitive Services User
Add your data using Azure AI Foundry portal
Tip
You canuse the Azure Developer CLIto programmatically create the resources needed for Azure OpenAI On Your Data
Navigate toAzure AI Foundry portaland sign-in with credentials that have access to your Azure OpenAI resource.
You can eithercreate an Azure AI Foundry projectby clickingCreate project, or continue directly by clicking the button on theFocused on Azure OpenAI Servicetile.
You can eithercreate an Azure AI Foundry projectby clickingCreate project, or continue directly by clicking the button on theFocused on Azure OpenAI Servicetile.

SelectChatunderPlaygroundsin the left pane, and select your model deployment.
SelectChatunderPlaygroundsin the left pane, and select your model deployment.
In theChat playground, SelectAdd your dataand thenAdd a data source
In theChat playground, SelectAdd your dataand thenAdd a data source

In the pane that appears, selectUpload files (preview)underSelect data source. Azure OpenAI needs both a storage resource and a search resource to access and index your data.TipSee the following resource for more information:Data source optionssupported file types and formatsFor documents and datasets with long text, we recommend using the availabledata preparation script.For Azure OpenAI to access your storage account, you will need to turn onCross-origin resource sharing (CORS). If CORS isn't already turned on for the Azure Blob Storage resource, selectTurn on CORS.Select your Azure AI Search resource, and select the acknowledgment that connecting it will incur usage on your account. Then selectNext.
In the pane that appears, selectUpload files (preview)underSelect data source. Azure OpenAI needs both a storage resource and a search resource to access and index your data.
Tip
See the following resource for more information:Data source optionssupported file types and formats
Data source options
supported file types and formats
For documents and datasets with long text, we recommend using the availabledata preparation script.
For Azure OpenAI to access your storage account, you will need to turn onCross-origin resource sharing (CORS). If CORS isn't already turned on for the Azure Blob Storage resource, selectTurn on CORS.
For Azure OpenAI to access your storage account, you will need to turn onCross-origin resource sharing (CORS). If CORS isn't already turned on for the Azure Blob Storage resource, selectTurn on CORS.
Select your Azure AI Search resource, and select the acknowledgment that connecting it will incur usage on your account. Then selectNext.
Select your Azure AI Search resource, and select the acknowledgment that connecting it will incur usage on your account. Then selectNext.

On theUpload filespane, selectBrowse for a fileand select the files you downloaded from theprerequisitessection, or your own data. Then selectUpload files. Then selectNext.
On theUpload filespane, selectBrowse for a fileand select the files you downloaded from theprerequisitessection, or your own data. Then selectUpload files. Then selectNext.
On theData managementpane, you can choose whether to enablesemantic search or vector searchfor your index.ImportantSemantic searchandvector searchare subject to additional pricing. You need to chooseBasic or higher SKUto enable semantic search or vector search. Seepricing tier differenceandservice limitsfor more information.To help improve the quality of the information retrieval and model response, we recommend enablingsemantic searchfor the following data source languages: English, French, Spanish, Portuguese, Italian, Germany, Chinese(Zh), Japanese, Korean, Russian, and Arabic.
On theData managementpane, you can choose whether to enablesemantic search or vector searchfor your index.
Important
Semantic searchandvector searchare subject to additional pricing. You need to chooseBasic or higher SKUto enable semantic search or vector search. Seepricing tier differenceandservice limitsfor more information.
To help improve the quality of the information retrieval and model response, we recommend enablingsemantic searchfor the following data source languages: English, French, Spanish, Portuguese, Italian, Germany, Chinese(Zh), Japanese, Korean, Russian, and Arabic.
Review the details you entered, and selectSave and close. You can now chat with the model and it will use information from your data to construct the response.
Review the details you entered, and selectSave and close. You can now chat with the model and it will use information from your data to construct the response.
Retrieve resource information
You need to retrieve the following information to authenticate your application with your Azure OpenAI resource. This quickstart assumes you've uploaded your data to an Azure blob storage account and have an Azure AI Search index created. SeeAdd your data using Azure AI Foundry portal.
Microsoft Entra ID
API key
AZURE_OPENAI_ENDPOINT
https://my-resoruce.openai.azure.com
AZURE_OPENAI_DEPLOYMENT_NAME
AZURE_AI_SEARCH_ENDPOINT
AZURE_AI_SEARCH_INDEX
Learn more aboutkeyless authenticationandsetting environment variables.
AZURE_OPENAI_ENDPOINT
https://my-resoruce.openai.azure.com
AZURE_OPENAI_API_KEY
KEY1
KEY2
AZURE_OPENAI_DEPLOYMENT_NAME
AZURE_AI_SEARCH_ENDPOINT
AZURE_AI_SEARCH_API_KEY
AZURE_AI_SEARCH_INDEX
Learn more aboutfinding API keysandsetting environment variables.
Important
Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, seeAPI keys with Azure Key Vault.
For more information about AI services security, seeAuthenticate requests to Azure AI services.
Set up
Create a new folderuse-data-quickstartand go to the quickstart folder with the following command:mkdir use-data-quickstart && cd use-data-quickstart
Create a new folderuse-data-quickstartand go to the quickstart folder with the following command:
use-data-quickstart
mkdir use-data-quickstart && cd use-data-quickstart
mkdir use-data-quickstart && cd use-data-quickstart
Create thepackage.jsonwith the following command:npm init -y
Create thepackage.jsonwith the following command:
package.json
npm init -y
npm init -y
Update thepackage.jsonto ECMAScript with the following command:npm pkg set type=module
Update thepackage.jsonto ECMAScript with the following command:
package.json
npm pkg set type=module
npm pkg set type=module
Install the OpenAI client library for JavaScript with:npm install openai
Install the OpenAI client library for JavaScript with:
npm install openai
npm install openai
For therecommendedpasswordless authentication:npm install @azure/identity
For therecommendedpasswordless authentication:
npm install @azure/identity
npm install @azure/identity
Add the TypeScript code
Microsoft Entra ID
API key
Create theindex.tsfile with the following code:import { AzureOpenAI } from "openai";
import { DefaultAzureCredential, getBearerTokenProvider } from "@azure/identity";
import "@azure/openai/types";

// Set the Azure and AI Search values from environment variables
const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";
const searchEndpoint = process.env.AZURE_AI_SEARCH_ENDPOINT || "Your search endpoint";
const searchIndex = process.env.AZURE_AI_SEARCH_INDEX || "Your search index";

// keyless authentication    
const credential = new DefaultAzureCredential();
const scope = "https://cognitiveservices.azure.com/.default";
const azureADTokenProvider = getBearerTokenProvider(credential, scope);

// Required Azure OpenAI deployment name and API version
const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "gpt-4";
const apiVersion = process.env.OPENAI_API_VERSION || "2024-07-01-preview";

function getClient(): AzureOpenAI {
  return new AzureOpenAI({
    endpoint,
    azureADTokenProvider,
    deployment: deploymentName,
    apiVersion,
  });
}

async function main() {
  const client = getClient();

  const messages = [
    { role: "user", content: "What are my available health plans?" },
  ];

  console.log(`Message: ${messages.map((m) => m.content).join("\n")}`);

  const events = await client.chat.completions.create({
    stream: true,
    messages: [
      {
        role: "user",
        content:
          "What's the most common feedback we received from our customers about the product?",
      },
    ],
    max_tokens: 128,
    model: "",
    data_sources: [
      {
        type: "azure_search",
        parameters: {
          endpoint: searchEndpoint,
          index_name: searchIndex,
          authentication: {
            type: "api_key",
            key: searchKey,
          },
        },
      },
    ],
  });

  let response = "";
  for await (const event of events) {
    for (const choice of event.choices) {
      const newText = choice.delta?.content;
      if (newText) {
        response += newText;
        // To see streaming results as they arrive, uncomment line below
        // console.log(newText);
      }
    }
  }
  console.log(response);
}

main().catch((err) => {
  console.error("The sample encountered an error:", err);
});
Create theindex.tsfile with the following code:
index.ts
import { AzureOpenAI } from "openai";
import { DefaultAzureCredential, getBearerTokenProvider } from "@azure/identity";
import "@azure/openai/types";

// Set the Azure and AI Search values from environment variables
const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";
const searchEndpoint = process.env.AZURE_AI_SEARCH_ENDPOINT || "Your search endpoint";
const searchIndex = process.env.AZURE_AI_SEARCH_INDEX || "Your search index";

// keyless authentication    
const credential = new DefaultAzureCredential();
const scope = "https://cognitiveservices.azure.com/.default";
const azureADTokenProvider = getBearerTokenProvider(credential, scope);

// Required Azure OpenAI deployment name and API version
const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "gpt-4";
const apiVersion = process.env.OPENAI_API_VERSION || "2024-07-01-preview";

function getClient(): AzureOpenAI {
  return new AzureOpenAI({
    endpoint,
    azureADTokenProvider,
    deployment: deploymentName,
    apiVersion,
  });
}

async function main() {
  const client = getClient();

  const messages = [
    { role: "user", content: "What are my available health plans?" },
  ];

  console.log(`Message: ${messages.map((m) => m.content).join("\n")}`);

  const events = await client.chat.completions.create({
    stream: true,
    messages: [
      {
        role: "user",
        content:
          "What's the most common feedback we received from our customers about the product?",
      },
    ],
    max_tokens: 128,
    model: "",
    data_sources: [
      {
        type: "azure_search",
        parameters: {
          endpoint: searchEndpoint,
          index_name: searchIndex,
          authentication: {
            type: "api_key",
            key: searchKey,
          },
        },
      },
    ],
  });

  let response = "";
  for await (const event of events) {
    for (const choice of event.choices) {
      const newText = choice.delta?.content;
      if (newText) {
        response += newText;
        // To see streaming results as they arrive, uncomment line below
        // console.log(newText);
      }
    }
  }
  console.log(response);
}

main().catch((err) => {
  console.error("The sample encountered an error:", err);
});
import { AzureOpenAI } from "openai";
import { DefaultAzureCredential, getBearerTokenProvider } from "@azure/identity";
import "@azure/openai/types";

// Set the Azure and AI Search values from environment variables
const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";
const searchEndpoint = process.env.AZURE_AI_SEARCH_ENDPOINT || "Your search endpoint";
const searchIndex = process.env.AZURE_AI_SEARCH_INDEX || "Your search index";

// keyless authentication    
const credential = new DefaultAzureCredential();
const scope = "https://cognitiveservices.azure.com/.default";
const azureADTokenProvider = getBearerTokenProvider(credential, scope);

// Required Azure OpenAI deployment name and API version
const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "gpt-4";
const apiVersion = process.env.OPENAI_API_VERSION || "2024-07-01-preview";

function getClient(): AzureOpenAI {
  return new AzureOpenAI({
    endpoint,
    azureADTokenProvider,
    deployment: deploymentName,
    apiVersion,
  });
}

async function main() {
  const client = getClient();

  const messages = [
    { role: "user", content: "What are my available health plans?" },
  ];

  console.log(`Message: ${messages.map((m) => m.content).join("\n")}`);

  const events = await client.chat.completions.create({
    stream: true,
    messages: [
      {
        role: "user",
        content:
          "What's the most common feedback we received from our customers about the product?",
      },
    ],
    max_tokens: 128,
    model: "",
    data_sources: [
      {
        type: "azure_search",
        parameters: {
          endpoint: searchEndpoint,
          index_name: searchIndex,
          authentication: {
            type: "api_key",
            key: searchKey,
          },
        },
      },
    ],
  });

  let response = "";
  for await (const event of events) {
    for (const choice of event.choices) {
      const newText = choice.delta?.content;
      if (newText) {
        response += newText;
        // To see streaming results as they arrive, uncomment line below
        // console.log(newText);
      }
    }
  }
  console.log(response);
}

main().catch((err) => {
  console.error("The sample encountered an error:", err);
});
Create thetsconfig.jsonfile to transpile the TypeScript code and copy the following code for ECMAScript.{
    "compilerOptions": {
      "module": "NodeNext",
      "target": "ES2022", // Supports top-level await
      "moduleResolution": "NodeNext",
      "skipLibCheck": true, // Avoid type errors from node_modules
      "strict": true // Enable strict type-checking options
    },
    "include": ["*.ts"]
}
Create thetsconfig.jsonfile to transpile the TypeScript code and copy the following code for ECMAScript.
tsconfig.json
{
    "compilerOptions": {
      "module": "NodeNext",
      "target": "ES2022", // Supports top-level await
      "moduleResolution": "NodeNext",
      "skipLibCheck": true, // Avoid type errors from node_modules
      "strict": true // Enable strict type-checking options
    },
    "include": ["*.ts"]
}
{
    "compilerOptions": {
      "module": "NodeNext",
      "target": "ES2022", // Supports top-level await
      "moduleResolution": "NodeNext",
      "skipLibCheck": true, // Avoid type errors from node_modules
      "strict": true // Enable strict type-checking options
    },
    "include": ["*.ts"]
}
Transpile from TypeScript to JavaScript.tsc
Transpile from TypeScript to JavaScript.
tsc
tsc
Sign in to Azure with the following command:az login
Sign in to Azure with the following command:
az login
az login
Run the code with the following command:node index.js
Run the code with the following command:
node index.js
node index.js
Create theindex.tsfile with the following code:import { AzureOpenAI } from "openai";
import "@azure/openai/types";

// Set the Azure and AI Search values from environment variables
const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";
const apiKey = process.env.AZURE_OPENAI_API_KEY || "Your API key";
const searchEndpoint = process.env.AZURE_AI_SEARCH_ENDPOINT || "Your search endpoint";
const searchKey = process.env.AZURE_AI_SEARCH_API_KEY || "Your search key";
const searchIndex = process.env.AZURE_AI_SEARCH_INDEX || "Your search index";

// Required Azure OpenAI deployment name and API version
const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "gpt-4";
const apiVersion = process.env.OPENAI_API_VERSION || "2024-07-01-preview";

function getClient(): AzureOpenAI {
  return new AzureOpenAI({
    endpoint,
    apiKey,
    deployment: deploymentName,
    apiVersion,
  });
}

async function main() {
  const client = getClient();

  const messages = [
    { role: "user", content: "What are my available health plans?" },
  ];

  console.log(`Message: ${messages.map((m) => m.content).join("\n")}`);

  const events = await client.chat.completions.create({
    stream: true,
    messages: [
      {
        role: "user",
        content:
          "What's the most common feedback we received from our customers about the product?",
      },
    ],
    max_tokens: 128,
    model: "",
    data_sources: [
      {
        type: "azure_search",
        parameters: {
          endpoint: searchEndpoint,
          index_name: searchIndex,
          authentication: {
            type: "api_key",
            key: searchKey,
          },
        },
      },
    ],
  });

  let response = "";
  for await (const event of events) {
    for (const choice of event.choices) {
      const newText = choice.delta?.content;
      if (newText) {
        response += newText;
        // To see streaming results as they arrive, uncomment line below
        // console.log(newText);
      }
    }
  }
  console.log(response);
}

main().catch((err) => {
  console.error("The sample encountered an error:", err);
});
Create theindex.tsfile with the following code:
index.ts
import { AzureOpenAI } from "openai";
import "@azure/openai/types";

// Set the Azure and AI Search values from environment variables
const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";
const apiKey = process.env.AZURE_OPENAI_API_KEY || "Your API key";
const searchEndpoint = process.env.AZURE_AI_SEARCH_ENDPOINT || "Your search endpoint";
const searchKey = process.env.AZURE_AI_SEARCH_API_KEY || "Your search key";
const searchIndex = process.env.AZURE_AI_SEARCH_INDEX || "Your search index";

// Required Azure OpenAI deployment name and API version
const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "gpt-4";
const apiVersion = process.env.OPENAI_API_VERSION || "2024-07-01-preview";

function getClient(): AzureOpenAI {
  return new AzureOpenAI({
    endpoint,
    apiKey,
    deployment: deploymentName,
    apiVersion,
  });
}

async function main() {
  const client = getClient();

  const messages = [
    { role: "user", content: "What are my available health plans?" },
  ];

  console.log(`Message: ${messages.map((m) => m.content).join("\n")}`);

  const events = await client.chat.completions.create({
    stream: true,
    messages: [
      {
        role: "user",
        content:
          "What's the most common feedback we received from our customers about the product?",
      },
    ],
    max_tokens: 128,
    model: "",
    data_sources: [
      {
        type: "azure_search",
        parameters: {
          endpoint: searchEndpoint,
          index_name: searchIndex,
          authentication: {
            type: "api_key",
            key: searchKey,
          },
        },
      },
    ],
  });

  let response = "";
  for await (const event of events) {
    for (const choice of event.choices) {
      const newText = choice.delta?.content;
      if (newText) {
        response += newText;
        // To see streaming results as they arrive, uncomment line below
        // console.log(newText);
      }
    }
  }
  console.log(response);
}

main().catch((err) => {
  console.error("The sample encountered an error:", err);
});
import { AzureOpenAI } from "openai";
import "@azure/openai/types";

// Set the Azure and AI Search values from environment variables
const endpoint = process.env.AZURE_OPENAI_ENDPOINT || "Your endpoint";
const apiKey = process.env.AZURE_OPENAI_API_KEY || "Your API key";
const searchEndpoint = process.env.AZURE_AI_SEARCH_ENDPOINT || "Your search endpoint";
const searchKey = process.env.AZURE_AI_SEARCH_API_KEY || "Your search key";
const searchIndex = process.env.AZURE_AI_SEARCH_INDEX || "Your search index";

// Required Azure OpenAI deployment name and API version
const deploymentName = process.env.AZURE_OPENAI_DEPLOYMENT_NAME || "gpt-4";
const apiVersion = process.env.OPENAI_API_VERSION || "2024-07-01-preview";

function getClient(): AzureOpenAI {
  return new AzureOpenAI({
    endpoint,
    apiKey,
    deployment: deploymentName,
    apiVersion,
  });
}

async function main() {
  const client = getClient();

  const messages = [
    { role: "user", content: "What are my available health plans?" },
  ];

  console.log(`Message: ${messages.map((m) => m.content).join("\n")}`);

  const events = await client.chat.completions.create({
    stream: true,
    messages: [
      {
        role: "user",
        content:
          "What's the most common feedback we received from our customers about the product?",
      },
    ],
    max_tokens: 128,
    model: "",
    data_sources: [
      {
        type: "azure_search",
        parameters: {
          endpoint: searchEndpoint,
          index_name: searchIndex,
          authentication: {
            type: "api_key",
            key: searchKey,
          },
        },
      },
    ],
  });

  let response = "";
  for await (const event of events) {
    for (const choice of event.choices) {
      const newText = choice.delta?.content;
      if (newText) {
        response += newText;
        // To see streaming results as they arrive, uncomment line below
        // console.log(newText);
      }
    }
  }
  console.log(response);
}

main().catch((err) => {
  console.error("The sample encountered an error:", err);
});
Create thetsconfig.jsonfile to transpile the TypeScript code and copy the following code for ECMAScript.{
    "compilerOptions": {
      "module": "NodeNext",
      "target": "ES2022", // Supports top-level await
      "moduleResolution": "NodeNext",
      "skipLibCheck": true, // Avoid type errors from node_modules
      "strict": true // Enable strict type-checking options
    },
    "include": ["*.ts"]
}
Create thetsconfig.jsonfile to transpile the TypeScript code and copy the following code for ECMAScript.
tsconfig.json
{
    "compilerOptions": {
      "module": "NodeNext",
      "target": "ES2022", // Supports top-level await
      "moduleResolution": "NodeNext",
      "skipLibCheck": true, // Avoid type errors from node_modules
      "strict": true // Enable strict type-checking options
    },
    "include": ["*.ts"]
}
{
    "compilerOptions": {
      "module": "NodeNext",
      "target": "ES2022", // Supports top-level await
      "moduleResolution": "NodeNext",
      "skipLibCheck": true, // Avoid type errors from node_modules
      "strict": true // Enable strict type-checking options
    },
    "include": ["*.ts"]
}
Transpile from TypeScript to JavaScript.tsc
Transpile from TypeScript to JavaScript.
tsc
tsc
Run the code with the following command:node index.js
Run the code with the following command:
node index.js
node index.js
Important
For production, use a secure way of storing and accessing your credentials likeAzure Key Vault. For more information about credential security, see the Azure AI servicessecurityarticle.
Output
Message: What are my available health plans?
The available health plans in the Contoso Electronics plan and benefit packages are the Northwind Health Plus and Northwind Standard plans.
Message: What are my available health plans?
The available health plans in the Contoso Electronics plan and benefit packages are the Northwind Health Plus and Northwind Standard plans.
Prerequisites
The following resources:
Azure OpenAI
Azure Blob Storage
Azure AI Search
AnAzure OpenAI resourcedeployed in asupported region and with a supported model.Be sure that you're assigned at least theCognitive Services Contributorrole for the Azure OpenAI resource.
Be sure that you're assigned at least theCognitive Services Contributorrole for the Azure OpenAI resource.
Download the example data fromGitHubif you don't have your own data.
Reference|Source code|Package (pypi)|Samples
These links reference the OpenAI API for Python. There's no Azure-specific OpenAI Python SDK.Learn how to switch between the OpenAI services and Azure OpenAI services.
Add your data using Azure AI Foundry portal
Tip
You canuse the Azure Developer CLIto programmatically create the resources needed for Azure OpenAI On Your Data
Navigate toAzure AI Foundry portaland sign-in with credentials that have access to your Azure OpenAI resource.
You can eithercreate an Azure AI Foundry projectby clickingCreate project, or continue directly by clicking the button on theFocused on Azure OpenAI Servicetile.
You can eithercreate an Azure AI Foundry projectby clickingCreate project, or continue directly by clicking the button on theFocused on Azure OpenAI Servicetile.

SelectChatunderPlaygroundsin the left pane, and select your model deployment.
SelectChatunderPlaygroundsin the left pane, and select your model deployment.
In theChat playground, SelectAdd your dataand thenAdd a data source
In theChat playground, SelectAdd your dataand thenAdd a data source

In the pane that appears, selectUpload files (preview)underSelect data source. Azure OpenAI needs both a storage resource and a search resource to access and index your data.TipSee the following resource for more information:Data source optionssupported file types and formatsFor documents and datasets with long text, we recommend using the availabledata preparation script.For Azure OpenAI to access your storage account, you will need to turn onCross-origin resource sharing (CORS). If CORS isn't already turned on for the Azure Blob Storage resource, selectTurn on CORS.Select your Azure AI Search resource, and select the acknowledgment that connecting it will incur usage on your account. Then selectNext.
In the pane that appears, selectUpload files (preview)underSelect data source. Azure OpenAI needs both a storage resource and a search resource to access and index your data.
Tip
See the following resource for more information:Data source optionssupported file types and formats
Data source options
supported file types and formats
For documents and datasets with long text, we recommend using the availabledata preparation script.
For Azure OpenAI to access your storage account, you will need to turn onCross-origin resource sharing (CORS). If CORS isn't already turned on for the Azure Blob Storage resource, selectTurn on CORS.
For Azure OpenAI to access your storage account, you will need to turn onCross-origin resource sharing (CORS). If CORS isn't already turned on for the Azure Blob Storage resource, selectTurn on CORS.
Select your Azure AI Search resource, and select the acknowledgment that connecting it will incur usage on your account. Then selectNext.
Select your Azure AI Search resource, and select the acknowledgment that connecting it will incur usage on your account. Then selectNext.

On theUpload filespane, selectBrowse for a fileand select the files you downloaded from theprerequisitessection, or your own data. Then selectUpload files. Then selectNext.
On theUpload filespane, selectBrowse for a fileand select the files you downloaded from theprerequisitessection, or your own data. Then selectUpload files. Then selectNext.
On theData managementpane, you can choose whether to enablesemantic search or vector searchfor your index.ImportantSemantic searchandvector searchare subject to additional pricing. You need to chooseBasic or higher SKUto enable semantic search or vector search. Seepricing tier differenceandservice limitsfor more information.To help improve the quality of the information retrieval and model response, we recommend enablingsemantic searchfor the following data source languages: English, French, Spanish, Portuguese, Italian, Germany, Chinese(Zh), Japanese, Korean, Russian, and Arabic.
On theData managementpane, you can choose whether to enablesemantic search or vector searchfor your index.
Important
Semantic searchandvector searchare subject to additional pricing. You need to chooseBasic or higher SKUto enable semantic search or vector search. Seepricing tier differenceandservice limitsfor more information.
To help improve the quality of the information retrieval and model response, we recommend enablingsemantic searchfor the following data source languages: English, French, Spanish, Portuguese, Italian, Germany, Chinese(Zh), Japanese, Korean, Russian, and Arabic.
Review the details you entered, and selectSave and close. You can now chat with the model and it will use information from your data to construct the response.
Review the details you entered, and selectSave and close. You can now chat with the model and it will use information from your data to construct the response.
Retrieve resource information
You need to retrieve the following information to authenticate your application with your Azure OpenAI resource. This quickstart assumes you've uploaded your data to an Azure blob storage account and have an Azure AI Search index created. SeeAdd your data using Azure AI Foundry portal.
Microsoft Entra ID
API key
AZURE_OPENAI_ENDPOINT
https://my-resoruce.openai.azure.com
AZURE_OPENAI_DEPLOYMENT_NAME
AZURE_AI_SEARCH_ENDPOINT
AZURE_AI_SEARCH_INDEX
Learn more aboutkeyless authenticationandsetting environment variables.
AZURE_OPENAI_ENDPOINT
https://my-resoruce.openai.azure.com
AZURE_OPENAI_API_KEY
KEY1
KEY2
AZURE_OPENAI_DEPLOYMENT_NAME
AZURE_AI_SEARCH_ENDPOINT
AZURE_AI_SEARCH_API_KEY
AZURE_AI_SEARCH_INDEX
Learn more aboutfinding API keysandsetting environment variables.
Important
Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, seeAPI keys with Azure Key Vault.
For more information about AI services security, seeAuthenticate requests to Azure AI services.
Create a Python environment
Create a new folder namedopenai-pythonfor your project and a new Python code file namedmain.py. Change into that directory:
mkdir openai-python
cd openai-python
mkdir openai-python
cd openai-python
Install the following Python Libraries:
OpenAI Python 1.x
OpenAI Python 0.28.1
pip install openai
pip install python-dotenv
pip install openai
pip install python-dotenv
Note
The OpenAI Python library version0.28.1is deprecated. We recommend using1.x. Consult ourmigration guidefor information on moving from0.28.1to1.x.
0.28.1
1.x
0.28.1
1.x
pip install openai==0.28.1
pip install python-dotenv
pip install openai==0.28.1
pip install python-dotenv
Create the Python app
From the project directory, open themain.pyfile and add the following code:
OpenAI Python 1.x
OpenAI Python 0.28.1
import os
import openai
import dotenv

dotenv.load_dotenv()

endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT")
api_key = os.environ.get("AZURE_OPENAI_API_KEY")
deployment = os.environ.get("AZURE_OPENAI_DEPLOYMENT_NAME")

client = openai.AzureOpenAI(
    azure_endpoint=endpoint,
    api_key=api_key,
    api_version="2024-10-21",
)

completion = client.chat.completions.create(
    model=deployment,
    messages=[
        {
            "role": "user",
            "content": "What are my available health plans?",
        },
    ],
    extra_body={
        "data_sources":[
            {
                "type": "azure_search",
                "parameters": {
                    "endpoint": os.environ["AZURE_AI_SEARCH_ENDPOINT"],
                    "index_name": os.environ["AZURE_AI_SEARCH_INDEX"],
                    "authentication": {
                        "type": "api_key",
                        "key": os.environ["AZURE_AI_SEARCH_API_KEY"],
                    }
                }
            }
        ],
    }
)

print(f"{completion.choices[0].message.role}: {completion.choices[0].message.content}")
import os
import openai
import dotenv

dotenv.load_dotenv()

endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT")
api_key = os.environ.get("AZURE_OPENAI_API_KEY")
deployment = os.environ.get("AZURE_OPENAI_DEPLOYMENT_NAME")

client = openai.AzureOpenAI(
    azure_endpoint=endpoint,
    api_key=api_key,
    api_version="2024-10-21",
)

completion = client.chat.completions.create(
    model=deployment,
    messages=[
        {
            "role": "user",
            "content": "What are my available health plans?",
        },
    ],
    extra_body={
        "data_sources":[
            {
                "type": "azure_search",
                "parameters": {
                    "endpoint": os.environ["AZURE_AI_SEARCH_ENDPOINT"],
                    "index_name": os.environ["AZURE_AI_SEARCH_INDEX"],
                    "authentication": {
                        "type": "api_key",
                        "key": os.environ["AZURE_AI_SEARCH_API_KEY"],
                    }
                }
            }
        ],
    }
)

print(f"{completion.choices[0].message.role}: {completion.choices[0].message.content}")
import os
import openai
import dotenv
import requests

dotenv.load_dotenv()

openai.api_base = os.environ.get("AZURE_OPENAI_ENDPOINT")

# Azure OpenAI on your own data is only supported by the 2023-08-01-preview API version
openai.api_version = "2023-08-01-preview"
openai.api_type = 'azure'
openai.api_key = os.environ.get("AZURE_OPENAI_API_KEY")

def setup_byod(deployment_id: str) -> None:
    """Sets up the OpenAI Python SDK to use your own data for the chat endpoint.
 
    :param deployment_id: The deployment ID for the model to use with your own data.

    To remove this configuration, simply set openai.requestssession to None.
    """

    class BringYourOwnDataAdapter(requests.adapters.HTTPAdapter):

     def send(self, request, **kwargs):
         request.url = f"{openai.api_base}/openai/deployments/{deployment_id}/extensions/chat/completions?api-version={openai.api_version}"
         return super().send(request, **kwargs)

    session = requests.Session()

    # Mount a custom adapter which will use the extensions endpoint for any call using the given `deployment_id`
    session.mount(
        prefix=f"{openai.api_base}/openai/deployments/{deployment_id}",
        adapter=BringYourOwnDataAdapter()
    )

    openai.requestssession = session

aoai_deployment_id = os.environ.get("AZURE_OPENAI_DEPLOYMENT_NAME")
setup_byod(aoai_deployment_id)

completion = openai.ChatCompletion.create(
    messages=[{"role": "user", "content": "What are my available health plans?"}],
    deployment_id=os.environ.get("AZURE_OPENAI_DEPLOYMENT_NAME"),
    dataSources=[  # camelCase is intentional, as this is the format the API expects
        {
            "type": "AzureCognitiveSearch",
            "parameters": {
                "endpoint": os.environ.get("AZURE_AI_SEARCH_ENDPOINT"),
                "key": os.environ.get("AZURE_AI_SEARCH_API_KEY"),
                "indexName": os.environ.get("AZURE_AI_SEARCH_INDEX"),
            }
        }
    ]
)
print(completion)
import os
import openai
import dotenv
import requests

dotenv.load_dotenv()

openai.api_base = os.environ.get("AZURE_OPENAI_ENDPOINT")

# Azure OpenAI on your own data is only supported by the 2023-08-01-preview API version
openai.api_version = "2023-08-01-preview"
openai.api_type = 'azure'
openai.api_key = os.environ.get("AZURE_OPENAI_API_KEY")

def setup_byod(deployment_id: str) -> None:
    """Sets up the OpenAI Python SDK to use your own data for the chat endpoint.
 
    :param deployment_id: The deployment ID for the model to use with your own data.

    To remove this configuration, simply set openai.requestssession to None.
    """

    class BringYourOwnDataAdapter(requests.adapters.HTTPAdapter):

     def send(self, request, **kwargs):
         request.url = f"{openai.api_base}/openai/deployments/{deployment_id}/extensions/chat/completions?api-version={openai.api_version}"
         return super().send(request, **kwargs)

    session = requests.Session()

    # Mount a custom adapter which will use the extensions endpoint for any call using the given `deployment_id`
    session.mount(
        prefix=f"{openai.api_base}/openai/deployments/{deployment_id}",
        adapter=BringYourOwnDataAdapter()
    )

    openai.requestssession = session

aoai_deployment_id = os.environ.get("AZURE_OPENAI_DEPLOYMENT_NAME")
setup_byod(aoai_deployment_id)

completion = openai.ChatCompletion.create(
    messages=[{"role": "user", "content": "What are my available health plans?"}],
    deployment_id=os.environ.get("AZURE_OPENAI_DEPLOYMENT_NAME"),
    dataSources=[  # camelCase is intentional, as this is the format the API expects
        {
            "type": "AzureCognitiveSearch",
            "parameters": {
                "endpoint": os.environ.get("AZURE_AI_SEARCH_ENDPOINT"),
                "key": os.environ.get("AZURE_AI_SEARCH_API_KEY"),
                "indexName": os.environ.get("AZURE_AI_SEARCH_INDEX"),
            }
        }
    ]
)
print(completion)
Important
For production, use a secure way of storing and accessing your credentials likeAzure Key Vault. For more information about credential security, see the Azure AI servicessecurityarticle.
Execute the following command:
python main.py
python main.py
The application prints the response in a JSON format suitable for use in many scenarios. It includes both answers to your query and citations from your uploaded files.
Prerequisites
The following resources:
Azure OpenAI
Azure Blob Storage
Azure AI Search
AnAzure OpenAI resourcedeployed in asupported region and with a supported model.Be sure that you're assigned at least theCognitive Services Contributorrole for the Azure OpenAI resource.
Be sure that you're assigned at least theCognitive Services Contributorrole for the Azure OpenAI resource.
Download the example data fromGitHubif you don't have your own data.
Add your data using Azure AI Foundry portal
Tip
You canuse the Azure Developer CLIto programmatically create the resources needed for Azure OpenAI On Your Data
Navigate toAzure AI Foundry portaland sign-in with credentials that have access to your Azure OpenAI resource.
You can eithercreate an Azure AI Foundry projectby clickingCreate project, or continue directly by clicking the button on theFocused on Azure OpenAI Servicetile.
You can eithercreate an Azure AI Foundry projectby clickingCreate project, or continue directly by clicking the button on theFocused on Azure OpenAI Servicetile.

SelectChatunderPlaygroundsin the left pane, and select your model deployment.
SelectChatunderPlaygroundsin the left pane, and select your model deployment.
In theChat playground, SelectAdd your dataand thenAdd a data source
In theChat playground, SelectAdd your dataand thenAdd a data source

In the pane that appears, selectUpload files (preview)underSelect data source. Azure OpenAI needs both a storage resource and a search resource to access and index your data.TipSee the following resource for more information:Data source optionssupported file types and formatsFor documents and datasets with long text, we recommend using the availabledata preparation script.For Azure OpenAI to access your storage account, you will need to turn onCross-origin resource sharing (CORS). If CORS isn't already turned on for the Azure Blob Storage resource, selectTurn on CORS.Select your Azure AI Search resource, and select the acknowledgment that connecting it will incur usage on your account. Then selectNext.
In the pane that appears, selectUpload files (preview)underSelect data source. Azure OpenAI needs both a storage resource and a search resource to access and index your data.
Tip
See the following resource for more information:Data source optionssupported file types and formats
Data source options
supported file types and formats
For documents and datasets with long text, we recommend using the availabledata preparation script.
For Azure OpenAI to access your storage account, you will need to turn onCross-origin resource sharing (CORS). If CORS isn't already turned on for the Azure Blob Storage resource, selectTurn on CORS.
For Azure OpenAI to access your storage account, you will need to turn onCross-origin resource sharing (CORS). If CORS isn't already turned on for the Azure Blob Storage resource, selectTurn on CORS.
Select your Azure AI Search resource, and select the acknowledgment that connecting it will incur usage on your account. Then selectNext.
Select your Azure AI Search resource, and select the acknowledgment that connecting it will incur usage on your account. Then selectNext.

On theUpload filespane, selectBrowse for a fileand select the files you downloaded from theprerequisitessection, or your own data. Then selectUpload files. Then selectNext.
On theUpload filespane, selectBrowse for a fileand select the files you downloaded from theprerequisitessection, or your own data. Then selectUpload files. Then selectNext.
On theData managementpane, you can choose whether to enablesemantic search or vector searchfor your index.ImportantSemantic searchandvector searchare subject to additional pricing. You need to chooseBasic or higher SKUto enable semantic search or vector search. Seepricing tier differenceandservice limitsfor more information.To help improve the quality of the information retrieval and model response, we recommend enablingsemantic searchfor the following data source languages: English, French, Spanish, Portuguese, Italian, Germany, Chinese(Zh), Japanese, Korean, Russian, and Arabic.
On theData managementpane, you can choose whether to enablesemantic search or vector searchfor your index.
Important
Semantic searchandvector searchare subject to additional pricing. You need to chooseBasic or higher SKUto enable semantic search or vector search. Seepricing tier differenceandservice limitsfor more information.
To help improve the quality of the information retrieval and model response, we recommend enablingsemantic searchfor the following data source languages: English, French, Spanish, Portuguese, Italian, Germany, Chinese(Zh), Japanese, Korean, Russian, and Arabic.
Review the details you entered, and selectSave and close. You can now chat with the model and it will use information from your data to construct the response.
Review the details you entered, and selectSave and close. You can now chat with the model and it will use information from your data to construct the response.
Retrieve resource information
You need to retrieve the following information to authenticate your application with your Azure OpenAI resource. This quickstart assumes you've uploaded your data to an Azure blob storage account and have an Azure AI Search index created. SeeAdd your data using Azure AI Foundry portal.
Microsoft Entra ID
API key
AZURE_OPENAI_ENDPOINT
https://my-resoruce.openai.azure.com
AZURE_OPENAI_DEPLOYMENT_NAME
AZURE_AI_SEARCH_ENDPOINT
AZURE_AI_SEARCH_INDEX
Learn more aboutkeyless authenticationandsetting environment variables.
AZURE_OPENAI_ENDPOINT
https://my-resoruce.openai.azure.com
AZURE_OPENAI_API_KEY
KEY1
KEY2
AZURE_OPENAI_DEPLOYMENT_NAME
AZURE_AI_SEARCH_ENDPOINT
AZURE_AI_SEARCH_API_KEY
AZURE_AI_SEARCH_INDEX
Learn more aboutfinding API keysandsetting environment variables.
Important
Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, seeAPI keys with Azure Key Vault.
For more information about AI services security, seeAuthenticate requests to Azure AI services.
Example PowerShell commands
The Azure OpenAI chat models are optimized to work with inputs formatted as a conversation. Themessagesvariable passes an array of dictionaries with different roles in the conversation delineated by system, user, tool, and assistant. ThedataSourcesvariable connects to your Azure Cognitive Search index, and enables Azure OpenAI models to respond using your data.
messages
dataSources
To trigger a response from the model, you should end with a user message indicating that it's the assistant's turn to respond.
Tip
There are several parameters you can use to change the model's response, such astemperatureortop_p. See thereference documentationfor more information.
temperature
top_p
# Azure OpenAI metadata variables
   $openai = @{
       api_key     = $Env:AZURE_OPENAI_API_KEY
       api_base    = $Env:AZURE_OPENAI_ENDPOINT # your endpoint should look like the following https://YOUR_RESOURCE_NAME.openai.azure.com/
       api_version = '2023-07-01-preview' # this may change in the future
       name        = 'YOUR-DEPLOYMENT-NAME-HERE' #This will correspond to the custom name you chose for your deployment when you deployed a model.
   }

   $acs = @{
       search_endpoint     = 'YOUR ACS ENDPOINT' # your endpoint should look like the following https://YOUR_RESOURCE_NAME.search.windows.net/
       search_key    = 'YOUR-ACS-KEY-HERE' # or use the Get-Secret cmdlet to retrieve the value
       search_index = 'YOUR-INDEX-NAME-HERE' # the name of your ACS index
   }

   # Completion text
   $body = @{
    dataSources = @(
        @{
            type = 'AzureCognitiveSearch'
            parameters = @{
                    endpoint = $acs.search_endpoint
                    key = $acs.search_key
                    indexName = $acs.search_index
                }
        }
    )
    messages = @(
            @{
                role = 'user'
                content = 'What are my available health plans?'
            }
    )
   } | convertto-json -depth 5

   # Header for authentication
   $headers = [ordered]@{
       'api-key' = $openai.api_key
   }

   # Send a completion call to generate an answer
   $url = "$($openai.api_base)/openai/deployments/$($openai.name)/extensions/chat/completions?api-version=$($openai.api_version)"

   $response = Invoke-RestMethod -Uri $url -Headers $headers -Body $body -Method Post -ContentType 'application/json'
   return $response.choices.messages[1].content
# Azure OpenAI metadata variables
   $openai = @{
       api_key     = $Env:AZURE_OPENAI_API_KEY
       api_base    = $Env:AZURE_OPENAI_ENDPOINT # your endpoint should look like the following https://YOUR_RESOURCE_NAME.openai.azure.com/
       api_version = '2023-07-01-preview' # this may change in the future
       name        = 'YOUR-DEPLOYMENT-NAME-HERE' #This will correspond to the custom name you chose for your deployment when you deployed a model.
   }

   $acs = @{
       search_endpoint     = 'YOUR ACS ENDPOINT' # your endpoint should look like the following https://YOUR_RESOURCE_NAME.search.windows.net/
       search_key    = 'YOUR-ACS-KEY-HERE' # or use the Get-Secret cmdlet to retrieve the value
       search_index = 'YOUR-INDEX-NAME-HERE' # the name of your ACS index
   }

   # Completion text
   $body = @{
    dataSources = @(
        @{
            type = 'AzureCognitiveSearch'
            parameters = @{
                    endpoint = $acs.search_endpoint
                    key = $acs.search_key
                    indexName = $acs.search_index
                }
        }
    )
    messages = @(
            @{
                role = 'user'
                content = 'What are my available health plans?'
            }
    )
   } | convertto-json -depth 5

   # Header for authentication
   $headers = [ordered]@{
       'api-key' = $openai.api_key
   }

   # Send a completion call to generate an answer
   $url = "$($openai.api_base)/openai/deployments/$($openai.name)/extensions/chat/completions?api-version=$($openai.api_version)"

   $response = Invoke-RestMethod -Uri $url -Headers $headers -Body $body -Method Post -ContentType 'application/json'
   return $response.choices.messages[1].content
Example output
The available health plans in the Contoso Electronics plan and benefit packages are the Northwind Health Plus and Northwind Standard plans.
The available health plans in the Contoso Electronics plan and benefit packages are the Northwind Health Plus and Northwind Standard plans.
Important
For production, use a secure way of storing and accessing your credentials likeThe PowerShell Secret Management with Azure Key Vault. For more information about credential security, see the Azure AI servicessecurityarticle.
Chat with your model using a web app
To start chatting with the Azure OpenAI model that uses your data, you can deploy a web app usingAzure AI Foundry portalor example code weprovide on GitHub. This app deploys using Azure app service, and provides a user interface for sending queries. This app can be used with Azure OpenAI models that use your data, or models that don't use your data. See the readme file in the repo for instructions on requirements, setup, and deployment. You can optionally customize thefrontend and backend logicof the web app by making changes to the source code.
Prerequisites
The following resources:
Azure OpenAI
Azure Blob Storage
Azure AI Search
AnAzure OpenAI resourcedeployed in asupported region and with a supported model.Be sure that you're assigned at least theCognitive Services Contributorrole for the Azure OpenAI resource.
Be sure that you're assigned at least theCognitive Services Contributorrole for the Azure OpenAI resource.
Download the example data fromGitHubif you don't have your own data.
Reference|Source code|Package (Go)|Samples
Add your data using Azure AI Foundry portal
Tip
You canuse the Azure Developer CLIto programmatically create the resources needed for Azure OpenAI On Your Data
Navigate toAzure AI Foundry portaland sign-in with credentials that have access to your Azure OpenAI resource.
You can eithercreate an Azure AI Foundry projectby clickingCreate project, or continue directly by clicking the button on theFocused on Azure OpenAI Servicetile.
You can eithercreate an Azure AI Foundry projectby clickingCreate project, or continue directly by clicking the button on theFocused on Azure OpenAI Servicetile.

SelectChatunderPlaygroundsin the left pane, and select your model deployment.
SelectChatunderPlaygroundsin the left pane, and select your model deployment.
In theChat playground, SelectAdd your dataand thenAdd a data source
In theChat playground, SelectAdd your dataand thenAdd a data source

In the pane that appears, selectUpload files (preview)underSelect data source. Azure OpenAI needs both a storage resource and a search resource to access and index your data.TipSee the following resource for more information:Data source optionssupported file types and formatsFor documents and datasets with long text, we recommend using the availabledata preparation script.For Azure OpenAI to access your storage account, you will need to turn onCross-origin resource sharing (CORS). If CORS isn't already turned on for the Azure Blob Storage resource, selectTurn on CORS.Select your Azure AI Search resource, and select the acknowledgment that connecting it will incur usage on your account. Then selectNext.
In the pane that appears, selectUpload files (preview)underSelect data source. Azure OpenAI needs both a storage resource and a search resource to access and index your data.
Tip
See the following resource for more information:Data source optionssupported file types and formats
Data source options
supported file types and formats
For documents and datasets with long text, we recommend using the availabledata preparation script.
For Azure OpenAI to access your storage account, you will need to turn onCross-origin resource sharing (CORS). If CORS isn't already turned on for the Azure Blob Storage resource, selectTurn on CORS.
For Azure OpenAI to access your storage account, you will need to turn onCross-origin resource sharing (CORS). If CORS isn't already turned on for the Azure Blob Storage resource, selectTurn on CORS.
Select your Azure AI Search resource, and select the acknowledgment that connecting it will incur usage on your account. Then selectNext.
Select your Azure AI Search resource, and select the acknowledgment that connecting it will incur usage on your account. Then selectNext.

On theUpload filespane, selectBrowse for a fileand select the files you downloaded from theprerequisitessection, or your own data. Then selectUpload files. Then selectNext.
On theUpload filespane, selectBrowse for a fileand select the files you downloaded from theprerequisitessection, or your own data. Then selectUpload files. Then selectNext.
On theData managementpane, you can choose whether to enablesemantic search or vector searchfor your index.ImportantSemantic searchandvector searchare subject to additional pricing. You need to chooseBasic or higher SKUto enable semantic search or vector search. Seepricing tier differenceandservice limitsfor more information.To help improve the quality of the information retrieval and model response, we recommend enablingsemantic searchfor the following data source languages: English, French, Spanish, Portuguese, Italian, Germany, Chinese(Zh), Japanese, Korean, Russian, and Arabic.
On theData managementpane, you can choose whether to enablesemantic search or vector searchfor your index.
Important
Semantic searchandvector searchare subject to additional pricing. You need to chooseBasic or higher SKUto enable semantic search or vector search. Seepricing tier differenceandservice limitsfor more information.
To help improve the quality of the information retrieval and model response, we recommend enablingsemantic searchfor the following data source languages: English, French, Spanish, Portuguese, Italian, Germany, Chinese(Zh), Japanese, Korean, Russian, and Arabic.
Review the details you entered, and selectSave and close. You can now chat with the model and it will use information from your data to construct the response.
Review the details you entered, and selectSave and close. You can now chat with the model and it will use information from your data to construct the response.
Microsoft Entra ID prerequisites
For the recommended keyless authentication with Microsoft Entra ID, you need to:
Install theAzure CLIused for keyless authentication with Microsoft Entra ID.
Assign theCognitive Services Userrole to your user account. You can assign roles in the Azure portal underAccess control (IAM)>Add role assignment.
Cognitive Services User
Set up
Create a new folderdall-e-quickstartand go to the quickstart folder with the following command:mkdir dall-e-quickstart && cd dall-e-quickstart
Create a new folderdall-e-quickstartand go to the quickstart folder with the following command:
dall-e-quickstart
mkdir dall-e-quickstart && cd dall-e-quickstart
mkdir dall-e-quickstart && cd dall-e-quickstart
For therecommendedkeyless authentication with Microsoft Entra ID, sign in to Azure with the following command:az login
For therecommendedkeyless authentication with Microsoft Entra ID, sign in to Azure with the following command:
az login
az login
Retrieve resource information
You need to retrieve the following information to authenticate your application with your Azure OpenAI resource. This quickstart assumes you've uploaded your data to an Azure blob storage account and have an Azure AI Search index created. SeeAdd your data using Azure AI Foundry portal.
Microsoft Entra ID
API key
AZURE_OPENAI_ENDPOINT
https://my-resoruce.openai.azure.com
AZURE_OPENAI_DEPLOYMENT_NAME
AZURE_AI_SEARCH_ENDPOINT
AZURE_AI_SEARCH_INDEX
Learn more aboutkeyless authenticationandsetting environment variables.
AZURE_OPENAI_ENDPOINT
https://my-resoruce.openai.azure.com
AZURE_OPENAI_API_KEY
KEY1
KEY2
AZURE_OPENAI_DEPLOYMENT_NAME
AZURE_AI_SEARCH_ENDPOINT
AZURE_AI_SEARCH_API_KEY
AZURE_AI_SEARCH_INDEX
Learn more aboutfinding API keysandsetting environment variables.
Important
Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, seeAPI keys with Azure Key Vault.
For more information about AI services security, seeAuthenticate requests to Azure AI services.
Run the quickstart
The sample code in this quickstart uses Microsoft Entra ID for the recommended keyless authentication. If you prefer to use an API key, you can replace theNewDefaultAzureCredentialimplementation withNewKeyCredential.
NewDefaultAzureCredential
NewKeyCredential
Microsoft Entra ID
API key
azureOpenAIEndpoint := os.Getenv("AZURE_OPENAI_ENDPOINT")
credential, err := azidentity.NewDefaultAzureCredential(nil)
client, err := azopenai.NewClient(azureOpenAIEndpoint, credential, nil)
azureOpenAIEndpoint := os.Getenv("AZURE_OPENAI_ENDPOINT")
credential, err := azidentity.NewDefaultAzureCredential(nil)
client, err := azopenai.NewClient(azureOpenAIEndpoint, credential, nil)
azureOpenAIEndpoint := os.Getenv("AZURE_OPENAI_ENDPOINT")
azureOpenAIKey := os.Getenv("AZURE_OPENAI_API_KEY")
credential := azcore.NewKeyCredential(azureOpenAIKey)
client, err := azopenai.NewClientWithKeyCredential(azureOpenAIEndpoint, credential, nil)
azureOpenAIEndpoint := os.Getenv("AZURE_OPENAI_ENDPOINT")
azureOpenAIKey := os.Getenv("AZURE_OPENAI_API_KEY")
credential := azcore.NewKeyCredential(azureOpenAIKey)
client, err := azopenai.NewClientWithKeyCredential(azureOpenAIEndpoint, credential, nil)
Microsoft Entra ID
API key
To run the sample:
Create a new file namedquickstart.go. Copy the following code into thequickstart.gofile.package main

import (
 "context"
 "fmt"
 "log"
 "os"

 "github.com/Azure/azure-sdk-for-go/sdk/ai/azopenai"
 "github.com/Azure/azure-sdk-for-go/sdk/azcore"
 "github.com/Azure/azure-sdk-for-go/sdk/azcore/to"
)

func main() {
 azureOpenAIEndpoint := os.Getenv("AZURE_OPENAI_ENDPOINT")
 credential, err := azidentity.NewDefaultAzureCredential(nil)
 client, err := azopenai.NewClient(azureOpenAIEndpoint, credential, nil)

 modelDeploymentID := os.Getenv("AZURE_OPENAI_DEPLOYMENT_NAME")

 // Azure AI Search configuration
 searchIndex := os.Getenv("AZURE_AI_SEARCH_INDEX")
 searchEndpoint := os.Getenv("AZURE_AI_SEARCH_ENDPOINT")
 searchAPIKey := os.Getenv("AZURE_AI_SEARCH_API_KEY")

 if modelDeploymentID == "" || azureOpenAIEndpoint == "" || searchIndex == "" || searchEndpoint == "" || searchAPIKey == "" {
 	fmt.Fprintf(os.Stderr, "Skipping example, environment variables missing\n")
 	return
 }

 client, err := azopenai.NewClientWithKeyCredential(azureOpenAIEndpoint, credential, nil)

 if err != nil {
 	// Implement application specific error handling logic.
 	log.Printf("ERROR: %s", err)
 	return
 }

 resp, err := client.GetChatCompletions(context.TODO(), azopenai.ChatCompletionsOptions{
 	Messages: []azopenai.ChatRequestMessageClassification{
 		&azopenai.ChatRequestUserMessage{Content: azopenai.NewChatRequestUserMessageContent("What are my available health plans?")},
 	},
 	MaxTokens: to.Ptr[int32](512),
 	AzureExtensionsOptions: []azopenai.AzureChatExtensionConfigurationClassification{
 		&azopenai.AzureSearchChatExtensionConfiguration{
 			// This allows Azure OpenAI to use an Azure AI Search index.
 			// Answers are based on the model's pretrained knowledge
 			// and the latest information available in the designated data source. 
 			Parameters: &azopenai.AzureSearchChatExtensionParameters{
 				Endpoint:  &searchEndpoint,
 				IndexName: &searchIndex,
 				Authentication: &azopenai.OnYourDataAPIKeyAuthenticationOptions{
 					Key: &searchAPIKey,
 				},
 			},
 		},
 	},
 	DeploymentName: &modelDeploymentID,
 }, nil)

 if err != nil {
 	// Implement application specific error handling logic.
 	log.Printf("ERROR: %s", err)
 	return
 }

 fmt.Fprintf(os.Stderr, "Extensions Context Role: %s\nExtensions Context (length): %d\n",
 	*resp.Choices[0].Message.Role,
 	len(*resp.Choices[0].Message.Content))

 fmt.Fprintf(os.Stderr, "ChatRole: %s\nChat content: %s\n",
 	*resp.Choices[0].Message.Role,
 	*resp.Choices[0].Message.Content,
 )
}
Create a new file namedquickstart.go. Copy the following code into thequickstart.gofile.
package main

import (
 "context"
 "fmt"
 "log"
 "os"

 "github.com/Azure/azure-sdk-for-go/sdk/ai/azopenai"
 "github.com/Azure/azure-sdk-for-go/sdk/azcore"
 "github.com/Azure/azure-sdk-for-go/sdk/azcore/to"
)

func main() {
 azureOpenAIEndpoint := os.Getenv("AZURE_OPENAI_ENDPOINT")
 credential, err := azidentity.NewDefaultAzureCredential(nil)
 client, err := azopenai.NewClient(azureOpenAIEndpoint, credential, nil)

 modelDeploymentID := os.Getenv("AZURE_OPENAI_DEPLOYMENT_NAME")

 // Azure AI Search configuration
 searchIndex := os.Getenv("AZURE_AI_SEARCH_INDEX")
 searchEndpoint := os.Getenv("AZURE_AI_SEARCH_ENDPOINT")
 searchAPIKey := os.Getenv("AZURE_AI_SEARCH_API_KEY")

 if modelDeploymentID == "" || azureOpenAIEndpoint == "" || searchIndex == "" || searchEndpoint == "" || searchAPIKey == "" {
 	fmt.Fprintf(os.Stderr, "Skipping example, environment variables missing\n")
 	return
 }

 client, err := azopenai.NewClientWithKeyCredential(azureOpenAIEndpoint, credential, nil)

 if err != nil {
 	// Implement application specific error handling logic.
 	log.Printf("ERROR: %s", err)
 	return
 }

 resp, err := client.GetChatCompletions(context.TODO(), azopenai.ChatCompletionsOptions{
 	Messages: []azopenai.ChatRequestMessageClassification{
 		&azopenai.ChatRequestUserMessage{Content: azopenai.NewChatRequestUserMessageContent("What are my available health plans?")},
 	},
 	MaxTokens: to.Ptr[int32](512),
 	AzureExtensionsOptions: []azopenai.AzureChatExtensionConfigurationClassification{
 		&azopenai.AzureSearchChatExtensionConfiguration{
 			// This allows Azure OpenAI to use an Azure AI Search index.
 			// Answers are based on the model's pretrained knowledge
 			// and the latest information available in the designated data source. 
 			Parameters: &azopenai.AzureSearchChatExtensionParameters{
 				Endpoint:  &searchEndpoint,
 				IndexName: &searchIndex,
 				Authentication: &azopenai.OnYourDataAPIKeyAuthenticationOptions{
 					Key: &searchAPIKey,
 				},
 			},
 		},
 	},
 	DeploymentName: &modelDeploymentID,
 }, nil)

 if err != nil {
 	// Implement application specific error handling logic.
 	log.Printf("ERROR: %s", err)
 	return
 }

 fmt.Fprintf(os.Stderr, "Extensions Context Role: %s\nExtensions Context (length): %d\n",
 	*resp.Choices[0].Message.Role,
 	len(*resp.Choices[0].Message.Content))

 fmt.Fprintf(os.Stderr, "ChatRole: %s\nChat content: %s\n",
 	*resp.Choices[0].Message.Role,
 	*resp.Choices[0].Message.Content,
 )
}
package main

import (
 "context"
 "fmt"
 "log"
 "os"

 "github.com/Azure/azure-sdk-for-go/sdk/ai/azopenai"
 "github.com/Azure/azure-sdk-for-go/sdk/azcore"
 "github.com/Azure/azure-sdk-for-go/sdk/azcore/to"
)

func main() {
 azureOpenAIEndpoint := os.Getenv("AZURE_OPENAI_ENDPOINT")
 credential, err := azidentity.NewDefaultAzureCredential(nil)
 client, err := azopenai.NewClient(azureOpenAIEndpoint, credential, nil)

 modelDeploymentID := os.Getenv("AZURE_OPENAI_DEPLOYMENT_NAME")

 // Azure AI Search configuration
 searchIndex := os.Getenv("AZURE_AI_SEARCH_INDEX")
 searchEndpoint := os.Getenv("AZURE_AI_SEARCH_ENDPOINT")
 searchAPIKey := os.Getenv("AZURE_AI_SEARCH_API_KEY")

 if modelDeploymentID == "" || azureOpenAIEndpoint == "" || searchIndex == "" || searchEndpoint == "" || searchAPIKey == "" {
 	fmt.Fprintf(os.Stderr, "Skipping example, environment variables missing\n")
 	return
 }

 client, err := azopenai.NewClientWithKeyCredential(azureOpenAIEndpoint, credential, nil)

 if err != nil {
 	// Implement application specific error handling logic.
 	log.Printf("ERROR: %s", err)
 	return
 }

 resp, err := client.GetChatCompletions(context.TODO(), azopenai.ChatCompletionsOptions{
 	Messages: []azopenai.ChatRequestMessageClassification{
 		&azopenai.ChatRequestUserMessage{Content: azopenai.NewChatRequestUserMessageContent("What are my available health plans?")},
 	},
 	MaxTokens: to.Ptr[int32](512),
 	AzureExtensionsOptions: []azopenai.AzureChatExtensionConfigurationClassification{
 		&azopenai.AzureSearchChatExtensionConfiguration{
 			// This allows Azure OpenAI to use an Azure AI Search index.
 			// Answers are based on the model's pretrained knowledge
 			// and the latest information available in the designated data source. 
 			Parameters: &azopenai.AzureSearchChatExtensionParameters{
 				Endpoint:  &searchEndpoint,
 				IndexName: &searchIndex,
 				Authentication: &azopenai.OnYourDataAPIKeyAuthenticationOptions{
 					Key: &searchAPIKey,
 				},
 			},
 		},
 	},
 	DeploymentName: &modelDeploymentID,
 }, nil)

 if err != nil {
 	// Implement application specific error handling logic.
 	log.Printf("ERROR: %s", err)
 	return
 }

 fmt.Fprintf(os.Stderr, "Extensions Context Role: %s\nExtensions Context (length): %d\n",
 	*resp.Choices[0].Message.Role,
 	len(*resp.Choices[0].Message.Content))

 fmt.Fprintf(os.Stderr, "ChatRole: %s\nChat content: %s\n",
 	*resp.Choices[0].Message.Role,
 	*resp.Choices[0].Message.Content,
 )
}
Run the following command to create a new Go module:go mod init quickstart.go
Run the following command to create a new Go module:
go mod init quickstart.go
go mod init quickstart.go
Rungo mod tidyto install the required dependencies:go mod tidy
Rungo mod tidyto install the required dependencies:
go mod tidy
go mod tidy
go mod tidy
Run the following command to run the sample:go run quickstart.go
Run the following command to run the sample:
go run quickstart.go
go run quickstart.go
To run the sample:
Create a new file namedquickstart.go. Copy the following code into thequickstart.gofile.package main

import (
 "context"
 "fmt"
 "log"
 "os"

 "github.com/Azure/azure-sdk-for-go/sdk/ai/azopenai"
 "github.com/Azure/azure-sdk-for-go/sdk/azcore"
 "github.com/Azure/azure-sdk-for-go/sdk/azcore/to"
)

func main() {
 azureOpenAIEndpoint := os.Getenv("AZURE_OPENAI_ENDPOINT")
 azureOpenAIKey := os.Getenv("AZURE_OPENAI_API_KEY")
 modelDeploymentID := os.Getenv("AZURE_OPENAI_DEPLOYMENT_NAME")

 // Azure AI Search configuration
 searchIndex := os.Getenv("AZURE_AI_SEARCH_INDEX")
 searchEndpoint := os.Getenv("AZURE_AI_SEARCH_ENDPOINT")
 searchAPIKey := os.Getenv("AZURE_AI_SEARCH_API_KEY")

 if azureOpenAIKey == "" || modelDeploymentID == "" || azureOpenAIEndpoint == "" || searchIndex == "" || searchEndpoint == "" || searchAPIKey == "" {
 	fmt.Fprintf(os.Stderr, "Skipping example, environment variables missing\n")
 	return
 }

 credential := azcore.NewKeyCredential(azureOpenAIKey)

 client, err := azopenai.NewClientWithKeyCredential(azureOpenAIEndpoint, credential, nil)

    	if err != nil {
 	// Implement application specific error handling logic.
 	log.Printf("ERROR: %s", err)
 	return
 }

 resp, err := client.GetChatCompletions(context.TODO(), azopenai.ChatCompletionsOptions{
 	Messages: []azopenai.ChatRequestMessageClassification{
 		&azopenai.ChatRequestUserMessage{Content: azopenai.NewChatRequestUserMessageContent("What are my available health plans?")},
 	},
 	MaxTokens: to.Ptr[int32](512),
 	AzureExtensionsOptions: []azopenai.AzureChatExtensionConfigurationClassification{
 		&azopenai.AzureSearchChatExtensionConfiguration{
 			// This allows Azure OpenAI to use an Azure AI Search index.
 			// Answers are based on the model's pretrained knowledge
 			// and the latest information available in the designated data source. 
 			Parameters: &azopenai.AzureSearchChatExtensionParameters{
 				Endpoint:  &searchEndpoint,
 				IndexName: &searchIndex,
 				Authentication: &azopenai.OnYourDataAPIKeyAuthenticationOptions{
 					Key: &searchAPIKey,
 				},
 			},
 		},
 	},
 	DeploymentName: &modelDeploymentID,
 }, nil)

 if err != nil {
 	// Implement application specific error handling logic.
 	log.Printf("ERROR: %s", err)
 	return
 }

 fmt.Fprintf(os.Stderr, "Extensions Context Role: %s\nExtensions Context (length): %d\n",
 	*resp.Choices[0].Message.Role,
 	len(*resp.Choices[0].Message.Content))

 fmt.Fprintf(os.Stderr, "ChatRole: %s\nChat content: %s\n",
 	*resp.Choices[0].Message.Role,
 	*resp.Choices[0].Message.Content,
 )
}
Create a new file namedquickstart.go. Copy the following code into thequickstart.gofile.
package main

import (
 "context"
 "fmt"
 "log"
 "os"

 "github.com/Azure/azure-sdk-for-go/sdk/ai/azopenai"
 "github.com/Azure/azure-sdk-for-go/sdk/azcore"
 "github.com/Azure/azure-sdk-for-go/sdk/azcore/to"
)

func main() {
 azureOpenAIEndpoint := os.Getenv("AZURE_OPENAI_ENDPOINT")
 azureOpenAIKey := os.Getenv("AZURE_OPENAI_API_KEY")
 modelDeploymentID := os.Getenv("AZURE_OPENAI_DEPLOYMENT_NAME")

 // Azure AI Search configuration
 searchIndex := os.Getenv("AZURE_AI_SEARCH_INDEX")
 searchEndpoint := os.Getenv("AZURE_AI_SEARCH_ENDPOINT")
 searchAPIKey := os.Getenv("AZURE_AI_SEARCH_API_KEY")

 if azureOpenAIKey == "" || modelDeploymentID == "" || azureOpenAIEndpoint == "" || searchIndex == "" || searchEndpoint == "" || searchAPIKey == "" {
 	fmt.Fprintf(os.Stderr, "Skipping example, environment variables missing\n")
 	return
 }

 credential := azcore.NewKeyCredential(azureOpenAIKey)

 client, err := azopenai.NewClientWithKeyCredential(azureOpenAIEndpoint, credential, nil)

    	if err != nil {
 	// Implement application specific error handling logic.
 	log.Printf("ERROR: %s", err)
 	return
 }

 resp, err := client.GetChatCompletions(context.TODO(), azopenai.ChatCompletionsOptions{
 	Messages: []azopenai.ChatRequestMessageClassification{
 		&azopenai.ChatRequestUserMessage{Content: azopenai.NewChatRequestUserMessageContent("What are my available health plans?")},
 	},
 	MaxTokens: to.Ptr[int32](512),
 	AzureExtensionsOptions: []azopenai.AzureChatExtensionConfigurationClassification{
 		&azopenai.AzureSearchChatExtensionConfiguration{
 			// This allows Azure OpenAI to use an Azure AI Search index.
 			// Answers are based on the model's pretrained knowledge
 			// and the latest information available in the designated data source. 
 			Parameters: &azopenai.AzureSearchChatExtensionParameters{
 				Endpoint:  &searchEndpoint,
 				IndexName: &searchIndex,
 				Authentication: &azopenai.OnYourDataAPIKeyAuthenticationOptions{
 					Key: &searchAPIKey,
 				},
 			},
 		},
 	},
 	DeploymentName: &modelDeploymentID,
 }, nil)

 if err != nil {
 	// Implement application specific error handling logic.
 	log.Printf("ERROR: %s", err)
 	return
 }

 fmt.Fprintf(os.Stderr, "Extensions Context Role: %s\nExtensions Context (length): %d\n",
 	*resp.Choices[0].Message.Role,
 	len(*resp.Choices[0].Message.Content))

 fmt.Fprintf(os.Stderr, "ChatRole: %s\nChat content: %s\n",
 	*resp.Choices[0].Message.Role,
 	*resp.Choices[0].Message.Content,
 )
}
package main

import (
 "context"
 "fmt"
 "log"
 "os"

 "github.com/Azure/azure-sdk-for-go/sdk/ai/azopenai"
 "github.com/Azure/azure-sdk-for-go/sdk/azcore"
 "github.com/Azure/azure-sdk-for-go/sdk/azcore/to"
)

func main() {
 azureOpenAIEndpoint := os.Getenv("AZURE_OPENAI_ENDPOINT")
 azureOpenAIKey := os.Getenv("AZURE_OPENAI_API_KEY")
 modelDeploymentID := os.Getenv("AZURE_OPENAI_DEPLOYMENT_NAME")

 // Azure AI Search configuration
 searchIndex := os.Getenv("AZURE_AI_SEARCH_INDEX")
 searchEndpoint := os.Getenv("AZURE_AI_SEARCH_ENDPOINT")
 searchAPIKey := os.Getenv("AZURE_AI_SEARCH_API_KEY")

 if azureOpenAIKey == "" || modelDeploymentID == "" || azureOpenAIEndpoint == "" || searchIndex == "" || searchEndpoint == "" || searchAPIKey == "" {
 	fmt.Fprintf(os.Stderr, "Skipping example, environment variables missing\n")
 	return
 }

 credential := azcore.NewKeyCredential(azureOpenAIKey)

 client, err := azopenai.NewClientWithKeyCredential(azureOpenAIEndpoint, credential, nil)

    	if err != nil {
 	// Implement application specific error handling logic.
 	log.Printf("ERROR: %s", err)
 	return
 }

 resp, err := client.GetChatCompletions(context.TODO(), azopenai.ChatCompletionsOptions{
 	Messages: []azopenai.ChatRequestMessageClassification{
 		&azopenai.ChatRequestUserMessage{Content: azopenai.NewChatRequestUserMessageContent("What are my available health plans?")},
 	},
 	MaxTokens: to.Ptr[int32](512),
 	AzureExtensionsOptions: []azopenai.AzureChatExtensionConfigurationClassification{
 		&azopenai.AzureSearchChatExtensionConfiguration{
 			// This allows Azure OpenAI to use an Azure AI Search index.
 			// Answers are based on the model's pretrained knowledge
 			// and the latest information available in the designated data source. 
 			Parameters: &azopenai.AzureSearchChatExtensionParameters{
 				Endpoint:  &searchEndpoint,
 				IndexName: &searchIndex,
 				Authentication: &azopenai.OnYourDataAPIKeyAuthenticationOptions{
 					Key: &searchAPIKey,
 				},
 			},
 		},
 	},
 	DeploymentName: &modelDeploymentID,
 }, nil)

 if err != nil {
 	// Implement application specific error handling logic.
 	log.Printf("ERROR: %s", err)
 	return
 }

 fmt.Fprintf(os.Stderr, "Extensions Context Role: %s\nExtensions Context (length): %d\n",
 	*resp.Choices[0].Message.Role,
 	len(*resp.Choices[0].Message.Content))

 fmt.Fprintf(os.Stderr, "ChatRole: %s\nChat content: %s\n",
 	*resp.Choices[0].Message.Role,
 	*resp.Choices[0].Message.Content,
 )
}
Run the following command to create a new Go module:go mod init quickstart.go
Run the following command to create a new Go module:
go mod init quickstart.go
go mod init quickstart.go
Rungo mod tidyto install the required dependencies:go mod tidy
Rungo mod tidyto install the required dependencies:
go mod tidy
go mod tidy
go mod tidy
Run the following command to run the sample:go run quickstart.go
Run the following command to run the sample:
go run quickstart.go
go run quickstart.go
The application prints the response including both answers to your query and citations from your uploaded files.
Prerequisites
The following resources:
Azure OpenAI
Azure Blob Storage
Azure AI Search
AnAzure OpenAI resourcedeployed in asupported region and with a supported model.Be sure that you're assigned at least theCognitive Services Contributorrole for the Azure OpenAI resource.
Be sure that you're assigned at least theCognitive Services Contributorrole for the Azure OpenAI resource.
Download the example data fromGitHubif you don't have your own data.
Add your data using Azure AI Foundry portal
Tip
You canuse the Azure Developer CLIto programmatically create the resources needed for Azure OpenAI On Your Data
Navigate toAzure AI Foundry portaland sign-in with credentials that have access to your Azure OpenAI resource.
You can eithercreate an Azure AI Foundry projectby clickingCreate project, or continue directly by clicking the button on theFocused on Azure OpenAI Servicetile.
You can eithercreate an Azure AI Foundry projectby clickingCreate project, or continue directly by clicking the button on theFocused on Azure OpenAI Servicetile.

SelectChatunderPlaygroundsin the left pane, and select your model deployment.
SelectChatunderPlaygroundsin the left pane, and select your model deployment.
In theChat playground, SelectAdd your dataand thenAdd a data source
In theChat playground, SelectAdd your dataand thenAdd a data source

In the pane that appears, selectUpload files (preview)underSelect data source. Azure OpenAI needs both a storage resource and a search resource to access and index your data.TipSee the following resource for more information:Data source optionssupported file types and formatsFor documents and datasets with long text, we recommend using the availabledata preparation script.For Azure OpenAI to access your storage account, you will need to turn onCross-origin resource sharing (CORS). If CORS isn't already turned on for the Azure Blob Storage resource, selectTurn on CORS.Select your Azure AI Search resource, and select the acknowledgment that connecting it will incur usage on your account. Then selectNext.
In the pane that appears, selectUpload files (preview)underSelect data source. Azure OpenAI needs both a storage resource and a search resource to access and index your data.
Tip
See the following resource for more information:Data source optionssupported file types and formats
Data source options
supported file types and formats
For documents and datasets with long text, we recommend using the availabledata preparation script.
For Azure OpenAI to access your storage account, you will need to turn onCross-origin resource sharing (CORS). If CORS isn't already turned on for the Azure Blob Storage resource, selectTurn on CORS.
For Azure OpenAI to access your storage account, you will need to turn onCross-origin resource sharing (CORS). If CORS isn't already turned on for the Azure Blob Storage resource, selectTurn on CORS.
Select your Azure AI Search resource, and select the acknowledgment that connecting it will incur usage on your account. Then selectNext.
Select your Azure AI Search resource, and select the acknowledgment that connecting it will incur usage on your account. Then selectNext.

On theUpload filespane, selectBrowse for a fileand select the files you downloaded from theprerequisitessection, or your own data. Then selectUpload files. Then selectNext.
On theUpload filespane, selectBrowse for a fileand select the files you downloaded from theprerequisitessection, or your own data. Then selectUpload files. Then selectNext.
On theData managementpane, you can choose whether to enablesemantic search or vector searchfor your index.ImportantSemantic searchandvector searchare subject to additional pricing. You need to chooseBasic or higher SKUto enable semantic search or vector search. Seepricing tier differenceandservice limitsfor more information.To help improve the quality of the information retrieval and model response, we recommend enablingsemantic searchfor the following data source languages: English, French, Spanish, Portuguese, Italian, Germany, Chinese(Zh), Japanese, Korean, Russian, and Arabic.
On theData managementpane, you can choose whether to enablesemantic search or vector searchfor your index.
Important
Semantic searchandvector searchare subject to additional pricing. You need to chooseBasic or higher SKUto enable semantic search or vector search. Seepricing tier differenceandservice limitsfor more information.
To help improve the quality of the information retrieval and model response, we recommend enablingsemantic searchfor the following data source languages: English, French, Spanish, Portuguese, Italian, Germany, Chinese(Zh), Japanese, Korean, Russian, and Arabic.
Review the details you entered, and selectSave and close. You can now chat with the model and it will use information from your data to construct the response.
Review the details you entered, and selectSave and close. You can now chat with the model and it will use information from your data to construct the response.
Retrieve resource information
You need to retrieve the following information to authenticate your application with your Azure OpenAI resource. This quickstart assumes you've uploaded your data to an Azure blob storage account and have an Azure AI Search index created. SeeAdd your data using Azure AI Foundry portal.
Microsoft Entra ID
API key
AZURE_OPENAI_ENDPOINT
https://my-resoruce.openai.azure.com
AZURE_OPENAI_DEPLOYMENT_NAME
AZURE_AI_SEARCH_ENDPOINT
AZURE_AI_SEARCH_INDEX
Learn more aboutkeyless authenticationandsetting environment variables.
AZURE_OPENAI_ENDPOINT
https://my-resoruce.openai.azure.com
AZURE_OPENAI_API_KEY
KEY1
KEY2
AZURE_OPENAI_DEPLOYMENT_NAME
AZURE_AI_SEARCH_ENDPOINT
AZURE_AI_SEARCH_API_KEY
AZURE_AI_SEARCH_INDEX
Learn more aboutfinding API keysandsetting environment variables.
Important
Use API keys with caution. Don't include the API key directly in your code, and never post it publicly. If you use an API key, store it securely in Azure Key Vault. For more information about using API keys securely in your apps, seeAPI keys with Azure Key Vault.
For more information about AI services security, seeAuthenticate requests to Azure AI services.
Example cURL commands
The Azure OpenAI chat models are optimized to work with inputs formatted as a conversation. Themessagesvariable passes an array of dictionaries with different roles in the conversation delineated by system, user, tool, and assistant. ThedataSourcesvariable connects to your Azure AI Search index, and enables Azure OpenAI models to respond using your data.
messages
dataSources
To trigger a response from the model, you should end with a user message indicating that it's the assistant's turn to respond.
Tip
There are several parameters you can use to change the model's response, such astemperatureortop_p. See thereference documentationfor more information.
temperature
top_p
curl -i -X POST $AZURE_OPENAI_ENDPOINT/openai/deployments/$AZURE_OPENAI_DEPLOYMENT_NAME/chat/completions?api-version=2024-10-21 \
-H "Content-Type: application/json" \
-H "api-key: $AZURE_OPENAI_API_KEY" \
-d \
'
{
    "data_sources": [
        {
            "type": "azure_search",
            "parameters": {
                "endpoint": "'$AZURE_AI_SEARCH_ENDPOINT'",
                "index_name": "'$AZURE_AI_SEARCH_INDEX'",
                "authentication": {
                    "type": "api_key",
                    "key": "'$AZURE_AI_SEARCH_API_KEY'"
                }
            }
        }
    ],
    "messages": [
        {
            "role": "user",
            "content": "What are my available health plans?"
        }
    ]
}
'
curl -i -X POST $AZURE_OPENAI_ENDPOINT/openai/deployments/$AZURE_OPENAI_DEPLOYMENT_NAME/chat/completions?api-version=2024-10-21 \
-H "Content-Type: application/json" \
-H "api-key: $AZURE_OPENAI_API_KEY" \
-d \
'
{
    "data_sources": [
        {
            "type": "azure_search",
            "parameters": {
                "endpoint": "'$AZURE_AI_SEARCH_ENDPOINT'",
                "index_name": "'$AZURE_AI_SEARCH_INDEX'",
                "authentication": {
                    "type": "api_key",
                    "key": "'$AZURE_AI_SEARCH_API_KEY'"
                }
            }
        }
    ],
    "messages": [
        {
            "role": "user",
            "content": "What are my available health plans?"
        }
    ]
}
'
Example output
{
    "id": "12345678-1a2b-3c4e5f-a123-12345678abcd",
    "model": "gpt-4",
    "created": 1709835345,
    "object": "extensions.chat.completion",
    "choices": [
        {
            "index": 0,
            "finish_reason": "stop",
            "message": {
                "role": "assistant",
                "content": "The available health plans in the Contoso Electronics plan and benefit packages are the Northwind Health Plus and Northwind Standard plans. [doc1].",
                "end_turn": true,
                "context": {
                    "citations": [
                        {
                            "content": "...",
                            "title": "...",
                            "url": "https://mysearch.blob.core.windows.net/xyz/001.txt",
                            "filepath": "001.txt",
                            "chunk_id": "0"
                        }
                    ],
                    "intent": "[\"Available health plans\"]"
                }
            }
        }
    ],
    "usage": {
        "prompt_tokens": 3779,
        "completion_tokens": 105,
        "total_tokens": 3884
    },
    "system_fingerprint": "fp_65792305e4"
}
{
    "id": "12345678-1a2b-3c4e5f-a123-12345678abcd",
    "model": "gpt-4",
    "created": 1709835345,
    "object": "extensions.chat.completion",
    "choices": [
        {
            "index": 0,
            "finish_reason": "stop",
            "message": {
                "role": "assistant",
                "content": "The available health plans in the Contoso Electronics plan and benefit packages are the Northwind Health Plus and Northwind Standard plans. [doc1].",
                "end_turn": true,
                "context": {
                    "citations": [
                        {
                            "content": "...",
                            "title": "...",
                            "url": "https://mysearch.blob.core.windows.net/xyz/001.txt",
                            "filepath": "001.txt",
                            "chunk_id": "0"
                        }
                    ],
                    "intent": "[\"Available health plans\"]"
                }
            }
        }
    ],
    "usage": {
        "prompt_tokens": 3779,
        "completion_tokens": 105,
        "total_tokens": 3884
    },
    "system_fingerprint": "fp_65792305e4"
}
Chat with your model using a web app
To start chatting with the Azure OpenAI model that uses your data, you can deploy a web app usingAzure AI Foundry portalor example code weprovide on GitHub. This app deploys using Azure app service, and provides a user interface for sending queries. This app can be used with Azure OpenAI models that use your data, or models that don't use your data. See the readme file in the repo for instructions on requirements, setup, and deployment. You can optionally customize thefrontend and backend logicof the web app by making changes to the source code.
Clean up resources
If you want to clean up and remove an Azure OpenAI or Azure AI Search resource, you can delete the resource or resource group. Deleting the resource group also deletes any other resources associated with it.
Azure AI services resources
Azure AI Search resources
Azure app service resources
Next steps
Learn more aboutusing your data in Azure OpenAI Service
Chat app sample code on GitHub.
Feedback
Was this page helpful?
Additional resources