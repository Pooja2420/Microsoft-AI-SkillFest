Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
GPU-enabled compute
Article
2025-03-10
2 contributors
In this article
Note
Some GPU-enabled instance types are inBetaand are marked as such in the drop-down list when you select the driver and worker types during compute creation.
Overview
Azure Databricks supports compute accelerated with graphics processing units (GPUs).
This article describes how to create compute with GPU-enabled instances and describes
the GPU drivers and libraries installed on those instances.
To learn more about deep learning on GPU-enabled compute, seeDeep learning.
Create a GPU compute
Creating a GPU compute is similar to creating any compute. You should keep in mind the following:
TheDatabricks Runtime Versionmust be a GPU-enabled version, such asRuntime 13.3 LTS ML (GPU, Scala 2.12.15, Spark 3.4.1).
TheWorker TypeandDriver Typemust be GPU instance types.
Supported instance types
Warning
Azure Databricks is deprecating will no longer support spinning up compute using NC v3 instance type series since Azure is deprecatingNc24rs by March 31, 2025andNC6s_v3, NC12s_v3, and NC24s_v3 by September 30, 2025.
Azure Databricks supports the following instance types:
GPU Type: NVIDIA H100 NVL GPU
GPU Type: NVIDIA A100 PCIe GPU
GPU Type: NVIDIA Ampere A100 40GB Tensor Core GPU
GPU Type: NVIDIA A10 GPU
GPU Type: NVIDIA T4 GPU
GPU Type: NVIDIA Tesla V100 GPU
SeeAzure Databricks Pricingfor an up-to-date list of supported GPU instance types and their availability regions.
Your Azure Databricks deployment must reside in a supported region to launch GPU-enabled compute.
GPU scheduling
GPU scheduling distributes Spark tasks efficiently across a large number of GPUs.
Databricks Runtime supports GPU-aware scheduling from Apache Spark 3.0. Azure Databricks preconfigures it on GPU compute.
Note
GPU scheduling is not enabled on single-node compute.
GPU scheduling for AI and ML
spark.task.resource.gpu.amountis the only Spark config related to GPU-aware scheduling that you may need to configure.
The default configuration uses one GPU per task, which is a good baseline for distributed inference workloads and distributed training if you use all GPU nodes.
spark.task.resource.gpu.amount
To reduce communication overhead during distributed training, Databricks recommends settingspark.task.resource.gpu.amountto the number of GPUs per worker node in the computeSpark configuration. This creates only one Spark task for each Spark worker and assigns all GPUs in that worker node to the same task.
spark.task.resource.gpu.amount
To increase parallelization for distributed deep learning inference, you can setspark.task.resource.gpu.amountto fractional values such as 1/2, 1/3, 1/4, â¦ 1/N. This creates more Spark tasks than there are GPUs, allowing more concurrent tasks to handle inference requests in parallel. For example, if you setspark.task.resource.gpu.amountto0.5,0.33, or0.25, then the available GPUs will be split among double, triple, or quadruple the number of tasks.
spark.task.resource.gpu.amount
spark.task.resource.gpu.amount
0.5
0.33
0.25
GPU indices
For PySpark tasks, Azure Databricks automatically remaps assigned GPU(s) to zero-based indices. For the default configuration that uses one GPU per task, you can use the default GPU without checking which GPU is assigned to the task.
If you set multiple GPUs per task, for example, 4, the indices of the assigned GPUs are always 0, 1, 2, and 3. If you do need the physical indices of the assigned GPUs, you can get them from theCUDA_VISIBLE_DEVICESenvironment variable.
CUDA_VISIBLE_DEVICES
If you use Scala, you can get the indices of the GPUs assigned to the task fromTaskContext.resources().get("gpu").
TaskContext.resources().get("gpu")
NVIDIA GPU driver, CUDA, and cuDNN
Azure Databricks installs the NVIDIA driver and libraries required to use GPUs on Spark driver and worker instances:
CUDA Toolkit, installed under/usr/local/cuda.
/usr/local/cuda
cuDNN: NVIDIA CUDA Deep Neural Network Library.
NCCL: NVIDIA Collective Communications Library.
The version of the NVIDIA driver included is 535.54.03, which supports CUDA 11.0. For theNV A10 v5 instance type series, the version of the NVIDIA driver included is535.154.05.
535.154.05
For the versions of the libraries included, see therelease notesfor the specific Databricks Runtime version you are using.
Note
This software contains source code provided by NVIDIA Corporation. Specifically, to support GPUs, Azure Databricks includes code fromCUDA Samples.
NVIDIA End User License Agreement (EULA)
When you select a GPU-enabled âDatabricks Runtime Versionâ in Azure Databricks, you implicitly agree to the terms and conditions outlined in the
NVIDIA EULA with respect to the CUDA, cuDNN, and Tesla libraries,
and theNVIDIA End User License Agreement (with NCCL Supplement)for the NCCL library.
Databricks Container Services on GPU compute
Important
This feature is inPublic Preview.
You can useDatabricks Container Serviceson compute with GPUs to create portable deep learning environments with customized libraries. SeeCustomize containers with Databricks Container Servicefor instructions.
To create custom images for GPU compute, you must select a standard runtime version instead of Databricks Runtime ML for GPU. When you selectUse your own Docker container, you can choose GPU compute with a standard runtime version. The custom images for GPU are based on theofficial CUDA containers, which is different from Databricks Runtime ML for GPU.
When you create custom images for GPU compute, you cannot change the NVIDIA driver version because it must match the driver version on the host machine.
ThedatabricksruntimeDocker Hubcontains example base images with GPU capability. The Dockerfiles used to generate these images are located in theexample containers GitHub repository, which also has details on what the example images provide and how to customize them.
databricksruntime
Feedback
Was this page helpful?
Additional resources