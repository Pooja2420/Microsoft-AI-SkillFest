Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Connect to Azure AI services from workflows in Azure Logic Apps
Article
2025-01-21
2 contributors
In this article
Applies to:Azure Logic Apps (Consumption + Standard)
To integrate enterprise services, systems, and data with AI technologies, your logic app workflows can connect toAzure OpenAIandAzure AI Searchresources that you use for these integration scenarios.
This guide provides an overview and examples that show how to useAzure OpenAIandAzure AI Searchconnector operations in your workflow.
What is Azure OpenAI Service
What is Azure AI Search
Why use Azure Logic Apps with AI services?
Usually, building AI solutions involves several key steps and requires a few building blocks. Primarily, you need to have a dynamic ingestion pipeline and a chat interface that can communicate with large language models (LLMs) and vector databases.
Tip
To learn more, you can ask Azure Copilot these questions:
What is a dynamic ingestion pipeline in AI?
What is a vector database in AI?
To find Azure Copilot, on theAzure portaltoolbar, selectCopilot.
You can assemble various components, not only to perform data ingestion but also to provide a robust backend for the chat interface. This backend facilitates entering prompts and generates dependable responses during interactions. However, creating the code to manage and control all these elements can pose challenges, which is the case for most solutions.
Azure Logic Apps offers a low code approach and simplifies backend management by providing prebuilt connectors that you use as building blocks to streamline the backend process. This approach lets you focus on sourcing your data and making sure that search results provide current and relevant information. With these AI connectors, your workflow acts as an orchestration engine that transfers data between AI services and other components that you want to integrate.
For more information, see the following resources:
Introduction to large language models
What is a vector database
Prerequisites
An Azure account and subscription. If you don't have an Azure subscription,sign up for a free Azure account.
An Azure account and subscription. If you don't have an Azure subscription,sign up for a free Azure account.
The Azure AI Search and Azure OpenAI resources to access and use in your workflow, including connection information:Azure OpenAI connection requirementsAzure AI Search connection requirements
The Azure AI Search and Azure OpenAI resources to access and use in your workflow, including connection information:
Azure OpenAI connection requirements
Azure AI Search connection requirements
A logic app workflow where you want to access your Azure OpenAI and Azure AI Search resources.The connectors for these services currently provide only actions, not triggers. Before you can add an Azure AI connector action, make sure your workflow starts with the appropriate trigger for your scenario.
A logic app workflow where you want to access your Azure OpenAI and Azure AI Search resources.
The connectors for these services currently provide only actions, not triggers. Before you can add an Azure AI connector action, make sure your workflow starts with the appropriate trigger for your scenario.
Connector technical reference
In Consumption workflows, theAzure OpenAIandAzure AI Searchmanaged or "shared" connectors are currently in preview and subject to theSupplemental Terms of Use for Microsoft Azure Previews.
Azure OpenAI
Azure OpenAI Service provides access toOpenAI's language models, which include GPT-4, GPT-4 Turbo with Vision, GPT-3.5-Turbo, and the Embeddings model series. With theAzure OpenAIconnector, your workflow can connect to Azure OpenAI Service and get OpenAI embeddings for your data or generate chat completions.
Tip
To learn more, you can ask Azure Copilot these questions:
What is an embedding in AI?
What is a chat completion in AI?
To find Azure Copilot, on theAzure portaltoolbar, selectCopilot.
TheAzure OpenAIconnector has different versions, based onlogic app type and hosting model:
Azure AI Search
Azure AI Search is platform for AI-powered information retrieval that helps developers build rich search experiences and generative AI apps by combining large language models with enterprise data. With theAzure AI Searchconnector, your workflow can connect to Azure AI Search to index documents and perform vector searches on your data.
TheAzure AI Searchconnector has different versions, based onlogic app type and hosting model:
Authentication
The AI managed connectors require an API key for authentication. However, the AI built-in connectors support multiple authentication types for your AI service endpoint. These options provide robust authentication that meets most customers' needs. Both built-in connectors can also directly connect to Azure OpenAI and Azure AI Search resources inside virtual networks or behind firewalls.
The following table describes the built-in connector authentication options, all which require that you provide the URL for the AI service endpoint:
Warning
Always secure and protect sensitive and personal data, such as credentials, secrets,
access keys, connection strings, certificates, thumbprints, and similar information
with the highest available or supported level of security.
For authentication and authorization, set up or useMicrosoft Entra IDwith amanaged identity.
This solution provides optimal and superior security without you having to manually
provide and manage credentials, secrets, access keys, and so on because Azure handles
the managed identity for you. To set up a managed identity for Azure Logic Apps, seeAuthenticate access and connections to Azure resources with managed identities in Azure Logic Apps.
If you can't use a managed identity, choose the next highest level security solution
available. For example, if you must use a connection string, which includes information
required to access a resource, service, or system, remember that this string includes
an access key that is similar to a root password.
Make sure that you securely store such information by using Microsoft Entra ID andAzure Key Vault. Don't hardcode this information,
share with other users, or save in plain text anywhere that others can access. Set up
a plan to rotate or revoke secrets in the case they become compromised. For more
information, see the following resources:
Automate secrets rotation in Azure Key Vault
Best practices for protecting secrets
Secrets in Azure Key Vault
For more information, see the following resources:
Authenticate requests to Azure AI services
What is Microsoft Entra ID
What are managed identities for Azure resources
Authenticate access and connections to Azure resources with managed identities in Azure Logic Apps
Add an Azure OpenAI or Azure AI Search action to your workflow
Currently, the connectors for Azure OpenAI and Azure AI Search provide only actions, not triggers. You can start your workflow with any trigger that fits your scenario or needs. Based on whether you have a Consumption or Standard workflow, you can thenfollow these general steps to add actions for Azure OpenAI, Azure AI Search, and other operations.
Scenarios
The following scenarios describe only two of the many ways that you can use AI connector operations in your workflows:
Create a knowledge base for your enterprise data
Azure Logic Apps providesover 1,400 Microsoft-managed connectorsandnatively running built-in connectorsfor your workflow to securely connect with almost any data source, such as SharePoint, Oracle DB, Salesforce, OneDrive, Dropbox, SAP, IBM, and so on. Each connector provides operations, which include triggers, actions, or both, for you to use in your workflow.
For example, you can select from many trigger types to make your automated workflow run on a schedule or based on specific events, such as the uploading of new documents to a SharePoint site. With so many operations for you to choose, you can create a knowledge base and easily build a document ingestion pipeline using vector embeddings for these documents in Azure AI Search.
For more information, see the following resources:
Vectors in Azure AI Search
What are embeddings
Understand embeddings in Azure OpenAI
Generate completions
An Azure Logic Apps workflow can accept input, while Azure OpenAI Service can perform completion operations. These capabilities mean that your workflow can ingest real-time questions, generate answers about your data, or send automated responses using Azure OpenAI. You can immediately send the responses back to the client or to an approval workflow for verification.
For more information, see the following resources:
Introduction to prompt engineering
Learn how to generate or manipulate text
Example scenario with sample code: Ingest data and create chat interactions
This Standard workflow example shows how to use the Azure OpenAI and Azure AI Search built-in connectors to break down the backend logic for ingesting data and conducting simple chat conversations into two key workflows. For faster performance, create stateless workflows that, by default, don't save and store the history for each run.
Sample code
Create a chat using ingested data
Other prerequisites
A Standard logic app workflow
A Standard logic app workflow
See thesample code requirements.
See thesample code requirements.
The followingcross-environment parameter valuesare also used by the workflow operations in this example:Parameter nameDescriptionaisearch_admin_keyThe admin key for Azure AI Searchaisearch_endpointThe endpoint URL for the Azure AI Search exampleaisearch_index_nameThe index to use for the Azure AI Search exampleopenapi_api_keyThe API key for Azure OpenAIopenai_deployment_idThe deployment ID for the Azure OpenAI exampleopenai_endpointThe endpoint URL for the Azure OpenAI exampletokenize_function_urlThe URL for a custom Azure function that batches and tokenizes data, which is required for Azure OpenAI to properly create embeddings for this example.For more information about this function, see thesample code for "Create a chat using ingested data".
The followingcross-environment parameter valuesare also used by the workflow operations in this example:
Video: Learn how to build AI applications using logic apps
Learn how to build AI applications using logic apps
Ingest data workflow
To save considerable time and effort when you build an ingestion pipeline, implement the following pattern with any data source. This pattern encapsulates all the advantages and benefits currently offered by Standard workflows in single-tenant Azure Logic Apps.
Each step in this pattern makes sure that the AI seamlessly extracts all the crucial information from your data files. If run as a stateless workflow, this pattern also provides faster performance. This approach simplifies not only the coding aspect but also guarantees that your workflows have effective authentication, monitoring, and deployment processes in place.

Chat workflow
As your vector databases continue to ingest data, make sure the data is easily searchable so that when a user asks a question, the backend Standard logic app workflow can process the prompt and generate a reliable response.
The following pattern is only one example that shows how a chat workflow might look:

Related content
Azure OpenAI and Azure AI Search connectors are now generally available
Azure OpenAI and AI Search connectors for Azure Logic Apps (Standard)
Feedback
Was this page helpful?
Additional resources