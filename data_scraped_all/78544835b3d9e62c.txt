Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Configure GPU monitoring with Container insights and/or Managed Prometheus
Article
2025-04-07
5 contributors
In this article
Container insights supports monitoring GPU clusters from the following GPU vendors:
NVIDIA
AMD
Note
If you are usingNvidia DCGM exporter, you can enable GPU monitoring with Managed Prometheus and Managed Grafana. For details on the setup and instructions, please seeEnable GPU monitoring with Nvidia DCGM exporter.
Container insights automatically starts monitoring GPU usage on nodes and GPU requesting pods and workloads by collecting the following metrics at 60-second intervals and storing them in theInsightMetricstable.
Note
After you provision clusters with GPU nodes, ensure that theGPU driveris installed as required by Azure Kubernetes Service (AKS) to run GPU workloads. Container insights collect GPU metrics through GPU driver pods running in the node.
GPU performance charts
Container insights includes preconfigured charts for the metrics listed earlier in the table as a GPU workbook for every cluster. For a description of the workbooks available for Container insights, seeWorkbooks in Container insights.
Next steps
SeeUse GPUs for compute-intensive workloads on Azure Kubernetes Serviceto learn how to deploy an AKS cluster that includes GPU-enabled nodes.
Learn more aboutGPU optimized VM SKUs in Azure.
ReviewGPU support in Kubernetesto learn more about Kubernetes experimental support for managing GPUs across one or more nodes in a cluster.
Feedback
Was this page helpful?
Additional resources