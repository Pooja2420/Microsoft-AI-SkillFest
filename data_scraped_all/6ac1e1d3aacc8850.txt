Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
What is Apache Hive and HiveQL on Azure HDInsight?
Article
2024-05-09
15 contributors
In this article
Apache Hiveis a data warehouse system for Apache Hadoop. Hive enables data summarization, querying, and analysis of data. Hive queries are written in HiveQL, which is a query language similar to SQL.
Hive allows you to project structure on largely unstructured data. After you define the structure, you can use HiveQL to query the data without knowledge of Java or MapReduce.
HDInsight provides several cluster types, which are tuned for specific workloads. The following cluster types are most often used for Hive queries:
How to use Hive
Use the following table to discover the different ways to use Hive with HDInsight:
HiveQL language reference
HiveQL language reference is available in thelanguage manual.
Hive and data structure
Hive understands how to work with structured and semi-structured data. For example, text files where the fields are delimited by specific characters. The following HiveQL statement creates a table over space-delimited data:
CREATE EXTERNAL TABLE log4jLogs (
    t1 string,
    t2 string,
    t3 string,
    t4 string,
    t5 string,
    t6 string,
    t7 string)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ' '
STORED AS TEXTFILE LOCATION '/example/data/';
CREATE EXTERNAL TABLE log4jLogs (
    t1 string,
    t2 string,
    t3 string,
    t4 string,
    t5 string,
    t6 string,
    t7 string)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ' '
STORED AS TEXTFILE LOCATION '/example/data/';
Hive also supports customserializer/deserializers (SerDe)for complex or irregularly structured data. For more information, see theHow to use a custom JSON SerDe with HDInsightdocument.
For more information on file formats supported by Hive, see theLanguage manual (https://cwiki.apache.org/confluence/display/Hive/LanguageManual)
Hive internal tables vs external tables
There are two types of tables that you can create with Hive:
Internal: Data is stored in the Hive data warehouse. The data warehouse is located at/hive/warehouse/on the default storage for the cluster.Use internal tables when one of the following conditions applies:Data is temporary.You want Hive to manage the lifecycle of the table and data.
Internal: Data is stored in the Hive data warehouse. The data warehouse is located at/hive/warehouse/on the default storage for the cluster.
/hive/warehouse/
Use internal tables when one of the following conditions applies:
Data is temporary.
You want Hive to manage the lifecycle of the table and data.
External: Data is stored outside the data warehouse. The data can be stored on any storage accessible by the cluster.Use external tables when one of the following conditions applies:The data is also used outside of Hive. For example, the data files are updated by another process (that doesn't lock the files.)Data needs to remain in the underlying location, even after dropping the table.You need a custom location, such as a non-default storage account.A program other than hive manages the data format, location, and so on.
External: Data is stored outside the data warehouse. The data can be stored on any storage accessible by the cluster.
Use external tables when one of the following conditions applies:
The data is also used outside of Hive. For example, the data files are updated by another process (that doesn't lock the files.)
Data needs to remain in the underlying location, even after dropping the table.
You need a custom location, such as a non-default storage account.
A program other than hive manages the data format, location, and so on.
For more information, see theHive Internal and External Tables Introblog post.
User-defined functions (UDF)
Hive can also be extended throughuser-defined functions (UDF). A UDF allows you to implement functionality or logic that isn't easily modeled in HiveQL. For an example of using UDFs with Hive, see the following documents:
Use a Java user-defined function with Apache Hive
Use a Java user-defined function with Apache Hive
Use a Python user-defined function with Apache Hive
Use a Python user-defined function with Apache Hive
Use a C# user-defined function with Apache Hive
Use a C# user-defined function with Apache Hive
How to add a custom Apache Hive user-defined function to HDInsight
How to add a custom Apache Hive user-defined function to HDInsight
An example Apache Hive user-defined function to convert date/time formats to Hive timestamp
An example Apache Hive user-defined function to convert date/time formats to Hive timestamp
Example data
Hive on HDInsight comes pre-loaded with an internal table namedhivesampletable. HDInsight also provides example data sets that can be used with Hive. These data sets are stored in the/example/dataand/HdiSamplesdirectories. These directories exist in the default storage for your cluster.
hivesampletable
/example/data
/HdiSamples
Example Hive query
The following HiveQL statements project columns onto the/example/data/sample.logfile:
/example/data/sample.log
DROP TABLE log4jLogs;
CREATE EXTERNAL TABLE log4jLogs (
    t1 string,
    t2 string,
    t3 string,
    t4 string,
    t5 string,
    t6 string,
    t7 string)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ' '
STORED AS TEXTFILE LOCATION '/example/data/';
SELECT t4 AS sev, COUNT(*) AS count FROM log4jLogs
    WHERE t4 = '[ERROR]' AND INPUT__FILE__NAME LIKE '%.log'
    GROUP BY t4;
DROP TABLE log4jLogs;
CREATE EXTERNAL TABLE log4jLogs (
    t1 string,
    t2 string,
    t3 string,
    t4 string,
    t5 string,
    t6 string,
    t7 string)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ' '
STORED AS TEXTFILE LOCATION '/example/data/';
SELECT t4 AS sev, COUNT(*) AS count FROM log4jLogs
    WHERE t4 = '[ERROR]' AND INPUT__FILE__NAME LIKE '%.log'
    GROUP BY t4;
In the previous example, the HiveQL statements perform the following actions:
example/data
Note
External tables should be used when you expect the underlying data to be updated by an external source. For example, an automated data upload process, or MapReduce operation.
Dropping an external table doesnotdelete the data, it only deletes the table definition.
To create aninternaltable instead of external, use the following HiveQL:
CREATE TABLE IF NOT EXISTS errorLogs (
    t1 string,
    t2 string,
    t3 string,
    t4 string,
    t5 string,
    t6 string,
    t7 string)
STORED AS ORC;
INSERT OVERWRITE TABLE errorLogs
SELECT t1, t2, t3, t4, t5, t6, t7 
    FROM log4jLogs WHERE t4 = '[ERROR]';
CREATE TABLE IF NOT EXISTS errorLogs (
    t1 string,
    t2 string,
    t3 string,
    t4 string,
    t5 string,
    t6 string,
    t7 string)
STORED AS ORC;
INSERT OVERWRITE TABLE errorLogs
SELECT t1, t2, t3, t4, t5, t6, t7 
    FROM log4jLogs WHERE t4 = '[ERROR]';
These statements perform the following actions:
log4jLogs
Note
Unlike external tables, dropping an internal table also deletes the underlying data.
Improve Hive query performance
Apache Tez
Apache Tezis a framework that allows data intensive applications, such as Hive, to run much more efficiently at scale. Tez is enabled by default.  TheApache Hive on Tez design documentscontains details about the implementation choices and tuning configurations.
Low Latency Analytical Processing (LLAP)
LLAP(sometimes known as Live Long and Process) is a new feature in Hive 2.0 that allows in-memory caching of queries.
HDInsight provides LLAP in the Interactive Query cluster type. For more information, see theStart with Interactive Querydocument.
Scheduling Hive queries
There are several services that can be used to run Hive queries as part of a scheduled or on-demand workflow.
Azure Data Factory
Azure Data Factory allows you to use HDInsight as part of a Data Factory pipeline. For more information on using Hive from a pipeline, see theTransform data using Hive activity in Azure Data Factorydocument.
Hive jobs and SQL Server Integration Services
You can use SQL Server Integration Services (SSIS) to run a Hive job. The Azure Feature Pack for SSIS provides the following components that work with Hive jobs on HDInsight.
Azure HDInsight Hive Task
Azure HDInsight Hive Task
Azure Subscription Connection Manager
Azure Subscription Connection Manager
For more information, see theAzure Feature Packdocumentation.
Apache Oozie
Apache Oozie is a workflow and coordination system that manages Hadoop jobs. For more information on using Oozie with Hive, see theUse Apache Oozie to define and run a workflowdocument.
Note
Phoenix Storage Handler for Hiveis not supported in HDInsight
Next steps
Now that you've learned what Hive is and how to use it with Hadoop in HDInsight, use the following links to explore other ways to work with Azure HDInsight.
Upload data to HDInsight
Use Python User Defined Functions (UDF) with Apache Hive and Apache Pig in HDInsight
Use MapReduce jobs with HDInsight
Feedback
Was this page helpful?
Additional resources