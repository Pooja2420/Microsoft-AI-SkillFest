Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Run a Databricks notebook with the Databricks Notebook Activity in Azure Data Factory
Article
2025-04-18
20 contributors
In this article
APPLIES TO:Azure Data FactoryAzure Synapse Analytics
Tip
Try outData Factory in Microsoft Fabric, an all-in-one analytics solution for enterprises.Microsoft Fabriccovers everything from data movement to data science, real-time analytics, business intelligence, and reporting. Learn how tostart a new trialfor free!
In this tutorial, you use the Azure portal to create an Azure Data Factory pipeline that executes a Databricks notebook against the Databricks jobs cluster. It also passes Azure Data Factory parameters to the Databricks notebook during execution.
You perform the following steps in this tutorial:
Create a data factory.
Create a data factory.
Create a pipeline that uses Databricks Notebook Activity.
Create a pipeline that uses Databricks Notebook Activity.
Trigger a pipeline run.
Trigger a pipeline run.
Monitor the pipeline run.
Monitor the pipeline run.
If you don't have an Azure subscription, create afree accountbefore you begin.
Note
For full details on how to use the Databricks Notebook Activity, including using libraries and passing input and output parameters, refer to theDatabricks Notebook Activitydocumentation.
Prerequisites
Azure Databricks workspace.Create a Databricks workspaceor use an existing one. You create a Python notebook in your Azure Databricks workspace. Then you execute the notebook and pass parameters to it using Azure Data Factory.
Create a data factory
LaunchMicrosoft EdgeorGoogle Chromeweb browser. Currently, Data Factory UI is supported only in Microsoft Edge and Google Chrome web browsers.
LaunchMicrosoft EdgeorGoogle Chromeweb browser. Currently, Data Factory UI is supported only in Microsoft Edge and Google Chrome web browsers.
SelectCreate a resourceon the Azure portal menu, then selectAnalytics>Data Factory:
SelectCreate a resourceon the Azure portal menu, then selectAnalytics>Data Factory:

On theCreate Data Factorypage, underBasicstab, select your AzureSubscriptionin which you want to create the data factory.
On theCreate Data Factorypage, underBasicstab, select your AzureSubscriptionin which you want to create the data factory.
ForResource Group, take one of the following steps:Select an existing resource group from the drop-down list.SelectCreate new, and enter the name of a new resource group.To learn about resource groups, seeUsing resource groups to manage your Azure resources.
ForResource Group, take one of the following steps:
Select an existing resource group from the drop-down list.
Select an existing resource group from the drop-down list.
SelectCreate new, and enter the name of a new resource group.
SelectCreate new, and enter the name of a new resource group.
To learn about resource groups, seeUsing resource groups to manage your Azure resources.
ForRegion, select the location for the data factory.The list shows only locations that Data Factory supports, and where your Azure Data Factory meta data will be stored. The associated data stores (like Azure Storage and Azure SQL Database) and computes (like Azure HDInsight) that Data Factory uses can run in other regions.
ForRegion, select the location for the data factory.
The list shows only locations that Data Factory supports, and where your Azure Data Factory meta data will be stored. The associated data stores (like Azure Storage and Azure SQL Database) and computes (like Azure HDInsight) that Data Factory uses can run in other regions.
ForName, enterADFTutorialDataFactory.The name of the Azure data factory must beglobally unique. If you see the following error, change the name of the data factory (For example, use<yourname>ADFTutorialDataFactory). For naming rules for Data Factory artifacts, see theData Factory - naming rulesarticle.
ForName, enterADFTutorialDataFactory.
The name of the Azure data factory must beglobally unique. If you see the following error, change the name of the data factory (For example, use<yourname>ADFTutorialDataFactory). For naming rules for Data Factory artifacts, see theData Factory - naming rulesarticle.

ForVersion, selectV2.
ForVersion, selectV2.
SelectNext: Git configuration, and then selectConfigure Git latercheck box.
SelectNext: Git configuration, and then selectConfigure Git latercheck box.
SelectReview + create, and selectCreateafter the validation is passed.
SelectReview + create, and selectCreateafter the validation is passed.
After the creation is complete, selectGo to resourceto navigate to theData Factorypage. Select theOpen Azure Data Factory Studiotile to start the Azure Data Factory user interface (UI) application on a separate browser tab.
After the creation is complete, selectGo to resourceto navigate to theData Factorypage. Select theOpen Azure Data Factory Studiotile to start the Azure Data Factory user interface (UI) application on a separate browser tab.

Create linked services
In this section, you author a Databricks linked service. This linked service contains the connection information to the Databricks cluster:
Create an Azure Databricks linked service
On the home page, switch to theManagetab in the left panel.
On the home page, switch to theManagetab in the left panel.

SelectLinked servicesunderConnections, and then select+ New.
SelectLinked servicesunderConnections, and then select+ New.

In theNew linked servicewindow, selectCompute>Azure Databricks, and then selectContinue.
In theNew linked servicewindow, selectCompute>Azure Databricks, and then selectContinue.

In theNew linked servicewindow, complete the following steps:ForName, enterAzureDatabricks_LinkedService.Select the appropriateDatabricks workspacethat you'll run your notebook in.ForSelect cluster, selectNew job cluster.ForDatabricks Workspace URL, the information should be autopopulated.ForAuthentication type, if you selectAccess Token, generate it from Azure Databricks workplace. You can find the stepshere. ForManaged  service identityandUser Assigned Managed Identity,  grantContributor roleto both identities  in Azure Databricks resource'sAccess controlmenu.ForCluster version, select the version you want to use.ForCluster node type, selectStandard_D3_v2underGeneral Purpose (HDD)category for this tutorial.ForWorkers, enter2.SelectCreate.
In theNew linked servicewindow, complete the following steps:
ForName, enterAzureDatabricks_LinkedService.
ForName, enterAzureDatabricks_LinkedService.
Select the appropriateDatabricks workspacethat you'll run your notebook in.
Select the appropriateDatabricks workspacethat you'll run your notebook in.
ForSelect cluster, selectNew job cluster.
ForSelect cluster, selectNew job cluster.
ForDatabricks Workspace URL, the information should be autopopulated.
ForDatabricks Workspace URL, the information should be autopopulated.
ForAuthentication type, if you selectAccess Token, generate it from Azure Databricks workplace. You can find the stepshere. ForManaged  service identityandUser Assigned Managed Identity,  grantContributor roleto both identities  in Azure Databricks resource'sAccess controlmenu.
ForAuthentication type, if you selectAccess Token, generate it from Azure Databricks workplace. You can find the stepshere. ForManaged  service identityandUser Assigned Managed Identity,  grantContributor roleto both identities  in Azure Databricks resource'sAccess controlmenu.
ForCluster version, select the version you want to use.
ForCluster version, select the version you want to use.
ForCluster node type, selectStandard_D3_v2underGeneral Purpose (HDD)category for this tutorial.
ForCluster node type, selectStandard_D3_v2underGeneral Purpose (HDD)category for this tutorial.
ForWorkers, enter2.
ForWorkers, enter2.
SelectCreate.
SelectCreate.

Create a pipeline
Select the+(plus) button, and then selectPipelineon the menu.
Select the+(plus) button, and then selectPipelineon the menu.

Create aparameterto be used in thePipeline. Later you pass this parameter to the Databricks Notebook Activity. In the empty pipeline, select theParameterstab, then select+ Newand name it as 'name'.
Create aparameterto be used in thePipeline. Later you pass this parameter to the Databricks Notebook Activity. In the empty pipeline, select theParameterstab, then select+ Newand name it as 'name'.


In theActivitiestoolbox, expandDatabricks. Drag theNotebookactivity from theActivitiestoolbox to the pipeline designer surface.
In theActivitiestoolbox, expandDatabricks. Drag theNotebookactivity from theActivitiestoolbox to the pipeline designer surface.

In the properties for theDatabricksNotebookactivity window at the bottom, complete the following steps:Switch to theAzure Databrickstab.SelectAzureDatabricks_LinkedService(which you created in the previous procedure).Switch to theSettingstab.Browse to select a DatabricksNotebook path. Letâs create a notebook and specify the path here. You get the Notebook Path by following the next few steps.Launch your Azure Databricks Workspace.Create aNew Folderin Workplace and call it asadftutorial.Create a new notebook, letâs call itmynotebook. Right-click theadftutorialFolder, and selectCreate.In the newly created notebook "mynotebook'" add the following code:# Creating widgets for leveraging parameters, and printing the parameters

dbutils.widgets.text("input", "","")
y = dbutils.widgets.get("input")
print ("Param -\'input':")
print (y)TheNotebook Pathin this case is/adftutorial/mynotebook.
In the properties for theDatabricksNotebookactivity window at the bottom, complete the following steps:
Switch to theAzure Databrickstab.
Switch to theAzure Databrickstab.
SelectAzureDatabricks_LinkedService(which you created in the previous procedure).
SelectAzureDatabricks_LinkedService(which you created in the previous procedure).
Switch to theSettingstab.
Switch to theSettingstab.
Browse to select a DatabricksNotebook path. Letâs create a notebook and specify the path here. You get the Notebook Path by following the next few steps.Launch your Azure Databricks Workspace.Create aNew Folderin Workplace and call it asadftutorial.Create a new notebook, letâs call itmynotebook. Right-click theadftutorialFolder, and selectCreate.In the newly created notebook "mynotebook'" add the following code:# Creating widgets for leveraging parameters, and printing the parameters

dbutils.widgets.text("input", "","")
y = dbutils.widgets.get("input")
print ("Param -\'input':")
print (y)TheNotebook Pathin this case is/adftutorial/mynotebook.
Browse to select a DatabricksNotebook path. Letâs create a notebook and specify the path here. You get the Notebook Path by following the next few steps.
Launch your Azure Databricks Workspace.
Launch your Azure Databricks Workspace.
Create aNew Folderin Workplace and call it asadftutorial.
Create aNew Folderin Workplace and call it asadftutorial.
Create a new notebook, letâs call itmynotebook. Right-click theadftutorialFolder, and selectCreate.
Create a new notebook, letâs call itmynotebook. Right-click theadftutorialFolder, and selectCreate.
In the newly created notebook "mynotebook'" add the following code:# Creating widgets for leveraging parameters, and printing the parameters

dbutils.widgets.text("input", "","")
y = dbutils.widgets.get("input")
print ("Param -\'input':")
print (y)
In the newly created notebook "mynotebook'" add the following code:
# Creating widgets for leveraging parameters, and printing the parameters

dbutils.widgets.text("input", "","")
y = dbutils.widgets.get("input")
print ("Param -\'input':")
print (y)
# Creating widgets for leveraging parameters, and printing the parameters

dbutils.widgets.text("input", "","")
y = dbutils.widgets.get("input")
print ("Param -\'input':")
print (y)
TheNotebook Pathin this case is/adftutorial/mynotebook.
TheNotebook Pathin this case is/adftutorial/mynotebook.
Switch back to theData Factory UI authoring tool. Navigate toSettingsTab under theNotebook1activity.a.  Add aparameterto the Notebook activity. You use the same parameter that you added earlier to thePipeline.b.  Name the parameter asinputand provide the value as expression@pipeline().parameters.name.
Switch back to theData Factory UI authoring tool. Navigate toSettingsTab under theNotebook1activity.
a.  Add aparameterto the Notebook activity. You use the same parameter that you added earlier to thePipeline.

b.  Name the parameter asinputand provide the value as expression@pipeline().parameters.name.
To validate the pipeline, select theValidatebutton on the toolbar. To close the validation window, select theClosebutton.
To validate the pipeline, select theValidatebutton on the toolbar. To close the validation window, select theClosebutton.

SelectPublish all. The Data Factory UI publishes entities (linked services and pipeline) to the Azure Data Factory service.
SelectPublish all. The Data Factory UI publishes entities (linked services and pipeline) to the Azure Data Factory service.

Trigger a pipeline run
SelectAdd triggeron the toolbar, and then selectTrigger now.

ThePipeline rundialog box asks for thenameparameter. Use/path/filenameas the parameter here. SelectOK.

Monitor the pipeline run
Switch to theMonitortab. Confirm that you see a pipeline run. It takes approximately 5-8 minutes to create a Databricks job cluster, where the notebook is executed.
Switch to theMonitortab. Confirm that you see a pipeline run. It takes approximately 5-8 minutes to create a Databricks job cluster, where the notebook is executed.

SelectRefreshperiodically to check the status of the pipeline run.
SelectRefreshperiodically to check the status of the pipeline run.
To see activity runs associated with the pipeline run, selectpipeline1link in thePipeline namecolumn.
To see activity runs associated with the pipeline run, selectpipeline1link in thePipeline namecolumn.
In theActivity runspage, selectOutputin theActivity namecolumn to view the output of each activity, and you can find the link to Databricks logs in theOutputpane for more detailed Spark logs.
In theActivity runspage, selectOutputin theActivity namecolumn to view the output of each activity, and you can find the link to Databricks logs in theOutputpane for more detailed Spark logs.
You can switch back to the pipeline runs view by selecting theAll pipeline runslink in the breadcrumb menu at the top.
You can switch back to the pipeline runs view by selecting theAll pipeline runslink in the breadcrumb menu at the top.
Verify the output
You can log on to theAzure Databricks workspace, go toJob Runsand you can see theJobstatus aspending execution, running, or terminated.
You can select theJob nameand navigate to see further details. On successful run, you can validate the parameters passed and the output of the Python notebook.
Summary
The pipeline in this sample triggers a Databricks Notebook activity and passes a parameter to it. You learned how to:
Create a data factory.
Create a data factory.
Create a pipeline that uses a Databricks Notebook activity.
Create a pipeline that uses a Databricks Notebook activity.
Trigger a pipeline run.
Trigger a pipeline run.
Monitor the pipeline run.
Monitor the pipeline run.
Feedback
Was this page helpful?
Additional resources