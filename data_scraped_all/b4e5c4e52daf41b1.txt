Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Model Catalog and Collections
Article
2024-12-11
14 contributors
In this article
The model catalog in Azure Machine Learning studio is the hub to discover and use a wide range of models that enable you to build Generative AI applications. The model catalog features hundreds of models from model providers such as Azure OpenAI service, Mistral, Meta, Cohere, NVIDIA, Hugging Face, including models trained by Microsoft. Models from providers other than Microsoft are Non-Microsoft Products, as defined inMicrosoft's Product Terms, and subject to the terms provided with the model.
Model Collections
Models are organized by Collections in the model catalog. There are three types of collections in the model catalog:
Models curated by Azure AI:The most popular third-party open weight and propriety models packaged and optimized to work seamlessly on the Azure AI platform. Use of these models is subject to the model provider's license terms provided with the model. When deployed in Azure Machine Learning, availability of the model is subject to the applicableAzure SLA, and Microsoft provides support for deployment issues. Models from partners such as Meta, NVIDIA, Mistral AI are examples of models available in the "Curated by Azure AI" collection on the catalog. These models can be identified by a green checkmark on the model tiles in the catalog or you can filter by the "Curated by Azure AI" collection.
Azure OpenAI models, exclusively available on Azure:Flagship Azure OpenAI models via the 'Azure OpenAI' collection through an integration with the Azure OpenAI Service. These models are supported by Microsoft and their use is subject to the product terms andSLA for Azure OpenAI Service.
Open models from the Hugging Face hub:Hundreds of models from the HuggingFace hub are accessible via the 'Hugging Face' collection for real time inference with online endpoints. Hugging face creates and maintains models listed inâ¯HuggingFaceâ¯collection. Useâ¯HuggingFace forumâ¯orâ¯HuggingFace supportâ¯for help. Learn more abouthow to deploy models from Hugging Face.
Suggesting additions to the model catalog:You can submit a request to add a model to the model catalog usingthis form.
Model catalog capabilities overview
For information on Azure OpenAI models, refer toAzure OpenAI Service.
For modelsCurated by Azure AIandOpen models from the Hugging Face hub, some of these can be deployed with a managed compute option, and some of these are available to be deployed using serverless APIs with pay-as-you-go billing. These models can be discovered, compared, evaluated, fine-tuned (when supported) and deployed at scale and integrated into your Generative AI applications with enterprise-grade security and data governance.
Discover:â¯Review model cards, try sample inference and browse code samples to evaluate, fine-tune, or deploy the model.
Compare:Compare benchmarks across models and datasets available in the industry to assess which one meets your business scenario.
Evaluate:â¯Evaluate if the model is suited for your specific workload by providing your own test data. Evaluation metrics make it easy to visualize how well the selected model performed in your scenario.
Fine-tune:â¯Customize fine-tunable models using your own training data and pick the best model by comparing metrics across all your fine-tuning jobs. Built-in optimizations speed up fine-tuning and reduce the memory and compute needed for fine-tuning.
Deploy:â¯Deploy pretrained models or fine-tuned models seamlessly for inference. Models that can be deployed to managed compute can also be downloaded.
Model deployment: Managed compute and serverless API (pay-as-you-go)
Model Catalog offers two distinct ways to deploy models from the catalog for your use: managed compute and serverless APIs. The deployment options available for each model vary; learn more about the features of the deployment options, and the options available for specific models, in the tables below. Learn more aboutdata processingwith the deployment options.
Deployment options

Managed compute
The capability to deploy models with managed compute builds on platform capabilities of Azure Machine Learning to enable seamless integration, across the entire GenAIOps (sometimes called LLMOps) lifecycle, of the wide collection of models in the model catalog.

How are models made available for managed compute?
The models are made available throughAzure Machine Learning registriesthat enable ML first approach tohosting and distributing Machine Learning assetssuch as model weights, container runtimes for running the models, pipelines for evaluating and fine-tuning the models and datasets for benchmarks and samples. These ML Registries build on top of highly scalable and enterprise ready infrastructure that:
Delivers low latency access model artifacts to all Azure regions with built-in geo-replication.
Delivers low latency access model artifacts to all Azure regions with built-in geo-replication.
Supports enterprise security requirements aslimiting access to models with Azure Policyandsecure deployment with managed virtual networks.
Supports enterprise security requirements aslimiting access to models with Azure Policyandsecure deployment with managed virtual networks.
Evaluate and fine-tune models deployed with managed compute
You can evaluate and fine-tune in the "Curated by Azure AI" collection in Azure Machine Learning using Azure Machine Learning Pipelines. You can either choose to bring your own evaluation and fine-tuning code and just access model weights or use Azure Machine Learning components that offer built-in evaluation and fine-tuning capabilities. To learn more,follow this link.
Deploy models for inference with managed compute
Models available for deployment with managed compute can be deployed to Azure Machine Learning online endpoints for real-time inference or can be used for Azure Machine Learning batch inference to batch process your data. Deploying to managed compute requires you to have Virtual Machine quota in your Azure Subscription for the specific SKUs needed to optimally run the model.  Some models allow you to deploy totemporarily shared quota for testing the model. Learn more about deploying models:
Deploy Meta Llama models
Deploy Open models Created by Azure AI
Deploy Hugging Face models
Build Generative AI Apps with managed compute
Prompt flow offers capabilities for prototyping, experimenting, iterating, and deploying your AI applications. You can use models deployed with managed compute in Prompt Flow with theOpen Model LLM tool.  You can also use the REST API exposed by the managed computes in popular LLM tools like LangChain with theAzure Machine Learning extension.
Content safety for models deployed with managed compute
Azure AI Content Safety (AACS)service is available for use with models deployed to managed compute to screen for various categories of harmful content such as sexual content, violence, hate, and self-harm and advanced threats such as Jailbreak risk detection and Protected material text detection. You can refer to this notebook for reference integration with AACS forLlama 2or use theContent Safety (Text) tool in Prompt Flowto pass responses from the model to AACS for screening. You'll be billed separately as perAACS pricingfor such use.
Work with models not in the model catalog
For models not available in the model catalog, Azure Machine Learning provides an open and extensible platform for working with models of your choice. You can bring a model with any framework or runtime using Azure Machine Learning's open and extensible platform capabilities such asAzure Machine Learning environmentsfor containers that can package frameworks and runtimes andAzure Machine Learning pipelinesfor code to evaluate or fine-tune the models. Refer to this notebook for sample reference to import models and work with thebuilt-in runtimes and pipelines.
Serverless APIs with Pay-as-you-go billing
Certain models in the model catalog can be deployed as serverless APIs with pay-as-you-go billing; this method of deployment is called Models-as-a Service (MaaS). Models available through MaaS are hosted in infrastructure managed by Microsoft, which enables API-based access to the model provider's model. API based access can dramatically reduce the cost of accessing a model and significantly simplify the provisioning experience. Most MaaS models come with token-based pricing.
How are third-party models made available in MaaS?

Models that are available for deployment as serverless APIs with pay-as-you-go billing are offered by the model provider but hosted in Microsoft-managed Azure infrastructure and accessed via API. Model providers define the license terms and set the price for use of their models, while Azure Machine Learning service manages the hosting infrastructure, makes the inference APIs available, and acts as the data processor for prompts submitted and content output by models deployed via MaaS. Learn more about data processing for MaaS at thedata privacyarticle.
Pay for model usage in MaaS
The discovery, subscription, and consumption experience for models deployed via MaaS is in theAzure AI Foundry portaland Azure Machine Learning studio. Users accept license terms for use of the models, and pricing information for consumption is provided during deployment. Models from third party providers are billed through Azure Marketplace, in accordance with theCommercial Marketplace Terms of Use; models from Microsoft are billed using Azure meters as First Party Consumption Services. As described in theProduct Terms, First Party Consumption Services are purchased using Azure meters but aren't subject to Azure service terms; use of these models is subject to the license terms provided.
Deploy models for inference through MaaS
Deploying a model through MaaS allows users to get access to ready to use inference APIs without the need to configure infrastructure or provision GPUs, saving engineering time and resources. These APIs can be integrated with several LLM tools and usage is billed as described in the previous section.
Fine-tune models through MaaS with Pay-as-you-go
For models that are available through MaaS and support fine-tuning, users can take advantage of hosted fine-tuning with pay-as-you-go billing to tailor the models using data they provide. For more information, seefine-tune a Llama 2 modelinAzure AI Foundry portal.
RAG with models deployed through MaaS
Azure AI Foundry enables users to make use of Vector Indexes and Retrieval Augmented Generation. Models that can be deployed as serverless APIs can be used to generate embeddings and inferencing based on custom data to generate answers specific to their use case. For more information, seeRetrieval augmented generation and indexes.
Regional availability of offers and models
PPay-as-you-go billing is available only to users whose Azure subscription belongs to a billing account in a country/region where the model provider has made the offer available. If the offer is available in the relevant region, the user then must have a Hub/Project in the Azure region where the model is available for deployment or fine-tuning, as applicable. SeeRegion availability for models in serverless API endpointsfor detailed information.
Content safety for models deployed via MaaS
Important
This feature is currently in public preview. This preview version is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities.
For more information, seeSupplemental Terms of Use for Microsoft Azure Previews.
For language models deployed via serverless APIs, Azure AI implements a default configuration ofAzure AI Content Safetytext moderation filters that detect harmful content such as hate, self-harm, sexual, and violent content. To learn more about content filtering (preview), seeContent safety for models curated by Azure AI in the model catalog.
Tip
Content filtering (preview) is not available for certain model types that are deployed via serverless APIs. These model types include embedding models and time series models.
Content filtering (preview) occurs synchronously as the service processes prompts to generate content. You might be billed separately according toAzure AI Content Safety pricingfor such use. You can disable content filtering (preview) for individual serverless endpoints either:
At the time when you first deploy a language model
Later, by selecting the content filtering toggle on the deployment details page
Suppose you decide to use an API other than theAzure AI Model Inference APIto work with a model that's deployed via a serverless API. In such a situation, content filtering (preview) isn't enabled unless you implement it separately by using Azure AI Content Safety.
To get started with Azure AI Content Safety, seeQuickstart: Analyze text content. If you don't use content filtering (preview) when working with models that are deployed via serverless APIs, you run a higher risk of exposing users to harmful content.
Network isolation for models deployed via Serverless APIs
Endpoints for models deployed as Serverless APIs follow the public network access (PNA) flag setting of the workspace in which the deployment exists. To secure your MaaS endpoint, disable the PNA flag on your workspace. You can secure inbound communication from a client to your endpoint by using a private endpoint for the workspace.
To set the PNA flag for the workspace:
Go to theAzure portal.
Search forAzure Machine Learning, and select your workspace from the list of workspaces.
On the Overview page, use the left pane to go toSettings>Networking.
Under thePublic accesstab, you can configure settings for the public network access flag.
Save your changes. Your changes might take up to five minutes to propagate.
If you have a workspace with a private endpoint created before July 11, 2024, new MaaS endpoints added to this workspace won't follow its networking configuration. Instead, you need to create a new private endpoint for the workspace and create new serverless API deployments in the workspace so that the new deployments can follow the workspace's networking configuration.
If you have a workspace with MaaS deployments created before July 11, 2024, and you enable a private endpoint on this workspace, the existing MaaS deployments won't follow the workspace's networking configuration. For serverless API deployments in the workspace to follow the workspace's configuration, you need to create the deployments again.
CurrentlyOn Your Datasupport isn't available for MaaS deployments in private workspaces, since private workspaces have the PNA flag disabled.
Any network configuration change (for example, enabling or disabling the PNA flag) might take up to five minutes to propagate.
Learn more
Model deprecation and retirement in Azure AI model catalog
How to use Open Source foundation models curated by Azure Machine Learning
Feedback
Was this page helpful?
Additional resources