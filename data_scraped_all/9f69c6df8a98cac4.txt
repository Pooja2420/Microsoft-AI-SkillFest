Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Visualize your traces (preview)
Article
2025-02-28
3 contributors
In this article
Important
Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, seeSupplemental Terms of Use for Microsoft Azure Previews.
After instrumenting your application to log traces, let's walkthrough how you can view your traces in both local and cloud solutions to debug your application.
View your traces for local debugging
To enable traces locally, you have two options:
UsingPrompty, you can trace your application with theAzure AI Inference SDK, which offers enhanced visibility and simplified troubleshooting for LLM-based applications. This method follows the OpenTelemetry specification, capturing and visualizing the internal execution details of any AI application, thereby enhancing the overall development experience. To learn more, seeDebugging Prompty.
Aspire Dashboard: A free & open-source OpenTelemetry dashboard for deep insights into your apps on your local development machine. To learn more, seeAspire Dashboard.
View your traces in Azure AI Foundry portal
Before you can log to Azure AI Foundry portal, attach an Application Insights resource to your project.
Navigate to your project inAzure AI Foundry portal.
Select theTracingpage on the left hand side.
SelectCreate Newto attach a new Application Insights resource to your project.
Supply a name and selectCreate.

Next, install theopentelemetrySDK:
opentelemetry
%pip install azure-monitor-opentelemetry
%pip install azure-monitor-opentelemetry
Now enable tracing with output to the console:
import os
from azure.monitor.opentelemetry import configure_azure_monitor

os.environ['AZURE_TRACING_GEN_AI_CONTENT_RECORDING_ENABLED'] = 'true'
# Enable Azure Monitor tracing
application_insights_connection_string = project.telemetry.get_connection_string()
if not application_insights_connection_string:
    print("Application Insights was not enabled for this project.")
    print("Enable it via the 'Tracing' tab in your Azure AI Foundry project page.")
    exit()
    
configure_azure_monitor(connection_string=application_insights_connection_string)
import os
from azure.monitor.opentelemetry import configure_azure_monitor

os.environ['AZURE_TRACING_GEN_AI_CONTENT_RECORDING_ENABLED'] = 'true'
# Enable Azure Monitor tracing
application_insights_connection_string = project.telemetry.get_connection_string()
if not application_insights_connection_string:
    print("Application Insights was not enabled for this project.")
    print("Enable it via the 'Tracing' tab in your Azure AI Foundry project page.")
    exit()
    
configure_azure_monitor(connection_string=application_insights_connection_string)
Finally, run an inferencing call. The call is logged to Azure AI Foundry. This code prints a link to the traces.
response = chat.complete(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": "You are an AI assistant that is a travel planning expert especially with National Parks."},
        {"role": "user", "content": "Hey, can you recommend me trails I should go on when I visit Mount Rainier?"},
    ]
)

print("View traces at:")
print(f"https://ai.azure.com/tracing?wsid=/subscriptions/{project.scope['subscription_id']}/resourceGroups/{project.scope['resource_group_name']}/providers/Microsoft.MachineLearningServices/workspaces/{project.scope['project_name']}")
response = chat.complete(
    model="gpt-4o-mini",
    messages=[
        {"role": "system", "content": "You are an AI assistant that is a travel planning expert especially with National Parks."},
        {"role": "user", "content": "Hey, can you recommend me trails I should go on when I visit Mount Rainier?"},
    ]
)

print("View traces at:")
print(f"https://ai.azure.com/tracing?wsid=/subscriptions/{project.scope['subscription_id']}/resourceGroups/{project.scope['resource_group_name']}/providers/Microsoft.MachineLearningServices/workspaces/{project.scope['project_name']}")
Select the link and begin viewing traces in Azure AI Foundry portal!
Debug and filter traces
In your project, you can filter your traces as you see fit.
By selecting a trace, I can step through each span and identify issues while observing how my application is responding.

Update your attached Application Insights resource
To update the Application Insights resource that is attached to your project, go toManage data sourceandEditto switch to a new Application Insights resource.

View your traces in Azure Monitor
If you logged traces using the previous code snippet, then you're all set to view your traces in Azure Monitor Application Insights. You can open in Application Insights fromManage data sourceand use theEnd-to-end transaction details viewto further investigate.
For more information on how to send Azure AI Inference traces to Azure Monitor and create Azure Monitor resource, seeAzure Monitor OpenTelemetry documentation.
View your generative AI spans and traces
From Azure AI Foundry project, you can also open your custom dashboard that provides you with insights specifically to help you monitor your generative AI application.
In this Azure Workbook, you can view your Gen AI spans and jump into the Azure MonitorEnd-to-end transaction details viewto deep dive and investigate.
Learn more about using this workbook to monitor your application, seeAzure Workbook documentation.
Related content
Trace your application with Azure AI Inference SDK
Feedback
Was this page helpful?
Additional resources