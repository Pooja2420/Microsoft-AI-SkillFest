Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Open Model LLM tool
Article
2024-08-28
5 contributors
In this article
The Open Model LLM tool enables the utilization of various Open Model and Foundational Models, such asFalconandLlama 2, for natural language processing in Azure Machine Learning prompt flow.
Caution
Deprecation notice:The Open Model LLM tool has been deprecated in favor of theLLM tool, which provide support for all the models supported by theAzure AI model inference APIand hence it provider greater flexibility.
Here's how it looks in action on the Visual Studio Code prompt flow extension. In this example, the tool is being used to call a LlaMa-2 chat endpoint and asking "What is CI?".

This prompt flow tool supports two different LLM API types:
Chat: Shown in the preceding example. The chat API type facilitates interactive conversations with text-based inputs and responses.
Completion: The Completion API type is used to generate single response text completions based on provided prompt input.
Quick overview: How do I use the Open Model LLM tool?
Choose a model from the Azure Machine Learning Model Catalog and get it deployed.
Connect to the model deployment.
Configure the open model llm tool settings.
Prepare the prompt.
Run the flow.
Prerequisites: Model deployment
Pick the model that matched your scenario from theAzure Machine Learning model catalog.
Use theDeploybutton to deploy the model to an Azure Machine Learning online inference endpoint.Use one of the Pay as you go deployment options.
Use one of the Pay as you go deployment options.
To learn more, seeDeploy foundation models to endpoints for inferencing.
Prerequisites: Connect to the model
In order for prompt flow to use your deployed model, you need to connect to it. There are two ways to connect.
Endpoint connections
Once your flow is associated to an Azure Machine Learning or Azure AI Foundry workspace, the Open Model LLM tool can use the endpoints on that workspace.
Using Azure Machine Learning or Azure AI Foundry workspaces: If you're using prompt flow in one of the web page based browsers workspaces, the online endpoints available on that workspace who up automatically.
Using Azure Machine Learning or Azure AI Foundry workspaces: If you're using prompt flow in one of the web page based browsers workspaces, the online endpoints available on that workspace who up automatically.
Using VS Code or code first: If you're using prompt flow in VS Code or one of the Code First offerings, you need to connect to the workspace. The Open Model LLM tool uses the azure.identity DefaultAzureCredential client for authorization. One way is throughsetting environment credential values.
Using VS Code or code first: If you're using prompt flow in VS Code or one of the Code First offerings, you need to connect to the workspace. The Open Model LLM tool uses the azure.identity DefaultAzureCredential client for authorization. One way is throughsetting environment credential values.
Custom connections
The Open Model LLM tool uses the CustomConnection. Prompt flow supports two types of connections:
Workspace connections- Connections that are stored as secrets on an Azure Machine Learning workspace. While these connections can be used, in many places, the are commonly created and maintained in the Studio UI. To learn how to create a custom connection in Studio UI, seehow to create a custom connection.
Workspace connections- Connections that are stored as secrets on an Azure Machine Learning workspace. While these connections can be used, in many places, the are commonly created and maintained in the Studio UI. To learn how to create a custom connection in Studio UI, seehow to create a custom connection.
Local connections- Connections that are stored locally on your machine. These connections aren't available in the Studio UX, but can be used with the VS Code extension. To learn how to create a local Custom Connection, seehow to create a local connection.
Local connections- Connections that are stored locally on your machine. These connections aren't available in the Studio UX, but can be used with the VS Code extension. To learn how to create a local Custom Connection, seehow to create a local connection.
The required keys to set are:
endpoint_urlThis value can be found at the previously created Inferencing endpoint.
This value can be found at the previously created Inferencing endpoint.
endpoint_api_keyEnsure to set it as a secret value.This value can be found at the previously created Inferencing endpoint.
Ensure to set it as a secret value.
This value can be found at the previously created Inferencing endpoint.
model_familySupported values: LLAMA, DOLLY, GPT2, or FALCONThis value is dependent on the type of deployment you're targeting.
Supported values: LLAMA, DOLLY, GPT2, or FALCON
This value is dependent on the type of deployment you're targeting.
Running the tool: Inputs
The Open Model LLM tool has many parameters, some of which are required. See the following table for details, you can match these parameters to the preceding screenshot for visual clarity.
Outputs
Deploying to an online endpoint
When you deploy a flow containing the Open Model LLM tool to an online endpoint, there's an extra step to set up permissions. During deployment through the web pages, there's a choice between System-assigned and User-assigned Identity types. Either way, using the Azure portal (or a similar functionality), add the "Reader" Job function role to the identity on the Azure Machine Learning workspace or Ai Studio project, which is hosting the endpoint. The prompt flow deployment may need to be refreshed.
Feedback
Was this page helpful?
Additional resources