Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Azure OpenAI Service quotas and limits
Article
2025-04-23
11 contributors
In this article
This article contains a quick reference and a detailed description of the quotas and limits for Azure OpenAI in Azure AI services.
Quotas and limits reference
The following sections provide you with a quick guide to the default quotas and limits that apply to Azure OpenAI:
/embeddings
/chat/completions
/chat/completions
/chat completions
vision-preview
turbo-2024-04-09
max_tokens
1Our current APIs allow up to 10 custom headers, which are passed through the pipeline, and returned. Some customers now exceed this header count resulting in HTTP 431 errors. There's no solution for this error, other than to reduce header volume.In future API versions we will no longer pass through custom headers. We recommend customers not depend on custom headers in future system architectures.
Note
Quota limits are subject to change.
Batch limits
Batch quota
The table shows the batch quota limit. Quota values for global batch are represented in terms of enqueued tokens. When you submit a file for batch processing the number of tokens present in the file are counted. Until the batch job reaches a terminal state, those tokens will count against your  total enqueued token limit.
Global batch
gpt-4o
gpt-4o-mini
gpt-4-turbo
gpt-4
gpt-35-turbo
o3-mini
B = billion | M = million | K = thousand
Data zone batch
gpt-4o
gpt-4o-mini
o3-mini
GPT 4.1 series
gpt-4.1
gpt-4.1
gpt-4.1-nano
gpt-4.1-nano
gpt-4.1-mini
gpt-4.1-mini
computer-use-preview global standard
computer-use-preview
computer-use-preview
GPT-4.5 Preview global standard
gpt-4.5
gpt-4.5
o-seriesrate limits
o-series
Important
The ratio of RPM/TPM for quota with o1-series models works differently than older chat completions models:
Older chat models:1 unit of capacity = 6 RPM and 1,000 TPM.
o1 & o1-preview:1 unit of capacity = 1 RPM and 6,000 TPM.
o31 unit of capacity = 1 RPM per 1,000 TPM
o4-mini1 unit of capacity = 1 RPM per 1,000 TPM
o3-mini:1 unit of capacity = 1 RPM per 10,000 TPM.
o1-mini:1 unit of capacity = 1 RPM per 10,000 TPM.
This is particularly important for programmatic model deployment as this change in RPM/TPM ratio can result in accidental under allocation of quota if one is still assuming the 1:1000 ratio followed by older chat completion models.
There's a known issue with thequota/usages APIwhere it assumes the old ratio applies to the new o1-series models. The API returns the correct base capacity number, but doesn't apply the correct ratio for the accurate calculation of TPM.
o-seriesglobal standard
o-series
o4-mini
o3
o3-mini
o1
o1-preview
o1-mini
o4-mini
o3
o3-mini
o1
o1-preview
o1-mini
o-seriesdata zone standard
o-series
o3-mini
o3-mini
o1
o1
o1-preview & o1-mini standard
o1-preview
o1-mini
o1-preview
o1-mini
gpt-4o & GPT-4 Turbo rate limits
gpt-4oandgpt-4o-mini, andgpt-4(turbo-2024-04-09) have rate limit tiers with higher limits for certain customer types.
gpt-4o
gpt-4o-mini
gpt-4
turbo-2024-04-09
gpt-4o & GPT-4 Turbo global standard
gpt-4o
gpt-4o-mini
gpt-4
gpt-4o
gpt-4o-mini
gpt-4
M = million | K = thousand
gpt-4o data zone standard
gpt-4o
gpt-4o-mini
gpt-4o
gpt-4o-mini
M = million | K = thousand
gpt-4o standard
gpt-4o
gpt-4o-mini
gpt-4o
gpt-4o-mini
M = million | K = thousand
gpt-4o audio
The rate limits for eachgpt-4oaudio model deployment are 100K TPM and 1K RPM. During the preview,Azure AI Foundry portaland APIs might inaccurately show different rate limits. Even if you try to set a different rate limit, the actual rate limit will be 100K TPM and 1K RPM.
gpt-4o
gpt-4o-audio-preview
gpt-4o-realtime-preview
gpt-4o-mini-audio-preview
gpt-4o-mini-realtime-preview
M = million | K = thousand
Global standard deployments use Azure's global infrastructure, dynamically routing customer traffic to the data center with best availability for the customerâs inference requests. Similarly, Data zone standard deployments allow you to leverage Azure global infrastructure to dynamically route traffic to the data center within the Microsoft defined data zone with the best availability for each request. This enables more consistent latency for customers with low to medium levels of traffic. Customers with high sustained levels of usage might see greater variability in response latency.
The Usage Limit determines the level of usage above which customers might see larger variability in response latency. A customerâs usage is defined per model and is the total tokens consumed across all deployments in all subscriptions in all regions for a given tenant.
Note
Usage tiers only apply to standard, data zone standard, and global standard deployment types. Usage tiers don't apply to global batch and provisioned throughput deployments.
gpt-4o
gpt-4o-mini
gpt-4
gpt-4-32k
Other offer types
If your Azure subscription is linked to certainoffer typesyour max quota values are lower than the values indicated in the above tables.
Azure for Students
MSDN
Pay-as-you-go
Azure_MS-AZR-0111P
Azure_MS-AZR-0035P
Azure_MS-AZR-0025P
Azure_MS-AZR-0052P
CSP Integration Sandbox
Lightweight trial
Free Trials
Azure Pass
*This only applies to a small number of legacy CSP sandbox subscriptions. Use the query below to determine whatquotaIdis associated with your subscription.
quotaId
To determine the offer type that is associated with your subscription you can check yourquotaId. If yourquotaIdisn't listed in this table your subscription qualifies for default quota.
quotaId
quotaId
REST
CLI
API reference
az login
access_token=$(az account get-access-token --query accessToken -o tsv)
az login
access_token=$(az account get-access-token --query accessToken -o tsv)
curl -X GET "https://management.azure.com/subscriptions/{subscriptionId}?api-version=2020-01-01" \
  -H "Authorization: Bearer $access_token" \
  -H "Content-Type: application/json"
curl -X GET "https://management.azure.com/subscriptions/{subscriptionId}?api-version=2020-01-01" \
  -H "Authorization: Bearer $access_token" \
  -H "Content-Type: application/json"
az rest --method GET --uri "https://management.azure.com/subscriptions/{sub-id}?api-version=2020-01-01"
az rest --method GET --uri "https://management.azure.com/subscriptions/{sub-id}?api-version=2020-01-01"
Output
{
  "authorizationSource": "Legacy",
  "displayName": "Pay-As-You-Go",
  "id": "/subscriptions/aaaaaa-bbbbb-cccc-ddddd-eeeeee",
  "state": "Enabled",
  "subscriptionId": "aaaaaa-bbbbb-cccc-ddddd-eeeeee",
  "subscriptionPolicies": {
    "locationPlacementId": "Public_2014-09-01",
    "quotaId": "PayAsYouGo_2014-09-01",
    "spendingLimit": "Off"
  }
}
{
  "authorizationSource": "Legacy",
  "displayName": "Pay-As-You-Go",
  "id": "/subscriptions/aaaaaa-bbbbb-cccc-ddddd-eeeeee",
  "state": "Enabled",
  "subscriptionId": "aaaaaa-bbbbb-cccc-ddddd-eeeeee",
  "subscriptionPolicies": {
    "locationPlacementId": "Public_2014-09-01",
    "quotaId": "PayAsYouGo_2014-09-01",
    "spendingLimit": "Off"
  }
}
EnterpriseAgreement_2014-09-01
PayAsYouGo_2014-09-01
MSDN_2014-09-01
CSPDEVTEST_2018-05-01
AzureForStudents_2018-01-01
FreeTrial_2014-09-01
AzurePass_2014-09-01
AzureInOpen_2014-09-01
LightweightTrial_2016-09-01
MPN_2014-09-01
MSDNDevTest_2014-09-01
General best practices to remain within rate limits
To minimize issues related to rate limits, it's a good idea to use the following techniques:
Implement retry logic in your application.
Avoid sharp changes in the workload. Increase the workload gradually.
Test different load increase patterns.
Increase the quota assigned to your deployment. Move quota from another deployment, if necessary.
How to request quota increases
Quota increase requests can be submitted via thequota increase request form. Due to high demand, quota increase requests are being accepted and will be filled in the order they're received. Priority is given to customers who generate traffic that consumes the existing quota allocation, and your request might be denied if this condition isn't met.
For other rate limits,submit a service request.
Regional quota capacity limits
You can view quota availability by region for your subscription in theAzure AI Foundry portal.
Alternatively to view quota capacity by region for a specific model/version you can query thecapacity APIfor your subscription. Provide asubscriptionId,model_name, andmodel_versionand the API will return the available capacity for that model across all regions, and deployment types for your subscription.
subscriptionId
model_name
model_version
Note
Currently both the Azure AI Foundry portal and the capacity API will return quota/capacity information for models that areretiredand no longer available.
API Reference
import requests
import json
from azure.identity import DefaultAzureCredential

subscriptionId = "Replace with your subscription ID" #replace with your subscription ID
model_name = "gpt-4o"     # Example value, replace with model name
model_version = "2024-08-06"   # Example value, replace with model version

token_credential = DefaultAzureCredential()
token = token_credential.get_token('https://management.azure.com/.default')
headers = {'Authorization': 'Bearer ' + token.token}

url = f"https://management.azure.com/subscriptions/{subscriptionId}/providers/Microsoft.CognitiveServices/modelCapacities"
params = {
    "api-version": "2024-06-01-preview",
    "modelFormat": "OpenAI",
    "modelName": model_name,
    "modelVersion": model_version
}

response = requests.get(url, params=params, headers=headers)
model_capacity = response.json()

print(json.dumps(model_capacity, indent=2))
import requests
import json
from azure.identity import DefaultAzureCredential

subscriptionId = "Replace with your subscription ID" #replace with your subscription ID
model_name = "gpt-4o"     # Example value, replace with model name
model_version = "2024-08-06"   # Example value, replace with model version

token_credential = DefaultAzureCredential()
token = token_credential.get_token('https://management.azure.com/.default')
headers = {'Authorization': 'Bearer ' + token.token}

url = f"https://management.azure.com/subscriptions/{subscriptionId}/providers/Microsoft.CognitiveServices/modelCapacities"
params = {
    "api-version": "2024-06-01-preview",
    "modelFormat": "OpenAI",
    "modelName": model_name,
    "modelVersion": model_version
}

response = requests.get(url, params=params, headers=headers)
model_capacity = response.json()

print(json.dumps(model_capacity, indent=2))
Next steps
Explore how tomanage quotafor your Azure OpenAI deployments.
Learn more about theunderlying models that power Azure OpenAI.
Feedback
Was this page helpful?
Additional resources