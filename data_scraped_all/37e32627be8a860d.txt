Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
What is Delta Lake?
Article
2024-12-18
9 contributors
In this article
Delta Lake is the optimized storage layer that provides the foundation for tables in a lakehouse on Databricks. Delta Lake isopen source softwarethat extends Parquet data files with a file-based transaction log forACID transactionsand scalable metadata handling. Delta Lake is fully compatible with Apache Spark APIs, and was developed for tight integration with Structured Streaming, allowing you to easily use a single copy of data for both batch and streaming operations and providing incremental processing at scale.
Delta Lake is the default format for all operations on Azure Databricks. Unless otherwise specified, all tables on Azure Databricks are Delta tables. Databricks originally developed the Delta Lake protocol and continues to actively contribute to the open source project. Many of the optimizations and products in the Databricks platform build upon the guarantees provided by Apache Spark and Delta Lake. For information on optimizations on Azure Databricks, seeOptimization recommendations on Azure Databricks.
For reference information on Delta Lake SQL commands, seeDelta Lake statements.
The Delta Lake transaction log has a well-defined open protocol that can be used by any system to read the log. SeeDelta Transaction Log Protocol.
Getting started with Delta Lake
All tables on Azure Databricks are Delta tables by default. Whether youâre using Apache SparkDataFramesor SQL, you get all the benefits of Delta Lake just by saving your data to the lakehouse with default settings.
For examples of basic Delta Lake operations such as creating tables, reading, writing, and updating data, seeTutorial: Delta Lake.
Databricks has many recommendations forbest practices for Delta Lake.
Converting and ingesting data to Delta Lake
Azure Databricks provides a number of products to accelerate and simplify loading data to your lakehouse.
DLT:Get started: Build an extract, transform, and load (ETL) pipeline in Azure DatabricksLoad data using streaming tables (Python/SQL notebook)Use streaming tables in Databricks SQL
DLT:
Get started: Build an extract, transform, and load (ETL) pipeline in Azure Databricks
Load data using streaming tables (Python/SQL notebook)
Use streaming tables in Databricks SQL
COPY INTO
COPY INTO
Auto Loader
Auto Loader
Add data UI
Add data UI
Incrementally convert Parquet or Iceberg data to Delta Lake
Incrementally convert Parquet or Iceberg data to Delta Lake
One-time conversion of Parquet or Iceberg data to Delta Lake
One-time conversion of Parquet or Iceberg data to Delta Lake
Third-party partners
Third-party partners
For a full list of ingestion options, seeIngest data into an Azure Databricks lakehouse.
Updating and modifying Delta Lake tables
Atomic transactions with Delta Lake provide many options for updating data and metadata. Databricks recommends you avoid interacting directly with data and transaction log files in Delta Lake file directories to avoid corrupting your tables.
Delta Lake supports upserts using the merge operation. SeeUpsert into a Delta Lake table using merge.
Delta Lake provides numerous options for selective overwrites based on filters and partitions. SeeSelectively overwrite data with Delta Lake.
You can manually or automatically update your table schema without rewriting data. SeeUpdate Delta Lake table schema.
Enable columns mapping to rename or delete columns without rewriting data. SeeRename and drop columns with Delta Lake column mapping.
Incremental and streaming workloads on Delta Lake
Delta Lake is optimized for Structured Streaming on Azure Databricks.DLTextends native capabilities with simplified infrastructure deployment, enhanced scaling, and managed data dependencies.
Delta table streaming reads and writes
Use Delta Lake change data feed on Azure Databricks
Querying previous versions of a table
Each write to a Delta table creates a new table version. You can use the transaction log to review modifications to your table and query previous table versions. SeeWork with Delta Lake table history.
Delta Lake schema enhancements
Delta Lake validates schema on write, ensuring that all data written to a table matches the requirements youâve set.
Schema enforcement
Constraints on Azure Databricks
Delta Lake generated columns
Enrich Delta Lake tables with custom metadata
Managing files and indexing data with Delta Lake
Azure Databricks sets many default parameters for Delta Lake that impact the size of data files and number of table versions that are retained in history. Delta Lake uses a combination of metadata parsing and physical data layout to reduce the number of files scanned to fulfill any query.
Use liquid clustering for Delta tables
Data skipping for Delta Lake
Optimize data file layout
Remove unused data files with vacuum
Configure Delta Lake to control data file size
Configuring and reviewing Delta Lake settings
Azure Databricks stores all data and metadata for Delta Lake tables in cloud object storage. Many configurations can be set at either the table level or within the Spark session. You can review the details of the Delta table to discover what options are configured.
Review Delta Lake table details with describe detail
Delta table properties reference
Data pipelines using Delta Lake and DLT
Azure Databricks encourages users to leverage amedallion architectureto process data through a series of tables as data is cleaned and enriched.DLTsimplifies ETL workloads through optimized execution and automated infrastructure deployment and scaling.
Delta Lake feature compatibility
Not all Delta Lake features are in all versions of Databricks Runtime. For information about Delta Lake versioning, seeDelta Lake feature compatibility and protocols.
Delta Lake API documentation
For most read and write operations on Delta tables, you can useSpark SQLor Apache SparkDataFrameAPIs.
For Delta Lake-specific SQL statements, seeDelta Lake statements.
Azure Databricks ensures binary compatibility with Delta Lake APIs in Databricks Runtime. To view the Delta Lake API version packaged in each Databricks Runtime version, see theSystem environmentsection on the relevant article in theDatabricks Runtime release notes. For documentation on Delta Lake APIs for Python, Scala, and Java, see theOSS Delta Lake documentation.
Feedback
Was this page helpful?
Additional resources