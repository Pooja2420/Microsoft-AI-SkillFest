Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Build large-scale data copy pipelines with metadata-driven approach in copy data tool
Article
2025-02-13
3 contributors
In this article
APPLIES TO:Azure Data FactoryAzure Synapse Analytics
Tip
Try outData Factory in Microsoft Fabric, an all-in-one analytics solution for enterprises.Microsoft Fabriccovers everything from data movement to data science, real-time analytics, business intelligence, and reporting. Learn how tostart a new trialfor free!
When you want to copy huge amounts of objects (for example, thousands of tables) or load data from large variety of sources, the appropriate approach is to input the name list of the objects with required copy behaviors in a control table, and then use parameterized pipelines to read the same from the control table and apply them to the jobs accordingly.  By doing so, you can maintain (for example, add/remove) the objects list to be copied easily by just updating the object names in control table instead of redeploying the pipelines. Whatâs more, you will have single place to easily check which objects copied by which pipelines/triggers with defined copy behaviors.
Copy data tool in ADF eases the journey of building such metadata driven data copy pipelines. After you go through an intuitive flow from a wizard-based experience, the tool can generate parameterized pipelines and SQL scripts for you to create external control tables accordingly. After you run the generated scripts to create the control table in your SQL database, your pipelines will read the metadata from the control table and apply them on the copy jobs automatically.
Create metadata-driven copy jobs from copy data tool
SelectMetadata-driven copy taskin copy data tool.You need to input the connection and table name of your control table, so that the generated pipeline will read metadata from that.
SelectMetadata-driven copy taskin copy data tool.
You need to input the connection and table name of your control table, so that the generated pipeline will read metadata from that.

Input theconnection of your source database. You can useparameterized linked serviceas well.
Input theconnection of your source database. You can useparameterized linked serviceas well.

Select thetable nameto copy.NoteIf you select tabular data store, you will have chance to further select either full load or delta load in the next page. If you select storage store, you can further select full load only in the next page. Incrementally loading new files only from storage store is currently not supported.
Select thetable nameto copy.

Note
If you select tabular data store, you will have chance to further select either full load or delta load in the next page. If you select storage store, you can further select full load only in the next page. Incrementally loading new files only from storage store is currently not supported.
Chooseloading behavior.TipIf you want to do full copy on all the tables, selectFull load all tables. If you want to do incremental copy, you can selectconfigure for each table individually, and selectDelta loadas well as watermark column name & value to start for each table.
Chooseloading behavior.
Tip
If you want to do full copy on all the tables, selectFull load all tables. If you want to do incremental copy, you can selectconfigure for each table individually, and selectDelta loadas well as watermark column name & value to start for each table.
SelectDestination data store.
SelectDestination data store.
InSettingspage, You can decide the max number of copy activities to copy data from your source store concurrently viaNumber of concurrent copy tasks. The default value is 20.
InSettingspage, You can decide the max number of copy activities to copy data from your source store concurrently viaNumber of concurrent copy tasks. The default value is 20.

After pipeline deployment, you can copy or download the SQL scripts from UI for creating control table and store procedure.You will see two SQL scripts.The first SQL script is used to create two control tables. The main control table stores the table list, file path or copy behaviors. The connection control table stores the connection value of your data store if you used parameterized linked service.The second SQL script is used to create a store procedure. It is used to update the watermark value in main control table when the incremental copy jobs complete every time.
After pipeline deployment, you can copy or download the SQL scripts from UI for creating control table and store procedure.

You will see two SQL scripts.
The first SQL script is used to create two control tables. The main control table stores the table list, file path or copy behaviors. The connection control table stores the connection value of your data store if you used parameterized linked service.
The second SQL script is used to create a store procedure. It is used to update the watermark value in main control table when the incremental copy jobs complete every time.
OpenSSMSto connect to your control table server, and run the two SQL scripts to create control tables and store procedure.
OpenSSMSto connect to your control table server, and run the two SQL scripts to create control tables and store procedure.

Query the main control table and connection control table to review the metadata in it.Main control tableConnection control table
Query the main control table and connection control table to review the metadata in it.
Main control table
Connection control table
Go back to ADF portal to view and debug pipelines. You will see a folder created by naming "MetadataDrivenCopyTask_#########".Clickthe pipeline naming with "MetadataDrivenCopyTask###_TopLevel" and clickdebug run.You are required to input the following parameters:Parameters nameDescriptionMaxNumberOfConcurrentTasksYou can always change the max number of concurrent copy activities run before pipeline run. The default value will be the one you input in copy data tool.MainControlTableNameYou can always change the main control table name, so the pipeline will get the metadata from that table before run.ConnectionControlTableNameYou can always change the connection control table name (optional), so the pipeline will get the metadata related to data store connection before run.MaxNumberOfObjectsReturnedFromLookupActivityIn order to avoid reaching the limit of output lookup activity, there is a way to define the max number of objects returned by lookup activity. In most cases, the default value is not required to be changed.windowStartWhen you input dynamic value (for example, yyyy/mm/dd) as folder path, the parameter is used to pass the current trigger time to pipeline in order to fill the dynamic folder path. When the pipeline is triggered by schedule trigger or tumbling windows trigger, users do not need to input the value of this parameter. Sample value: 2021-01-25T01:49:28Z
Go back to ADF portal to view and debug pipelines. You will see a folder created by naming "MetadataDrivenCopyTask_#########".Clickthe pipeline naming with "MetadataDrivenCopyTask###_TopLevel" and clickdebug run.
You are required to input the following parameters:
Enable the trigger to operationalize the pipelines.
Enable the trigger to operationalize the pipelines.

Update control table by copy data tool
You can always directly update the control table by adding or removing the object to be copied or changing the copy behavior for each table. We also create UI experience in copy data tool to ease the journey of editing the control table.
Right-click the top-level pipeline:MetadataDrivenCopyTask_xxx_TopLevel, and then selectEdit control table.
Right-click the top-level pipeline:MetadataDrivenCopyTask_xxx_TopLevel, and then selectEdit control table.

Select rows from the control table to edit.
Select rows from the control table to edit.

Go throughput the copy data tool, and it will come up with a new SQL script for you. Rerun the SQL script to update your control table.NoteThe pipeline will NOT be redeployed. The new created SQL script help you to update the control table only.
Go throughput the copy data tool, and it will come up with a new SQL script for you. Rerun the SQL script to update your control table.

Note
The pipeline will NOT be redeployed. The new created SQL script help you to update the control table only.
Control tables
Main control table
Each row in control table contains the metadata for one object (for example, one table) to be copied.
Connection control table
Each row in control table contains one connection setting for the data store.
Pipelines
You will see three levels of pipelines are generated by copy data tool.
MetadataDrivenCopyTask_xxx_TopLevel
This pipeline will calculate the total number of objects (tables etc.) required to be copied in this run, come up with the number of sequential batches based on the max allowed concurrent copy task, and then execute another pipeline to copy different batches sequentially.
MetadataDrivenCopyTask_xxx_ MiddleLevel
This pipeline will copy one batch of objects. The objects belonging to this batch will be copied in parallel.
MetadataDrivenCopyTask_xxx_ BottomLevel
This pipeline will copy objects from one group. The objects belonging to this group will be copied in parallel.
Known limitations
IR name, database type, file format type cannot be parameterized in ADF. For example, if you want to ingest data from both Oracle Server and SQL Server, you will need two different parameterized pipelines. But the single control table can be shared by two sets of pipelines.
OPENJSON is used in generated SQL scripts by copy data tool. If you are using SQL Server to host control table, it must be SQL Server 2016 (13.x) and later in order to support OPENJSON function.
Related content
Try these tutorials that use the Copy Data tool:
Quickstart: Create a data factory using the Copy Data tool
Tutorial: Copy data in Azure using the Copy Data tool
Tutorial: Copy on-premises data to Azure using the Copy Data tool
Feedback
Was this page helpful?
Additional resources