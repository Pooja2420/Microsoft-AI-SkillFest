Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Architecture best practices for Azure OpenAI Service
Article
2024-03-14
10 contributors
In this article
Azure OpenAI Service provides REST API access to OpenAI large language models (LLMs), adding Azure networking and security capabilities. This article provides architectural recommendations to help you make informed decisions when you use Azure OpenAI as part of your workload's architecture. The guidance is based on theAzure Well-Architected Framework pillars.
Important
How to use this guide
Each section has adesign checklistthat presents architectural areas of concern along with design strategies localized to the technology scope.
Also included arerecommendationson the technology capabilities that can help materialize those strategies. The recommendations don't represent an exhaustive list of all configurations available for Azure OpenAI and its dependencies. Instead, they list the key recommendations mapped to the design perspectives. Use the recommendations to build your proof-of-concept or optimize your existing environments.
Foundational architecture that demonstrates the key recommendations:Baseline OpenAI end-to-end chat reference architecture.
Technology scope
This review focuses solely on Azure OpenAI.
Reliability
The purpose of the Reliability pillar is to provide continued functionality bybuilding enough resilience and the ability to recover quickly from failures.
TheReliability design principlesprovide a high-level design strategy applied for individual components, system flows, and the system as a whole.
Design checklist
Start your design strategy based on thedesign review checklist for Reliability. Determine its relevance to your business requirements. Extend the strategy to include more approaches as needed.
Resiliency: Choose the appropriate deployment option of either pay-as-you-go orprovisioned throughputbased on your use case. Because reserved capacity increases resiliency, choose provisioned throughput for production solutions. The pay-as-you-go approach is ideal for dev/test environments.
Resiliency: Choose the appropriate deployment option of either pay-as-you-go orprovisioned throughputbased on your use case. Because reserved capacity increases resiliency, choose provisioned throughput for production solutions. The pay-as-you-go approach is ideal for dev/test environments.
Redundancy: Add the appropriate gateways in front of your Azure OpenAI deployments. The gateway must have the capability to withstand transient failures like throttling and also route to multiple Azure OpenAI instances. Consider routing to instances in different regions to build regional redundancy.
Redundancy: Add the appropriate gateways in front of your Azure OpenAI deployments. The gateway must have the capability to withstand transient failures like throttling and also route to multiple Azure OpenAI instances. Consider routing to instances in different regions to build regional redundancy.
Resiliency: If you're usingprovisioned throughput, consider also deploying a pay-as-you-go instance to handle overflow. You can route calls to the pay-as-you-go instance via your gateway when your provisioned throughput model is throttled.
Resiliency: If you're usingprovisioned throughput, consider also deploying a pay-as-you-go instance to handle overflow. You can route calls to the pay-as-you-go instance via your gateway when your provisioned throughput model is throttled.
Resiliency: Monitor capacity usage to ensure you aren't exceeding throughput limits. Regularly review capacity usage to achieve more accurate forecasting and help prevent service interruptions due to capacity constraints.
Resiliency: Monitor capacity usage to ensure you aren't exceeding throughput limits. Regularly review capacity usage to achieve more accurate forecasting and help prevent service interruptions due to capacity constraints.
Resiliency: Follow theguidance for fine-tuning with large data filesand import the data from an Azure blob store. Large files, 100 MB or larger, can become unstable when uploaded through multipart forms because the requests are atomic and can't be retried or resumed.
Resiliency: Follow theguidance for fine-tuning with large data filesand import the data from an Azure blob store. Large files, 100 MB or larger, can become unstable when uploaded through multipart forms because the requests are atomic and can't be retried or resumed.
Recovery: Define a recovery strategy that includes a recovery plan for models that are fine-tuned and for training data uploaded to Azure OpenAI. Because Azure OpenAI doesn't have automatic failover, you must design a strategy that encompasses the entire service and all dependencies, such as storage that contains training data.
Recovery: Define a recovery strategy that includes a recovery plan for models that are fine-tuned and for training data uploaded to Azure OpenAI. Because Azure OpenAI doesn't have automatic failover, you must design a strategy that encompasses the entire service and all dependencies, such as storage that contains training data.
Recommendations
Security
The purpose of the Security pillar is to provideconfidentiality, integrity, and availabilityguarantees to the workload.
TheSecurity design principlesprovide a high-level design strategy for achieving those goals by applying approaches to the technical design around Azure OpenAI.
Design checklist
Start your design strategy based on thedesign review checklist for Securityand identify vulnerabilities and controls to improve the security posture. Then, review theAzure security baseline for Azure OpenAI. Finally, extend the strategy to include more approaches as needed.
Protect confidentiality: If you upload training data to Azure OpenAI, usecustomer-managed keysfor data encryption, implement a key-rotation strategy, anddelete training, validation, and training results data. If you use an external data store for training data, follow security best practices for that store. For example, for Azure Blob Storage, use customer-managed keys for encryption and implement a key-rotation strategy. Use managed identity-based access, implement a network perimeter by using private endpoints, and enable access logs.
Protect confidentiality: If you upload training data to Azure OpenAI, usecustomer-managed keysfor data encryption, implement a key-rotation strategy, anddelete training, validation, and training results data. If you use an external data store for training data, follow security best practices for that store. For example, for Azure Blob Storage, use customer-managed keys for encryption and implement a key-rotation strategy. Use managed identity-based access, implement a network perimeter by using private endpoints, and enable access logs.
Protect confidentiality: Guard against data exfiltration by limiting the outbound URLs that Azure OpenAI resources can access.
Protect confidentiality: Guard against data exfiltration by limiting the outbound URLs that Azure OpenAI resources can access.
Protect integrity: Implement access controls to authenticate and authorize user access to the system by using the least-privilege principle and by using individual identities instead of keys.
Protect integrity: Implement access controls to authenticate and authorize user access to the system by using the least-privilege principle and by using individual identities instead of keys.
Protect integrity: Implementjailbreak risk detectionto safeguard your language model deployments against prompt injection attacks.
Protect integrity: Implementjailbreak risk detectionto safeguard your language model deployments against prompt injection attacks.
Protect availability: Use security controls to prevent attacks that might exhaust model usage quotas. You might configure controls to isolate the service on a network. If the service must be accessible from the internet, consider using a gateway to block suspected abuse by using routing or throttling.
Protect availability: Use security controls to prevent attacks that might exhaust model usage quotas. You might configure controls to isolate the service on a network. If the service must be accessible from the internet, consider using a gateway to block suspected abuse by using routing or throttling.
Recommendations
disableLocalAuth
true
Cost Optimization
Cost Optimization focuses ondetecting spend patterns, prioritizing investments in critical areas, and optimizing in othersto meet the organization's budget while meeting business requirements.
Read theCost Optimization design principlesto learn about approaches for achieving those goals and the tradeoffs necessary in technical design choices related to Azure OpenAI.
Design checklist
Start your design strategy based on thedesign review checklist for Cost Optimizationfor investments. Fine-tune the design so that the workload is aligned with its allocated budget. Your design should use the appropriate Azure capabilities, monitor investments, and find opportunities to optimize over time.
Cost management: Develop your cost model, considering prompt sizes. Understanding prompt input and response sizes and how text translates into tokens helps you create a viable cost model.
Cost management: Develop your cost model, considering prompt sizes. Understanding prompt input and response sizes and how text translates into tokens helps you create a viable cost model.
Usage optimization: Start withpay-as-you-go pricingfor Azure OpenAI until your token usage is predictable.
Usage optimization: Start withpay-as-you-go pricingfor Azure OpenAI until your token usage is predictable.
Rate optimization: When your token usage is sufficiently high and predictable over a period of time, use theprovisioned throughputpricing model for better cost optimization.
Rate optimization: When your token usage is sufficiently high and predictable over a period of time, use theprovisioned throughputpricing model for better cost optimization.
Usage optimization: Considermodel pricingand capabilities when you choose models. Start with less-costly models for less-complex tasks like text generation or completion tasks. For more complex tasks like language translation or content understanding, consider using more advanced models. Consider differentmodel capabilitiesand maximum token usage limits when you choose a model that's appropriate for use cases like text embedding, image generation, or transcription scenarios. By carefully selecting the model that best fits your needs, you can optimize costs while still achieving the desired application performance.
Usage optimization: Considermodel pricingand capabilities when you choose models. Start with less-costly models for less-complex tasks like text generation or completion tasks. For more complex tasks like language translation or content understanding, consider using more advanced models. Consider differentmodel capabilitiesand maximum token usage limits when you choose a model that's appropriate for use cases like text embedding, image generation, or transcription scenarios. By carefully selecting the model that best fits your needs, you can optimize costs while still achieving the desired application performance.
Usage optimization: Use the token-limiting constraints offered by the API calls, such asmax_tokensandn, which indicate the number of completions to generate.
Usage optimization: Use the token-limiting constraints offered by the API calls, such asmax_tokensandn, which indicate the number of completions to generate.
max_tokens
n
Usage optimization: Maximize Azure OpenAI price breakpoints, for example, fine-tuning and model breakpoints like image generation. Because fine-tuning is charged per hour, use as much time as you have available per hour to improve fine-tuning results while avoiding slipping into the next billing period. Similarly, the cost for generating 100 images is the same as the cost for 1 image. Maximize price breakpoints to your advantage.
Usage optimization: Maximize Azure OpenAI price breakpoints, for example, fine-tuning and model breakpoints like image generation. Because fine-tuning is charged per hour, use as much time as you have available per hour to improve fine-tuning results while avoiding slipping into the next billing period. Similarly, the cost for generating 100 images is the same as the cost for 1 image. Maximize price breakpoints to your advantage.
Usage optimization: Remove unused fine-tuned models when they're no longer being consumed to avoid incurring an ongoing hosting fee.
Usage optimization: Remove unused fine-tuned models when they're no longer being consumed to avoid incurring an ongoing hosting fee.
Adjust usage: Optimize prompt input and response length. Longer prompts raise costs by consuming more tokens. However, prompts that are missing sufficient context don't help the models yield good results. Create concise prompts that provide enough context for the model to generate a useful response. Also ensure that you optimize the limit of the response length.
Adjust usage: Optimize prompt input and response length. Longer prompts raise costs by consuming more tokens. However, prompts that are missing sufficient context don't help the models yield good results. Create concise prompts that provide enough context for the model to generate a useful response. Also ensure that you optimize the limit of the response length.
Cost efficiency: Batch requests where possible to minimize the per-call overhead, which can reduce overall costs. Ensure that you optimize batch size.
Cost efficiency: Batch requests where possible to minimize the per-call overhead, which can reduce overall costs. Ensure that you optimize batch size.
Cost efficiency: Because models have different fine-tuning costs, consider these costs if your solution requires fine-tuning.
Cost efficiency: Because models have different fine-tuning costs, consider these costs if your solution requires fine-tuning.
Monitor and optimize: Set up a cost-tracking system that monitors model usage. Use that information to help inform model choices and prompt sizes.
Monitor and optimize: Set up a cost-tracking system that monitors model usage. Use that information to help inform model choices and prompt sizes.
Recommendations
max_tokens
n
Operational Excellence
Operational Excellence primarily focuses on procedures fordevelopment practices,observability, andrelease management.
TheOperational Excellence design principlesprovide a high-level design strategy for achieving those goals toward the workload's operational requirements.
Design checklist
Start your design strategy based on thedesign review checklist for Operational Excellence. This checklist defines processes for observability, testing, and deployment related to Azure OpenAI.
Azure DevOps culture: Ensure deployment of Azure OpenAI instances across your various environments, such as development, test, and production. Ensure that you have environments to support continuous learning and experimentation throughout the development cycle.
Azure DevOps culture: Ensure deployment of Azure OpenAI instances across your various environments, such as development, test, and production. Ensure that you have environments to support continuous learning and experimentation throughout the development cycle.
Observability: Monitor, aggregate, and visualize appropriate metrics.
Observability: Monitor, aggregate, and visualize appropriate metrics.
Observability: If Azure OpenAI diagnostics are insufficient for your needs, consider using a gateway like Azure API Management in front of Azure OpenAI to log both incoming prompts and outgoing responses where permitted. This information can help you understand the effectiveness of the model for incoming prompts.
Observability: If Azure OpenAI diagnostics are insufficient for your needs, consider using a gateway like Azure API Management in front of Azure OpenAI to log both incoming prompts and outgoing responses where permitted. This information can help you understand the effectiveness of the model for incoming prompts.
Deploy with confidence: Use infrastructure as code (IaC) to deploy Azure OpenAI, model deployments, and other infrastructure required for fine-tuning models.
Deploy with confidence: Use infrastructure as code (IaC) to deploy Azure OpenAI, model deployments, and other infrastructure required for fine-tuning models.
Deploy with confidence: Followlarge language model operations (LLMOps)practices to operationalize the management of your Azure OpenAI LLMs, including deployment, fine-tuning, and prompt engineering.
Deploy with confidence: Followlarge language model operations (LLMOps)practices to operationalize the management of your Azure OpenAI LLMs, including deployment, fine-tuning, and prompt engineering.
Automate for efficiency: If you use key-based authentication, implement an automated key-rotation strategy.
Automate for efficiency: If you use key-based authentication, implement an automated key-rotation strategy.
Recommendations
Performance Efficiency
Performance Efficiency is aboutmaintaining user experience even when there's an increase in loadby managing capacity. The strategy includes scaling resources, identifying and optimizing potential bottlenecks, and optimizing for peak performance.
ThePerformance Efficiency design principlesprovide a high-level design strategy for achieving those capacity goals against the expected usage.
Design checklist
Start your design strategy based on thedesign review checklist for Performance Efficiencyfor defining a baseline based on key performance indicators for Azure OpenAI workloads.
Capacity: Estimate consumers' elasticity demands. Identify high-priority traffic that requires synchronous responses and low-priority traffic that can be asynchronous and batched.
Capacity: Estimate consumers' elasticity demands. Identify high-priority traffic that requires synchronous responses and low-priority traffic that can be asynchronous and batched.
Capacity: Benchmark token consumption requirements based on estimated demands from consumers. Consider using theAzure OpenAI benchmarking toolto help you validate the throughput if you're using provisioned throughput unit (PTU) deployments.
Capacity: Benchmark token consumption requirements based on estimated demands from consumers. Consider using theAzure OpenAI benchmarking toolto help you validate the throughput if you're using provisioned throughput unit (PTU) deployments.
Capacity: Use provisioned throughput for production workloads. Provisioned throughput offers dedicated memory and compute, reserved capacity, and consistent maximum latency for the specified model version. The pay-as-you-go offering can suffer fromnoisy neighborproblems like increased latency and throttling in regions under heavy use. Also, the pay-as-you-go approach doesn't offer guaranteed capacity.
Capacity: Use provisioned throughput for production workloads. Provisioned throughput offers dedicated memory and compute, reserved capacity, and consistent maximum latency for the specified model version. The pay-as-you-go offering can suffer fromnoisy neighborproblems like increased latency and throttling in regions under heavy use. Also, the pay-as-you-go approach doesn't offer guaranteed capacity.
Capacity: Add the appropriate gateways in front of your Azure OpenAI deployments. Ensure that the gateway can route to multiple instances in the same or different regions.
Capacity: Add the appropriate gateways in front of your Azure OpenAI deployments. Ensure that the gateway can route to multiple instances in the same or different regions.
Capacity: Allocate PTUs to cover your predicted usage, and complement these PTUs with a TPM deployment to handle elasticity above that limit. This approach combines base throughput with elastic throughput for efficiency. Like other considerations, this approach requires a custom gateway implementation to route requests to the TPM deployment when the PTU limits are reached.
Capacity: Allocate PTUs to cover your predicted usage, and complement these PTUs with a TPM deployment to handle elasticity above that limit. This approach combines base throughput with elastic throughput for efficiency. Like other considerations, this approach requires a custom gateway implementation to route requests to the TPM deployment when the PTU limits are reached.
Capacity: Send high-priority requests synchronously. Queue low-priority requests and send them through in batches when demand is low.
Capacity: Send high-priority requests synchronously. Queue low-priority requests and send them through in batches when demand is low.
Capacity: Select a model that aligns with your performance requirements, considering the tradeoff between speed and output complexity. Model performance can vary significantly based on the chosen model type. Models designed for speed offer faster response times, which can be beneficial for applications that require quick interactions. Conversely, more sophisticated models might deliver higher-quality outputs at the expense of increased response time.
Capacity: Select a model that aligns with your performance requirements, considering the tradeoff between speed and output complexity. Model performance can vary significantly based on the chosen model type. Models designed for speed offer faster response times, which can be beneficial for applications that require quick interactions. Conversely, more sophisticated models might deliver higher-quality outputs at the expense of increased response time.
Achieve performance: For applications like chatbots or conversational interfaces, consider implementing streaming. Streaming can enhance the perceived performance of Azure OpenAI applications by delivering responses to users in an incremental manner, improving the user experience.
Achieve performance: For applications like chatbots or conversational interfaces, consider implementing streaming. Streaming can enhance the perceived performance of Azure OpenAI applications by delivering responses to users in an incremental manner, improving the user experience.
Achieve performance:Determine when to use fine-tuningbefore you commit to fine-tuning. Although there are good use cases for fine-tuning, such as when the information needed to steer the model is too long or complex to fit into the prompt, make sure that prompt engineering and retrieval-augmented generation (RAG) approaches don't work or are demonstrably more expensive.
Achieve performance:Determine when to use fine-tuningbefore you commit to fine-tuning. Although there are good use cases for fine-tuning, such as when the information needed to steer the model is too long or complex to fit into the prompt, make sure that prompt engineering and retrieval-augmented generation (RAG) approaches don't work or are demonstrably more expensive.
Achieve performance: Consider using dedicated model deployments per consumer group to provide per-model usage isolation that can help prevent noisy neighbors between your consumer groups.
Achieve performance: Consider using dedicated model deployments per consumer group to provide per-model usage isolation that can help prevent noisy neighbors between your consumer groups.
Recommendations
There are no recommended configurations for Performance Efficiency for Azure OpenAI.
Azure Policy
Azure provides an extensive set of built-in policies related to Azure OpenAI and its dependencies. Some of the preceding recommendations can be audited through Azure Policy. Consider the following policy definitions:
Disable key access
Restrict network access
Disable public network access
Use Azure Private Link
Enable data encryption with customer-managed keys
These Azure Policy definitions are alsoAzure Advisorsecurity best-practice recommendations for Azure OpenAI.
Next steps
Consider the following articles as resources that demonstrate the recommendations highlighted in this article.
Use this reference architecture as an example of how you can apply this article's guidance to a workload:Baseline OpenAI end-to-end chat reference architecture.
Build implementation expertise by usingAzure Machine Learningproduct documentation.
Feedback
Was this page helpful?
Additional resources