Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Accessing diagnostic logs for Azure Data Lake Analytics
Article
2023-12-20
1 contributor
In this article
Important
Azure Data Lake Analytics retired on 29 February 2024. Learn morewith this announcement.
For data analytics, your organization can useAzure Synapse AnalyticsorMicrosoft Fabric.
Diagnostic logging allows you to collect data access audit trails. These logs provide information such as:
A list of users that accessed the data.
How frequently the data is accessed.
How much data is stored in the account.
Enable logging
Sign on to theAzure portal.
Sign on to theAzure portal.
Open your Data Lake Analytics account and selectDiagnostic settingsfrom theMonitoringsection. Next, select+ Add diagnostic setting.
Open your Data Lake Analytics account and selectDiagnostic settingsfrom theMonitoringsection. Next, select+ Add diagnostic setting.

FromDiagnostics setting, enter a name for this logging configuration and then select logging options.You can choose to store/process the data in four different ways.SelectArchive to a storage accountto store logs in an Azure storage account. Use this option if you want to archive the data. If you select this option, you must provide an Azure storage account to save the logs to.SelectStream to an event hubto stream log data to an Azure Event Hubs. Use this option if you have a downstream processing pipeline that is analyzing incoming logs in real time. If you select this option, you must provide the details for the Azure Event Hubs you want to use.SelectSend to Log Analytics workspaceto send the data to the Azure Monitor service. Use this option if you want to use Azure Monitor logs to gather and analyze logs.Selectsend to partner solutionif you want to use our partner integration. For more information, you canfollow this link.Specify whether you want to get audit logs or request logs or both.  A request log captures every API request. An audit log records all operations that are triggered by that API request.ForArchive to a storage account, specify the number of days to retain the data.SelectSave.NoteYou must select eitherArchive to a storage account,Stream to an Event Hub,Send to Log Analytics workspace, orSend to partner solutionbefore selecting theSavebutton.
FromDiagnostics setting, enter a name for this logging configuration and then select logging options.

You can choose to store/process the data in four different ways.SelectArchive to a storage accountto store logs in an Azure storage account. Use this option if you want to archive the data. If you select this option, you must provide an Azure storage account to save the logs to.SelectStream to an event hubto stream log data to an Azure Event Hubs. Use this option if you have a downstream processing pipeline that is analyzing incoming logs in real time. If you select this option, you must provide the details for the Azure Event Hubs you want to use.SelectSend to Log Analytics workspaceto send the data to the Azure Monitor service. Use this option if you want to use Azure Monitor logs to gather and analyze logs.Selectsend to partner solutionif you want to use our partner integration. For more information, you canfollow this link.
You can choose to store/process the data in four different ways.
SelectArchive to a storage accountto store logs in an Azure storage account. Use this option if you want to archive the data. If you select this option, you must provide an Azure storage account to save the logs to.
SelectArchive to a storage accountto store logs in an Azure storage account. Use this option if you want to archive the data. If you select this option, you must provide an Azure storage account to save the logs to.
SelectStream to an event hubto stream log data to an Azure Event Hubs. Use this option if you have a downstream processing pipeline that is analyzing incoming logs in real time. If you select this option, you must provide the details for the Azure Event Hubs you want to use.
SelectStream to an event hubto stream log data to an Azure Event Hubs. Use this option if you have a downstream processing pipeline that is analyzing incoming logs in real time. If you select this option, you must provide the details for the Azure Event Hubs you want to use.
SelectSend to Log Analytics workspaceto send the data to the Azure Monitor service. Use this option if you want to use Azure Monitor logs to gather and analyze logs.
SelectSend to Log Analytics workspaceto send the data to the Azure Monitor service. Use this option if you want to use Azure Monitor logs to gather and analyze logs.
Selectsend to partner solutionif you want to use our partner integration. For more information, you canfollow this link.
Selectsend to partner solutionif you want to use our partner integration. For more information, you canfollow this link.
Specify whether you want to get audit logs or request logs or both.  A request log captures every API request. An audit log records all operations that are triggered by that API request.
Specify whether you want to get audit logs or request logs or both.  A request log captures every API request. An audit log records all operations that are triggered by that API request.
ForArchive to a storage account, specify the number of days to retain the data.
ForArchive to a storage account, specify the number of days to retain the data.
SelectSave.NoteYou must select eitherArchive to a storage account,Stream to an Event Hub,Send to Log Analytics workspace, orSend to partner solutionbefore selecting theSavebutton.
SelectSave.
Note
You must select eitherArchive to a storage account,Stream to an Event Hub,Send to Log Analytics workspace, orSend to partner solutionbefore selecting theSavebutton.
Use the Azure Storage account that contains log data
To display the blob containers that hold logging data, open the Azure Storage account used for Data Lake Analytics for logging, and then selectContainers.The containerinsights-logs-auditcontains the audit logs.The containerinsights-logs-requestscontains the request logs.
To display the blob containers that hold logging data, open the Azure Storage account used for Data Lake Analytics for logging, and then selectContainers.
The containerinsights-logs-auditcontains the audit logs.
The containerinsights-logs-requestscontains the request logs.
Within the containers, the logs are stored under the following file structure:resourceId=/
  SUBSCRIPTIONS/
    <<SUBSCRIPTION_ID>>/
      RESOURCEGROUPS/
        <<RESOURCE_GRP_NAME>>/
          PROVIDERS/
            MICROSOFT.DATALAKEANALYTICS/
              ACCOUNTS/
                <DATA_LAKE_ANALYTICS_NAME>>/
                  y=####/
                    m=##/
                      d=##/
                        h=##/
                          m=00/
                            PT1H.jsonNoteThe##entries in the path contain the year, month, day, and hour in which the log was created. Data Lake Analytics creates one file every hour, som=always contains a value of00.As an example, the complete path to an audit log could be:https://adllogs.blob.core.windows.net/insights-logs-audit/resourceId=/SUBSCRIPTIONS/<sub-id>/RESOURCEGROUPS/myresourcegroup/PROVIDERS/MICROSOFT.DATALAKEANALYTICS/ACCOUNTS/mydatalakeanalytics/y=2016/m=07/d=18/h=04/m=00/PT1H.jsonSimilarly, the complete path to a request log could be:https://adllogs.blob.core.windows.net/insights-logs-requests/resourceId=/SUBSCRIPTIONS/<sub-id>/RESOURCEGROUPS/myresourcegroup/PROVIDERS/MICROSOFT.DATALAKEANALYTICS/ACCOUNTS/mydatalakeanalytics/y=2016/m=07/d=18/h=14/m=00/PT1H.json
Within the containers, the logs are stored under the following file structure:
resourceId=/
  SUBSCRIPTIONS/
    <<SUBSCRIPTION_ID>>/
      RESOURCEGROUPS/
        <<RESOURCE_GRP_NAME>>/
          PROVIDERS/
            MICROSOFT.DATALAKEANALYTICS/
              ACCOUNTS/
                <DATA_LAKE_ANALYTICS_NAME>>/
                  y=####/
                    m=##/
                      d=##/
                        h=##/
                          m=00/
                            PT1H.json
resourceId=/
  SUBSCRIPTIONS/
    <<SUBSCRIPTION_ID>>/
      RESOURCEGROUPS/
        <<RESOURCE_GRP_NAME>>/
          PROVIDERS/
            MICROSOFT.DATALAKEANALYTICS/
              ACCOUNTS/
                <DATA_LAKE_ANALYTICS_NAME>>/
                  y=####/
                    m=##/
                      d=##/
                        h=##/
                          m=00/
                            PT1H.json
Note
The##entries in the path contain the year, month, day, and hour in which the log was created. Data Lake Analytics creates one file every hour, som=always contains a value of00.
##
m=
00
As an example, the complete path to an audit log could be:
https://adllogs.blob.core.windows.net/insights-logs-audit/resourceId=/SUBSCRIPTIONS/<sub-id>/RESOURCEGROUPS/myresourcegroup/PROVIDERS/MICROSOFT.DATALAKEANALYTICS/ACCOUNTS/mydatalakeanalytics/y=2016/m=07/d=18/h=04/m=00/PT1H.json
https://adllogs.blob.core.windows.net/insights-logs-audit/resourceId=/SUBSCRIPTIONS/<sub-id>/RESOURCEGROUPS/myresourcegroup/PROVIDERS/MICROSOFT.DATALAKEANALYTICS/ACCOUNTS/mydatalakeanalytics/y=2016/m=07/d=18/h=04/m=00/PT1H.json
Similarly, the complete path to a request log could be:
https://adllogs.blob.core.windows.net/insights-logs-requests/resourceId=/SUBSCRIPTIONS/<sub-id>/RESOURCEGROUPS/myresourcegroup/PROVIDERS/MICROSOFT.DATALAKEANALYTICS/ACCOUNTS/mydatalakeanalytics/y=2016/m=07/d=18/h=14/m=00/PT1H.json
https://adllogs.blob.core.windows.net/insights-logs-requests/resourceId=/SUBSCRIPTIONS/<sub-id>/RESOURCEGROUPS/myresourcegroup/PROVIDERS/MICROSOFT.DATALAKEANALYTICS/ACCOUNTS/mydatalakeanalytics/y=2016/m=07/d=18/h=14/m=00/PT1H.json
Process the log data
Azure Data Lake Analytics provides a sample on how to process and analyze the log data. You can find the sample athttps://github.com/Azure/AzureDataLake/tree/master/Samples/AzureDiagnosticsSample.
Log structure
The audit and request logs are in a structured JSON format.
Request logs
Here's a sample entry in the JSON-formatted request log. Each blob has one root object calledrecordsthat contains an array of log objects.
{
"records":
  [
    . . . .
    ,
    {
         "time": "2016-07-07T21:02:53.456Z",
         "resourceId": "/SUBSCRIPTIONS/<subscription_id>/RESOURCEGROUPS/<resource_group_name>/PROVIDERS/MICROSOFT.DATALAKEANALYTICS/ACCOUNTS/<data_lake_analytics_account_name>",
         "category": "Requests",
         "operationName": "GetAggregatedJobHistory",
         "resultType": "200",
         "callerIpAddress": "::ffff:1.1.1.1",
         "correlationId": "4a11c709-05f5-417c-a98d-6e81b3e29c58",
         "identity": "1808bd5f-62af-45f4-89d8-03c5e81bac30",
         "properties": {
             "HttpMethod":"POST",
             "Path":"/JobAggregatedHistory",
             "RequestContentLength":122,
             "ClientRequestId":"3b7adbd9-3519-4f28-a61c-bd89506163b8",
             "StartTime":"2016-07-07T21:02:52.472Z",
             "EndTime":"2016-07-07T21:02:53.456Z"
             }
    }
    ,
    . . . .
  ]
}
{
"records":
  [
    . . . .
    ,
    {
         "time": "2016-07-07T21:02:53.456Z",
         "resourceId": "/SUBSCRIPTIONS/<subscription_id>/RESOURCEGROUPS/<resource_group_name>/PROVIDERS/MICROSOFT.DATALAKEANALYTICS/ACCOUNTS/<data_lake_analytics_account_name>",
         "category": "Requests",
         "operationName": "GetAggregatedJobHistory",
         "resultType": "200",
         "callerIpAddress": "::ffff:1.1.1.1",
         "correlationId": "4a11c709-05f5-417c-a98d-6e81b3e29c58",
         "identity": "1808bd5f-62af-45f4-89d8-03c5e81bac30",
         "properties": {
             "HttpMethod":"POST",
             "Path":"/JobAggregatedHistory",
             "RequestContentLength":122,
             "ClientRequestId":"3b7adbd9-3519-4f28-a61c-bd89506163b8",
             "StartTime":"2016-07-07T21:02:52.472Z",
             "EndTime":"2016-07-07T21:02:53.456Z"
             }
    }
    ,
    . . . .
  ]
}
Audit logs
Here's a sample entry in the JSON-formatted audit log. Each blob has one root object calledrecordsthat contains an array of log objects.
{
"records":
  [
    {
         "time": "2016-07-28T19:15:16.245Z",
         "resourceId": "/SUBSCRIPTIONS/<subscription_id>/RESOURCEGROUPS/<resource_group_name>/PROVIDERS/MICROSOFT.DATALAKEANALYTICS/ACCOUNTS/<data_lake_ANALYTICS_account_name>",
         "category": "Audit",
         "operationName": "JobSubmitted",
         "identity": "user@somewhere.com",
         "properties": {
             "JobId":"D74B928F-5194-4E6C-971F-C27026C290E6",
             "JobName": "New Job",
             "JobRuntimeName": "default",
             "SubmitTime": "7/28/2016 7:14:57 PM"
             }
    }
  ]
}
{
"records":
  [
    {
         "time": "2016-07-28T19:15:16.245Z",
         "resourceId": "/SUBSCRIPTIONS/<subscription_id>/RESOURCEGROUPS/<resource_group_name>/PROVIDERS/MICROSOFT.DATALAKEANALYTICS/ACCOUNTS/<data_lake_ANALYTICS_account_name>",
         "category": "Audit",
         "operationName": "JobSubmitted",
         "identity": "user@somewhere.com",
         "properties": {
             "JobId":"D74B928F-5194-4E6C-971F-C27026C290E6",
             "JobName": "New Job",
             "JobRuntimeName": "default",
             "SubmitTime": "7/28/2016 7:14:57 PM"
             }
    }
  ]
}
Note
resultTypeandresultSignatureprovide information on the result of an operation, and only contain a value if an operation has completed. For example, they only contain a value whenoperationNamecontains a value ofJobStartedorJobEnded.
Note
SubmitTime,StartTime,EndTime, andParallelismprovide information on an operation. These entries only contain a value if that operation has started or completed. For example,SubmitTimeonly contains a value afteroperationNamehas the valueJobSubmitted.
Next steps
Overview of Azure Data Lake Analytics
Troubleshoot U-SQL jobs
Additional resources