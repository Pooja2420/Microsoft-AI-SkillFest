Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Azure AI Search performance benchmarks
Article
2024-04-22
1 contributor
In this article
Important
These benchmarks apply to search services createdbefore April 3, 2024on deployments that run on older infrastructure. The benchmarks also apply to nonvector workloads only. Updates are pending for services and workloads on the new limits.
Performance benchmarks are useful for estimating potential performance under similar configurations. Actual performance depends on avariety of factors, including the size of your search service and the types of queries you're sending.
To help you estimate the size of search service needed for your workload, we ran several benchmarks to document the performance for different search services and configurations.
To cover a range of different use cases, we ran benchmarks for two main scenarios:
E-commerce search- This benchmark emulates a real e-commerce scenario and is based on the Nordic e-commerce companyCDON.
Document search- This scenario is comprised of keyword search over full text documents fromSemantic Scholar. This emulates a typical document search solution.
While these scenarios reflect different use cases, every scenario is different so we always recommend performance testing your individual workload. We've published aperformance testing solution using JMeterso you can run similar tests against your own service.
Testing methodology
To benchmark Azure AI Search's performance, we ran tests for two different scenarios at different tiers and replica/partition combinations.
To create these benchmarks, the following methodology was used:
The test begins atXqueries per second (QPS) for 180 seconds. This was usually 5 or 10 QPS.
X
QPS then increased byXand ran for another 180 seconds
X
Every 180 seconds, the test increased byXQPS until average latency increased above 1000 ms or less than 99% of queries succeeded.
X
The following graph gives a visual example of what the test's query load looks like:

Each scenario used at least 10,000 unique queries to avoid tests being overly skewed by caching.
Important
These tests only include query workloads. If you expect to have a high volume of indexing operations, be sure to factor that into your estimation and performance testing. Sample code for simulating indexing can be found in thistutorial.
Definitions
Maximum QPS-  the maximum QPS numbers are based on the highest QPS achieved in a test where 99% of queries completed successfully without throttling and average latency stayed under 1000 ms.
Maximum QPS-  the maximum QPS numbers are based on the highest QPS achieved in a test where 99% of queries completed successfully without throttling and average latency stayed under 1000 ms.
Percentage of max QPS- A percentage of the maximum QPS achieved for a particular test. For example, if a given test reached a maximum of 100 QPS, 20% of max QPS would be 20 QPS.
Percentage of max QPS- A percentage of the maximum QPS achieved for a particular test. For example, if a given test reached a maximum of 100 QPS, 20% of max QPS would be 20 QPS.
Latency- The server's latency for a query; these numbers don't includeround trip delay (RTT). Values are in milliseconds (ms).
Latency- The server's latency for a query; these numbers don't includeround trip delay (RTT). Values are in milliseconds (ms).
Testing disclaimer
The code we used to run these benchmarks is available on theazure-search-performance-testingrepository. It's worth noting that we observed slightly lower QPS levels with theJMeter performance testing solutionthan in the benchmarks. The differences can be attributed to differences in the style of the tests. This speaks to the importance of making your performance tests as similar to your production workload as possible.
Important
These benchmarks in no way guarantee a certain level of performance from your service but can give you an idea of the performance you can expect based on your scenario.
If you have any questions or concerns, reach out to us at azuresearch_contact@microsoft.com.
Benchmark 1: E-commerce search

This benchmark was created in partnership with the e-commerce company,CDON, the Nordic region's largest online marketplace with operations in Sweden, Finland, Norway, and Denmark. Through its 1,500 merchants, CDON offers a wide range assortment that includes over 8 million products. In 2020, CDON had over 120 million visitors and 2 million active customers. You can learn more about CDON's use of Azure AI Search inthis article.
To run these tests, we used a snapshot of CDON's production search index and thousands of unique queries from theirwebsite.
Scenario Details
Document Count: 6,000,000
Index Size: 20 GB
Index Schema: a wide index with 250 fields total, 25 searchable fields, and 200 facetable/filterable fields
Query Types: full text search queries including facets, filters, ordering, and scoring profiles
S1 Performance
The following chart shows the highest query load a service could handle for an extended period of time in terms of queries per second (QPS).

Query latency varies based on the load of the service and services under higher stress have a higher average query latency. The following table shows the 25th, 50th, 75th, 90th, 95th, and 99th percentiles of query latency for three different usage levels.
S2 Performance
The following chart shows the highest query load a service could handle for an extended period of time in terms of queries per second (QPS).

Query latency varies based on the load of the service and services under higher stress have a higher average query latency. The following table shows the 25th, 50th, 75th, 90th, 95th, and 99th percentiles of query latency for three different usage levels.
S3 Performance
The following chart shows the highest query load a service could handle for an extended period of time in terms of queries per second (QPS).

In this case, we see that adding a second partition significantly increases the maximum QPS but adding a third partition provides diminishing marginal returns. The smaller improvement is likely because all of the data is already being pulled into the S3's active memory with just two partitions.
Query latency varies based on the load of the service and services under higher stress have a higher average query latency. The following table shows the 25th, 50th, 75th, 90th, 95th, and 99th percentiles of query latency for three different usage levels.
Benchmark 2: Document search
Scenario Details
Document Count: 7.5 million
Index Size: 22 GB
Index Schema: 23 fields; 8 searchable, 10 filterable/facetable
Query Types: keyword searches with facets and hit highlighting
S1 Performance
The following chart shows the highest query load a service could handle for an extended period of time in terms of queries per second (QPS).

Query latency varies based on the load of the service and services under higher stress have a higher average query latency. The following table shows the 25th, 50th, 75th, 90th, 95th, and 99th percentiles of query latency for three different usage levels.
S2 Performance
The following chart shows the highest query load a service could handle for an extended period of time in terms of queries per second (QPS).

Query latency varies based on the load of the service and services under higher stress have a higher average query latency. The following table shows the 25th, 50th, 75th, 90th, 95th, and 99th percentiles of query latency for three different usage levels.
S3 Performance
The following chart shows the highest query load a service could handle for an extended period of time in terms of queries per second (QPS).

Query latency varies based on the load of the service and services under higher stress have a higher average query latency. The following table shows the 25th, 50th, 75th, 90th, 95th, and 99th percentiles of query latency for three different usage levels.
Takeaways
Through these benchmarks, you can get an idea of the performance Azure AI Search offers. You can also see difference between services at different tiers.
Some key take ways from these benchmarks are:
An S2 can typically handle at least four times the query volume as an S1
An S2 typically has lower latency than an S1 at comparable query volumes
As you add replicas, the QPS a service can handle typically scales linearly (for example, if one replica can handle 10 QPS then five replicas can usually handle 50 QPS)
The higher the load on the service, the higher the average latency
You can also see that performance can vary drastically between scenarios. If you're not getting the performance you expect, check out thetips for better performance.
Next steps
Now that you've seen the performance benchmarks, you can learn more about how to analyze Azure AI Search's performance and key factors that influence performance.
Analyze performance
Tips for better performance
Case Study: Use Cognitive Search to Support Complex AI Scenarios
Feedback
Was this page helpful?
Additional resources