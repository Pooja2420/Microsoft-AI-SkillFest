Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Collect text file from virtual machine with Azure Monitor
Article
2025-03-11
5 contributors
In this article
Many applications and services on a virtual machine will log information to text files instead of standard logging services such as Windows Event log or Syslog. Collect custom text logs from virtual machines can be collected using adata collection rule (DCR)with aCustom Text Logsdata source.
Details for the creation of the DCR are provided inCollect data from VM client with Azure Monitor. This article provides additional details for the Custom Text Logs data source type.
Note
To work with the DCR definition directly or to deploy with other methods such as ARM templates, seeData collection rule (DCR) samples in Azure Monitor.
Prerequisites
In addition to the prerequisites listed inCollect data from virtual machine client with Azure Monitor, you need a custom table in a Log Analytics workspace to receive the data. SeeLog Analytics workspace tablefor details about the requirements of this table. Note that Aarch64 is not supported.
Configure custom text file data source
Create the DCR using the process inCollect data from virtual machine client with Azure Monitor. On theCollect and delivertab of the DCR, selectCustom Text Logsfrom theData source typedropdown.

The options available in theCustom Text Logsconfiguration are described in the following table.
TimeStamp
timeFormat
source
RawData
Add destinations
Custom text logs can only be sent to a Log Analytics workspace where it's stored in thecustom tablethat you create. Add a destination of typeAzure Monitor Logsand select a Log Analytics workspace. You can only add a single workspace to a DCR for a custom text log data source. If you need multiple destinations, create multiple DCRs. Be aware though that this will send duplicate data to each which will result in additional cost.

Time formats
The following table describes the time formats that are supported in thetimeFormatsetting of the DCR. If a time with the specified format is included in the log entry, it will be used to identify a new log entry. If no date in the specified format is found, then end of line is used as the delimiter. SeeMultiline log filesfor further description on how this setting is used.
timeFormat
ISO 8601
yyyy-MM-ddTHH:mm:ssk
YYYY-MM-DD HH:MM:SS
M/D/YYYY HH:MM:SS AM/PM
Mon DD, YYYY HH:MM:SS
yyMMdd HH:mm:ss
ddMMyy HH:mm:ss
MMM d HH:mm:ss
dd/MMM/yyyy:HH:mm:ss zzz
Text file requirements and best practices
The file that Azure Monitor collects must meet the following requirements:
The file must be stored on the local drive of the agent machine in the directory that is being monitored.
The file must use ASCII or UTF-8 encoding. Other formats such as UTF-16 aren't supported.
New records should be appended to the end of the file and not overwrite old records. Overwriting will cause data loss.
Following is a sample of a typical custom text file that can be collected by Azure Monitor. While each line does start with a date, this isn't required since end of line will be used to identify each entry if no date is found.
2024-06-21 19:17:34,1423,Error,Sales,Unable to connect to pricing service.
2024-06-21 19:18:23,1420,Information,Sales,Pricing service connection established.
2024-06-21 21:45:13,2011,Warning,Procurement,Module failed and was restarted.
2024-06-21 23:53:31,4100,Information,Data,Nightly backup complete.
2024-06-21 19:17:34,1423,Error,Sales,Unable to connect to pricing service.
2024-06-21 19:18:23,1420,Information,Sales,Pricing service connection established.
2024-06-21 21:45:13,2011,Warning,Procurement,Module failed and was restarted.
2024-06-21 23:53:31,4100,Information,Data,Nightly backup complete.
Adhere to the following recommendations to ensure that you don't experience data loss or performance issues:
Don't target more than 10 directories with log files. Polling too many directories leads to poor performance.
Continuously clean up log files in the monitored directory. Tracking many log files can drive up agent CPU and Memory usage. Wait for at least two days to allow ample time for all logs to be processed.
Don't rename a file that matches the file scan pattern to another name that also matches the file scan pattern. This will cause duplicate data to be ingested.
Don't rename or copy large log files that match the file scan pattern into the monitored directory. If you must, do not exceed 50MB per minute.
Log Analytics workspace table
Each entry in the log file is collected as it's created and sent to the specified table in a Log Analytics workspace. The custom table in the Log Analytics workspace that will receive the data must exist before you create the DCR.
The following table describes the required and optional columns in the workspace table. The table can include other columns, but they won't be populated unless you parse the data with a transformation as described inDelimited log files.
TimeGenerated
TimeGenerated
RawData
Computer
FilePath
1The table doesn't have to include aRawDatacolumn if you use a transformation to parse the data into multiple columns.
RawData
When collected using default settings, the data from the sample log file shown above would appear as follows when retrieved with a log query.

Create custom table
If the destination table doesn't already exist then you must create it before creating the DCR. SeeCreate a custom tablefor different methods to create a table.
For example, you can use the following PowerShell script to create a custom table to receive the data from a custom text log. This example also adds the optional columns.
$tableParams = @'
{
    "properties": {
        "schema": {
               "name": "{TableName}_CL",
               "columns": [
                    {
                        "name": "TimeGenerated",
                        "type": "DateTime"
                    }, 
                    {
                        "name": "Computer",
                        "type": "string"
                    },
                    {
                        "name": "FilePath",
                        "type": "string"
                    },
                    {
                        "name": "RawData",
                        "type": "string"
                    }
              ]
        }
    }
}
'@

Invoke-AzRestMethod -Path "/subscriptions/{subscription}/resourcegroups/{resourcegroup}/providers/microsoft.operationalinsights/workspaces/{workspace}/tables/MyTable_CL?api-version=2021-12-01-preview" -Method PUT -payload $tableParams
$tableParams = @'
{
    "properties": {
        "schema": {
               "name": "{TableName}_CL",
               "columns": [
                    {
                        "name": "TimeGenerated",
                        "type": "DateTime"
                    }, 
                    {
                        "name": "Computer",
                        "type": "string"
                    },
                    {
                        "name": "FilePath",
                        "type": "string"
                    },
                    {
                        "name": "RawData",
                        "type": "string"
                    }
              ]
        }
    }
}
'@

Invoke-AzRestMethod -Path "/subscriptions/{subscription}/resourcegroups/{resourcegroup}/providers/microsoft.operationalinsights/workspaces/{workspace}/tables/MyTable_CL?api-version=2021-12-01-preview" -Method PUT -payload $tableParams
Multiline log files
Some log files may contain entries that span multiple lines. If each log entry starts with a date, then this date can be used as the delimiter to define each log entry. In this case, the extra lines will be joined together in theRawDatacolumn.
RawData
For example, the text file in the previous example might be formatted as follows:
2024-06-21 19:17:34,1423,Error,Sales,
Unable to connect to pricing service.
2024-06-21 19:18:23,1420,Information,Sales,
Pricing service connection established.
2024-06-21 21:45:13,2011,Warning,Procurement,
Module failed and was restarted.
2024-06-21 23:53:31,4100,Information,Data,
Nightly backup complete.
2024-06-21 19:17:34,1423,Error,Sales,
Unable to connect to pricing service.
2024-06-21 19:18:23,1420,Information,Sales,
Pricing service connection established.
2024-06-21 21:45:13,2011,Warning,Procurement,
Module failed and was restarted.
2024-06-21 23:53:31,4100,Information,Data,
Nightly backup complete.
If the time stamp formatYYYY-MM-DD HH:MM:SSis used in the DCR, then the data would be collected in the same way as the previous example. The extra lines would be included in theRawDatacolumn. If another time stamp format were used that doesn't match the date in the log entry, then each entry would be collected as two separate records.
YYYY-MM-DD HH:MM:SS
RawData
Delimited log files
Many text log files have entries with columns delimited by a character such as a comma. Instead of sending the entire entry to theRawDatacolumn, you can parse the data into separate columns so that each can be populated in the destination table. Use a transformation with thesplit functionto perform this parsing.
RawData
The sample text file shown above is comma-delimited, and the fields could be described as:Time,Code,Severity,Module, andMessage. To parse this data into separate columns, add each of the columns to the destination table and add the following transformation to the DCR.
Time
Code
Severity
Module
Message
Important
Prior to adding this transformation to the DCR, you must add these columns to the destination table. You can modify the PowerShell script above to include the additional columns when the table is created. Or use the Azure portal as described inAdd or delete a custom columnto add the columns to an existing table.
Notable details of the transformation query include the following:
The query outputs properties that each match a column name in the target table.
This example renames theTimeproperty in the log file so that this value is used forTimeGenerated. If this was not provided, thenTimeGeneratedwould be populated with the ingestion time.
Time
TimeGenerated
TimeGenerated
Becausesplitreturns dynamic data, you must use functions such astostringandtointto convert the data to the correct scalar type.
split
tostring
toint
source | project d = split(RawData,",") | project TimeGenerated=todatetime(d[0]), Code=toint(d[1]), Severity=tostring(d[2]), Module=tostring(d[3]), Message=tostring(d[4])
source | project d = split(RawData,",") | project TimeGenerated=todatetime(d[0]), Code=toint(d[1]), Severity=tostring(d[2]), Module=tostring(d[3]), Message=tostring(d[4])

Retrieving this data with a log query would return the following results.

Troubleshooting
Go through the following steps if you aren't collecting data from the text log that you're expecting.
Verify that data is being written to the log file being collected.
Verify that the name and location of the log file matches the file pattern you specified.
Verify that the schema of the target table matches the incoming stream or that you have a transformation that will convert the incoming stream to the correct schema.
SeeVerify operationto verify whether the agent is operational and data is being received.
Next steps
Learn more aboutAzure Monitor Agent.
Learn more aboutdata collection rules.
Feedback
Was this page helpful?
Additional resources