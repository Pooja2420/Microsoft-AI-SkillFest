Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Manage node pools for a cluster in Azure Kubernetes Service (AKS)
Article
2024-08-01
8 contributors
In this article
In Azure Kubernetes Service (AKS), nodes of the same configuration are grouped together intonode pools. These node pools contain the underlying VMs that run your applications. When you create an AKS cluster, you define the initial number of nodes and their size (SKU). As application demands change, you may need to change the settings on your node pools. For example, you may need to scale the number of nodes in a node pool or upgrade the Kubernetes version of a node pool.
This article shows you how to manage one or more node pools in an AKS cluster.
Before you begin
ReviewCreate node pools for a cluster in Azure Kubernetes Service (AKS)to learn how to create node pools for your AKS clusters.
You need the Azure CLI version 2.2.0 or later installed and configured. Runaz --versionto find the version. If you need to install or upgrade, seeInstall Azure CLI.
az --version
ReviewStorage options for applications in Azure Kubernetes Serviceto plan your storage configuration.
Limitations
The following limitations apply when you create and manage AKS clusters that support multiple node pools:
SeeQuotas, virtual machine size restrictions, and region availability in Azure Kubernetes Service (AKS).
System poolsmust contain at least one node, and user node pools may contain zero or more nodes.
You can't change the VM size of a node pool after you create it.
When you create multiple node pools at cluster creation time, all Kubernetes versions used by node pools must match the version set for the control plane. You can make updates after provisioning the cluster using per node pool operations.
You can't simultaneously run upgrade and scale operations on a cluster or node pool. If you attempt to run them at the same time, you receive an error. Each operation type must complete on the target resource prior to the next request on that same resource. For more information, see thetroubleshooting guide.
Upgrade a single node pool
Note
The node pool OS image version is tied to the Kubernetes version of the cluster. You only get OS image upgrades, following a cluster upgrade.
In this example, we upgrade themynodepoolnode pool. Since there are two node pools, we must use theaz aks nodepool upgradecommand to upgrade.
az aks nodepool upgrade
Check for any available upgrades using theaz aks get-upgradescommand.az aks get-upgrades --resource-group myResourceGroup --name myAKSCluster
Check for any available upgrades using theaz aks get-upgradescommand.
az aks get-upgrades
az aks get-upgrades --resource-group myResourceGroup --name myAKSCluster
az aks get-upgrades --resource-group myResourceGroup --name myAKSCluster
Upgrade themynodepoolnode pool using theaz aks nodepool upgradecommand.az aks nodepool upgrade \
    --resource-group myResourceGroup \
    --cluster-name myAKSCluster \
    --name mynodepool \
    --kubernetes-version KUBERNETES_VERSION \
    --no-wait
Upgrade themynodepoolnode pool using theaz aks nodepool upgradecommand.
az aks nodepool upgrade
az aks nodepool upgrade \
    --resource-group myResourceGroup \
    --cluster-name myAKSCluster \
    --name mynodepool \
    --kubernetes-version KUBERNETES_VERSION \
    --no-wait
az aks nodepool upgrade \
    --resource-group myResourceGroup \
    --cluster-name myAKSCluster \
    --name mynodepool \
    --kubernetes-version KUBERNETES_VERSION \
    --no-wait
List the status of your node pools using theaz aks nodepool listcommand.az aks nodepool list --resource-group myResourceGroup --cluster-name myAKSClusterThe following example output showsmynodepoolis in theUpgradingstate:[
  {
    ...
    "count": 3,
    ...
    "name": "mynodepool",
    "orchestratorVersion": "KUBERNETES_VERSION",
    ...
    "provisioningState": "Upgrading",
    ...
    "vmSize": "Standard_DS2_v2",
    ...
  },
  {
    ...
    "count": 2,
    ...
    "name": "nodepool1",
    "orchestratorVersion": "1.15.7",
    ...
    "provisioningState": "Succeeded",
    ...
    "vmSize": "Standard_DS2_v2",
    ...
  }
]It takes a few minutes to upgrade the nodes to the specified version.
List the status of your node pools using theaz aks nodepool listcommand.
az aks nodepool list
az aks nodepool list --resource-group myResourceGroup --cluster-name myAKSCluster
az aks nodepool list --resource-group myResourceGroup --cluster-name myAKSCluster
The following example output showsmynodepoolis in theUpgradingstate:
[
  {
    ...
    "count": 3,
    ...
    "name": "mynodepool",
    "orchestratorVersion": "KUBERNETES_VERSION",
    ...
    "provisioningState": "Upgrading",
    ...
    "vmSize": "Standard_DS2_v2",
    ...
  },
  {
    ...
    "count": 2,
    ...
    "name": "nodepool1",
    "orchestratorVersion": "1.15.7",
    ...
    "provisioningState": "Succeeded",
    ...
    "vmSize": "Standard_DS2_v2",
    ...
  }
]
[
  {
    ...
    "count": 3,
    ...
    "name": "mynodepool",
    "orchestratorVersion": "KUBERNETES_VERSION",
    ...
    "provisioningState": "Upgrading",
    ...
    "vmSize": "Standard_DS2_v2",
    ...
  },
  {
    ...
    "count": 2,
    ...
    "name": "nodepool1",
    "orchestratorVersion": "1.15.7",
    ...
    "provisioningState": "Succeeded",
    ...
    "vmSize": "Standard_DS2_v2",
    ...
  }
]
It takes a few minutes to upgrade the nodes to the specified version.
As a best practice, you should upgrade all node pools in an AKS cluster to the same Kubernetes version. The default behavior ofaz aks upgradeis to upgrade all node pools together with the control plane to achieve this alignment. The ability to upgrade individual node pools lets you perform a rolling upgrade and schedule pods between node pools to maintain application uptime within the above constraints mentioned.
az aks upgrade
Upgrade a cluster control plane with multiple node pools
Note
Kubernetes uses the standardSemantic Versioningversioning scheme. The version number is expressed asx.y.z, wherexis the major version,yis the minor version, andzis the patch version. For example, in version1.12.6, 1 is the major version, 12 is the minor version, and 6 is the patch version. The Kubernetes version of the control plane and the initial node pool are set during cluster creation. Other node pools have their Kubernetes version set when they are added to the cluster. The Kubernetes versions may differ between node pools and between a node pool and the control plane.
An AKS cluster has two cluster resource objects with Kubernetes versions associated to them:
The cluster control plane Kubernetes version, and
A node pool with a Kubernetes version.
The control plane maps to one or many node pools. The behavior of an upgrade operation depends on which Azure CLI command you use.
az aks upgradeupgrades the control plane and all node pools in the cluster to the same Kubernetes version.
az aks upgrade
az aks upgradewith the--control-plane-onlyflag upgrades only the cluster control plane and leaves all node pools unchanged.
az aks upgrade
--control-plane-only
az aks nodepool upgradeupgrades only the target node pool with the specified Kubernetes version.
az aks nodepool upgrade
Validation rules for upgrades
Kubernetes upgrades for a cluster control plane and node pools are validated using the following sets of rules:
Rules for valid versions to upgrade node pools:The node pool version must have the samemajorversion as the control plane.The node poolminorversion must be within twominorversions of the control plane version.The node pool version can't be greater than the controlmajor.minor.patchversion.
Rules for valid versions to upgrade node pools:
The node pool version must have the samemajorversion as the control plane.
The node poolminorversion must be within twominorversions of the control plane version.
The node pool version can't be greater than the controlmajor.minor.patchversion.
major.minor.patch
Rules for submitting an upgrade operation:You can't downgrade the control plane or a node pool Kubernetes version.If a node pool Kubernetes version isn't specified, the behavior depends on the client. In Resource Manager templates, declaration falls back to the existing version defined for the node pool. If nothing is set, it uses the control plane version to fall back on.You can't simultaneously submit multiple operations on a single control plane or node pool resource. You can either upgrade or scale a control plane or a node pool at a given time.
Rules for submitting an upgrade operation:
You can't downgrade the control plane or a node pool Kubernetes version.
If a node pool Kubernetes version isn't specified, the behavior depends on the client. In Resource Manager templates, declaration falls back to the existing version defined for the node pool. If nothing is set, it uses the control plane version to fall back on.
You can't simultaneously submit multiple operations on a single control plane or node pool resource. You can either upgrade or scale a control plane or a node pool at a given time.
Scale a node pool manually
As your application workload demands change, you may need to scale the number of nodes in a node pool. The number of nodes can be scaled up or down.
Scale the number of nodes in a node pool using theaz aks node pool scalecommand.az aks nodepool scale \
    --resource-group myResourceGroup \
    --cluster-name myAKSCluster \
    --name mynodepool \
    --node-count 5 \
    --no-wait
Scale the number of nodes in a node pool using theaz aks node pool scalecommand.
az aks node pool scale
az aks nodepool scale \
    --resource-group myResourceGroup \
    --cluster-name myAKSCluster \
    --name mynodepool \
    --node-count 5 \
    --no-wait
az aks nodepool scale \
    --resource-group myResourceGroup \
    --cluster-name myAKSCluster \
    --name mynodepool \
    --node-count 5 \
    --no-wait
List the status of your node pools using theaz aks node pool listcommand.az aks nodepool list --resource-group myResourceGroup --cluster-name myAKSClusterThe following example output showsmynodepoolis in theScalingstate with a new count of five nodes:[
  {
    ...
    "count": 5,
    ...
    "name": "mynodepool",
    "orchestratorVersion": "1.15.7",
    ...
    "provisioningState": "Scaling",
    ...
    "vmSize": "Standard_DS2_v2",
    ...
  },
  {
    ...
    "count": 2,
    ...
    "name": "nodepool1",
    "orchestratorVersion": "1.15.7",
    ...
    "provisioningState": "Succeeded",
    ...
    "vmSize": "Standard_DS2_v2",
    ...
  }
]It takes a few minutes for the scale operation to complete.
List the status of your node pools using theaz aks node pool listcommand.
az aks node pool list
az aks nodepool list --resource-group myResourceGroup --cluster-name myAKSCluster
az aks nodepool list --resource-group myResourceGroup --cluster-name myAKSCluster
The following example output showsmynodepoolis in theScalingstate with a new count of five nodes:
[
  {
    ...
    "count": 5,
    ...
    "name": "mynodepool",
    "orchestratorVersion": "1.15.7",
    ...
    "provisioningState": "Scaling",
    ...
    "vmSize": "Standard_DS2_v2",
    ...
  },
  {
    ...
    "count": 2,
    ...
    "name": "nodepool1",
    "orchestratorVersion": "1.15.7",
    ...
    "provisioningState": "Succeeded",
    ...
    "vmSize": "Standard_DS2_v2",
    ...
  }
]
[
  {
    ...
    "count": 5,
    ...
    "name": "mynodepool",
    "orchestratorVersion": "1.15.7",
    ...
    "provisioningState": "Scaling",
    ...
    "vmSize": "Standard_DS2_v2",
    ...
  },
  {
    ...
    "count": 2,
    ...
    "name": "nodepool1",
    "orchestratorVersion": "1.15.7",
    ...
    "provisioningState": "Succeeded",
    ...
    "vmSize": "Standard_DS2_v2",
    ...
  }
]
It takes a few minutes for the scale operation to complete.
Scale a specific node pool automatically using the cluster autoscaler
AKS offers a separate feature to automatically scale node pools with a feature called thecluster autoscaler. You can enable this feature with unique minimum and maximum scale counts per node pool.
For more information, seeuse the cluster autoscaler.
Remove specific VMs in an existing node pool
For more information, seeRemove specific VMs in an existing node pool.
Associate capacity reservation groups to node pools
As your workload demands change, you can associate existing capacity reservation groups to node pools to guarantee allocated capacity for your node pools.
Prerequisites to use capacity reservation groups with AKS
Use CLI version 2.56 or above and API version 2023-10-01 or higher.
Use CLI version 2.56 or above and API version 2023-10-01 or higher.
The capacity reservation group should already exist and should contain minimum one capacity reservation, otherwise the node pool is added to the cluster with a warning and no capacity reservation group gets associated. For more information, seecapacity reservation groups.
The capacity reservation group should already exist and should contain minimum one capacity reservation, otherwise the node pool is added to the cluster with a warning and no capacity reservation group gets associated. For more information, seecapacity reservation groups.
You need to create a user-assigned managed identity for the resource group that contains the capacity reservation group (CRG). System-assigned managed identities won't work for this feature. In the following example, replace the environment variables with your own values.IDENTITY_NAME=myID
RG_NAME=myResourceGroup
CLUSTER_NAME=myAKSCluster
VM_SKU=Standard_D4s_v3
NODE_COUNT=2
LOCATION=westus2
az identity create --name $IDENTITY_NAME --resource-group $RG_NAME  
IDENTITY_ID=$(az identity show --name $IDENTITY_NAME --resource-group $RG_NAME --query identity.id -o tsv)
You need to create a user-assigned managed identity for the resource group that contains the capacity reservation group (CRG). System-assigned managed identities won't work for this feature. In the following example, replace the environment variables with your own values.
IDENTITY_NAME=myID
RG_NAME=myResourceGroup
CLUSTER_NAME=myAKSCluster
VM_SKU=Standard_D4s_v3
NODE_COUNT=2
LOCATION=westus2
az identity create --name $IDENTITY_NAME --resource-group $RG_NAME  
IDENTITY_ID=$(az identity show --name $IDENTITY_NAME --resource-group $RG_NAME --query identity.id -o tsv)
IDENTITY_NAME=myID
RG_NAME=myResourceGroup
CLUSTER_NAME=myAKSCluster
VM_SKU=Standard_D4s_v3
NODE_COUNT=2
LOCATION=westus2
az identity create --name $IDENTITY_NAME --resource-group $RG_NAME  
IDENTITY_ID=$(az identity show --name $IDENTITY_NAME --resource-group $RG_NAME --query identity.id -o tsv)
You need to assign theContributorrole to the user-assigned identity created above. For more details, seeSteps to assign an Azure role.
You need to assign theContributorrole to the user-assigned identity created above. For more details, seeSteps to assign an Azure role.
Contributor
Create a new cluster and assign the newly created identity.az aks create \
      --resource-group $RG_NAME \
      --name $CLUSTER_NAME \
      --location $LOCATION \
      --node-vm-size $VM_SKU --node-count $NODE_COUNT \
      --assign-identity $IDENTITY_ID \
      --generate-ssh-keys
Create a new cluster and assign the newly created identity.
az aks create \
      --resource-group $RG_NAME \
      --name $CLUSTER_NAME \
      --location $LOCATION \
      --node-vm-size $VM_SKU --node-count $NODE_COUNT \
      --assign-identity $IDENTITY_ID \
      --generate-ssh-keys
az aks create \
      --resource-group $RG_NAME \
      --name $CLUSTER_NAME \
      --location $LOCATION \
      --node-vm-size $VM_SKU --node-count $NODE_COUNT \
      --assign-identity $IDENTITY_ID \
      --generate-ssh-keys
You can also assign the user-managed identity on an existing managed cluster with update command.az aks update \
    --resource-group $RG_NAME \
    --name $CLUSTER_NAME \
    --location $LOCATION \
    --node-vm-size $VM_SKU \
    --node-count $NODE_COUNT \
    --enable-managed-identity \
    --assign-identity $IDENTITY_ID
You can also assign the user-managed identity on an existing managed cluster with update command.
az aks update \
    --resource-group $RG_NAME \
    --name $CLUSTER_NAME \
    --location $LOCATION \
    --node-vm-size $VM_SKU \
    --node-count $NODE_COUNT \
    --enable-managed-identity \
    --assign-identity $IDENTITY_ID
az aks update \
    --resource-group $RG_NAME \
    --name $CLUSTER_NAME \
    --location $LOCATION \
    --node-vm-size $VM_SKU \
    --node-count $NODE_COUNT \
    --enable-managed-identity \
    --assign-identity $IDENTITY_ID
Associate an existing capacity reservation group with a node pool
Associate an existing capacity reservation group with a node pool using theaz aks nodepool addcommand and specify a capacity reservation group with the--crg-idflag. The following example assumes you have a CRG named "myCRG".
az aks nodepool add
--crg-id
RG_NAME=myResourceGroup
CLUSTER_NAME=myAKSCluster
NODEPOOL_NAME=myNodepool
CRG_NAME=myCRG
CRG_ID=$(az capacity reservation group show --capacity-reservation-group $CRG_NAME --resource-group $RG_NAME --query id -o tsv)
az aks nodepool add --resource-group $RG_NAME --cluster-name $CLUSTER_NAME --name $NODEPOOL_NAME --crg-id $CRG_ID
RG_NAME=myResourceGroup
CLUSTER_NAME=myAKSCluster
NODEPOOL_NAME=myNodepool
CRG_NAME=myCRG
CRG_ID=$(az capacity reservation group show --capacity-reservation-group $CRG_NAME --resource-group $RG_NAME --query id -o tsv)
az aks nodepool add --resource-group $RG_NAME --cluster-name $CLUSTER_NAME --name $NODEPOOL_NAME --crg-id $CRG_ID
Associate an existing capacity reservation group with a system node pool
To associate an existing capacity reservation group with a system node pool, associate the cluster with the user-assigned identity with the Contributor role on your CRG and the CRG itself during cluster creation. Use theaz aks createcommand with the--assign-identityand--crg-idflags.
az aks create
--assign-identity
--crg-id
IDENTITY_NAME=myID
RG_NAME=myResourceGroup
CLUSTER_NAME=myAKSCluster
NODEPOOL_NAME=myNodepool
CRG_NAME=myCRG
CRG_ID=$(az capacity reservation group show --capacity-reservation-group $CRG_NAME --resource-group $RG_NAME --query id -o tsv)
IDENTITY_ID=$(az identity show --name $IDENTITY_NAME --resource-group $RG_NAME --query identity.id -o tsv)

az aks create \
    --resource-group $RG_NAME \
    --cluster-name $CLUSTER_NAME \
    --crg-id $CRG_ID \
    --assign-identity $IDENTITY_ID \
    --generate-ssh-keys
IDENTITY_NAME=myID
RG_NAME=myResourceGroup
CLUSTER_NAME=myAKSCluster
NODEPOOL_NAME=myNodepool
CRG_NAME=myCRG
CRG_ID=$(az capacity reservation group show --capacity-reservation-group $CRG_NAME --resource-group $RG_NAME --query id -o tsv)
IDENTITY_ID=$(az identity show --name $IDENTITY_NAME --resource-group $RG_NAME --query identity.id -o tsv)

az aks create \
    --resource-group $RG_NAME \
    --cluster-name $CLUSTER_NAME \
    --crg-id $CRG_ID \
    --assign-identity $IDENTITY_ID \
    --generate-ssh-keys
Note
Deleting a node pool implicitly dissociates that node pool from any associated capacity reservation group before the node pool is deleted. Deleting a cluster implicitly dissociates all node pools in that cluster from their associated capacity reservation groups.
Note
You cannot update an existing node pool with a capacity reservation group. The recommended approach is to associate a capacity reservation group during the node pool creation.
Specify a VM size for a node pool
You may need to create node pools with different VM sizes and capabilities. For example, you may create a node pool that contains nodes with large amounts of CPU or memory or a node pool that provides GPU support. In the next section, youuse taints and tolerationsto tell the Kubernetes scheduler how to limit access to pods that can run on these nodes.
In the following example, we create a GPU-based node pool that uses theStandard_NC6s_v3VM size. These VMs are powered by the NVIDIA Tesla K80 card. For information, seeAvailable sizes for Linux virtual machines in Azure.
Create a node pool using theaz aks nodepool addcommand. Specify the namegpunodepooland use the--node-vm-sizeparameter to specify theStandard_NC6size.az aks nodepool add \
    --resource-group myResourceGroup \
    --cluster-name myAKSCluster \
    --name gpunodepool \
    --node-count 1 \
    --node-vm-size Standard_NC6s_v3 \
    --no-wait
Create a node pool using theaz aks nodepool addcommand. Specify the namegpunodepooland use the--node-vm-sizeparameter to specify theStandard_NC6size.
az aks nodepool add
--node-vm-size
az aks nodepool add \
    --resource-group myResourceGroup \
    --cluster-name myAKSCluster \
    --name gpunodepool \
    --node-count 1 \
    --node-vm-size Standard_NC6s_v3 \
    --no-wait
az aks nodepool add \
    --resource-group myResourceGroup \
    --cluster-name myAKSCluster \
    --name gpunodepool \
    --node-count 1 \
    --node-vm-size Standard_NC6s_v3 \
    --no-wait
Check the status of the node pool using theaz aks nodepool listcommand.az aks nodepool list --resource-group myResourceGroup --cluster-name myAKSClusterThe following example output shows thegpunodepoolnode pool isCreatingnodes with the specifiedVmSize:[
  {
    ...
    "count": 1,
    ...
    "name": "gpunodepool",
    "orchestratorVersion": "1.15.7",
    ...
    "provisioningState": "Creating",
    ...
    "vmSize": "Standard_NC6s_v3",
    ...
  },
  {
    ...
    "count": 2,
    ...
    "name": "nodepool1",
    "orchestratorVersion": "1.15.7",
    ...
    "provisioningState": "Succeeded",
    ...
    "vmSize": "Standard_DS2_v2",
    ...
  }
]It takes a few minutes for thegpunodepoolto be successfully created.
Check the status of the node pool using theaz aks nodepool listcommand.
az aks nodepool list
az aks nodepool list --resource-group myResourceGroup --cluster-name myAKSCluster
az aks nodepool list --resource-group myResourceGroup --cluster-name myAKSCluster
The following example output shows thegpunodepoolnode pool isCreatingnodes with the specifiedVmSize:
[
  {
    ...
    "count": 1,
    ...
    "name": "gpunodepool",
    "orchestratorVersion": "1.15.7",
    ...
    "provisioningState": "Creating",
    ...
    "vmSize": "Standard_NC6s_v3",
    ...
  },
  {
    ...
    "count": 2,
    ...
    "name": "nodepool1",
    "orchestratorVersion": "1.15.7",
    ...
    "provisioningState": "Succeeded",
    ...
    "vmSize": "Standard_DS2_v2",
    ...
  }
]
[
  {
    ...
    "count": 1,
    ...
    "name": "gpunodepool",
    "orchestratorVersion": "1.15.7",
    ...
    "provisioningState": "Creating",
    ...
    "vmSize": "Standard_NC6s_v3",
    ...
  },
  {
    ...
    "count": 2,
    ...
    "name": "nodepool1",
    "orchestratorVersion": "1.15.7",
    ...
    "provisioningState": "Succeeded",
    ...
    "vmSize": "Standard_DS2_v2",
    ...
  }
]
It takes a few minutes for thegpunodepoolto be successfully created.
Specify a taint, label, or tag for a node pool
When creating a node pool, you can add taints, labels, or tags to it. When you add a taint, label, or tag, all nodes within that node pool also get that taint, label, or tag.
Important
Adding taints, labels, or tags to nodes should be done for the entire node pool usingaz aks nodepool. We don't recommend usingkubectlto apply taints, labels, or tags to individual nodes in a node pool.
az aks nodepool
kubectl
Set node pool taints
AKS supports two kinds of node taints: node taints and node initialization taints (preview). For more information, seeUse node taints in an Azure Kubernetes Service (AKS) cluster.
For more information on how to use advanced Kubernetes scheduled features, seeBest practices for advanced scheduler features in AKS
Set node pool tolerations
In the previous step, you applied thesku=gpu:NoScheduletaint when creating your node pool. The following example YAML manifest uses a toleration to allow the Kubernetes scheduler to run an NGINX pod on a node in that node pool.
Create a file namednginx-toleration.yamland copy in the following example YAML.apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
 - image: mcr.microsoft.com/oss/nginx/nginx:1.15.9-alpine
    name: mypod
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 1
        memory: 2G
  tolerations:
 - key: "sku"
    operator: "Equal"
    value: "gpu"
    effect: "NoSchedule"
Create a file namednginx-toleration.yamland copy in the following example YAML.
nginx-toleration.yaml
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
 - image: mcr.microsoft.com/oss/nginx/nginx:1.15.9-alpine
    name: mypod
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 1
        memory: 2G
  tolerations:
 - key: "sku"
    operator: "Equal"
    value: "gpu"
    effect: "NoSchedule"
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
 - image: mcr.microsoft.com/oss/nginx/nginx:1.15.9-alpine
    name: mypod
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 1
        memory: 2G
  tolerations:
 - key: "sku"
    operator: "Equal"
    value: "gpu"
    effect: "NoSchedule"
Schedule the pod using thekubectl applycommand.kubectl apply -f nginx-toleration.yamlIt takes a few seconds to schedule the pod and pull the NGINX image.
Schedule the pod using thekubectl applycommand.
kubectl apply
kubectl apply -f nginx-toleration.yaml
kubectl apply -f nginx-toleration.yaml
It takes a few seconds to schedule the pod and pull the NGINX image.
Check the status using thekubectl describe podcommand.kubectl describe pod mypodThe following condensed example output shows thesku=gpu:NoScheduletoleration is applied. In the events section, the scheduler assigned the pod to theaks-taintnp-28993262-vmss000000node:[...]
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
                 sku=gpu:NoSchedule
Events:
  Type    Reason     Age    From                Message
  ----    ------     ----   ----                -------
  Normal  Scheduled  4m48s  default-scheduler   Successfully assigned default/mypod to aks-taintnp-28993262-vmss000000
  Normal  Pulling    4m47s  kubelet             pulling image "mcr.microsoft.com/oss/nginx/nginx:1.15.9-alpine"
  Normal  Pulled     4m43s  kubelet             Successfully pulled image "mcr.microsoft.com/oss/nginx/nginx:1.15.9-alpine"
  Normal  Created    4m40s  kubelet             Created container
  Normal  Started    4m40s  kubelet             Started containerOnly pods that have this toleration applied can be scheduled on nodes intaintnp. Any other pods are scheduled in thenodepool1node pool. If you create more node pools, you can use taints and tolerations to limit what pods can be scheduled on those node resources.
Check the status using thekubectl describe podcommand.
kubectl describe pod
kubectl describe pod mypod
kubectl describe pod mypod
The following condensed example output shows thesku=gpu:NoScheduletoleration is applied. In the events section, the scheduler assigned the pod to theaks-taintnp-28993262-vmss000000node:
[...]
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
                 sku=gpu:NoSchedule
Events:
  Type    Reason     Age    From                Message
  ----    ------     ----   ----                -------
  Normal  Scheduled  4m48s  default-scheduler   Successfully assigned default/mypod to aks-taintnp-28993262-vmss000000
  Normal  Pulling    4m47s  kubelet             pulling image "mcr.microsoft.com/oss/nginx/nginx:1.15.9-alpine"
  Normal  Pulled     4m43s  kubelet             Successfully pulled image "mcr.microsoft.com/oss/nginx/nginx:1.15.9-alpine"
  Normal  Created    4m40s  kubelet             Created container
  Normal  Started    4m40s  kubelet             Started container
[...]
Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
                 node.kubernetes.io/unreachable:NoExecute for 300s
                 sku=gpu:NoSchedule
Events:
  Type    Reason     Age    From                Message
  ----    ------     ----   ----                -------
  Normal  Scheduled  4m48s  default-scheduler   Successfully assigned default/mypod to aks-taintnp-28993262-vmss000000
  Normal  Pulling    4m47s  kubelet             pulling image "mcr.microsoft.com/oss/nginx/nginx:1.15.9-alpine"
  Normal  Pulled     4m43s  kubelet             Successfully pulled image "mcr.microsoft.com/oss/nginx/nginx:1.15.9-alpine"
  Normal  Created    4m40s  kubelet             Created container
  Normal  Started    4m40s  kubelet             Started container
Only pods that have this toleration applied can be scheduled on nodes intaintnp. Any other pods are scheduled in thenodepool1node pool. If you create more node pools, you can use taints and tolerations to limit what pods can be scheduled on those node resources.
Setting node pool labels
For more information, seeUse labels in an Azure Kubernetes Service (AKS) cluster.
Setting node pool Azure tags
For more information, seeUse Azure tags in Azure Kubernetes Service (AKS).
Manage node pools using a Resource Manager template
When you use an Azure Resource Manager template to create and manage resources, you can change settings in your template and redeploy it to update resources. With AKS node pools, you can't update the initial node pool profile once the AKS cluster has been created. This behavior means you can't update an existing Resource Manager template, make a change to the node pools, and then redeploy the template. Instead, you must create a separate Resource Manager template that updates the node pools for the existing AKS cluster.
Create a template, such asaks-agentpools.json, and paste in the following example manifest. Make sure to edit the values as needed. This example template configures the following settings:Updates theLinuxnode pool namedmyagentpoolto run three nodes.Sets the nodes in the node pool to run Kubernetes version1.15.7.Defines the node size asStandard_DS2_v2.{
    "$schema": "https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
    "contentVersion": "1.0.0.0",
    "parameters": {
        "clusterName": {
            "type": "string",
            "metadata": {
                "description": "The name of your existing AKS cluster."
            }
        },
        "location": {
            "type": "string",
            "metadata": {
                "description": "The location of your existing AKS cluster."
            }
        },
        "agentPoolName": {
            "type": "string",
            "defaultValue": "myagentpool",
            "metadata": {
                "description": "The name of the agent pool to create or update."
            }
        },
        "vnetSubnetId": {
            "type": "string",
            "defaultValue": "",
            "metadata": {
                "description": "The Vnet subnet resource ID for your existing AKS cluster."
            }
        }
    },
    "variables": {
        "apiVersion": {
            "aks": "2020-01-01"
        },
        "agentPoolProfiles": {
            "maxPods": 30,
            "osDiskSizeGB": 0,
            "agentCount": 3,
            "agentVmSize": "Standard_DS2_v2",
            "osType": "Linux",
            "vnetSubnetId": "[parameters('vnetSubnetId')]"
        }
    },
    "resources": [
        {
            "apiVersion": "2020-01-01",
            "type": "Microsoft.ContainerService/managedClusters/agentPools",
            "name": "[concat(parameters('clusterName'),'/', parameters('agentPoolName'))]",
            "location": "[parameters('location')]",
            "properties": {
                "maxPods": "[variables('agentPoolProfiles').maxPods]",
                "osDiskSizeGB": "[variables('agentPoolProfiles').osDiskSizeGB]",
                "count": "[variables('agentPoolProfiles').agentCount]",
                "vmSize": "[variables('agentPoolProfiles').agentVmSize]",
                "osType": "[variables('agentPoolProfiles').osType]",
                "type": "VirtualMachineScaleSets",
                "vnetSubnetID": "[variables('agentPoolProfiles').vnetSubnetId]",
                "orchestratorVersion": "1.15.7"
            }
        }
    ]
}
Create a template, such asaks-agentpools.json, and paste in the following example manifest. Make sure to edit the values as needed. This example template configures the following settings:
aks-agentpools.json
Updates theLinuxnode pool namedmyagentpoolto run three nodes.
Sets the nodes in the node pool to run Kubernetes version1.15.7.
Defines the node size asStandard_DS2_v2.
{
    "$schema": "https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
    "contentVersion": "1.0.0.0",
    "parameters": {
        "clusterName": {
            "type": "string",
            "metadata": {
                "description": "The name of your existing AKS cluster."
            }
        },
        "location": {
            "type": "string",
            "metadata": {
                "description": "The location of your existing AKS cluster."
            }
        },
        "agentPoolName": {
            "type": "string",
            "defaultValue": "myagentpool",
            "metadata": {
                "description": "The name of the agent pool to create or update."
            }
        },
        "vnetSubnetId": {
            "type": "string",
            "defaultValue": "",
            "metadata": {
                "description": "The Vnet subnet resource ID for your existing AKS cluster."
            }
        }
    },
    "variables": {
        "apiVersion": {
            "aks": "2020-01-01"
        },
        "agentPoolProfiles": {
            "maxPods": 30,
            "osDiskSizeGB": 0,
            "agentCount": 3,
            "agentVmSize": "Standard_DS2_v2",
            "osType": "Linux",
            "vnetSubnetId": "[parameters('vnetSubnetId')]"
        }
    },
    "resources": [
        {
            "apiVersion": "2020-01-01",
            "type": "Microsoft.ContainerService/managedClusters/agentPools",
            "name": "[concat(parameters('clusterName'),'/', parameters('agentPoolName'))]",
            "location": "[parameters('location')]",
            "properties": {
                "maxPods": "[variables('agentPoolProfiles').maxPods]",
                "osDiskSizeGB": "[variables('agentPoolProfiles').osDiskSizeGB]",
                "count": "[variables('agentPoolProfiles').agentCount]",
                "vmSize": "[variables('agentPoolProfiles').agentVmSize]",
                "osType": "[variables('agentPoolProfiles').osType]",
                "type": "VirtualMachineScaleSets",
                "vnetSubnetID": "[variables('agentPoolProfiles').vnetSubnetId]",
                "orchestratorVersion": "1.15.7"
            }
        }
    ]
}
{
    "$schema": "https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
    "contentVersion": "1.0.0.0",
    "parameters": {
        "clusterName": {
            "type": "string",
            "metadata": {
                "description": "The name of your existing AKS cluster."
            }
        },
        "location": {
            "type": "string",
            "metadata": {
                "description": "The location of your existing AKS cluster."
            }
        },
        "agentPoolName": {
            "type": "string",
            "defaultValue": "myagentpool",
            "metadata": {
                "description": "The name of the agent pool to create or update."
            }
        },
        "vnetSubnetId": {
            "type": "string",
            "defaultValue": "",
            "metadata": {
                "description": "The Vnet subnet resource ID for your existing AKS cluster."
            }
        }
    },
    "variables": {
        "apiVersion": {
            "aks": "2020-01-01"
        },
        "agentPoolProfiles": {
            "maxPods": 30,
            "osDiskSizeGB": 0,
            "agentCount": 3,
            "agentVmSize": "Standard_DS2_v2",
            "osType": "Linux",
            "vnetSubnetId": "[parameters('vnetSubnetId')]"
        }
    },
    "resources": [
        {
            "apiVersion": "2020-01-01",
            "type": "Microsoft.ContainerService/managedClusters/agentPools",
            "name": "[concat(parameters('clusterName'),'/', parameters('agentPoolName'))]",
            "location": "[parameters('location')]",
            "properties": {
                "maxPods": "[variables('agentPoolProfiles').maxPods]",
                "osDiskSizeGB": "[variables('agentPoolProfiles').osDiskSizeGB]",
                "count": "[variables('agentPoolProfiles').agentCount]",
                "vmSize": "[variables('agentPoolProfiles').agentVmSize]",
                "osType": "[variables('agentPoolProfiles').osType]",
                "type": "VirtualMachineScaleSets",
                "vnetSubnetID": "[variables('agentPoolProfiles').vnetSubnetId]",
                "orchestratorVersion": "1.15.7"
            }
        }
    ]
}
Deploy the template using theaz deployment group createcommand.az deployment group create \
    --resource-group myResourceGroup \
    --template-file aks-agentpools.jsonTipYou can add a tag to your node pool by adding thetagproperty in the template, as shown in the following example:...
"resources": [
{
  ...
  "properties": {
    ...
    "tags": {
      "name1": "val1"
    },
    ...
  }
}
...It may take a few minutes to update your AKS cluster depending on the node pool settings and operations you define in your Resource Manager template.
Deploy the template using theaz deployment group createcommand.
az deployment group create
az deployment group create \
    --resource-group myResourceGroup \
    --template-file aks-agentpools.json
az deployment group create \
    --resource-group myResourceGroup \
    --template-file aks-agentpools.json
Tip
You can add a tag to your node pool by adding thetagproperty in the template, as shown in the following example:
...
"resources": [
{
  ...
  "properties": {
    ...
    "tags": {
      "name1": "val1"
    },
    ...
  }
}
...
...
"resources": [
{
  ...
  "properties": {
    ...
    "tags": {
      "name1": "val1"
    },
    ...
  }
}
...
It may take a few minutes to update your AKS cluster depending on the node pool settings and operations you define in your Resource Manager template.
Next steps
For more information about how to control pods across node pools, seeBest practices for advanced scheduler features in AKS.
Useproximity placement groupsto reduce latency for your AKS applications.
Useinstance-level public IP addressesto enable your nodes to directly serve traffic.
Azure Kubernetes Service

Additional resources