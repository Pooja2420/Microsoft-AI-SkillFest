Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
High-performance serving with Triton Inference Server
Article
2025-02-12
24 contributors
In this article
APPLIES TO:Azure CLI ml extension v2 (current)Python SDK azure-ai-ml v2 (current)
Learn how to useNVIDIA Triton Inference Serverin Azure Machine Learning withonline endpoints.
Triton is multi-framework, open-source software that is optimized for inference. It supports popular machine learning frameworks like TensorFlow, ONNX Runtime, PyTorch, NVIDIA TensorRT, and more. It can be used for your CPU or GPU workloads.
There are mainly two approaches you can take to leverage Triton models when deploying them to online endpoint: No-code deployment or full-code (Bring your own container) deployment.
No-code deployment for Triton models is a simple way to deploy them as you only need to bring Triton models to deploy.
Full-code deployment (Bring your own container) for Triton models is more advanced way to deploy them as you have full control on customizing the configurations available for Triton inference server.
For both options, Triton inference server will perform inferencing based on theTriton model as defined by NVIDIA. For instance,ensemble modelscan be used for more advanced scenarios.
Triton is supported in bothmanaged online endpoints and Kubernetes online endpoints.
In this article, you will learn how to deploy a model using no-code deployment for Triton to amanaged online endpoint. Information is provided on using the CLI (command line), Python SDK v2, and Azure Machine Learning studio. If you want to customize further directly using Triton inference server's configuration, refer toUse a custom container to deploy a modeland the BYOC example for Triton (deployment definitionandend-to-end script).
Note
Use of the NVIDIA Triton Inference Server container is governed by theNVIDIA AI Enterprise Software license agreementand can be used for 90 days without an enterprise product subscription. For more information, seeNVIDIA AI Enterprise on Azure Machine Learning.
Prerequisites
Azure CLI
Python
Studio
TheAzure CLIand themlextension to the Azure CLI, installed and configured. For more information, seeInstall and set up the CLI (v2).
TheAzure CLIand themlextension to the Azure CLI, installed and configured. For more information, seeInstall and set up the CLI (v2).
ml
A Bash shell or a compatible shell, for example, a shell on a Linux system orWindows Subsystem for Linux. The Azure CLI examples in this article assume that you use this type of shell.
A Bash shell or a compatible shell, for example, a shell on a Linux system orWindows Subsystem for Linux. The Azure CLI examples in this article assume that you use this type of shell.
An Azure Machine Learning workspace. For instructions to create a workspace, seeSet up.
An Azure Machine Learning workspace. For instructions to create a workspace, seeSet up.
A working Python 3.8 (or higher) environment.
A working Python 3.8 (or higher) environment.
You must have additional Python packages installed for scoring and may install them with the code below. They include:NumPy - An array and numerical computing libraryTriton Inference Server Client- Facilitates requests to the Triton Inference ServerPillow - A library for image operationsGevent - A networking library used when connecting to the Triton Server
You must have additional Python packages installed for scoring and may install them with the code below. They include:
NumPy - An array and numerical computing library
Triton Inference Server Client- Facilitates requests to the Triton Inference Server
Pillow - A library for image operations
Gevent - A networking library used when connecting to the Triton Server
pip install numpy
pip install tritonclient[http]
pip install pillow
pip install gevent
pip install numpy
pip install tritonclient[http]
pip install pillow
pip install gevent
Access to NCv3-series VMs for your Azure subscription.ImportantYou might need to request a quota increase for your subscription before you can use this series of VMs. For more information, seeNCv3-series.
Access to NCv3-series VMs for your Azure subscription.
Important
You might need to request a quota increase for your subscription before you can use this series of VMs. For more information, seeNCv3-series.
NVIDIA Triton Inference Server requires a specific model repository structure, where there is a directory for each model and subdirectories for the model version. The contents of each model version subdirectory is determined by the type of the model and the requirements of the backend that supports the model. To see all the model repository structurehttps://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_repository.md#model-files
The information in this document is based on using a model stored in ONNX format, so the directory structure of the model repository is<model-repository>/<model-name>/1/model.onnx. Specifically, this model performs image identification.
<model-repository>/<model-name>/1/model.onnx
The information in this article is based on code samples contained in theazureml-examplesrepository. To run the commands locally without having to copy/paste YAML and other files, clone the repo and then change directories to theclidirectory in the repo:
cli
git clone https://github.com/Azure/azureml-examples --depth 1
cd azureml-examples
cd cli
git clone https://github.com/Azure/azureml-examples --depth 1
cd azureml-examples
cd cli
If you haven't already set the defaults for the Azure CLI, save your default settings. To avoid passing in the values for your subscription, workspace, and resource group multiple times, use the following commands. Replace the following parameters with values for your specific configuration:
Replace<subscription>with your Azure subscription ID.
<subscription>
Replace<workspace>with your Azure Machine Learning workspace name.
<workspace>
Replace<resource-group>with the Azure resource group that contains your workspace.
<resource-group>
Replace<location>with the Azure region that contains your workspace.
<location>
Tip
You can see what your current defaults are by using theaz configure -lcommand.
az configure -l
az account set --subscription <subscription>
az configure --defaults workspace=<workspace> group=<resource-group> location=<location>
az account set --subscription <subscription>
az configure --defaults workspace=<workspace> group=<resource-group> location=<location>
APPLIES TO:Python SDK azure-ai-mlv2 (current)
An Azure Machine Learning workspace. For steps for creating a workspace, seeCreate the workspace.
An Azure Machine Learning workspace. For steps for creating a workspace, seeCreate the workspace.
The Azure Machine Learning SDK for Python v2. To install the SDK, use the following command:pip install azure-ai-ml azure-identityTo update an existing installation of the SDK to the latest version, use the following command:pip install --upgrade azure-ai-ml azure-identityFor more information, seeAzure Machine Learning Package client library for Python.
The Azure Machine Learning SDK for Python v2. To install the SDK, use the following command:
pip install azure-ai-ml azure-identity
pip install azure-ai-ml azure-identity
To update an existing installation of the SDK to the latest version, use the following command:
pip install --upgrade azure-ai-ml azure-identity
pip install --upgrade azure-ai-ml azure-identity
For more information, seeAzure Machine Learning Package client library for Python.
A working Python 3.8 (or higher) environment.
A working Python 3.8 (or higher) environment.
You must have additional Python packages installed for scoring and may install them with the code below. They include:NumPy - An array and numerical computing libraryTriton Inference Server Client- Facilitates requests to the Triton Inference ServerPillow - A library for image operationsGevent - A networking library used when connecting to the Triton Serverpip install numpy
pip install tritonclient[http]
pip install pillow
pip install gevent
You must have additional Python packages installed for scoring and may install them with the code below. They include:
NumPy - An array and numerical computing library
Triton Inference Server Client- Facilitates requests to the Triton Inference Server
Pillow - A library for image operations
Gevent - A networking library used when connecting to the Triton Server
pip install numpy
pip install tritonclient[http]
pip install pillow
pip install gevent
pip install numpy
pip install tritonclient[http]
pip install pillow
pip install gevent
Access to NCv3-series VMs for your Azure subscription.ImportantYou might need to request a quota increase for your subscription before you can use this series of VMs. For more information, seeNCv3-series.
Access to NCv3-series VMs for your Azure subscription.
Important
You might need to request a quota increase for your subscription before you can use this series of VMs. For more information, seeNCv3-series.
The information in this article is based on theonline-endpoints-triton.ipynbnotebook contained in theazureml-examplesrepository. To run the commands locally without having to copy/paste files, clone the repo, and then change directories to thesdk/endpoints/online/triton/single-model/directory in the repo:
sdk/endpoints/online/triton/single-model/
git clone https://github.com/Azure/azureml-examples --depth 1
cd azureml-examples/sdk/python/endpoints/online/triton/single-model/
git clone https://github.com/Azure/azureml-examples --depth 1
cd azureml-examples/sdk/python/endpoints/online/triton/single-model/
An Azure subscription. If you don't have an Azure subscription, create a free account before you begin. Try thefree or paid version of Azure Machine Learning.
An Azure subscription. If you don't have an Azure subscription, create a free account before you begin. Try thefree or paid version of Azure Machine Learning.
An Azure Machine Learning workspace. If you don't have one, use the steps inManage Azure Machine Learning workspaces in the portal, or with the Python SDKto create one.
An Azure Machine Learning workspace. If you don't have one, use the steps inManage Azure Machine Learning workspaces in the portal, or with the Python SDKto create one.
Define the deployment configuration
Azure CLI
Python
Studio
APPLIES TO:Azure CLI ml extensionv2 (current)
This section shows how you can deploy to a managed online endpoint using the Azure CLI with the Machine Learning extension (v2).
Important
For Triton no-code-deployment,testing via local endpointsis currently not supported.
To avoid typing in a path for multiple commands, use the following command to set aBASE_PATHenvironment variable. This variable points to the directory where the model and associated YAML configuration files are located:BASE_PATH=endpoints/online/triton/single-model
To avoid typing in a path for multiple commands, use the following command to set aBASE_PATHenvironment variable. This variable points to the directory where the model and associated YAML configuration files are located:
BASE_PATH
BASE_PATH=endpoints/online/triton/single-model
BASE_PATH=endpoints/online/triton/single-model
Use the following command to set the name of the endpoint that will be created. In this example, a random name is created for the endpoint:export ENDPOINT_NAME=triton-single-endpt-`echo $RANDOM`
Use the following command to set the name of the endpoint that will be created. In this example, a random name is created for the endpoint:
export ENDPOINT_NAME=triton-single-endpt-`echo $RANDOM`
export ENDPOINT_NAME=triton-single-endpt-`echo $RANDOM`
Create a YAML configuration file for your endpoint. The following example configures the name and authentication mode of the endpoint. The one used in the following commands is located at/cli/endpoints/online/triton/single-model/create-managed-endpoint.ymlin the azureml-examples repo you cloned earlier:create-managed-endpoint.yaml$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineEndpoint.schema.json
name: my-endpoint
auth_mode: aml_token
Create a YAML configuration file for your endpoint. The following example configures the name and authentication mode of the endpoint. The one used in the following commands is located at/cli/endpoints/online/triton/single-model/create-managed-endpoint.ymlin the azureml-examples repo you cloned earlier:
/cli/endpoints/online/triton/single-model/create-managed-endpoint.yml
create-managed-endpoint.yaml
$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineEndpoint.schema.json
name: my-endpoint
auth_mode: aml_token
$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineEndpoint.schema.json
name: my-endpoint
auth_mode: aml_token
Create a YAML configuration file for the deployment. The following example configures a deployment namedblueto the endpoint defined in the previous step. The one used in the following commands is located at/cli/endpoints/online/triton/single-model/create-managed-deployment.ymlin the azureml-examples repo you cloned earlier:ImportantFor Triton no-code-deployment (NCD) to work, settingtypetotriton_modelâis required,type: triton_modelâ. For more information, seeCLI (v2) model YAML schema.This deployment uses a Standard_NC6s_v3 VM. You might need to request a quota increase for your subscription before you can use this VM. For more information, seeNCv3-series.$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json
name: blue
endpoint_name: my-endpoint
model:
  name: sample-densenet-onnx-model
  version: 1
  path: ./models
  type: triton_model
instance_count: 1
instance_type: Standard_NC6s_v3
Create a YAML configuration file for the deployment. The following example configures a deployment namedblueto the endpoint defined in the previous step. The one used in the following commands is located at/cli/endpoints/online/triton/single-model/create-managed-deployment.ymlin the azureml-examples repo you cloned earlier:
/cli/endpoints/online/triton/single-model/create-managed-deployment.yml
Important
For Triton no-code-deployment (NCD) to work, settingtypetotriton_modelâis required,type: triton_modelâ. For more information, seeCLI (v2) model YAML schema.
type
triton_modelâ
type: triton_modelâ
This deployment uses a Standard_NC6s_v3 VM. You might need to request a quota increase for your subscription before you can use this VM. For more information, seeNCv3-series.
$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json
name: blue
endpoint_name: my-endpoint
model:
  name: sample-densenet-onnx-model
  version: 1
  path: ./models
  type: triton_model
instance_count: 1
instance_type: Standard_NC6s_v3
$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json
name: blue
endpoint_name: my-endpoint
model:
  name: sample-densenet-onnx-model
  version: 1
  path: ./models
  type: triton_model
instance_count: 1
instance_type: Standard_NC6s_v3
APPLIES TO:Python SDK azure-ai-mlv2 (current)
This section shows how you can define a Triton deployment to deploy to a managed online endpoint using the Azure Machine Learning Python SDK (v2).
Important
For Triton no-code-deployment,testing via local endpointsis currently not supported.
To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name.subscription_id = "<SUBSCRIPTION_ID>"
resource_group = "<RESOURCE_GROUP>"
workspace_name = "<AML_WORKSPACE_NAME>"
To connect to a workspace, we need identifier parameters - a subscription, resource group and workspace name.
subscription_id = "<SUBSCRIPTION_ID>"
resource_group = "<RESOURCE_GROUP>"
workspace_name = "<AML_WORKSPACE_NAME>"
subscription_id = "<SUBSCRIPTION_ID>"
resource_group = "<RESOURCE_GROUP>"
workspace_name = "<AML_WORKSPACE_NAME>"
Use the following command to set the name of the endpoint that will be created. In this example, a random name is created for the endpoint:import random

endpoint_name = f"endpoint-{random.randint(0, 10000)}"
Use the following command to set the name of the endpoint that will be created. In this example, a random name is created for the endpoint:
import random

endpoint_name = f"endpoint-{random.randint(0, 10000)}"
import random

endpoint_name = f"endpoint-{random.randint(0, 10000)}"
We use these details above in theMLClientfromazure.ai.mlto get a handle to the required Azure Machine Learning workspace. Check theconfiguration notebookfor more details on how to configure credentials and connect to a workspace.from azure.ai.ml import MLClient
from azure.identity import DefaultAzureCredential

ml_client = MLClient(
    DefaultAzureCredential(),
    subscription_id,
    resource_group,
    workspace_name,
)
We use these details above in theMLClientfromazure.ai.mlto get a handle to the required Azure Machine Learning workspace. Check theconfiguration notebookfor more details on how to configure credentials and connect to a workspace.
MLClient
azure.ai.ml
from azure.ai.ml import MLClient
from azure.identity import DefaultAzureCredential

ml_client = MLClient(
    DefaultAzureCredential(),
    subscription_id,
    resource_group,
    workspace_name,
)
from azure.ai.ml import MLClient
from azure.identity import DefaultAzureCredential

ml_client = MLClient(
    DefaultAzureCredential(),
    subscription_id,
    resource_group,
    workspace_name,
)
Create aManagedOnlineEndpointobject to configure the endpoint. The following example configures the name and authentication mode of the endpoint.from azure.ai.ml.entities import ManagedOnlineEndpoint

endpoint = ManagedOnlineEndpoint(name=endpoint_name, auth_mode="key")
Create aManagedOnlineEndpointobject to configure the endpoint. The following example configures the name and authentication mode of the endpoint.
ManagedOnlineEndpoint
from azure.ai.ml.entities import ManagedOnlineEndpoint

endpoint = ManagedOnlineEndpoint(name=endpoint_name, auth_mode="key")
from azure.ai.ml.entities import ManagedOnlineEndpoint

endpoint = ManagedOnlineEndpoint(name=endpoint_name, auth_mode="key")
Create aManagedOnlineDeploymentobject to configure the deployment. The following example configures a deployment namedblueto the endpoint defined in the previous step and defines a local model inline.from azure.ai.ml.entities import ManagedOnlineDeployment, Model

model_name = "densenet-onnx-model"
model_version = 1

deployment = ManagedOnlineDeployment(
    name="blue",
    endpoint_name=endpoint_name,
    model=Model(
        name=model_name, 
        version=model_version,
        path="./models",
        type="triton_model"
    ),
    instance_type="Standard_NC6s_v3",
    instance_count=1,
)
Create aManagedOnlineDeploymentobject to configure the deployment. The following example configures a deployment namedblueto the endpoint defined in the previous step and defines a local model inline.
ManagedOnlineDeployment
from azure.ai.ml.entities import ManagedOnlineDeployment, Model

model_name = "densenet-onnx-model"
model_version = 1

deployment = ManagedOnlineDeployment(
    name="blue",
    endpoint_name=endpoint_name,
    model=Model(
        name=model_name, 
        version=model_version,
        path="./models",
        type="triton_model"
    ),
    instance_type="Standard_NC6s_v3",
    instance_count=1,
)
from azure.ai.ml.entities import ManagedOnlineDeployment, Model

model_name = "densenet-onnx-model"
model_version = 1

deployment = ManagedOnlineDeployment(
    name="blue",
    endpoint_name=endpoint_name,
    model=Model(
        name=model_name, 
        version=model_version,
        path="./models",
        type="triton_model"
    ),
    instance_type="Standard_NC6s_v3",
    instance_count=1,
)
This section shows how you can define a Triton deployment on a managed online endpoint usingAzure Machine Learning studio.
Register your model in Triton format using the following YAML and CLI command. The YAML uses a densenet-onnx model fromhttps://github.com/Azure/azureml-examples/tree/main/cli/endpoints/online/triton/single-modelcreate-triton-model.yamlname: densenet-onnx-model
version: 1
path: ./models
type: triton_modelâ
description: Registering my Triton format model.az ml model create -f create-triton-model.yamlThe following screenshot shows how your registered model will look on theModels pageof Azure Machine Learning studio.
Register your model in Triton format using the following YAML and CLI command. The YAML uses a densenet-onnx model fromhttps://github.com/Azure/azureml-examples/tree/main/cli/endpoints/online/triton/single-model
create-triton-model.yaml
name: densenet-onnx-model
version: 1
path: ./models
type: triton_modelâ
description: Registering my Triton format model.
name: densenet-onnx-model
version: 1
path: ./models
type: triton_modelâ
description: Registering my Triton format model.
az ml model create -f create-triton-model.yaml
az ml model create -f create-triton-model.yaml
The following screenshot shows how your registered model will look on theModels pageof Azure Machine Learning studio.

Fromstudio, select your workspace and then use either theendpointsormodelspage to create the endpoint deployment:Endpoints pageModels pageFrom theEndpointspage, selectCreate.Provide a name and authentication type for the endpoint, and then selectNext.When selecting a model, select the Triton model registered previously. SelectNextto continue.When you select a model registered in Triton format, in the Environment step of the wizard, you don't need scoring script and environment.Select the Triton model, and then selectDeploy. When prompted, selectDeploy to real-time endpoint.
Fromstudio, select your workspace and then use either theendpointsormodelspage to create the endpoint deployment:
Endpoints page
Models page
From theEndpointspage, selectCreate.
From theEndpointspage, selectCreate.

Provide a name and authentication type for the endpoint, and then selectNext.
Provide a name and authentication type for the endpoint, and then selectNext.
When selecting a model, select the Triton model registered previously. SelectNextto continue.
When selecting a model, select the Triton model registered previously. SelectNextto continue.
When you select a model registered in Triton format, in the Environment step of the wizard, you don't need scoring script and environment.
When you select a model registered in Triton format, in the Environment step of the wizard, you don't need scoring script and environment.

Select the Triton model, and then selectDeploy. When prompted, selectDeploy to real-time endpoint.
Select the Triton model, and then selectDeploy. When prompted, selectDeploy to real-time endpoint.

Deploy to Azure
Azure CLI
Python
Studio
APPLIES TO:Azure CLI ml extensionv2 (current)
To create a new endpoint using the YAML configuration, use the following command:az ml online-endpoint create -n $ENDPOINT_NAME -f $BASE_PATH/create-managed-endpoint.yaml
To create a new endpoint using the YAML configuration, use the following command:
az ml online-endpoint create -n $ENDPOINT_NAME -f $BASE_PATH/create-managed-endpoint.yaml
az ml online-endpoint create -n $ENDPOINT_NAME -f $BASE_PATH/create-managed-endpoint.yaml
To create the deployment using the YAML configuration, use the following command:az ml online-deployment create --name blue --endpoint $ENDPOINT_NAME -f $BASE_PATH/create-managed-deployment.yaml --all-traffic
To create the deployment using the YAML configuration, use the following command:
az ml online-deployment create --name blue --endpoint $ENDPOINT_NAME -f $BASE_PATH/create-managed-deployment.yaml --all-traffic
az ml online-deployment create --name blue --endpoint $ENDPOINT_NAME -f $BASE_PATH/create-managed-deployment.yaml --all-traffic
APPLIES TO:Python SDK azure-ai-mlv2 (current)
To create a new endpoint using theManagedOnlineEndpointobject, use the following command:endpoint = ml_client.online_endpoints.begin_create_or_update(endpoint)
To create a new endpoint using theManagedOnlineEndpointobject, use the following command:
ManagedOnlineEndpoint
endpoint = ml_client.online_endpoints.begin_create_or_update(endpoint)
endpoint = ml_client.online_endpoints.begin_create_or_update(endpoint)
To create the deployment using theManagedOnlineDeploymentobject, use the following command:ml_client.online_deployments.begin_create_or_update(deployment)
To create the deployment using theManagedOnlineDeploymentobject, use the following command:
ManagedOnlineDeployment
ml_client.online_deployments.begin_create_or_update(deployment)
ml_client.online_deployments.begin_create_or_update(deployment)
Once the deployment completes, its traffic value will be set to0%. Update the traffic to 100%.endpoint.traffic = {"blue": 100}
ml_client.online_endpoints.begin_create_or_update(endpoint)
Once the deployment completes, its traffic value will be set to0%. Update the traffic to 100%.
0%
endpoint.traffic = {"blue": 100}
ml_client.online_endpoints.begin_create_or_update(endpoint)
endpoint.traffic = {"blue": 100}
ml_client.online_endpoints.begin_create_or_update(endpoint)
Complete the wizard to deploy to the endpoint.
Complete the wizard to deploy to the endpoint.

Once the deployment completes, its traffic value will be set to0%. Update the traffic to 100% from the Endpoint page by clickingUpdate Trafficon the second menu row.
Once the deployment completes, its traffic value will be set to0%. Update the traffic to 100% from the Endpoint page by clickingUpdate Trafficon the second menu row.
0%
Update Traffic
Test the endpoint
Azure CLI
Python
Studio
APPLIES TO:Azure CLI ml extensionv2 (current)
Once your deployment completes, use the following command to make a scoring request to the deployed endpoint.
Tip
The file/cli/endpoints/online/triton/single-model/triton_densenet_scoring.pyin the azureml-examples repo is used for scoring. The image passed to the endpoint needs pre-processing to meet the size, type, and format requirements, and post-processing to show the predicted label. Thetriton_densenet_scoring.pyuses thetritonclient.httplibrary to communicate with the Triton inference server. This file runs on the client side.
/cli/endpoints/online/triton/single-model/triton_densenet_scoring.py
triton_densenet_scoring.py
tritonclient.http
To get the endpoint scoring uri, use the following command:scoring_uri=$(az ml online-endpoint show -n $ENDPOINT_NAME --query scoring_uri -o tsv)
scoring_uri=${scoring_uri%/*}
To get the endpoint scoring uri, use the following command:
scoring_uri=$(az ml online-endpoint show -n $ENDPOINT_NAME --query scoring_uri -o tsv)
scoring_uri=${scoring_uri%/*}
scoring_uri=$(az ml online-endpoint show -n $ENDPOINT_NAME --query scoring_uri -o tsv)
scoring_uri=${scoring_uri%/*}
To get an authentication key, use the following command:auth_token=$(az ml online-endpoint get-credentials -n $ENDPOINT_NAME --query accessToken -o tsv)
To get an authentication key, use the following command:
auth_token=$(az ml online-endpoint get-credentials -n $ENDPOINT_NAME --query accessToken -o tsv)
auth_token=$(az ml online-endpoint get-credentials -n $ENDPOINT_NAME --query accessToken -o tsv)
To score data with the endpoint, use the following command. It submits the image of a peacock (https://aka.ms/peacock-pic) to the endpoint:python $BASE_PATH/triton_densenet_scoring.py --base_url=$scoring_uri --token=$auth_token --image_path $BASE_PATH/data/peacock.jpgThe response from the script is similar to the following text:Is server ready - True
Is model ready - True
/azureml-examples/cli/endpoints/online/triton/single-model/densenet_labels.txt
84 : PEACOCK
To score data with the endpoint, use the following command. It submits the image of a peacock (https://aka.ms/peacock-pic) to the endpoint:
python $BASE_PATH/triton_densenet_scoring.py --base_url=$scoring_uri --token=$auth_token --image_path $BASE_PATH/data/peacock.jpg
python $BASE_PATH/triton_densenet_scoring.py --base_url=$scoring_uri --token=$auth_token --image_path $BASE_PATH/data/peacock.jpg
The response from the script is similar to the following text:
Is server ready - True
Is model ready - True
/azureml-examples/cli/endpoints/online/triton/single-model/densenet_labels.txt
84 : PEACOCK
Is server ready - True
Is model ready - True
/azureml-examples/cli/endpoints/online/triton/single-model/densenet_labels.txt
84 : PEACOCK
APPLIES TO:Python SDK azure-ai-mlv2 (current)
To get the endpoint scoring uri, use the following command:endpoint = ml_client.online_endpoints.get(endpoint_name)
scoring_uri = endpoint.scoring_uri
To get the endpoint scoring uri, use the following command:
endpoint = ml_client.online_endpoints.get(endpoint_name)
scoring_uri = endpoint.scoring_uri
endpoint = ml_client.online_endpoints.get(endpoint_name)
scoring_uri = endpoint.scoring_uri
To get an authentication key, use the following command:
keys = ml_client.online_endpoints.list_keys(endpoint_name)
auth_key = keys.primary_key
To get an authentication key, use the following command:
keys = ml_client.online_endpoints.list_keys(endpoint_name)
auth_key = keys.primary_key
The following scoring code uses theTriton Inference Server Clientto submit the image of a peacock to the endpoint. This script is available in the companion notebook to this example -Deploy a model to online endpoints using Triton.# Test the blue deployment with some sample data
import requests
import gevent.ssl
import numpy as np
import tritonclient.http as tritonhttpclient
from pathlib import Path
import prepost

img_uri = "http://aka.ms/peacock-pic"

# We remove the scheme from the url
url = scoring_uri[8:]

# Initialize client handler
triton_client = tritonhttpclient.InferenceServerClient(
    url=url,
    ssl=True,
    ssl_context_factory=gevent.ssl._create_default_https_context,
)

# Create headers
headers = {}
headers["Authorization"] = f"Bearer {auth_key}"

# Check status of triton server
health_ctx = triton_client.is_server_ready(headers=headers)
print("Is server ready - {}".format(health_ctx))

# Check status of model
model_name = "model_1"
status_ctx = triton_client.is_model_ready(model_name, "1", headers)
print("Is model ready - {}".format(status_ctx))

if Path(img_uri).exists():
    img_content = open(img_uri, "rb").read()
else:
    agent = f"Python Requests/{requests.__version__} (https://github.com/Azure/azureml-examples)"
    img_content = requests.get(img_uri, headers={"User-Agent": agent}).content

img_data = prepost.preprocess(img_content)

# Populate inputs and outputs
input = tritonhttpclient.InferInput("data_0", img_data.shape, "FP32")
input.set_data_from_numpy(img_data)
inputs = [input]
output = tritonhttpclient.InferRequestedOutput("fc6_1")
outputs = [output]

result = triton_client.infer(model_name, inputs, outputs=outputs, headers=headers)
max_label = np.argmax(result.as_numpy("fc6_1"))
label_name = prepost.postprocess(max_label)
print(label_name)
The following scoring code uses theTriton Inference Server Clientto submit the image of a peacock to the endpoint. This script is available in the companion notebook to this example -Deploy a model to online endpoints using Triton.
# Test the blue deployment with some sample data
import requests
import gevent.ssl
import numpy as np
import tritonclient.http as tritonhttpclient
from pathlib import Path
import prepost

img_uri = "http://aka.ms/peacock-pic"

# We remove the scheme from the url
url = scoring_uri[8:]

# Initialize client handler
triton_client = tritonhttpclient.InferenceServerClient(
    url=url,
    ssl=True,
    ssl_context_factory=gevent.ssl._create_default_https_context,
)

# Create headers
headers = {}
headers["Authorization"] = f"Bearer {auth_key}"

# Check status of triton server
health_ctx = triton_client.is_server_ready(headers=headers)
print("Is server ready - {}".format(health_ctx))

# Check status of model
model_name = "model_1"
status_ctx = triton_client.is_model_ready(model_name, "1", headers)
print("Is model ready - {}".format(status_ctx))

if Path(img_uri).exists():
    img_content = open(img_uri, "rb").read()
else:
    agent = f"Python Requests/{requests.__version__} (https://github.com/Azure/azureml-examples)"
    img_content = requests.get(img_uri, headers={"User-Agent": agent}).content

img_data = prepost.preprocess(img_content)

# Populate inputs and outputs
input = tritonhttpclient.InferInput("data_0", img_data.shape, "FP32")
input.set_data_from_numpy(img_data)
inputs = [input]
output = tritonhttpclient.InferRequestedOutput("fc6_1")
outputs = [output]

result = triton_client.infer(model_name, inputs, outputs=outputs, headers=headers)
max_label = np.argmax(result.as_numpy("fc6_1"))
label_name = prepost.postprocess(max_label)
print(label_name)
# Test the blue deployment with some sample data
import requests
import gevent.ssl
import numpy as np
import tritonclient.http as tritonhttpclient
from pathlib import Path
import prepost

img_uri = "http://aka.ms/peacock-pic"

# We remove the scheme from the url
url = scoring_uri[8:]

# Initialize client handler
triton_client = tritonhttpclient.InferenceServerClient(
    url=url,
    ssl=True,
    ssl_context_factory=gevent.ssl._create_default_https_context,
)

# Create headers
headers = {}
headers["Authorization"] = f"Bearer {auth_key}"

# Check status of triton server
health_ctx = triton_client.is_server_ready(headers=headers)
print("Is server ready - {}".format(health_ctx))

# Check status of model
model_name = "model_1"
status_ctx = triton_client.is_model_ready(model_name, "1", headers)
print("Is model ready - {}".format(status_ctx))

if Path(img_uri).exists():
    img_content = open(img_uri, "rb").read()
else:
    agent = f"Python Requests/{requests.__version__} (https://github.com/Azure/azureml-examples)"
    img_content = requests.get(img_uri, headers={"User-Agent": agent}).content

img_data = prepost.preprocess(img_content)

# Populate inputs and outputs
input = tritonhttpclient.InferInput("data_0", img_data.shape, "FP32")
input.set_data_from_numpy(img_data)
inputs = [input]
output = tritonhttpclient.InferRequestedOutput("fc6_1")
outputs = [output]

result = triton_client.infer(model_name, inputs, outputs=outputs, headers=headers)
max_label = np.argmax(result.as_numpy("fc6_1"))
label_name = prepost.postprocess(max_label)
print(label_name)
The response from the script is similar to the following text:Is server ready - True
Is model ready - True
/azureml-examples/sdk/endpoints/online/triton/single-model/densenet_labels.txt
84 : PEACOCK
The response from the script is similar to the following text:
Is server ready - True
Is model ready - True
/azureml-examples/sdk/endpoints/online/triton/single-model/densenet_labels.txt
84 : PEACOCK
Is server ready - True
Is model ready - True
/azureml-examples/sdk/endpoints/online/triton/single-model/densenet_labels.txt
84 : PEACOCK
Triton Inference Server requires using Triton Client for inference, and it supports tensor-typed input. Azure Machine Learning studio doesn't currently support this. Instead, use CLI or SDK to invoke endpoints with Triton.
Delete the endpoint and model
Azure CLI
Python
Studio
APPLIES TO:Azure CLI ml extensionv2 (current)
Once you're done with the endpoint, use the following command to delete it:az ml online-endpoint delete -n $ENDPOINT_NAME --yes
Once you're done with the endpoint, use the following command to delete it:
az ml online-endpoint delete -n $ENDPOINT_NAME --yes
az ml online-endpoint delete -n $ENDPOINT_NAME --yes
Use the following command to archive your model:az ml model archive --name $MODEL_NAME --version $MODEL_VERSION
Use the following command to archive your model:
az ml model archive --name $MODEL_NAME --version $MODEL_VERSION
az ml model archive --name $MODEL_NAME --version $MODEL_VERSION
APPLIES TO:Python SDK azure-ai-mlv2 (current)
Delete the endpoint. Deleting the endpoint also deletes any child deployments, however it will not archive associated Environments or Models.ml_client.online_endpoints.begin_delete(name=endpoint_name)
Delete the endpoint. Deleting the endpoint also deletes any child deployments, however it will not archive associated Environments or Models.
ml_client.online_endpoints.begin_delete(name=endpoint_name)
ml_client.online_endpoints.begin_delete(name=endpoint_name)
Archive the model with the following code.ml_client.models.archive(name=model_name, version=model_version)
Archive the model with the following code.
ml_client.models.archive(name=model_name, version=model_version)
ml_client.models.archive(name=model_name, version=model_version)
From the endpoint's page, clickDeletein the second row below the endpoint's name.
From the endpoint's page, clickDeletein the second row below the endpoint's name.
Delete
From the model's page, clickDeletein the first row below the model's name.
From the model's page, clickDeletein the first row below the model's name.
Delete
Next steps
To learn more, review these articles:
Deploy models with REST
Create and use managed online endpoints in the studio
Safe rollout for online endpoints
How to autoscale managed online endpoints
View costs for an Azure Machine Learning managed online endpoint
Access Azure resources with a managed online endpoint and managed identity
Troubleshoot managed online endpoints deployment
Feedback
Was this page helpful?
Additional resources