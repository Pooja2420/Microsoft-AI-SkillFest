Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Upgrade an Azure Kubernetes Service (AKS) cluster
Article
2025-03-21
12 contributors
In this article
Part of the AKS cluster lifecycle involves performing periodic upgrades to the latest Kubernetes version. It's important you apply the latest security releases and upgrades to get the latest features. This article shows you how to check for and apply upgrades to your AKS cluster.
Kubernetes version upgrades
When you upgrade a supported AKS cluster, you can't skip Kubernetes minor versions. You must perform all upgrades sequentially by minor version number. For example, upgrades between1.14.x->1.15.xor1.15.x->1.16.xare allowed.1.14.x->1.16.xisn't allowed. You can only skip multiple versions when upgrading from anunsupported versionback to asupported version. For example, you can perform an upgrade from an unsupported1.10.xto a supported1.12.xif available.
When you perform an upgrade from anunsupported versionthat skips two or more minor versions, the upgrade has no guarantee of functionality and is excluded from the service-level agreements and limited warranty. If your version is out of date, we recommend you recreate your cluster instead.
Before you begin
If you're using the Azure CLI, this article requires Azure CLI version 2.34.1 or later. Runaz --versionto find the version. If you need to install or upgrade, seeInstall Azure CLI.
az --version
If you're using Azure PowerShell, this article requires Azure PowerShell version 5.9.0 or later. RunGet-InstalledModule -Name Azto find the version. If you need to install or upgrade, seeInstall Azure PowerShell.
Get-InstalledModule -Name Az
Performing upgrade operations requires theMicrosoft.ContainerService/managedClusters/agentPools/writeRBAC role. For more on Azure RBAC roles, see theAzure resource provider operations.
Microsoft.ContainerService/managedClusters/agentPools/write
Starting with 1.30 kubernetes version and 1.27 LTS versions the beta APIs will be disabled by default when you upgrade to them.
Warning
An AKS cluster upgrade triggers a cordon and drain of your nodes. If you have a low compute quota available, the upgrade might fail. For more information, seeincrease quotas.
Check for available AKS cluster upgrades
Note
To stay up to date with AKS fixes, releases, and updates, see theAKS release tracker.
Azure CLI
Azure PowerShell
Azure portal
Check which Kubernetes releases are available for your cluster using theaz aks get-upgradescommand.az aks get-upgrades --resource-group myResourceGroup --name myAKSCluster --output tableThe following example output shows the current version as1.26.6and lists the available versions underupgrades:{
  "agentPoolProfiles": null,
  "controlPlaneProfile": {
    "kubernetesVersion": "1.26.6",
    ...
    "upgrades": [
      {
        "isPreview": null,
        "kubernetesVersion": "1.27.1"
      },
      {
        "isPreview": null,
        "kubernetesVersion": "1.27.3"
      }
    ]
  },
  ...
}
Check which Kubernetes releases are available for your cluster using theaz aks get-upgradescommand.
az aks get-upgrades
az aks get-upgrades --resource-group myResourceGroup --name myAKSCluster --output table
az aks get-upgrades --resource-group myResourceGroup --name myAKSCluster --output table
The following example output shows the current version as1.26.6and lists the available versions underupgrades:
upgrades
{
  "agentPoolProfiles": null,
  "controlPlaneProfile": {
    "kubernetesVersion": "1.26.6",
    ...
    "upgrades": [
      {
        "isPreview": null,
        "kubernetesVersion": "1.27.1"
      },
      {
        "isPreview": null,
        "kubernetesVersion": "1.27.3"
      }
    ]
  },
  ...
}
{
  "agentPoolProfiles": null,
  "controlPlaneProfile": {
    "kubernetesVersion": "1.26.6",
    ...
    "upgrades": [
      {
        "isPreview": null,
        "kubernetesVersion": "1.27.1"
      },
      {
        "isPreview": null,
        "kubernetesVersion": "1.27.3"
      }
    ]
  },
  ...
}
Check which Kubernetes releases are available for your cluster and the region in which it resides using theGet-AzAksVersioncmdlet.Get-AzAksVersion -Location eastus | Where-Object OrchestratorVersionThe following example output shows the available versions underOrchestratorVersion:Default     IsPreview     OrchestratorType     OrchestratorVersion
-------     ---------     ----------------     -------------------
                          Kubernetes           1.27.1
                          Kubernetes           1.27.3
Check which Kubernetes releases are available for your cluster and the region in which it resides using theGet-AzAksVersioncmdlet.
Get-AzAksVersion
Get-AzAksVersion -Location eastus | Where-Object OrchestratorVersion
Get-AzAksVersion -Location eastus | Where-Object OrchestratorVersion
The following example output shows the available versions underOrchestratorVersion:
OrchestratorVersion
Default     IsPreview     OrchestratorType     OrchestratorVersion
-------     ---------     ----------------     -------------------
                          Kubernetes           1.27.1
                          Kubernetes           1.27.3
Default     IsPreview     OrchestratorType     OrchestratorVersion
-------     ---------     ----------------     -------------------
                          Kubernetes           1.27.1
                          Kubernetes           1.27.3
Check which Kubernetes releases are available for your cluster using the following steps:
Sign in to theAzure portal.
Navigate to your AKS cluster.
In the service menu, underSettings, selectCluster configuration.
ForKubernetes version, selectUpgrade version.
On theUpgrade Kubernetes versionpage, select theKubernetes versionto check for available upgrades.
The Azure portal highlights all the deprecated APIs between your current version and new available versions you intend to migrate to. For more information, seethe Kubernetes API Removal and Deprecation process.

Troubleshoot AKS cluster upgrade error messages
Azure CLI
Azure PowerShell
Azure portal
The following example output means theappservice-kubeextension isn't compatible with your Azure CLI version (a minimum of version 2.34.1 is required):
appservice-kube
The 'appservice-kube' extension is not compatible with this version of the CLI.
You have CLI core version 2.0.81 and this extension requires a min of 2.34.1.
Table output unavailable. Use the --query option to specify an appropriate query. Use --debug for more info.
The 'appservice-kube' extension is not compatible with this version of the CLI.
You have CLI core version 2.0.81 and this extension requires a min of 2.34.1.
Table output unavailable. Use the --query option to specify an appropriate query. Use --debug for more info.
If you receive this output, you need to update your Azure CLI version. Theaz upgradecommand was added in version 2.11.0 and doesn't work with versions prior to 2.11.0. You can update older versions by reinstalling Azure CLI as described inInstall the Azure CLI. If your Azure CLI version is 2.11.0 or later, runaz upgradeto upgrade Azure CLI to the latest version.
az upgrade
az upgrade
If your Azure CLI is updated and you receive the following example output, it means that no upgrades are available:
ERROR: Table output unavailable. Use the --query option to specify an appropriate query. Use --debug for more info.
ERROR: Table output unavailable. Use the --query option to specify an appropriate query. Use --debug for more info.
If no upgrades are available, create a new cluster with a supported version of Kubernetes and migrate your workloads from the existing cluster to the new cluster. AKS doesn't support upgrading a cluster to a newer Kubernetes version whenaz aks get-upgradesshows that no upgrades are available.
az aks get-upgrades
If no upgrades are available, create a new cluster with a supported version of Kubernetes and migrate your workloads from the existing cluster to the new cluster. AKS doesn't support upgrading a cluster to a newer Kubernetes version whenGet-AzAksUpgradeProfileshows that no upgrades are available.
Get-AzAksUpgradeProfile
If no upgrades are available, create a new cluster with a supported version of Kubernetes and migrate your workloads from the existing cluster to the new cluster. AKS doesn't support upgrading a cluster to a newer Kubernetes version when no upgrades are available.
Upgrade an AKS cluster
During the cluster upgrade process, AKS performs the following operations:
Add a new buffer node (or as many nodes as configured inmax surge) to the cluster that runs the specified Kubernetes version.
Cordon and drainone of the old nodes to minimize disruption to running applications. If you're using max surge, itcordons and drainsas many nodes at the same time as the number of buffer nodes specified.
For long running pods, you can configure the node drain time-out, which allows for custom wait time on the eviction of pods and graceful termination per node. If not specified, the default is 30 minutes. Minimum allowed time-out value is 5 minutes. The maximum limit for drain time-out is 24 hours.
When the old node is fully drained, it's reimaged to receive the new version and becomes the buffer node for the following node to be upgraded.
Optionally, you can set a duration of time to wait between draining a node and proceeding to reimage it and move on to the next node. A short interval allows you to complete other tasks, such as checking application health from a Grafana dashboard during the upgrade process. We recommend a short timeframe for the upgrade process, as close to 0 minutes as reasonably possible. Otherwise, a higher node soak time affects how long before you discover an issue. The minimum soak time value is 0 minutes, with a maximum of 30 minutes. If not specified, the default value is 0 minutes.
This process repeats until all nodes in the cluster are upgraded.
At the end of the process, the last buffer node is deleted, maintaining the existing agent node count and zone balance.
Note
If no patch is specified, the cluster automatically upgrades to the specified minor version's latest GA patch. For example, setting--kubernetes-versionto1.28results in the cluster upgrading to1.28.9.
--kubernetes-version
1.28
1.28.9
For more information, seeSupported Kubernetes minor version upgrades in AKS.
Azure CLI
Azure PowerShell
Azure portal
Upgrade your cluster using theaz aks upgradecommand.az aks upgrade \
    --resource-group myResourceGroup \
    --name myAKSCluster \
    --kubernetes-version <KUBERNETES_VERSION>
Upgrade your cluster using theaz aks upgradecommand.
az aks upgrade
az aks upgrade \
    --resource-group myResourceGroup \
    --name myAKSCluster \
    --kubernetes-version <KUBERNETES_VERSION>
az aks upgrade \
    --resource-group myResourceGroup \
    --name myAKSCluster \
    --kubernetes-version <KUBERNETES_VERSION>
Confirm the upgrade was successful using theaz aks showcommand.az aks show --resource-group myResourceGroup --name myAKSCluster --output tableThe following example output shows that the cluster now runs1.27.3:Name          Location    ResourceGroup    KubernetesVersion    ProvisioningState    Fqdn
------------  ----------  ---------------  -------------------  -------------------  ----------------------------------------------
myAKSCluster  eastus      myResourceGroup  1.27.3               Succeeded            myakscluster-dns-379cbbb9.hcp.eastus.azmk8s.io
Confirm the upgrade was successful using theaz aks showcommand.
az aks show
az aks show --resource-group myResourceGroup --name myAKSCluster --output table
az aks show --resource-group myResourceGroup --name myAKSCluster --output table
The following example output shows that the cluster now runs1.27.3:
Name          Location    ResourceGroup    KubernetesVersion    ProvisioningState    Fqdn
------------  ----------  ---------------  -------------------  -------------------  ----------------------------------------------
myAKSCluster  eastus      myResourceGroup  1.27.3               Succeeded            myakscluster-dns-379cbbb9.hcp.eastus.azmk8s.io
Name          Location    ResourceGroup    KubernetesVersion    ProvisioningState    Fqdn
------------  ----------  ---------------  -------------------  -------------------  ----------------------------------------------
myAKSCluster  eastus      myResourceGroup  1.27.3               Succeeded            myakscluster-dns-379cbbb9.hcp.eastus.azmk8s.io
Upgrade your cluster using theSet-AzAksClustercommand.Set-AzAksCluster -ResourceGroupName myResourceGroup -Name myAKSCluster -KubernetesVersion <KUBERNETES_VERSION>
Upgrade your cluster using theSet-AzAksClustercommand.
Set-AzAksCluster
Set-AzAksCluster -ResourceGroupName myResourceGroup -Name myAKSCluster -KubernetesVersion <KUBERNETES_VERSION>
Set-AzAksCluster -ResourceGroupName myResourceGroup -Name myAKSCluster -KubernetesVersion <KUBERNETES_VERSION>
Confirm the upgrade was successful using theGet-AzAksClustercommand.Get-AzAksCluster -ResourceGroupName myResourceGroup -Name myAKSCluster |
 Format-Table -Property Name, Location, KubernetesVersion, ProvisioningState, FqdnThe following example output shows that the cluster now runs1.27.3:Name         Location KubernetesVersion ProvisioningState Fqdn                                   
----         -------- ----------------- ----------------- ----                                   
myAKSCluster eastus   1.27.3            Succeeded         myakscluster-dns-379cbbb9.hcp.eastus.azmk8s.io
Confirm the upgrade was successful using theGet-AzAksClustercommand.
Get-AzAksCluster
Get-AzAksCluster -ResourceGroupName myResourceGroup -Name myAKSCluster |
 Format-Table -Property Name, Location, KubernetesVersion, ProvisioningState, Fqdn
Get-AzAksCluster -ResourceGroupName myResourceGroup -Name myAKSCluster |
 Format-Table -Property Name, Location, KubernetesVersion, ProvisioningState, Fqdn
The following example output shows that the cluster now runs1.27.3:
Name         Location KubernetesVersion ProvisioningState Fqdn                                   
----         -------- ----------------- ----------------- ----                                   
myAKSCluster eastus   1.27.3            Succeeded         myakscluster-dns-379cbbb9.hcp.eastus.azmk8s.io
Name         Location KubernetesVersion ProvisioningState Fqdn                                   
----         -------- ----------------- ----------------- ----                                   
myAKSCluster eastus   1.27.3            Succeeded         myakscluster-dns-379cbbb9.hcp.eastus.azmk8s.io
Sign in to theAzure portal.
Navigate to your AKS cluster.
In the service menu, underSettings, selectCluster configuration.
ForKubernetes version, selectUpgrade version.
On theUpgrade Kubernetes versionpage, select your desiredKubernetes versionand then selectSave.
Navigate to your AKS clusterOverviewpage, and view theKubernetes versionto confirm the upgrade was successful.
Set auto-upgrade channel
You can set an auto-upgrade channel on your cluster. For more information, seeAuto-upgrading an AKS cluster.
Customize node surge upgrade
Important
Node surges require subscription quota for the requested max surge count for each upgrade operation. For example, a cluster that has five node pools, each with a count of four nodes, has a total of 20 nodes. If each node pool has a max surge value of 50%, additional compute and IP quota of 10 nodes (2 nodes * 5 pools) is required to complete the upgrade.
Node surges require subscription quota for the requested max surge count for each upgrade operation. For example, a cluster that has five node pools, each with a count of four nodes, has a total of 20 nodes. If each node pool has a max surge value of 50%, additional compute and IP quota of 10 nodes (2 nodes * 5 pools) is required to complete the upgrade.
The max surge setting on a node pool is persistent. Subsequent Kubernetes upgrades or node version upgrades will use this setting. You can change the max surge value for your node pools at any time. For production node pools, we recommend a max-surge setting of 33%.
The max surge setting on a node pool is persistent. Subsequent Kubernetes upgrades or node version upgrades will use this setting. You can change the max surge value for your node pools at any time. For production node pools, we recommend a max-surge setting of 33%.
If you're using Azure CNI, validate there are available IPs in the subnet tosatisfy IP requirements of Azure CNI.
If you're using Azure CNI, validate there are available IPs in the subnet tosatisfy IP requirements of Azure CNI.
AKS configures upgrades to surge with one extra node by default. A default value ofonefor the max surge settings enables AKS to minimize workload disruption by creating an extra node before the cordon/drain of existing applications to replace an older versioned node. You can customize the max surge value per node pool. When you increase the max surge value, the upgrade process completes faster, and you might experience disruptions during the upgrade process.
For example, a max surge value of100%provides the fastest possible upgrade process but also causes all nodes in the node pool to be drained simultaneously. You might want to use a higher value like this for testing environments. For production node pools, we recommend amax_surgesetting of33%.
100%
max_surge
33%
AKS accepts both integer values and a percentage value for max surge. For example, an integer value of5indicates five extra nodes to surge. A percentage value of50%indicates a surge value of half the current node count in the pool. Max surge percent values can be a minimum of1%and a maximum of100%. A percent value is rounded up to the nearest node count. If the max surge value is higher than the required number of nodes to be upgraded, the number of nodes to be upgraded is used for the max surge value. During an upgrade, the max surge value can be a minimum of0and a maximum value equal to the number of nodes in your node pool. You can set larger values, but you can't set the maximum number of nodes used for max surge higher than the number of nodes in the pool at the time of upgrade.
5
50%
1%
100%
0
Set max surge values for new or existing node pools using theaz aks nodepool addoraz aks nodepool updatecommand.# Set max surge for a new node pool
az aks nodepool add --name mynodepool --resource-group MyResourceGroup --cluster-name MyManagedCluster --max-surge 33%

# Update max surge for an existing node pool 
az aks nodepool update --name mynodepool --resource-group MyResourceGroup --cluster-name MyManagedCluster --max-surge 5
Set max surge values for new or existing node pools using theaz aks nodepool addoraz aks nodepool updatecommand.
az aks nodepool add
az aks nodepool update
# Set max surge for a new node pool
az aks nodepool add --name mynodepool --resource-group MyResourceGroup --cluster-name MyManagedCluster --max-surge 33%

# Update max surge for an existing node pool 
az aks nodepool update --name mynodepool --resource-group MyResourceGroup --cluster-name MyManagedCluster --max-surge 5
# Set max surge for a new node pool
az aks nodepool add --name mynodepool --resource-group MyResourceGroup --cluster-name MyManagedCluster --max-surge 33%

# Update max surge for an existing node pool 
az aks nodepool update --name mynodepool --resource-group MyResourceGroup --cluster-name MyManagedCluster --max-surge 5
Customize unavailable nodes during upgrade (Preview)
Important
maxSurgemust be set to0formaxUnavailableto be set. The two values can't both be active at the same time.
maxSurge
0
maxUnavailable
maxUnavailablewon't create surge nodes during the upgrade process. Instead, AKS cordonsnmaxUnavailablenodes at a time and evict the pods to other nodes in the agent pool. This might cause workload disruptions if the pods can't be scheduled.
maxUnavailable
maxUnavailable
maxUnavailablemight cause more failures due to unsatisfied PodDisruptionBudgets (PDBs) since there will be fewer resources for pods to be scheduled on. Please see thetroubleshooting for PodDisruptionBudgetsfor mitigation suggestions if you are faced with failures while using this feature.
maxUnavailable
You can't setmaxUnavailableon System node pools.
maxUnavailable
AKS can also configure upgrades to not use a surge node and upgrade the nodes in place. ThemaxUnavailablevalue can be used to determine how many nodes can be cordoned and drained from the existing node pool nodes.
maxUnavailable
AKS accepts both integer values and a percentage value formaxUnavailable. For example, an integer value of5indicates five nodes will be cordoned from the existing nodes on the node pool. A percentage value of50%indicates amaxUnavailablevalue of half the current node count in the pool.maxUnavailablepercent values can be a minimum of1%and a maximum of100%. A percent value is rounded up to the nearest node count. During an upgrade, themaxUnavailablevalue can be a minimum of0and a maximum value equal to the number of nodes in your node pool.
maxUnavailable
5
50%
maxUnavailable
maxUnavailable
1%
100%
maxUnavailable
0
SetmaxUnvailablevalues for new or existing node pools using theaz aks nodepool addoraz aks nodepool updatecommand.# Set maxUnavailable for a new node pool
az aks nodepool add --name mynodepool --resource-group myResourceGroup --cluster-name myManagedCluster --max-surge 0 --max-unavailable 5
# Update maxUnavailable for an existing node pool 
az aks nodepool update --name mynodepool --resource-group myResourceGroup --cluster-name myManagedCluster --max-surge 0 --max-unavailable 5
# Set maxUnavailable at upgrade time for an existing node pool
az aks nodepool upgrade --name mynodepool --resource-group myResourceGroup --cluster-name myManagedCluster --max-surge 0 --max-unavailable 5
SetmaxUnvailablevalues for new or existing node pools using theaz aks nodepool addoraz aks nodepool updatecommand.
maxUnvailable
az aks nodepool add
az aks nodepool update
# Set maxUnavailable for a new node pool
az aks nodepool add --name mynodepool --resource-group myResourceGroup --cluster-name myManagedCluster --max-surge 0 --max-unavailable 5
# Update maxUnavailable for an existing node pool 
az aks nodepool update --name mynodepool --resource-group myResourceGroup --cluster-name myManagedCluster --max-surge 0 --max-unavailable 5
# Set maxUnavailable at upgrade time for an existing node pool
az aks nodepool upgrade --name mynodepool --resource-group myResourceGroup --cluster-name myManagedCluster --max-surge 0 --max-unavailable 5
# Set maxUnavailable for a new node pool
az aks nodepool add --name mynodepool --resource-group myResourceGroup --cluster-name myManagedCluster --max-surge 0 --max-unavailable 5
# Update maxUnavailable for an existing node pool 
az aks nodepool update --name mynodepool --resource-group myResourceGroup --cluster-name myManagedCluster --max-surge 0 --max-unavailable 5
# Set maxUnavailable at upgrade time for an existing node pool
az aks nodepool upgrade --name mynodepool --resource-group myResourceGroup --cluster-name myManagedCluster --max-surge 0 --max-unavailable 5
Set node drain timeout value
At times, you may have a long running workload on a certain pod and it can't be rescheduled to another node during runtime, for example, a memory intensive stateful workload that must finish running. In these cases, you can configure a node drain time-out that AKS will respect in the upgrade workflow. If no node drain time-out value is specified, the default is 30 minutes. Minimum allowed drain time-out value is 5 minutes and the maximum limit of drain time-out is 24 hours.
If the drain time-out value elapses and pods are still running, then the upgrade operation is stopped. Any subsequent PUT operation shall resume the stopped upgrade. It's also recommended for long running pods to configure the [terminationGracePeriodSeconds][https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/].
terminationGracePeriodSeconds
Set node drain time-out for new or existing node pools using theaz aks nodepool addoraz aks nodepool updatecommand.# Set drain time-out for a new node pool
az aks nodepool add --name mynodepool --resource-group MyResourceGroup --cluster-name MyManagedCluster  --drain-time-out 100

# Update drain time-out for an existing node pool
az aks nodepool update --name mynodepool --resource-group MyResourceGroup --cluster-name MyManagedCluster --drain-time-out 45
Set node drain time-out for new or existing node pools using theaz aks nodepool addoraz aks nodepool updatecommand.
az aks nodepool add
az aks nodepool update
# Set drain time-out for a new node pool
az aks nodepool add --name mynodepool --resource-group MyResourceGroup --cluster-name MyManagedCluster  --drain-time-out 100

# Update drain time-out for an existing node pool
az aks nodepool update --name mynodepool --resource-group MyResourceGroup --cluster-name MyManagedCluster --drain-time-out 45
# Set drain time-out for a new node pool
az aks nodepool add --name mynodepool --resource-group MyResourceGroup --cluster-name MyManagedCluster  --drain-time-out 100

# Update drain time-out for an existing node pool
az aks nodepool update --name mynodepool --resource-group MyResourceGroup --cluster-name MyManagedCluster --drain-time-out 45
Set node soak time value
To allow for a duration of time to wait between draining a node and proceeding to reimage it and move on to the next node, you can set the soak time to a value between 0 and 30 minutes. If no node soak time value is specified, the default is 0 minutes.
Set node soak time for new or existing node pools using theaz aks nodepool add,az aks nodepool update, oraz aks nodepool upgradecommand.# Set node soak time for a new node pool
az aks nodepool add --name MyNodePool --resource-group MyResourceGroup --cluster-name MyManagedCluster --node-soak-duration 10

# Update node soak time for an existing node pool
az aks nodepool update --name MyNodePool --resource-group MyResourceGroup --cluster-name MyManagedCluster --max-surge 33% --node-soak-duration 5

# Set node soak time when upgrading an existing node pool
az aks nodepool upgrade --name MyNodePool --resource-group MyResourceGroup --cluster-name MyManagedCluster --max-surge 33% --node-soak-duration 20
Set node soak time for new or existing node pools using theaz aks nodepool add,az aks nodepool update, oraz aks nodepool upgradecommand.
az aks nodepool add
az aks nodepool update
az aks nodepool upgrade
# Set node soak time for a new node pool
az aks nodepool add --name MyNodePool --resource-group MyResourceGroup --cluster-name MyManagedCluster --node-soak-duration 10

# Update node soak time for an existing node pool
az aks nodepool update --name MyNodePool --resource-group MyResourceGroup --cluster-name MyManagedCluster --max-surge 33% --node-soak-duration 5

# Set node soak time when upgrading an existing node pool
az aks nodepool upgrade --name MyNodePool --resource-group MyResourceGroup --cluster-name MyManagedCluster --max-surge 33% --node-soak-duration 20
# Set node soak time for a new node pool
az aks nodepool add --name MyNodePool --resource-group MyResourceGroup --cluster-name MyManagedCluster --node-soak-duration 10

# Update node soak time for an existing node pool
az aks nodepool update --name MyNodePool --resource-group MyResourceGroup --cluster-name MyManagedCluster --max-surge 33% --node-soak-duration 5

# Set node soak time when upgrading an existing node pool
az aks nodepool upgrade --name MyNodePool --resource-group MyResourceGroup --cluster-name MyManagedCluster --max-surge 33% --node-soak-duration 20
Upgrading through significant version drift
When upgrading from an unsupported version of Kubernetes to a supported version, it's important to test your workloads on the target version.  While AKS will make every effort to upgrade your control plane and data plane, where is no guarantee your workloads will continue to work.   If your workloads rely on deprecated Kubernetes APIs, the platform has introduced breaking changes or behaviors (documented in the AKS release notes), these need to be resolved.
In these situations we recommend testing your workloads on the new version, and resolving any version issues before doing an in place upgrade of your cluster.
A common pattern in this situation is to carry out a blue / green deployment of your modified workloads to a new cluster that is on a supported Kubernetes version, and route requests to the new cluster.
View upgrade events
View upgrade events using thekubectl get eventscommand.kubectl get eventsThe following example output shows some of the above events listed during an upgrade:...
default 2m1s Normal Drain node/aks-nodepool1-96663640-vmss000001 Draining node: [aks-nodepool1-96663640-vmss000001]
...
default 1m45s Normal Upgrade node/aks-nodepool1-96663640-vmss000001   Soak duration 5m0s after draining node: aks-nodepool1-96663640-vmss000001
...
default 9m22s Normal Surge node/aks-nodepool1-96663640-vmss000002 Created a surge node [aks-nodepool1-96663640-vmss000002 nodepool1] for agentpool nodepool1
...
View upgrade events using thekubectl get eventscommand.
kubectl get events
kubectl get events
kubectl get events
The following example output shows some of the above events listed during an upgrade:
...
default 2m1s Normal Drain node/aks-nodepool1-96663640-vmss000001 Draining node: [aks-nodepool1-96663640-vmss000001]
...
default 1m45s Normal Upgrade node/aks-nodepool1-96663640-vmss000001   Soak duration 5m0s after draining node: aks-nodepool1-96663640-vmss000001
...
default 9m22s Normal Surge node/aks-nodepool1-96663640-vmss000002 Created a surge node [aks-nodepool1-96663640-vmss000002 nodepool1] for agentpool nodepool1
...
...
default 2m1s Normal Drain node/aks-nodepool1-96663640-vmss000001 Draining node: [aks-nodepool1-96663640-vmss000001]
...
default 1m45s Normal Upgrade node/aks-nodepool1-96663640-vmss000001   Soak duration 5m0s after draining node: aks-nodepool1-96663640-vmss000001
...
default 9m22s Normal Surge node/aks-nodepool1-96663640-vmss000002 Created a surge node [aks-nodepool1-96663640-vmss000002 nodepool1] for agentpool nodepool1
...
Next steps
To learn how to configure automatic upgrades, seeConfigure automatic upgrades for an AKS cluster.
For a detailed discussion of upgrade best practices and other considerations, seeAKS patch and upgrade guidance.
Azure Kubernetes Service

Additional resources