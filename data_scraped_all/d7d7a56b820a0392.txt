Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Use Apache Sqoop with Hadoop in HDInsight
Article
2024-09-06
17 contributors
In this article
Overview
SSH
Curl
PowerShell
.NET
Learn how to use Apache Sqoop in HDInsight to import and export data between an HDInsight cluster and Azure SQL Database.
Although Apache Hadoop is a natural choice for processing unstructured and semi-structured data, such as logs and files, there may also be a need to process structured data that is stored in relational databases.
Apache Sqoopis a tool designed to transfer data between Hadoop clusters and relational databases. You can use it to import data from a relational database management system (RDBMS) such as SQL Server, MySQL, or Oracle into the Hadoop distributed file system (HDFS), transform the data in Hadoop with MapReduce or Apache Hive, and then export the data back into an RDBMS. In this article, you're using Azure SQL Database for your relational database.
Important
This article sets up a test environment to perform the data transfer. You then choose a data transfer method for this environment from one of the methods in sectionRun Sqoop jobs.
For Sqoop versions that are supported on HDInsight clusters, seeWhat's new in the cluster versions provided by HDInsight?
Understand the scenario
HDInsight cluster comes with some sample data. You use the following two samples:
An ApacheLog4jlog file, which is located at/example/data/sample.log. The following logs are extracted from the file:
Log4j
/example/data/sample.log
2012-02-03 18:35:34 SampleClass6 [INFO] everything normal for id 577725851
2012-02-03 18:35:34 SampleClass4 [FATAL] system problem at id 1991281254
2012-02-03 18:35:34 SampleClass3 [DEBUG] detail for id 1304807656
...
2012-02-03 18:35:34 SampleClass6 [INFO] everything normal for id 577725851
2012-02-03 18:35:34 SampleClass4 [FATAL] system problem at id 1991281254
2012-02-03 18:35:34 SampleClass3 [DEBUG] detail for id 1304807656
...
A Hive table namedhivesampletable, which references the data file located at/hive/warehouse/hivesampletable. The table contains some mobile device data.FieldData typeclientidstringquerytimestringmarketstringdeviceplatformstringdevicemakestringdevicemodelstringstatestringcountrystringquerydwelltimedoublesessionidbigintsessionpagevieworderbigint
A Hive table namedhivesampletable, which references the data file located at/hive/warehouse/hivesampletable. The table contains some mobile device data.
hivesampletable
/hive/warehouse/hivesampletable
sessionid
In this article, you use these two datasets to test Sqoop import and export.
Set up test environment
The cluster, SQL database, and other objects are created through the Azure portal using an Azure Resource Manager template. The template can be found inAzure Quickstart Templates. The Resource Manager template calls a bacpac package to deploy the table schemas to an SQL database. If you want to use a private container for the bacpac files, use the following values in the template:
"storageKeyType": "Primary",
"storageKey": "<TheAzureStorageAccountKey>",
"storageKeyType": "Primary",
"storageKey": "<TheAzureStorageAccountKey>",
Note
Import using a template or the Azure portal only supports importing a BACPAC file from Azure blob storage.
Select the following image to open the Resource Manager template in the Azure portal.
Select the following image to open the Resource Manager template in the Azure portal.

Enter the following properties:FieldValueSubscriptionSelect your Azure subscription from the drop-down list.Resource groupSelect your resource group from the drop-down list, or create a new oneLocationSelect a region from the drop-down list.Cluster NameEnter a name for the Hadoop cluster. Use lowercase letter only.Cluster sign-in User NameKeep the prepopulated valueadmin.Cluster sign in PasswordEnter a password.Ssh User NameKeep the prepopulated valuesshuser.Ssh PasswordEnter a password.Sql Admin sign-inKeep the prepopulated valuesqluser.Sql Admin PasswordEnter a password._artifacts LocationUse the default value unless you want to use your own bacpac file in a different location._artifacts Location Sas TokenLeave blank.Bacpac File NameUse the default value unless you want to use your own bacpac file.LocationUse the default value.Thelogical SQL servername is<ClusterName>dbserver. The database name is<ClusterName>db. The default storage account name ise6qhezrh2pdqu.
Enter the following properties:
admin
sshuser
sqluser
Thelogical SQL servername is<ClusterName>dbserver. The database name is<ClusterName>db. The default storage account name ise6qhezrh2pdqu.
<ClusterName>dbserver
<ClusterName>db
e6qhezrh2pdqu
SelectI agree to the terms and conditions stated above.
SelectI agree to the terms and conditions stated above.
SelectPurchase. You see a new tile titled Submitting deployment for Template deployment. It takes about around 20 minutes to create the cluster and SQL database.
SelectPurchase. You see a new tile titled Submitting deployment for Template deployment. It takes about around 20 minutes to create the cluster and SQL database.
Run Sqoop jobs
HDInsight can run Sqoop jobs by using various methods. Use the following table to decide which method is right for you, then follow the link for a walkthrough.
Limitations
Bulk export - With Linux-based HDInsight, the Sqoop connector used to export data to Microsoft SQL Server or SQL Database doesn't currently support bulk inserts.
Batching - With Linux-based HDInsight, When using the-batchswitch when performing inserts, Sqoop performs multiple inserts instead of batching the insert operations.
-batch
Next steps
Now you've learned how to use Sqoop. To learn more, see:
Use Apache Hive with HDInsight
Upload data to HDInsight: Find other methods for uploading data to HDInsight/Azure Blob storage.
Use Apache Sqoop to import and export data between Apache Hadoop on HDInsight and SQL Database
Feedback
Was this page helpful?
Additional resources