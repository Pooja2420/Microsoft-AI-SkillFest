Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Compute configuration reference
Article
2025-02-20
2 contributors
In this article
Note
The organization of this article assumes you are using the simple form compute UI. For an overview of the simple form updates, seeUse the simple form to manage compute.
This article explains the configuration settings available when creating a new all-purpose or job compute resource. Most users create compute resources using their assigned policies, which limits the configurable settings. If you donât see a particular setting in your UI, itâs because the policy youâve selected does not allow you to configure that setting.

The configurations and management tools described in this article apply to both all-purpose and job compute. For more considerations on configuring job compute, seeConfigure compute for jobs.
Create a new all-purpose compute resource
To create a new all-purpose compute resource:
In the workspace sidebar, clickCompute.
Click theCreate computebutton.
Configure the compute resource.
ClickCreate.
You new compute resource will automatically start spinning up and be ready to use shortly.
Compute policy
Policies are a set of rules used to limit the configuration options available to users when they create compute resources. If a user doesnât have theUnrestricted cluster creationentitlement, then they can only create compute resources using their granted policies.
To create compute resources according to a policy, select a policy from thePolicydrop-down menu.
By default, all users have access to thePersonal Computepolicy, allowing them to create single-machine compute resources. If you need access to Personal Compute or any additional policies, reach out to your workspace admin.
Performance settings
The following settings appear under thePerformancesection of the simple form compute UI:
Databricks Runtime versions
Use Photon acceleration
Worker node type
Single-node compute
Enable autoscaling
Advanced performance settings
Databricks Runtime versions
Databricks Runtime is the set of core components that run on your compute. Select the runtime using theDatabricks Runtime Versiondrop-down menu. For details on specific Databricks Runtime versions, seeDatabricks Runtime release notes versions and compatibility. All versions include Apache Spark. Databricks recommends the following:
For all-purpose compute, use the most current version to ensure you have the latest optimizations and the most up-to-date compatibility between your code and preloaded packages.
For job compute running operational workloads, consider using the Long Term Support (LTS) Databricks Runtime version. Using the LTS version will ensure you donât run into compatibility issues and can thoroughly test your workload before upgrading.
For data science and machine learning use cases, consider Databricks Runtime ML version.
Use Photon acceleration
Photon is enabled by default on compute running Databricks Runtime 9.1 LTS and above.
To enable or disable Photon acceleration, select theUse Photon Accelerationcheckbox. To learn more about Photon, seeWhat is Photon?.
Worker node type
A compute resource consists of one driver node and zero or more worker nodes. You can pick separate cloud provider instance types for the driver and worker nodes, although by default the driver node uses the same instance type as the worker node. The driver node setting is underneath theAdvanced performancesection.
Different families of instance types fit different use cases, such as memory-intensive or compute-intensive workloads. You can also select a pool to use as the worker or driver node.
Important
Do not use a pool with spot instances as your driver type. Select an on-demand driver type to prevent your driver from being reclaimed. SeeConnect to pools.
In multi-node compute, worker nodes run the Spark executors and other services required for a properly functioning compute resource. When you distribute your workload with Spark, all of the distributed processing happens on worker nodes. Azure Databricks runs one executor per worker node. Therefore, the terms executor and worker are used interchangeably in the context of the Databricks architecture.
Tip
To run a Spark job, you need at least one worker node. If the compute resource has zero workers, you can run non-Spark commands on the driver node, but Spark commands will fail.
Azure Databricks launches worker nodes with two private IP addresses each. The nodeâs primary private IP address hosts Azure Databricks internal traffic. The secondary private IP address is used by the Spark container for intra-cluster communication. This model allows Azure Databricks to provide isolation between multiple compute resources in the same workspace.
For computationally challenging tasks that demand high performance, like those associated with deep learning, Azure Databricks supports compute resources that are accelerated with graphics processing units (GPUs). For more information, seeGPU-enabled compute.
Azure confidential computing VM types prevent unauthorized access to data while itâs in use, including from the cloud operator. This VM type is beneficial to highly regulated industries and regions, as well as businesses with sensitive data in the cloud. For more information on Azureâs confidential computing, seeAzure confidential computing.
To run your workloads using Azure confidential computing VMs, select from the DC or EC series VM types in the worker and driver node dropdowns. SeeAzure Confidential VM options.
Single-node compute
TheSingle nodecheckbox allows you to create a single node compute resource.
Single node compute is intended for jobs that use small amounts of data or non-distributed workloads such as single-node machine learning libraries. Multi-node compute should be used for larger jobs with distributed workloads.
A single node compute resource has the following properties:
Runs Spark locally.
Driver acts as both master and worker, with no worker nodes.
Spawns one executor thread per logical core in the compute resource, minus 1 core for the driver.
Saves allstderr,stdout, andlog4jlog outputs in the driver log.
stderr
stdout
log4j
Canât be converted to a multi-node compute resource.
Consider your use case when deciding between single or multi-node compute:
Large-scale data processing will exhaust the resources on a single node compute resource. For these workloads, Databricks recommends using multi-node compute.
Large-scale data processing will exhaust the resources on a single node compute resource. For these workloads, Databricks recommends using multi-node compute.
Single-node compute is not designed to be shared. To avoid resource conflicts, Databricks recommends using a multi-node compute resource when the compute must be shared.
Single-node compute is not designed to be shared. To avoid resource conflicts, Databricks recommends using a multi-node compute resource when the compute must be shared.
A multi-node compute resource canât be scaled to 0 workers. Use single node compute instead.
A multi-node compute resource canât be scaled to 0 workers. Use single node compute instead.
Single-node compute is not compatible with process isolation.
Single-node compute is not compatible with process isolation.
GPU scheduling is not enabled on single node compute.
GPU scheduling is not enabled on single node compute.
On single-node compute, Spark cannot read Parquet files with a UDT column. The following error message results:The Spark driver has stopped unexpectedly and is restarting. Your notebook will be automatically reattached.To work around this problem, disable the native Parquet reader:spark.conf.set("spark.databricks.io.parquet.nativeReader.enabled", False)
On single-node compute, Spark cannot read Parquet files with a UDT column. The following error message results:
The Spark driver has stopped unexpectedly and is restarting. Your notebook will be automatically reattached.
The Spark driver has stopped unexpectedly and is restarting. Your notebook will be automatically reattached.
To work around this problem, disable the native Parquet reader:
spark.conf.set("spark.databricks.io.parquet.nativeReader.enabled", False)
spark.conf.set("spark.databricks.io.parquet.nativeReader.enabled", False)
Enable autoscaling
WhenEnable autoscalingis checked, you can provide a minimum and maximum number of workers for the compute resource. Databricks then chooses the appropriate number of workers required to run your job.
To set the minimum and the maximum number of workers your compute resource will autoscale between, use theMinandMaxfields next to theWorker typedropdown.
If you donât enable autoscaling, you must enter a fixed number of workers in theWorkersfield next to theWorker typedropdown.
Note
When the compute resource is running, the compute details page displays the number of allocated workers. You can compare number of allocated workers with the worker configuration and make adjustments as needed.
With autoscaling, Azure Databricks dynamically reallocates workers to account for the characteristics of your job. Certain parts of your pipeline may be more computationally demanding than others, and Databricks automatically adds additional workers during these phases of your job (and removes them when theyâre no longer needed).
Autoscaling makes it easier to achieve high utilization because you donât need to provision the compute to match a workload. This applies especially to workloads whose requirements change over time (like exploring a dataset during the course of a day), but it can also apply to a one-time shorter workload whose provisioning requirements are unknown. Autoscaling thus offers two advantages:
Workloads can run faster compared to a constant-sized under-provisioned compute resource.
Autoscaling can reduce overall costs compared to a statically-sized compute resource.
Depending on the constant size of the compute resource and the workload, autoscaling gives you one or both of these benefits at the same time. The compute size can go below the minimum number of workers selected when the cloud provider terminates instances. In this case, Azure Databricks continuously retries to re-provision instances in order to maintain the minimum number of workers.
Note
Autoscaling is not available forspark-submitjobs.
spark-submit
Note
Compute auto-scaling has limitations scaling down cluster size for Structured Streaming workloads. Databricks recommends using DLT with enhanced autoscaling for streaming workloads. SeeOptimize the cluster utilization of DLT pipelines with Autoscaling.
Workspace on the Premium plan use optimized autoscaling. Workspaces on the standard pricing plan use standard autoscaling.
Optimized autoscaling has the following characteristics:
Scales up from min to max in 2 steps.
Can scale down, even if the compute resource is not idle, by looking at the shuffle file state.
Scales down based on a percentage of current nodes.
On job compute, scales down if the compute resource is underutilized over the last 40 seconds.
On all-purpose compute, scales down if the compute resource is underutilized over the last 150 seconds.
Thespark.databricks.aggressiveWindowDownSSpark configuration property specifies in seconds how often the compute makes down-scaling decisions. Increasing the value causes the compute to scale down more slowly. The maximum value is 600.
spark.databricks.aggressiveWindowDownS
Standard autoscaling is used in standard plan workspaces. Standard autoscaling has the following characteristics:
Starts by adding 8 nodes. Then scales up exponentially, taking as many steps as required to reach the max.
Scales down when 90% of the nodes are not busy for 10 minutes and the compute has been idle for at least 30 seconds.
Scales down exponentially, starting with 1 node.
If you are attaching your compute resource to a pool, consider the following:
Make sure the compute size requested is less than or equal to theminimum number of idle instancesin the pool. If it is larger, compute startup time will be equivalent to compute that doesnât use a pool.
Make sure the maximum compute size is less than or equal to themaximum capacityof the pool. If it is larger, the compute creation will fail.
If you reconfigure a static compute resource to autoscale, Azure Databricks immediately resizes the compute resource within the minimum and maximum bounds and then starts autoscaling. As an example, the following table demonstrates what happens to a compute resource with a certain initial size if you reconfigure the compute resource to autoscale between 5 and 10 nodes.
Advanced performance settings
The following setting appear under theAdvanced performancesection in the simple form compute UI.
Spot instances
Automatic termination
Driver type
To save cost, you can choose to usespot instances, also known as Azure Spot VMsby checking theSpot instancescheckbox.

The first instance will always be on-demand (the driver node is always on-demand) and subsequent instances will be spot instances.
If instances are evicted due to unavailability, Azure Databricks will attempt to acquire new spot instances to replace the evicted instances. If spot instances can't be acquired, on-demand instances are deployed to replace the evicted instances. This on-demand failback is only supported for spot instances that have been fully acquired and are running. Spot instances that fail during setup are not automatically replaced.
Additionally, when new nodes are added to existing compute resources, Azure Databricks attempts to acquire spot instances for those nodes.
You can set auto termination for compute under theAdvanced performancesection. During compute creation, specify an inactivity period in minutes after which you want the compute resource to terminate.
If the difference between the current time and the last command run on the compute resource is more than the inactivity period specified, Azure Databricks automatically terminates that compute. resource For more information on compute termination, seeTerminate a compute.
You can select the driver type under theAdvanced performancesection. The driver node maintains state information of all notebooks attached to the compute resource. The driver node also maintains the SparkContext, interprets all the commands you run from a notebook or a library on the compute resource, and runs the Apache Spark master that coordinates with the Spark executors.
The default value of the driver node type is the same as the worker node type. You can choose a larger driver node type with more memory if you are planning tocollect()a lot of data from Spark workers and analyze them in the notebook.
collect()
Tip
Since the driver node maintains all of the state information of the notebooks attached, make sure to detach unused notebooks from the driver node.
Tags
Tags allow you to easily monitor the cost of compute resources used by various groups in your organization. Specify tags as key-value pairs when you create compute, and Azure Databricks applies these tags to cloud resources like VMs and disk volumes, as well as the Databricks usage logs.
For compute launched from pools, the custom tags are only applied to DBU usage reports and do not propagate to cloud resources.
For detailed information about how pool and compute tag types work together, seeUse tags to attribute and track usage
To add tags to your compute resource:
In theTagssection, add a key-value pair for each custom tag.
ClickAdd.
Advanced settings
The following settings appear under theAdvancedsection of the simple form compute UI:
Access modes
Enable autoscaling local storage
Local disk encryption
Spark configuration
SSH access to compute
Environment variables
Compute log delivery
Access modes
Access mode is a security feature that determines who can use the compute resource and the data they can access using the compute resource. Every compute resource in Azure Databricks has an access mode. Access mode settings are found under theAdvancedsection of the simple form compute UI.
Access mode selection isAutoby default, meaning the access mode is automatically chosen for you based on your selected Databricks Runtime. Machine learning runtimes and Databricks Runtimes lower than 14.3 default toDedicated, otherwiseStandardis used.
Databricks recommends that you use standard access mode for all workloads. Use dedicated access mode only if your required functionality is not supported by standard access mode.
For detailed information about the functionality support for each of these access modes, seeCompute access mode limitations for Unity Catalog.
Note
In Databricks Runtime 13.3 LTS and above, init scripts and libraries are supported by all access modes. Requirements and levels of support vary. SeeWhere can init scripts be installed?andCompute-scoped libraries.
Enable autoscaling local storage
It can often be difficult to estimate how much disk space a particular job will take. To save you
from having to estimate how many gigabytes of managed disk to attach to your compute at creation
time, Azure Databricks automatically enables autoscaling local storage on all Azure Databricks compute.
With autoscaling local storage, Azure Databricks monitors the amount of free disk space available on your
computeâs Spark workers. If a worker begins to run too low on disk, Databricks automatically
attaches a new managed disk to the worker before it runs out of disk space. Disks are attached up to
a limit of 5 TB of total disk space per virtual machine (including the virtual machineâs initial
local storage).
The managed disks attached to a virtual machine are detached only when the virtual machine is
returned to Azure. That is, managed disks are never detached from a virtual machine as long as they are
part of a running compute. To scale down managed disk usage, Azure Databricks recommends using this
feature in compute configured withautoscaling computeorautomatic termination.
Local disk encryption
Important
This feature is inPublic Preview.
Some instance types you use to run compute may have locally attached disks. Azure Databricks may store shuffle data or ephemeral data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data that is stored temporarily on your compute resourceâs local disks, you can enable local disk encryption.
Important
Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes.
When local disk encryption is enabled, Azure Databricks generates an encryption key locally that is unique to each compute node and is used to encrypt all data stored on local disks. The scope of the key is local to each compute node and is destroyed along with the compute node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk.
To enable local disk encryption, you must use theClusters API. During compute creation or edit, setenable_local_disk_encryptiontotrue.
enable_local_disk_encryption
true
Spark configuration
To fine-tune Spark jobs, you can provide customSpark configuration properties.
On the compute configuration page, click theAdvancedtoggle.
On the compute configuration page, click theAdvancedtoggle.
Click theSparktab.InSpark config, enter the configuration properties as one key-value pair per line.
Click theSparktab.

InSpark config, enter the configuration properties as one key-value pair per line.
When you configure compute using theClusters API, set Spark properties in thespark_conffield in thecreate cluster APIorUpdate cluster API.
spark_conf
To enforce Spark configurations on compute, workspace admins can usecompute policies.
Databricks recommends storing sensitive information, such as passwords, in asecretinstead of plaintext. To reference a secret in the Spark configuration, use the following syntax:
spark.<property-name> {{secrets/<scope-name>/<secret-name>}}
spark.<property-name> {{secrets/<scope-name>/<secret-name>}}
For example, to set a Spark configuration property calledpasswordto the value of the secret stored insecrets/acme_app/password:
password
secrets/acme_app/password
spark.password {{secrets/acme-app/password}}
spark.password {{secrets/acme-app/password}}
For more information, seeManage secrets.
SSH access to compute
For security reasons, in Azure Databricks the SSH port is closed by default. If you want to enable SSH access to your Spark clusters, seeSSH to the driver node.
Note
SSH can be enabled only if your workspace is deployed in your ownAzure virtual network.
Environment variables
Configure custom environment variables that you can access frominit scriptsrunning on the compute resource. Databricks also provides predefinedenvironment variablesthat you can use in init scripts. You cannot override these predefined environment variables.
On the compute configuration page, click theAdvancedtoggle.
On the compute configuration page, click theAdvancedtoggle.
Click theSparktab.
Click theSparktab.
Set the environment variables in theEnvironment variablesfield.
Set the environment variables in theEnvironment variablesfield.

You can also set environment variables using thespark_env_varsfield in theCreate cluster APIorUpdate cluster API.
spark_env_vars
Compute log delivery
When you create an all-purpose or jobs compute, you can specify a location to deliver the cluster logs for the Spark driver node, worker nodes, and events. Logs are delivered every five minutes and archived hourly in your chosen destination. Databricks will deliver all logs generated up until the compute resource is terminated.
To configure the log delivery location:
On the compute page, click theAdvancedtoggle.
Click theLoggingtab.
Select a destination type.
Enter theLog path.
To store the logs, Databricks creates a subfolder in your chosen log path named after the compute'scluster_id.
cluster_id
For example, if the specified log path is/Volumes/catalog/schema/volume, logs for06308418893214are delivered to/Volumes/catalog/schema/volume/06308418893214.
/Volumes/catalog/schema/volume
06308418893214
/Volumes/catalog/schema/volume/06308418893214
Note
Delivering logs to volumes is inPublic Previewand is only supported on Unity-Catalog-enabled compute withStandardaccess mode orDedicatedaccess mode assigned to a user. This feature is not supported on compute withDedicatedaccess mode assigned to a group. If you select a volume as the path, ensure you have theREAD VOLUMEandWRITE VOLUMEpermissions on the volume. SeeWhat are the privileges for volumes?.
READ VOLUME
WRITE VOLUME
Note
This feature is also available in the REST API. See theClusters API.
Feedback
Was this page helpful?
Additional resources