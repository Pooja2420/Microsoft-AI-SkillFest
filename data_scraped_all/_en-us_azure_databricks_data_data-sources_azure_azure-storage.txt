Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Connect to Azure Data Lake Storage and Blob Storage
Article
2025-03-17
2 contributors
In this article
Note
This article describes legacy patterns for configuring access to Azure Data Lake Storage. Databricks recommends using Unity Catalog to configure access to Azure Data Lake Storage and volumes for direct interaction with files. SeeConnect to cloud object storage and services using Unity Catalog.
This article explains how to connect to Azure Data Lake Storage and Blob Storage from Azure Databricks.
Note
The legacy Windows Azure Storage Blob driver (WASB) has been deprecated. ABFS has numerous benefits over WASB. SeeAzure documentation on ABFS. For documentation for working with the legacy WASB driver, seeConnect to Azure Blob Storage with WASB (legacy).
Connect to Azure Data Lake Storage or Blob Storage using Azure credentials
The following credentials can be used to access Azure Data Lake Storage or Blob Storage:
OAuth 2.0 with a Microsoft Entra ID service principal: Databricks recommends using Microsoft Entra ID service principals to connect to Azure Data Lake Storage. To create a Microsoft Entra ID service principal and provide it access to Azure storage accounts, seeAccess storage using a service principal & Microsoft Entra ID(Azure Active Directory).To create a Microsoft Entra ID service principal, you must have theApplication Administratorrole or theApplication.ReadWrite.Allpermission in Microsoft Entra ID. To assign roles on a storage account you must be an Owner or a user with the User Access Administrator Azure RBAC role on the storage account.ImportantBlob storage does not support Microsoft Entra ID service principals.
OAuth 2.0 with a Microsoft Entra ID service principal: Databricks recommends using Microsoft Entra ID service principals to connect to Azure Data Lake Storage. To create a Microsoft Entra ID service principal and provide it access to Azure storage accounts, seeAccess storage using a service principal & Microsoft Entra ID(Azure Active Directory).
To create a Microsoft Entra ID service principal, you must have theApplication Administratorrole or theApplication.ReadWrite.Allpermission in Microsoft Entra ID. To assign roles on a storage account you must be an Owner or a user with the User Access Administrator Azure RBAC role on the storage account.
Application Administrator
Application.ReadWrite.All
Important
Blob storage does not support Microsoft Entra ID service principals.
Shared access signatures (SAS): You can use storageSAS tokensto access Azure storage. With SAS, you can restrict access to a storage account using temporary tokens with fine-grained access control.You can only grant a SAS token permissions that you have on the storage account, container, or file yourself.
Shared access signatures (SAS): You can use storageSAS tokensto access Azure storage. With SAS, you can restrict access to a storage account using temporary tokens with fine-grained access control.
You can only grant a SAS token permissions that you have on the storage account, container, or file yourself.
Account keys: You can usestorage account access keysto manage access to Azure Storage. Storage account access keys provide full access to the configuration of a storage account, as well as the data. Databricks recommends using a Microsoft Entra ID service principal or a SAS token to connect to Azure storage instead of account keys.To view an accountâs access keys, you must have the Owner, Contributor, or Storage Account Key Operator Service role on the storage account.
Account keys: You can usestorage account access keysto manage access to Azure Storage. Storage account access keys provide full access to the configuration of a storage account, as well as the data. Databricks recommends using a Microsoft Entra ID service principal or a SAS token to connect to Azure storage instead of account keys.
To view an accountâs access keys, you must have the Owner, Contributor, or Storage Account Key Operator Service role on the storage account.
Databricks recommends using secret scopes for storing all credentials. You can grant users, service principals, and groups in your workspace access to read the secret scope. This protects the Azure credentials while allowing users to access Azure storage. To create a secret scope, seeManage secret scopes.
Set Spark properties to configure Azure credentials to access Azure storage
You can set Spark properties to configure a Azure credentials to access Azure storage. The credentials can be scoped to either a cluster or a notebook. Use both cluster access control and notebook access control together to protect access to Azure storage. SeeCompute permissionsandCollaborate using Databricks notebooks.
Note
Microsoft Entra ID service principals can also be used to access Azure storage from a SQL warehouse, seeData access configurations.
To set Spark properties, use the following snippet in a clusterâs Spark configuration or a notebook:
Use the following format to set the cluster Spark configuration:
spark.hadoop.fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net OAuth
spark.hadoop.fs.azure.account.oauth.provider.type.<storage-account>.dfs.core.windows.net org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider
spark.hadoop.fs.azure.account.oauth2.client.id.<storage-account>.dfs.core.windows.net <application-id>
spark.hadoop.fs.azure.account.oauth2.client.secret.<storage-account>.dfs.core.windows.net {{secrets/<secret-scope>/<service-credential-key>}}
spark.hadoop.fs.azure.account.oauth2.client.endpoint.<storage-account>.dfs.core.windows.net https://login.microsoftonline.com/<directory-id>/oauth2/token
spark.hadoop.fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net OAuth
spark.hadoop.fs.azure.account.oauth.provider.type.<storage-account>.dfs.core.windows.net org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider
spark.hadoop.fs.azure.account.oauth2.client.id.<storage-account>.dfs.core.windows.net <application-id>
spark.hadoop.fs.azure.account.oauth2.client.secret.<storage-account>.dfs.core.windows.net {{secrets/<secret-scope>/<service-credential-key>}}
spark.hadoop.fs.azure.account.oauth2.client.endpoint.<storage-account>.dfs.core.windows.net https://login.microsoftonline.com/<directory-id>/oauth2/token
You can usespark.conf.setin notebooks, as shown in the following example:
spark.conf.set
service_credential = dbutils.secrets.get(scope="<secret-scope>",key="<service-credential-key>")

spark.conf.set("fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net", "OAuth")
spark.conf.set("fs.azure.account.oauth.provider.type.<storage-account>.dfs.core.windows.net", "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider")
spark.conf.set("fs.azure.account.oauth2.client.id.<storage-account>.dfs.core.windows.net", "<application-id>")
spark.conf.set("fs.azure.account.oauth2.client.secret.<storage-account>.dfs.core.windows.net", service_credential)
spark.conf.set("fs.azure.account.oauth2.client.endpoint.<storage-account>.dfs.core.windows.net", "https://login.microsoftonline.com/<directory-id>/oauth2/token")
service_credential = dbutils.secrets.get(scope="<secret-scope>",key="<service-credential-key>")

spark.conf.set("fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net", "OAuth")
spark.conf.set("fs.azure.account.oauth.provider.type.<storage-account>.dfs.core.windows.net", "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider")
spark.conf.set("fs.azure.account.oauth2.client.id.<storage-account>.dfs.core.windows.net", "<application-id>")
spark.conf.set("fs.azure.account.oauth2.client.secret.<storage-account>.dfs.core.windows.net", service_credential)
spark.conf.set("fs.azure.account.oauth2.client.endpoint.<storage-account>.dfs.core.windows.net", "https://login.microsoftonline.com/<directory-id>/oauth2/token")
Replace
<secret-scope>with the Databricks secret scope name.
<secret-scope>
<service-credential-key>with the name of the key containing the client secret.
<service-credential-key>
<storage-account>with the name of the Azure storage account.
<storage-account>
<application-id>with theApplication (client) IDfor the Microsoft Entra ID application.
<application-id>
<directory-id>with theDirectory (tenant) IDfor the Microsoft Entra ID application.
<directory-id>
You can configure SAS tokens for multiple storage accounts in the same Spark session.
spark.conf.set("fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net", "SAS")
spark.conf.set("fs.azure.sas.token.provider.type.<storage-account>.dfs.core.windows.net", "org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider")
spark.conf.set("fs.azure.sas.fixed.token.<storage-account>.dfs.core.windows.net", dbutils.secrets.get(scope="<scope>", key="<sas-token-key>"))
spark.conf.set("fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net", "SAS")
spark.conf.set("fs.azure.sas.token.provider.type.<storage-account>.dfs.core.windows.net", "org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider")
spark.conf.set("fs.azure.sas.fixed.token.<storage-account>.dfs.core.windows.net", dbutils.secrets.get(scope="<scope>", key="<sas-token-key>"))
Replace
<storage-account>with the Azure Storage account name.
<storage-account>
<scope>with the Azure Databricks secret scope name.
<scope>
<sas-token-key>with the name of the key containing the Azure storage SAS token.
<sas-token-key>
spark.conf.set(
    "fs.azure.account.key.<storage-account>.dfs.core.windows.net",
    dbutils.secrets.get(scope="<scope>", key="<storage-account-access-key>"))
spark.conf.set(
    "fs.azure.account.key.<storage-account>.dfs.core.windows.net",
    dbutils.secrets.get(scope="<scope>", key="<storage-account-access-key>"))
Replace
<storage-account>with the Azure Storage account name.
<storage-account>
<scope>with the Azure Databricks secret scope name.
<scope>
<storage-account-access-key>with the name of the key containing the Azure storage account access key.
<storage-account-access-key>
Access Azure storage
Once you have properly configured credentials to access your Azure storage container, you can interact with resources in the storage account using URIs. Databricks recommends using theabfssdriver for greater security.
abfss
spark.read.load("abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-data>")

dbutils.fs.ls("abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-data>")
spark.read.load("abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-data>")

dbutils.fs.ls("abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-data>")
CREATE TABLE <database-name>.<table-name>;

COPY INTO <database-name>.<table-name>
FROM 'abfss://container@storageAccount.dfs.core.windows.net/path/to/folder'
FILEFORMAT = CSV
COPY_OPTIONS ('mergeSchema' = 'true');
CREATE TABLE <database-name>.<table-name>;

COPY INTO <database-name>.<table-name>
FROM 'abfss://container@storageAccount.dfs.core.windows.net/path/to/folder'
FILEFORMAT = CSV
COPY_OPTIONS ('mergeSchema' = 'true');
Example notebook
Get notebook
Azure Data Lake Storage known issues
If you try accessing a storage container created through the Azure portal, you might receive the following error:
StatusCode=404
StatusDescription=The specified filesystem does not exist.
ErrorCode=FilesystemNotFound
ErrorMessage=The specified filesystem does not exist.
StatusCode=404
StatusDescription=The specified filesystem does not exist.
ErrorCode=FilesystemNotFound
ErrorMessage=The specified filesystem does not exist.
When a hierarchical namespace is enabled, you donât need to create containers through Azure portal. If you see this issue, delete the Blob container through Azure portal. After a few minutes, you can access the container. Alternatively, you can change yourabfssURI to use a different container, as long as this container is not created through Azure portal.
abfss
SeeKnown issues with Azure Data Lake Storagein the Microsoft documentation.
Deprecated patterns for storing and accessing data from Azure Databricks
The following are deprecated storage patterns:
Databricks no longer recommends mounting external data locations to Databricks Filesystem. SeeMounting cloud object storage on Azure Databricks.
Databricks no longer recommends using credential passthrough with Azure Data Lake Storage. SeeAccess Azure Data Lake Storage using Microsoft Entra ID credential passthrough (legacy).
Feedback
Was this page helpful?
Additional resources