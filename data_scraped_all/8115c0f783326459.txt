Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Copy data to or from a file system by using Azure Data Factory or Azure Synapse Analytics
Article
2024-09-25
23 contributors
In this article
APPLIES TO:Azure Data FactoryAzure Synapse Analytics
Tip
Try outData Factory in Microsoft Fabric, an all-in-one analytics solution for enterprises.Microsoft Fabriccovers everything from data movement to data science, real-time analytics, business intelligence, and reporting. Learn how tostart a new trialfor free!
This article outlines how to copy data to and from file system. To learn more, read the introductory article forAzure Data FactoryorAzure Synapse Analytics.
Supported capabilities
This file system connector is supported for the following capabilities:
â  Azure integration runtime â¡ Self-hosted integration runtime
Specifically, this file system connector supports:
Copying files from/to network file share. To use a Linux file share, installSambaon your Linux server.
Copying files usingWindowsauthentication.
Copying files as-is or parsing/generating files with thesupported file formats and compression codecs.
Prerequisites
If your data store is located inside an on-premises network, an Azure virtual network, or Amazon Virtual Private Cloud, you need to configure aself-hosted integration runtimeto connect to it.
If your data store is a managed cloud data service, you can use the Azure Integration Runtime. If the access is restricted to IPs that are approved in the firewall rules, you can addAzure Integration Runtime IPsto the allow list.
You can also use themanaged virtual network integration runtimefeature in Azure Data Factory to access the on-premises network without installing and configuring a self-hosted integration runtime.
For more information about the network security mechanisms and options supported by Data Factory, seeData access strategies.
Getting started
To perform the Copy activity with a pipeline, you can use one of the following tools or SDKs:
The Copy Data tool
The Azure portal
The .NET SDK
The Python SDK
Azure PowerShell
The REST API
The Azure Resource Manager template
Create a file system linked service using UI
Use the following steps to create a file system linked service in the Azure portal UI.
Browse to the Manage tab in your Azure Data Factory or Synapse workspace and select Linked Services, then select New:Azure Data FactoryAzure Synapse
Browse to the Manage tab in your Azure Data Factory or Synapse workspace and select Linked Services, then select New:
Azure Data Factory
Azure Synapse


Search for file and select the File System connector.
Search for file and select the File System connector.

Configure the service details, test the connection, and create the new linked service.
Configure the service details, test the connection, and create the new linked service.

Connector configuration details
The following sections provide details about properties that are used to define Data Factory and Synapse pipeline entities specific to file system.
Linked service properties
The following properties are supported for file system linked service:
Sample linked service and dataset definitions
\\\\myserver\\share
\\myserver\share
.\\
folder\\subfolder
.\
folder\subfolder
Note
When authoring via UI, you don't need to input double backslash (\\) to escape like you do via JSON, specify single backslash.
\\
Note
Copying files from local machine is not supported under Azure Integration Runtime.Refer to the command line fromhereto enable the access to the local machine under Self-hosted integration runtime. By default, it's disabled.
Example:
{
    "name": "FileLinkedService",
    "properties": {
        "type": "FileServer",
        "typeProperties": {
            "host": "<host>",
            "userId": "<domain>\\<user>",
            "password": {
                "type": "SecureString",
                "value": "<password>"
            }
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
{
    "name": "FileLinkedService",
    "properties": {
        "type": "FileServer",
        "typeProperties": {
            "host": "<host>",
            "userId": "<domain>\\<user>",
            "password": {
                "type": "SecureString",
                "value": "<password>"
            }
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
Dataset properties
For a full list of sections and properties available for defining datasets, see theDatasetsarticle.
Azure Data Factory supports the following file formats. Refer to each article for format-based settings.
Avro format
Binary format
Delimited text format
Excel format
JSON format
ORC format
Parquet format
XML format
The following properties are supported for file system underlocationsettings in format-based dataset:
location
location
Example:
{
    "name": "DelimitedTextDataset",
    "properties": {
        "type": "DelimitedText",
        "linkedServiceName": {
            "referenceName": "<File system linked service name>",
            "type": "LinkedServiceReference"
        },
        "schema": [ < physical schema, optional, auto retrieved during authoring > ],
        "typeProperties": {
            "location": {
                "type": "FileServerLocation",
                "folderPath": "root/folder/subfolder"
            },
            "columnDelimiter": ",",
            "quoteChar": "\"",
            "firstRowAsHeader": true,
            "compressionCodec": "gzip"
        }
    }
}
{
    "name": "DelimitedTextDataset",
    "properties": {
        "type": "DelimitedText",
        "linkedServiceName": {
            "referenceName": "<File system linked service name>",
            "type": "LinkedServiceReference"
        },
        "schema": [ < physical schema, optional, auto retrieved during authoring > ],
        "typeProperties": {
            "location": {
                "type": "FileServerLocation",
                "folderPath": "root/folder/subfolder"
            },
            "columnDelimiter": ",",
            "quoteChar": "\"",
            "firstRowAsHeader": true,
            "compressionCodec": "gzip"
        }
    }
}
Copy activity properties
For a full list of sections and properties available for defining activities, see thePipelinesarticle. This section provides a list of properties supported by file system source and sink.
File system as source
Azure Data Factory supports the following file formats. Refer to each article for format-based settings.
Avro format
Binary format
Delimited text format
Excel format
JSON format
ORC format
Parquet format
XML format
The following properties are supported for file system understoreSettingssettings in format-based copy source:
storeSettings
storeSettings
wildcardFileName
*
*
?
*
?
^
*
?
^
fileListPath
modifiedDatetimeStart
modifiedDatetimeEnd
modifiedDatetimeStart
modifiedDatetimeEnd
modifiedDatetimeEnd
modifiedDatetimeStart
fileListPath
month
day
Example:
"activities":[
    {
        "name": "CopyFromFileSystem",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "<Delimited text input dataset name>",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "<output dataset name>",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "DelimitedTextSource",
                "formatSettings":{
                    "type": "DelimitedTextReadSettings",
                    "skipLineCount": 10
                },
                "storeSettings":{
                    "type": "FileServerReadSettings",
                    "recursive": true,
                    "wildcardFolderPath": "myfolder*A",
                    "wildcardFileName": "*.csv"
                }
            },
            "sink": {
                "type": "<sink type>"
            }
        }
    }
]
"activities":[
    {
        "name": "CopyFromFileSystem",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "<Delimited text input dataset name>",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "<output dataset name>",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "DelimitedTextSource",
                "formatSettings":{
                    "type": "DelimitedTextReadSettings",
                    "skipLineCount": 10
                },
                "storeSettings":{
                    "type": "FileServerReadSettings",
                    "recursive": true,
                    "wildcardFolderPath": "myfolder*A",
                    "wildcardFileName": "*.csv"
                }
            },
            "sink": {
                "type": "<sink type>"
            }
        }
    }
]
File system as sink
Azure Data Factory supports the following file formats. Refer to each article for format-based settings.
Avro format
Binary format
Delimited text format
JSON format
ORC format
Parquet format
Note
TheMergeFilescopyBehavioroption is only available in Azure Data Factory pipelines and not Synapse Analytics pipelines.
The following properties are supported for file system understoreSettingssettings in format-based copy sink:
storeSettings
storeSettings
Example:
"activities":[
    {
        "name": "CopyToFileSystem",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "<input dataset name>",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "<Parquet output dataset name>",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "<source type>"
            },
            "sink": {
                "type": "ParquetSink",
                "storeSettings":{
                    "type": "FileServerWriteSettings",
                    "copyBehavior": "PreserveHierarchy"
                }
            }
        }
    }
]
"activities":[
    {
        "name": "CopyToFileSystem",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "<input dataset name>",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "<Parquet output dataset name>",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "<source type>"
            },
            "sink": {
                "type": "ParquetSink",
                "storeSettings":{
                    "type": "FileServerWriteSettings",
                    "copyBehavior": "PreserveHierarchy"
                }
            }
        }
    }
]
Folder and file filter examples
This section describes the resulting behavior of the folder path and file name with wildcard filters.
Folder*
Folder*
Folder*
*.csv
Folder*
*.csv
File list examples
This section describes the resulting behavior of using file list path in copy activity source.
Assuming you have the following source folder structure and want to copy the files in bold:
root/FolderA
root/Metadata/FileListToCopy.txt
recursive and copyBehavior examples
This section describes the resulting behavior of the Copy operation for different combinations of recursive and copyBehavior values.
Lookup activity properties
To learn details about the properties, checkLookup activity.
GetMetadata activity properties
To learn details about the properties, checkGetMetadata activity.
Delete activity properties
To learn details about the properties, checkDelete activity.
Legacy models
Note
The following models are still supported as-is for backward compatibility. You are suggested to use the new model mentioned in above sections going forward, and the authoring UI has switched to generating the new model.
Legacy dataset model
*
?
^
*
?
"fileName": "*.csv"
"fileName": "???20180427.txt"
^
modifiedDatetimeStart
modifiedDatetimeEnd
modifiedDatetimeStart
modifiedDatetimeEnd
modifiedDatetimeEnd
modifiedDatetimeStart
modifiedDatetimeStart
modifiedDatetimeEnd
modifiedDatetimeStart
modifiedDatetimeEnd
modifiedDatetimeEnd
modifiedDatetimeStart
Tip
To copy all files under a folder, specifyfolderPathonly.To copy a single file with a given name, specifyfolderPathwith folder part andfileNamewith file name.To copy a subset of files under a folder, specifyfolderPathwith folder part andfileNamewith wildcard filter.
Note
If you were using "fileFilter" property for file filter, it is still supported as-is, while you are suggested to use the new filter capability added to "fileName" going forward.
Example:
{
    "name": "FileSystemDataset",
    "properties": {
        "type": "FileShare",
        "linkedServiceName":{
            "referenceName": "<file system linked service name>",
            "type": "LinkedServiceReference"
        },
        "typeProperties": {
            "folderPath": "folder/subfolder/",
            "fileName": "*",
            "modifiedDatetimeStart": "2018-12-01T05:00:00Z",
            "modifiedDatetimeEnd": "2018-12-01T06:00:00Z",
            "format": {
                "type": "TextFormat",
                "columnDelimiter": ",",
                "rowDelimiter": "\n"
            },
            "compression": {
                "type": "GZip",
                "level": "Optimal"
            }
        }
    }
}
{
    "name": "FileSystemDataset",
    "properties": {
        "type": "FileShare",
        "linkedServiceName":{
            "referenceName": "<file system linked service name>",
            "type": "LinkedServiceReference"
        },
        "typeProperties": {
            "folderPath": "folder/subfolder/",
            "fileName": "*",
            "modifiedDatetimeStart": "2018-12-01T05:00:00Z",
            "modifiedDatetimeEnd": "2018-12-01T06:00:00Z",
            "format": {
                "type": "TextFormat",
                "columnDelimiter": ",",
                "rowDelimiter": "\n"
            },
            "compression": {
                "type": "GZip",
                "level": "Optimal"
            }
        }
    }
}
Legacy copy activity source model
Example:
"activities":[
    {
        "name": "CopyFromFileSystem",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "<file system input dataset name>",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "<output dataset name>",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "FileSystemSource",
                "recursive": true
            },
            "sink": {
                "type": "<sink type>"
            }
        }
    }
]
"activities":[
    {
        "name": "CopyFromFileSystem",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "<file system input dataset name>",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "<output dataset name>",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "FileSystemSource",
                "recursive": true
            },
            "sink": {
                "type": "<sink type>"
            }
        }
    }
]
Legacy copy activity sink model
Example:
"activities":[
    {
        "name": "CopyToFileSystem",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "<input dataset name>",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "<file system output dataset name>",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "<source type>"
            },
            "sink": {
                "type": "FileSystemSink",
                "copyBehavior": "PreserveHierarchy"
            }
        }
    }
]
"activities":[
    {
        "name": "CopyToFileSystem",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "<input dataset name>",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "<file system output dataset name>",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "<source type>"
            },
            "sink": {
                "type": "FileSystemSink",
                "copyBehavior": "PreserveHierarchy"
            }
        }
    }
]
Related content
For a list of data stores supported as sources and sinks by the copy activity, seesupported data stores.
Feedback
Was this page helpful?
Additional resources