Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
What is Apache Spark in Azure Synapse Analytics?
Article
2024-11-08
12 contributors
In this article
Apache Spark is a parallel processing framework that supports in-memory processing to boost the performance of big data analytic applications. Apache Spark in Azure Synapse Analytics is one of Microsoft's implementations of Apache Spark in the cloud. Azure Synapse makes it easy to create and configure a serverless Apache Spark pool in Azure. Spark pools in Azure Synapse are compatible with Azure Storage and Azure Data Lake Generation 2 Storage. So you can use Spark pools to process your data stored in Azure.

What is Apache Spark
Apache Spark provides primitives for in-memory cluster computing. A Spark job can load and cache data into memory and query it repeatedly. In-memory computing is faster than disk-based applications. Spark also integrates withÂ multiple programming languages to let you manipulate distributed data sets like local collections. There's no need to structure everything as map and reduce operations. You can learn more from theApache Spark for Synapse video.

Spark pools in Azure Synapse offer a fully managed Spark service. The benefits of creating a Spark pool in Azure Synapse Analytics are listed here.
Spark pools in Azure Synapse include the following components that are available on the pools by default:
Spark Core. Includes Spark Core, Spark SQL, GraphX, and MLlib.
Anaconda
Apache Livy
nteract notebook
Spark pool architecture
Spark applications run as independent sets of processes on a pool, coordinated by theSparkContextobject in your main program, called thedriver program.
SparkContext
TheSparkContextcan connect to the cluster manager, which allocates resources across applications. The cluster manager isApache Hadoop YARN. Once connected, Spark acquires executors on nodes in the pool, which are processes that run computations and store data for your application. Next, it sends your application code, defined by JAR or Python files passed toSparkContext, to the executors. Finally,SparkContextsends tasks to the executors to run.
SparkContext
SparkContext
SparkContext
TheSparkContextruns the user's main function and executes the various parallel operations on the nodes. Then, theSparkContextcollects the results of the operations. The nodes read and write data from and to the file system. The nodes also cache transformed data in-memory as Resilient Distributed Datasets (RDDs).
SparkContext
SparkContext
TheSparkContextconnects to the Spark pool and is responsible for converting an application to a directed acyclic graph (DAG). The graph consists of individual tasks that run within an executor process on the nodes. Each application gets its own executor processes, which stay up during the whole application and run tasks in multiple threads.
SparkContext
Apache Spark in Azure Synapse Analytics use cases
Spark pools in Azure Synapse Analytics enable the following key scenarios:
Data Engineering/Data Preparation
Apache Spark includes many language features to support preparation and processing of large volumes of data so that it can be made more valuable and then consumed by other services within Azure Synapse Analytics. This is enabled through multiple languages (C#, Scala, PySpark, Spark SQL) and supplied libraries for processing and connectivity.
Machine Learning
Apache Spark comes withMLlib, a machine learning library built on top of Spark that you can use from a Spark pool in Azure Synapse Analytics. Spark pools in Azure Synapse Analytics also include Anaconda, a Python distribution with various packages for data science including machine learning. When combined with built-in support for notebooks, you have an environment for creating machine learning applications.
Streaming Data
Synapse Spark supports Spark structured streaming as long as you're running supported version of Azure Synapse Spark runtime release. All jobs are supported to live for seven days. This applies to both batch and streaming jobs, and generally, customers automate restart process using Azure Functions.
Related content
Use the following articles to learn more about Apache Spark in Azure Synapse Analytics:
Quickstart: Create a Spark pool in Azure Synapse
Quickstart: Create an Apache Spark notebook
Tutorial: Machine learning using Apache Spark
Note
Some of the official Apache Spark documentation relies on using the Spark console, which is not available on Azure Synapse Spark. Use the notebook or IntelliJ experiences instead.
Feedback
Was this page helpful?
Additional resources