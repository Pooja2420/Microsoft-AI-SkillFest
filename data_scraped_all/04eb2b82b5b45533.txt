Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Use external Hive Metastore for Synapse Spark Pool
Article
2025-01-17
8 contributors
In this article
Note
External Hive metastores will no longer be supported in subsequent versions afterAzure Synapse Runtime for Apache Spark 3.4in Synapse.
Azure Synapse Analytics allows Apache Spark pools in the same workspace to share a managed HMS (Hive Metastore) compatible metastore as their catalog. When customers want to persist the Hive catalog metadata outside of the workspace, and share catalog objects with other computational engines outside of the workspace, such as HDInsight and Azure Databricks, they can connect to an external Hive Metastore. In this article, you can learn how to connect Synapse Spark to an external Apache Hive Metastore.
Supported Hive Metastore versions
The feature works with Spark 3.3. The following table shows the supported Hive Metastore versions for each Spark version.
Set up linked service to Hive Metastore
Note
OnlyAzure SQL DatabaseandAzure Database for MySQLare supported as an external Hive Metastore. SQL(username-password) authentication is supported for both kinds of databases. Additionally, system-sssigned managed identity authentication is supported only for Azure SQL Database and Spark 3.4+. If the provided database is blank, please provision it viaHive Schema Toolto create database schema.
Follow below steps to set up a linked service to the external Hive Metastore in Synapse workspace.
Azure SQL Database
Azure Database for MySQL
Open Synapse Studio, go toManage > Linked servicesat left, clickNewto create a new linked service.
Open Synapse Studio, go toManage > Linked servicesat left, clickNewto create a new linked service.

ChooseAzure SQL Database, clickContinue.
ChooseAzure SQL Database, clickContinue.
ProvideNameof the linked service. Record the name of the linked service, this info will be used to configure Spark shortly.
ProvideNameof the linked service. Record the name of the linked service, this info will be used to configure Spark shortly.
ChooseLegacyversion and selectConnection String.
ChooseLegacyversion and selectConnection String.
Either selectAzure SQL Databasefor the external Hive Metastore from Azure subscription list, or enter the info manually.
Either selectAzure SQL Databasefor the external Hive Metastore from Azure subscription list, or enter the info manually.
SetAuthentication typeas one ofSQL AuthenticationorSystem-assigned managed identity. ForSQL Authentication, provideUser nameandPasswordto set up the connection. ForSystem-assigned managed identity, the page will automatically populate the management identity associated with the current workspace.
SetAuthentication typeas one ofSQL AuthenticationorSystem-assigned managed identity. ForSQL Authentication, provideUser nameandPasswordto set up the connection. ForSystem-assigned managed identity, the page will automatically populate the management identity associated with the current workspace.
SQL Authentication
System-assigned managed identity
SQL Authentication
System-assigned managed identity
Test connectionto verify the authentication.
Test connectionto verify the authentication.
ClickCreateto create the linked service.
ClickCreateto create the linked service.
Open Synapse Studio, go toManage > Linked servicesat left, clickNewto create a new linked service.
Open Synapse Studio, go toManage > Linked servicesat left, clickNewto create a new linked service.

ChooseAzure Database for MySQL, clickContinue.
ChooseAzure Database for MySQL, clickContinue.
ProvideNameof the linked service. Record the name of the linked service, this info will be used to configure Spark shortly.
ProvideNameof the linked service. Record the name of the linked service, this info will be used to configure Spark shortly.
Either selectAzure Database for MySQLfor the external Hive Metastore from Azure subscription list, or enter the info manually.
Either selectAzure Database for MySQLfor the external Hive Metastore from Azure subscription list, or enter the info manually.
ProvideUser nameandPasswordto set up the connection.
ProvideUser nameandPasswordto set up the connection.
Test connectionto verify the username and password.
Test connectionto verify the username and password.
ClickCreateto create the linked service.
ClickCreateto create the linked service.
Test connection and get the metastore version in notebook
Some network security rule settings could block access from Spark pool to the external Hive Metastore DB. Before you configure the Spark pool, run below code in any Spark pool notebook to test connection to the external Hive Metastore DB.
You can also get your Hive Metastore version from the output results. The Hive Metastore version will be used in the Spark configuration.
Warning
Don't publish the test scripts in your notebook with your password hardcoded as this could cause a potential security risk for your Hive Metastore.
%%spark 
import java.sql.DriverManager 
/** this JDBC url could be copied from Azure portal > Azure SQL database > Connection strings > JDBC **/ 
val url = s"jdbc:sqlserver://{your_servername_here}.database.windows.net:1433;database={your_database_here};user={your_username_here};password={your_password_here};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;" 
try { 
    val connection = DriverManager.getConnection(url) 
    val result = connection.createStatement().executeQuery("select t.SCHEMA_VERSION from VERSION t") 
    result.next(); 
    println(s"Successful to test connection. Hive Metastore version is ${result.getString(1)}") 
} catch { 
    case ex: Throwable => println(s"Failed to establish connection:\n $ex") 
}
%%spark 
import java.sql.DriverManager 
/** this JDBC url could be copied from Azure portal > Azure SQL database > Connection strings > JDBC **/ 
val url = s"jdbc:sqlserver://{your_servername_here}.database.windows.net:1433;database={your_database_here};user={your_username_here};password={your_password_here};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;" 
try { 
    val connection = DriverManager.getConnection(url) 
    val result = connection.createStatement().executeQuery("select t.SCHEMA_VERSION from VERSION t") 
    result.next(); 
    println(s"Successful to test connection. Hive Metastore version is ${result.getString(1)}") 
} catch { 
    case ex: Throwable => println(s"Failed to establish connection:\n $ex") 
}
%%spark 
import java.sql.DriverManager 
/** this JDBC url could be copied from Azure portal > Azure Database for MySQL > Connection strings > JDBC **/ 
val url = s"jdbc:mysql://{your_servername_here}.mysql.database.azure.com:3306/{your_database_here}?useSSL=true"
try { 
    val connection = DriverManager.getConnection(url, "{your_username_here}", "{your_password_here}");
    val result = connection.createStatement().executeQuery("select t.SCHEMA_VERSION from VERSION t") 
    result.next(); 
    println(s"Successful to test connection. Hive Metastore version is ${result.getString(1)}") 
} catch { 
    case ex: Throwable => println(s"Failed to establish connection:\n $ex") 
}
%%spark 
import java.sql.DriverManager 
/** this JDBC url could be copied from Azure portal > Azure Database for MySQL > Connection strings > JDBC **/ 
val url = s"jdbc:mysql://{your_servername_here}.mysql.database.azure.com:3306/{your_database_here}?useSSL=true"
try { 
    val connection = DriverManager.getConnection(url, "{your_username_here}", "{your_password_here}");
    val result = connection.createStatement().executeQuery("select t.SCHEMA_VERSION from VERSION t") 
    result.next(); 
    println(s"Successful to test connection. Hive Metastore version is ${result.getString(1)}") 
} catch { 
    case ex: Throwable => println(s"Failed to establish connection:\n $ex") 
}
Configure Spark to use the external Hive Metastore
After creating the linked service to the external Hive Metastore successfully, you need to set up a few Spark configurations to use the external Hive Metastore. You can both set up the configuration at Spark pool level, or at Spark session level.
Here are the configurations and descriptions:
Note
Synapse aims to work smoothly with computes from HDI. However HMS 3.1 in HDI 4.0 is not fully compatible with the OSS HMS 3.1. For OSS HMS 3.1, please checkhere.
spark.sql.hive.metastore.version
2.3
2.3
3.1
3.1
spark.sql.hive.metastore.jars
Version 2.3:/opt/hive-metastore/lib-2.3/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-client/*
/opt/hive-metastore/lib-2.3/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-client/*
Version 3.1:/opt/hive-metastore/lib-3.1/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-client/*
/opt/hive-metastore/lib-3.1/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-client/*
spark.hadoop.hive.synapse.externalmetastore.linkedservice.name
spark.sql.hive.metastore.sharedPrefixes
com.mysql.jdbc,com.microsoft.vegas
Configure at Spark pool level
When creating the Spark pool, underAdditional Settingstab, put below configurations in a text file and upload it inApache Spark configurationsection. You can also use the context menu for an existing Spark pool, choose Apache Spark configuration to add these configurations.

Update metastore version and linked service name, and save below configs in a text file for Spark pool configuration:
spark.sql.hive.metastore.version <your hms version, Make sure you use the first 2 parts without the 3rd part>
spark.hadoop.hive.synapse.externalmetastore.linkedservice.name <your linked service name>
spark.sql.hive.metastore.jars /opt/hive-metastore/lib-<your hms version, 2 parts>/*:/usr/hdp/current/hadoop-client/lib/*
spark.sql.hive.metastore.sharedPrefixes com.mysql.jdbc,com.microsoft.vegas
spark.sql.hive.metastore.version <your hms version, Make sure you use the first 2 parts without the 3rd part>
spark.hadoop.hive.synapse.externalmetastore.linkedservice.name <your linked service name>
spark.sql.hive.metastore.jars /opt/hive-metastore/lib-<your hms version, 2 parts>/*:/usr/hdp/current/hadoop-client/lib/*
spark.sql.hive.metastore.sharedPrefixes com.mysql.jdbc,com.microsoft.vegas
Here's an example for metastore version 2.3 with linked service named as HiveCatalog21:
spark.sql.hive.metastore.version 2.3
spark.hadoop.hive.synapse.externalmetastore.linkedservice.name HiveCatalog21
spark.sql.hive.metastore.jars /opt/hive-metastore/lib-2.3/*:/usr/hdp/current/hadoop-client/lib/*
spark.sql.hive.metastore.sharedPrefixes com.mysql.jdbc,com.microsoft.vegas
spark.sql.hive.metastore.version 2.3
spark.hadoop.hive.synapse.externalmetastore.linkedservice.name HiveCatalog21
spark.sql.hive.metastore.jars /opt/hive-metastore/lib-2.3/*:/usr/hdp/current/hadoop-client/lib/*
spark.sql.hive.metastore.sharedPrefixes com.mysql.jdbc,com.microsoft.vegas
Configure at Spark session level
For notebook session, you can also configure the Spark session in notebook using%%configuremagic command. Here's the code.
%%configure
%%configure -f
{
    "conf":{
        "spark.sql.hive.metastore.version":"<your hms version, 2 parts>",
        "spark.hadoop.hive.synapse.externalmetastore.linkedservice.name":"<your linked service name>",
        "spark.sql.hive.metastore.jars":"/opt/hive-metastore/lib-<your hms version, 2 parts>/*:/usr/hdp/current/hadoop-client/lib/*",
        "spark.sql.hive.metastore.sharedPrefixes":"com.mysql.jdbc,com.microsoft.vegas"
    }
}
%%configure -f
{
    "conf":{
        "spark.sql.hive.metastore.version":"<your hms version, 2 parts>",
        "spark.hadoop.hive.synapse.externalmetastore.linkedservice.name":"<your linked service name>",
        "spark.sql.hive.metastore.jars":"/opt/hive-metastore/lib-<your hms version, 2 parts>/*:/usr/hdp/current/hadoop-client/lib/*",
        "spark.sql.hive.metastore.sharedPrefixes":"com.mysql.jdbc,com.microsoft.vegas"
    }
}
For batch job, same configuration can also be applied viaSparkConf.
SparkConf
Run queries to verify the connection
After all these settings, try listing catalog objects by running below query in Spark notebook to check the connectivity to the external Hive Metastore.
spark.sql("show databases").show()
spark.sql("show databases").show()
Set up storage connection
The linked service to Hive Metastore database just provides access to Hive catalog metadata. To query the existing tables, you need to set up connection to the storage account that stores the underlying data for your Hive tables as well.
Set up connection to Azure Data Lake Storage Gen 2
If the underlying data of your Hive tables is stored in the workspace primary storage account, you don't need to do extra settings. It will just work as long as you followed storage setting up instructions during workspace creation.
If the underlying data of your Hive catalogs is stored in another ADLS Gen 2 account, you need to make sure the users who run Spark queries haveStorage Blob Data Contributorrole on the ADLS Gen2 storage account.
Set up connection to Blob Storage
If the underlying data of your Hive tables are stored in Azure Blob storage account, set up the connection follow below steps:
Open Synapse Studio, go toData > Linked tab > Addbutton >Connect to external data.
Open Synapse Studio, go toData > Linked tab > Addbutton >Connect to external data.

ChooseAzure Blob Storageand selectContinue.
ChooseAzure Blob Storageand selectContinue.
ProvideNameof the linked service. Record the name of the linked service, this info will be used in Spark configuration shortly.
ProvideNameof the linked service. Record the name of the linked service, this info will be used in Spark configuration shortly.
Select the Azure Blob Storage account. Make sure Authentication method isAccount key. Currently Spark pool can only access Blob Storage account via account key.
Select the Azure Blob Storage account. Make sure Authentication method isAccount key. Currently Spark pool can only access Blob Storage account via account key.
Test connectionand selectCreate.
Test connectionand selectCreate.
After creating the linked service to Blob Storage account, when you run Spark queries, make sure you run below Spark code in the notebook to get access to the Blob Storage account for the Spark session. Learn more about why you need to do thishere.
After creating the linked service to Blob Storage account, when you run Spark queries, make sure you run below Spark code in the notebook to get access to the Blob Storage account for the Spark session. Learn more about why you need to do thishere.
%%pyspark
blob_account_name = "<your blob storage account name>"
blob_container_name = "<your container name>"
from pyspark.sql import SparkSession
sc = SparkSession.builder.getOrCreate()
token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary
blob_sas_token = token_library.getConnectionString("<blob storage linked service name>")
spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token)
%%pyspark
blob_account_name = "<your blob storage account name>"
blob_container_name = "<your container name>"
from pyspark.sql import SparkSession
sc = SparkSession.builder.getOrCreate()
token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary
blob_sas_token = token_library.getConnectionString("<blob storage linked service name>")
spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token)
After setting up storage connections, you can query the existing tables in the Hive Metastore.
Known limitations
Synapse Studio object explorer will continue to show objects in managed Synapse metastore instead of the external HMS.
SQL <-> Spark synchronizationdoesn't work when using external HMS.
Only Azure SQL Database and Azure Database for MySQL are supported as external Hive Metastore database. Only SQL authorization is supported.
Currently Spark only works on external Hive tables and non-transactional/non-ACID managed Hive tables. It doesn't support Hive ACID/transactional tables.
Apache Ranger integration isn't supported.
Troubleshooting
See below error when querying a Hive table with data stored in Blob Storage
No credentials found for account xxxxx.blob.core.windows.net in the configuration, and its container xxxxx is not accessible using anonymous credentials. Please check if the container exists first. If it is not publicly available, you have to provide account credentials.
No credentials found for account xxxxx.blob.core.windows.net in the configuration, and its container xxxxx is not accessible using anonymous credentials. Please check if the container exists first. If it is not publicly available, you have to provide account credentials.
When using key authentication to your storage account via linked service, you need to take an extra step to get the token for Spark session. Run below code to configure your Spark session before running the query. Learn more about why you need to do this here.
%%pyspark
blob_account_name = "<your blob storage account name>"
blob_container_name = "<your container name>"
from pyspark.sql import SparkSession
sc = SparkSession.builder.getOrCreate()
token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary
blob_sas_token = token_library.getConnectionString("<blob storage linked service name>")
spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token)
%%pyspark
blob_account_name = "<your blob storage account name>"
blob_container_name = "<your container name>"
from pyspark.sql import SparkSession
sc = SparkSession.builder.getOrCreate()
token_library = sc._jvm.com.microsoft.azure.synapse.tokenlibrary.TokenLibrary
blob_sas_token = token_library.getConnectionString("<blob storage linked service name>")
spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name), blob_sas_token)
See below error when query a table stored in ADLS Gen2 account
Operation failed: "This request is not authorized to perform this operation using this permission.", 403, HEAD
Operation failed: "This request is not authorized to perform this operation using this permission.", 403, HEAD
This could happen because the user who runs Spark query doesn't have enough access to the underlying storage account. Make sure the user who runs Spark queries hasStorage Blob Data Contributorrole on the ADLS Gen2 storage account. This step can be done after creating the linked service.
HMS schema related settings
To avoid changing HMS backend schema/version, following hive configs are set by system by default:
spark.hadoop.hive.metastore.schema.verification true 
spark.hadoop.hive.metastore.schema.verification.record.version false 
spark.hadoop.datanucleus.fixedDatastore true 
spark.hadoop.datanucleus.schema.autoCreateAll false
spark.hadoop.hive.metastore.schema.verification true 
spark.hadoop.hive.metastore.schema.verification.record.version false 
spark.hadoop.datanucleus.fixedDatastore true 
spark.hadoop.datanucleus.schema.autoCreateAll false
If your HMS version is1.2.1or1.2.2, there's an issue in Hive that claims requiring only1.2.0if you turnspark.hadoop.hive.metastore.schema.verificationtotrue. Our suggestion is either you can modify your HMS version to1.2.0, or overwrite below two configurations to work around:
1.2.1
1.2.2
1.2.0
spark.hadoop.hive.metastore.schema.verification
true
1.2.0
spark.hadoop.hive.metastore.schema.verification false 
spark.hadoop.hive.synapse.externalmetastore.schema.usedefault false
spark.hadoop.hive.metastore.schema.verification false 
spark.hadoop.hive.synapse.externalmetastore.schema.usedefault false
If you need to migrate your HMS version, we recommend usinghive schema tool. And if the HMS has been used by HDInsight clusters, we suggest usingHDI provided version.
HMS schema change for OSS HMS 3.1
Synapse aims to work smoothly with computes from HDI. However HMS 3.1 in HDI 4.0 isn't fully compatible with the OSS HMS 3.1. Apply the following manually to your HMS 3.1 if itâs not provisioned by HDI.
-- HIVE-19416
ALTER TABLE TBLS ADD WRITE_ID bigint NOT NULL DEFAULT(0);
ALTER TABLE PARTITIONS ADD WRITE_ID bigint NOT NULL DEFAULT(0);
-- HIVE-19416
ALTER TABLE TBLS ADD WRITE_ID bigint NOT NULL DEFAULT(0);
ALTER TABLE PARTITIONS ADD WRITE_ID bigint NOT NULL DEFAULT(0);
When sharing the metastore with HDInsight 4.0 Spark cluster, I can't see the tables
If you want to share the Hive catalog with a spark cluster in HDInsight 4.0, ensure your propertyspark.hadoop.metastore.catalog.defaultin Synapse spark aligns with the value in HDInsight spark. The default value for HDI spark issparkand the default value for Synapse spark ishive.
spark.hadoop.metastore.catalog.default
spark
hive
When sharing the Hive Metastore with HDInsight 4.0 Hive cluster, I can list the tables successfully, but only get empty result when I query the table
As mentioned in the limitations, Synapse Spark pool only supports external hive tables and non-transactional/ACID managed tables, it doesn't support Hive ACID/transactional tables currently. In HDInsight 4.0 Hive clusters, all managed tables are created as ACID/transactional tables by default, that's why you get empty results when querying those tables.
See below error when an external metastore is used while Intelligent cache is enabled
java.lang.ClassNotFoundException: Class com.microsoft.vegas.vfs.SecureVegasFileSystem not found
java.lang.ClassNotFoundException: Class com.microsoft.vegas.vfs.SecureVegasFileSystem not found
You can easily fix this issue by appending/usr/hdp/current/hadoop-client/*to yourspark.sql.hive.metastore.jars.
/usr/hdp/current/hadoop-client/*
spark.sql.hive.metastore.jars
Eg: 
spark.sql.hive.metastore.jars":"/opt/hive-metastore/lib-2.3/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-client/*
Eg: 
spark.sql.hive.metastore.jars":"/opt/hive-metastore/lib-2.3/*:/usr/hdp/current/hadoop-client/lib/*:/usr/hdp/current/hadoop-client/*
Feedback
Was this page helpful?
Additional resources