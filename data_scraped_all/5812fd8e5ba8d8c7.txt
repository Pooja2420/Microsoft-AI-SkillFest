Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
How to ingest data by using Azure Data Factory in Azure Cosmos DB for PostgreSQL
Article
2024-08-14
4 contributors
In this article
APPLIES TO:Azure Cosmos DB for PostgreSQL (powered by theCitus database
extensionto PostgreSQL)
Azure Data Factoryis a cloud-basedETLand data integration service. It allows you to create data-driven workflows to move and transform data at scale.
Using Data Factory, you can create and schedule data-driven workflows
(called pipelines) that ingest data from disparate data stores. Pipelines can
run on-premises, in Azure, or on other cloud providers for analytics and
reporting.
Data Factory has a data sink for Azure Cosmos DB for PostgreSQL. The data sink allows you to bring
your data (relational, NoSQL, data lake files) into Azure Cosmos DB for PostgreSQL tables
for storage, processing, and reporting.

Important
Data Factory doesn't support private endpoints for Azure Cosmos DB for PostgreSQL at this time.
Data Factory for real-time ingestion
Here are key reasons to choose Azure Data Factory for ingesting data into
Azure Cosmos DB for PostgreSQL:
Easy-to-use- Offers a code-free visual environment for orchestrating and automating data movement.
Powerful- Uses the full capacity of underlying network bandwidth, up to 5 GiB/s throughput.
Built-in connectors- Integrates all your data sources, with more than 90 built-in connectors.
Cost effective- Supports a pay-as-you-go, fully managed serverless cloud service that scales on demand.
Steps to use Data Factory
In this article, you create a data pipeline by using the Data Factory
user interface (UI). The pipeline in this data factory copies data from Azure
Blob storage to a database. For a list of data stores
supported as sources and sinks, see thesupported data
storestable.
In Data Factory, you can use theCopyactivity to copy data among
data stores located on-premises and in the cloud to Azure Cosmos DB for PostgreSQL. If you're
new to Data Factory, here's a quick guide on how to get started:
Once Data Factory is provisioned, go to your data factory and launch Azure Data Factory Studio. You see the Data Factory home page as shown in the following image:
Once Data Factory is provisioned, go to your data factory and launch Azure Data Factory Studio. You see the Data Factory home page as shown in the following image:

On the Azure Data Factory Studio home page, selectOrchestrate.
On the Azure Data Factory Studio home page, selectOrchestrate.

UnderProperties, enter a name for the pipeline.
UnderProperties, enter a name for the pipeline.
In theActivitiestoolbox, expand theMove & transformcategory,
and drag and drop theCopy dataactivity to the pipeline designer
surface. At the bottom of the designer pane, on theGeneraltab, enter a name for the copy activity.
In theActivitiestoolbox, expand theMove & transformcategory,
and drag and drop theCopy dataactivity to the pipeline designer
surface. At the bottom of the designer pane, on theGeneraltab, enter a name for the copy activity.

ConfigureSource.On theActivitiespage, select theSourcetab. SelectNewto create a source dataset.In theNew Datasetdialog box, selectAzure Blob Storage, and then selectContinue.Choose the format type of your data, and then selectContinue.On theSet propertiespage, underLinked service, selectNew.On theNew linked servicepage, enter a name for the linked service, and select your storage account from theStorage account namelist.UnderTest connection, selectTo file path, enter the container and directory to connect to, and then selectTest connection.SelectCreateto save the configuration.On theSet propertiesscreen, selectOK.
ConfigureSource.
On theActivitiespage, select theSourcetab. SelectNewto create a source dataset.
On theActivitiespage, select theSourcetab. SelectNewto create a source dataset.
In theNew Datasetdialog box, selectAzure Blob Storage, and then selectContinue.
In theNew Datasetdialog box, selectAzure Blob Storage, and then selectContinue.
Choose the format type of your data, and then selectContinue.
Choose the format type of your data, and then selectContinue.
On theSet propertiespage, underLinked service, selectNew.
On theSet propertiespage, underLinked service, selectNew.
On theNew linked servicepage, enter a name for the linked service, and select your storage account from theStorage account namelist.
On theNew linked servicepage, enter a name for the linked service, and select your storage account from theStorage account namelist.

UnderTest connection, selectTo file path, enter the container and directory to connect to, and then selectTest connection.
UnderTest connection, selectTo file path, enter the container and directory to connect to, and then selectTest connection.
SelectCreateto save the configuration.
SelectCreateto save the configuration.
On theSet propertiesscreen, selectOK.
On theSet propertiesscreen, selectOK.
ConfigureSink.On theActivitiespage, select theSinktab. SelectNewto create a sink dataset.In theNew Datasetdialog box, selectAzure Database for PostgreSQL, and then selectContinue.On theSet propertiespage, underLinked service, selectNew.On theNew linked servicepage, enter a name for the linked service, and selectEnter manuallyin theAccount selection method.Enter your cluster's coordinator name in theFully qualified domain namefield. You can copy the coordinator's name from theOverviewpage of your Azure Cosmos DB for PostgreSQL cluster.Leave default port 5432 in thePortfield for direct connection to the coordinator or replace it with port 6432 to connect tothe managed PgBouncerport.Enter database name on your cluster and provide credentials to connect to it.SelectSSLin theEncryption methoddrop-down list.SelectTest connectionat the bottom of the panel to validate sink configuration.SelectCreateto save the configuration.On theSet propertiesscreen, selectOK.In theSinktab on theActivitiespage, selectOpennext to theSink datasetdrop-down list and select the table name on destination cluster where you want to ingest the data.UnderWrite method, selectCopy command.
ConfigureSink.
On theActivitiespage, select theSinktab. SelectNewto create a sink dataset.
On theActivitiespage, select theSinktab. SelectNewto create a sink dataset.
In theNew Datasetdialog box, selectAzure Database for PostgreSQL, and then selectContinue.
In theNew Datasetdialog box, selectAzure Database for PostgreSQL, and then selectContinue.
On theSet propertiespage, underLinked service, selectNew.
On theSet propertiespage, underLinked service, selectNew.
On theNew linked servicepage, enter a name for the linked service, and selectEnter manuallyin theAccount selection method.
On theNew linked servicepage, enter a name for the linked service, and selectEnter manuallyin theAccount selection method.
Enter your cluster's coordinator name in theFully qualified domain namefield. You can copy the coordinator's name from theOverviewpage of your Azure Cosmos DB for PostgreSQL cluster.
Enter your cluster's coordinator name in theFully qualified domain namefield. You can copy the coordinator's name from theOverviewpage of your Azure Cosmos DB for PostgreSQL cluster.
Leave default port 5432 in thePortfield for direct connection to the coordinator or replace it with port 6432 to connect tothe managed PgBouncerport.
Leave default port 5432 in thePortfield for direct connection to the coordinator or replace it with port 6432 to connect tothe managed PgBouncerport.
Enter database name on your cluster and provide credentials to connect to it.
Enter database name on your cluster and provide credentials to connect to it.
SelectSSLin theEncryption methoddrop-down list.
SelectSSLin theEncryption methoddrop-down list.

SelectTest connectionat the bottom of the panel to validate sink configuration.
SelectTest connectionat the bottom of the panel to validate sink configuration.
SelectCreateto save the configuration.
SelectCreateto save the configuration.
On theSet propertiesscreen, selectOK.
On theSet propertiesscreen, selectOK.
In theSinktab on theActivitiespage, selectOpennext to theSink datasetdrop-down list and select the table name on destination cluster where you want to ingest the data.
In theSinktab on theActivitiespage, selectOpennext to theSink datasetdrop-down list and select the table name on destination cluster where you want to ingest the data.
UnderWrite method, selectCopy command.
UnderWrite method, selectCopy command.

From the toolbar above the canvas, selectValidateto validate pipeline
settings. Fix any errors, revalidate, and ensure that the pipeline is successfully validated.
From the toolbar above the canvas, selectValidateto validate pipeline
settings. Fix any errors, revalidate, and ensure that the pipeline is successfully validated.
SelectDebugfrom the toolbar to execute the pipeline.
SelectDebugfrom the toolbar to execute the pipeline.

Once the pipeline can run successfully, in the top toolbar, selectPublish all. This action publishes entities (datasets and pipelines) you created
to Data Factory.
Once the pipeline can run successfully, in the top toolbar, selectPublish all. This action publishes entities (datasets and pipelines) you created
to Data Factory.
Call a stored procedure in Data Factory
In some specific scenarios, you might want to call a stored procedure/function
to push aggregated data from the staging table to the summary table. Data Factory doesn't offer a stored procedure activity for Azure Cosmos DB for PostgreSQL, but as
a workaround you can use the Lookup activity with a query to call a stored procedure
as shown below:

Next steps
Learn how to create areal-time dashboardwith Azure Cosmos DB for PostgreSQL.
Learn how tomove your workload to Azure Cosmos DB for PostgreSQL
Feedback
Was this page helpful?
Additional resources