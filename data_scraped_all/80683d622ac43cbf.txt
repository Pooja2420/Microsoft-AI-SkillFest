Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Configure liveness probes
Article
2024-08-29
8 contributors
In this article
Containerized applications may run for extended periods of time, resulting in broken states that may need to be repaired by restarting the container. Azure Container Instances supports liveness probes so that you can configure your containers within your container group to restart if critical functionality isn't working. The liveness probe behaves like aKubernetes liveness probe.
This article explains how to deploy a container group that includes a liveness probe, demonstrating the automatic restart of a simulated unhealthy container.
Azure Container Instances also supportsreadiness probes, which you can configure to ensure that traffic reaches a container only when it's ready for it.
YAML deployment
Create aliveness-probe.yamlfile with the following snippet. This file defines a container group that consists of an NGINX container that eventually becomes unhealthy.
liveness-probe.yaml
apiVersion: 2019-12-01
location: eastus
name: livenesstest
properties:
  containers:
  - name: mycontainer
    properties:
      image: mcr.microsoft.com/oss/nginx/nginx:1.15.5-alpine
      command:
        - "/bin/sh"
        - "-c"
        - "touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600"
      ports: []
      resources:
        requests:
          cpu: 1.0
          memoryInGB: 1.5
      livenessProbe:
        exec:
            command:
                - "cat"
                - "/tmp/healthy"
        periodSeconds: 5
  osType: Linux
  restartPolicy: Always
tags: null
type: Microsoft.ContainerInstance/containerGroups
apiVersion: 2019-12-01
location: eastus
name: livenesstest
properties:
  containers:
  - name: mycontainer
    properties:
      image: mcr.microsoft.com/oss/nginx/nginx:1.15.5-alpine
      command:
        - "/bin/sh"
        - "-c"
        - "touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600"
      ports: []
      resources:
        requests:
          cpu: 1.0
          memoryInGB: 1.5
      livenessProbe:
        exec:
            command:
                - "cat"
                - "/tmp/healthy"
        periodSeconds: 5
  osType: Linux
  restartPolicy: Always
tags: null
type: Microsoft.ContainerInstance/containerGroups
Run the following command to deploy this container group with the preceding YAML configuration:
az container create --resource-group myResourceGroup --name livenesstest -f liveness-probe.yaml
az container create --resource-group myResourceGroup --name livenesstest -f liveness-probe.yaml
Start command
The deployment includes acommandproperty defining a starting command that runs when the container first starts running. This property accepts an array of strings. This command simulates the container entering an unhealthy state.
command
First, it starts a bash session and creates a file calledhealthywithin the/tmpdirectory. It then sleeps for 30 seconds before deleting the file, then enters a 10-minute sleep:
healthy
/tmp
/bin/sh -c "touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600"
/bin/sh -c "touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600"
Liveness command
This deployment defines alivenessProbethat supports anexecliveness command that acts as the liveness check. If this command exits with a nonzero value, the container is killed and restarted, signaling thehealthyfile couldn't be found. If this command exits successfully with exit code 0, no action is taken.
livenessProbe
exec
healthy
TheperiodSecondsproperty designates the liveness command should execute every 5 seconds.
periodSeconds
Verify liveness output
Within the first 30 seconds, thehealthyfile created by the start command exists. When the liveness command checks for thehealthyfile's existence, the status code returns 0, signaling success, so no restarting occurs.
healthy
healthy
After 30 seconds, thecat /tmp/healthycommand begins to fail, causing unhealthy and killing events to occur.
cat /tmp/healthy
These events can be viewed from the Azure portal or Azure CLI.

By viewing the events in the Azure portal, events of typeUnhealthyare triggered upon the liveness command failing. The subsequent event is of typeKilling, signifying a container deletion so a restart can begin. The restart count for the container increments each time this event occurs.
Unhealthy
Killing
Restarts are completed in-place so resources like public IP addresses and node-specific contents are preserved.

If the liveness probe continuously fails and triggers too many restarts, your container enters an exponential back-off delay.
Liveness probes and restart policies
Restart policies supersede the restart behavior triggered by liveness probes. For example, if you set arestartPolicy = Neveranda liveness probe, the container group won't restart because of a failed liveness check. The container group instead adheres to the container group's restart policy ofNever.
restartPolicy = Never
Never
Next steps
Task-based scenarios may require a liveness probe to enable automatic restarts if a prerequisite function isn't working properly. For more information about running task-based containers, seeRun containerized tasks in Azure Container Instances.
Feedback
Was this page helpful?
Additional resources