Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Integrate Azure Data Explorer with Azure Data Factory
Article
2024-02-14
15 contributors
In this article
Azure Data Factory(ADF) is a cloud-based data integration service that allows you to integrate different data stores and perform activities on the data. ADF allows you to create data-driven workflows for orchestrating and automating data movement and data transformation. Azure Data Explorer is one of thesupported data storesin Azure Data Factory.
Azure Data Factory activities for Azure Data Explorer
Various integrations with Azure Data Factory are available for Azure Data Explorer users:
Copy activity
Azure Data Factory Copy activity is used to transfer data between data stores. Azure Data Explorer is supported as a source, where data is copied from Azure Data Explorer to any supported data store, and a sink, where data is copied from any supported data store to Azure Data Explorer. For more information, seecopy data to or from Azure Data Explorer using Azure Data Factory. For a detailed walk-through seeload data from Azure Data Factory into Azure Data Explorer.
Azure Data Explorer is supported by Azure IR (Integration Runtime), used when data is copied within Azure, and self-hosted IR, used when copying data from/to data stores located on-premises or in a network with access control, such as an Azure Virtual Network. For more information, seewhich IR to use.
Tip
When using the copy activity and creating aLinked Serviceor aDataset, select the data storeAzure Data Explorer (Kusto)and not the old data storeKusto.
Lookup activity
The Lookup activity is used for executing queries on Azure Data Explorer. The result of the query will be returned as the output of the Lookup activity, and can be used in the next activity in the pipeline as described in theADF Lookup documentation.
In addition to the response size limit of 5,000 rows and 2 MB, the activity also has a query timeout limit of 1 hour.
Command activity
The Command activity allows the execution of Azure Data Explorermanagement commands. Unlike queries, the management commands can potentially modify data or metadata. Some of the management commands are targeted to ingest data into Azure Data Explorer, using commands such as.ingestor.set-or-append) or copy data from Azure Data Explorer to external data stores using commands such as.export.
For a detailed walk-through of the command activity, seeuse Azure Data Factory command activity to run Azure Data Explorer management commands.  Using a management command to copy data can, at times, be a faster and cheaper option than the Copy activity. To determine when to use the Command activity versus the Copy activity, seeselect between Copy and Command activities when copying data.
.ingest
.set-or-append
.export
Copy in bulk from a database template
TheCopy in bulk from a database to Azure Data Explorer by using the Azure Data Factory templateis a predefined Azure Data Factory pipeline. The template is used to create many pipelines per database or per table for faster data copying.
Mapping data flows
Azure Data Factory mapping data flowsare visually designed data transformations that allow data engineers to develop graphical data transformation logic without writing code. To create a data flow and ingest data into Azure Data Explorer, use the following method:
Create themapping data flow.
Export the data into Azure Blob.
DefineEvent GridorADF copy activityto ingest the data to Azure Data Explorer.
Select between Copy and Azure Data Explorer Command activities when copy data
This section assists you in selecting the correct activity for your data copying needs.
When copying data from or to Azure Data Explorer, there are two available options in Azure Data Factory:
Copy activity.
Azure Data Explorer Command activity, which executes one of the management commands that transfer data in Azure Data Explorer.
Copy data from Azure Data Explorer
You can copy data from Azure Data Explorer using the copy activity or the.exportcommand. The.exportcommand executes a query, and then exports the results of the query.
.export
.export
See the following table for a comparison of the Copy activity and.exportcommand for copying data from Azure Data Explorer.
.export
.export
Distributed (default), exporting data from multiple nodes concurrently
Faster and COGS (cost of goods sold) efficient.
Size limit of 500,000 records or 64 MB.
Time limit of 10 minutes.
noTruncationset to false.
noTruncation
Size limits are disabled.
Server timeout is extended to 1 hour.
MaxMemoryConsumptionPerIteratorandMaxMemoryConsumptionPerQueryPerNodeare extended to max (5 GB, TotalPhysicalMemory/2).
MaxMemoryConsumptionPerIterator
MaxMemoryConsumptionPerQueryPerNode
Tip
If your copy destination is one of the data stores supported by the.exportcommand, and if none of the Copy activity features is crucial to your needs, select the.exportcommand.
.export
.export
Copying data to Azure Data Explorer
You can copy data to Azure Data Explorer using the copy activity or ingestion commands such asingest from query(.set-or-append,.set-or-replace,.set,.replace), andingest from storage(.ingest).
.set-or-append
.set-or-replace
.set
.replace)
.ingest
See the following table for a comparison of the Copy activity, and ingestion commands for copying data to Azure Data Explorer.
.set-or-append
.set-or-replace
.set
.replace
.ingest
.show
Those commands weren't designed for high volume data importing.
Works as expected and cheaper. But for production scenarios and when traffic rates and data sizes are large, use the Copy activity.
No size limit.
Max timeout limit: One hour per ingested blob.
There's only a size limit on the query part, which can be skipped by specifyingnoTruncation=true.
noTruncation=true
Max timeout limit: One hour.
No size limit.
Max timeout limit: One hour.
Tip
When copying data from ADF to Azure Data Explorer use theingest from querycommands.
ingest from query
For large datasets (>1GB), use the Copy activity.
Required permissions
The following table lists the required permissions for various steps in the integration with Azure Data Factory.
.show
TestConnection verifies the connection to the cluster, and not to the database. It can succeed even if the database doesn't exist.
Table admin permissions aren't sufficient.
.show
.show
All mandatory operations work withtable ingestor.
Some optional operations can fail.
Create a CSV mapping on the table
Drop the mapping
Performance
If Azure Data Explorer is the source and you use the Lookup, copy, or command activity that contains a query where, refer toquery best practicesfor performance information andADF documentation for copy activity.
This section addresses the use of copy activity where Azure Data Explorer is the sink. The estimated throughput for Azure Data Explorer sink is 11-13 MBps. The following table details the parameters influencing the performance of the Azure Data Explorer sink.
source and sink data stores.
ADF integration runtime.
Your Azure Data Explorer cluster.
For Azure copy, ADF VMs and machine SKUs can't be changed.
For on-premises to Azure copy, determine that the VM hosting your self-hosted IR is strong enough.
Tips and common pitfalls
Monitor activity progress
When monitoring the activity progress, theData writtenproperty may be larger than theData readproperty
becauseData readis calculated according to the binary file size, whileData writtenis calculated according to the in-memory size, after data is deserialized and decompressed.
When monitoring the activity progress, theData writtenproperty may be larger than theData readproperty
becauseData readis calculated according to the binary file size, whileData writtenis calculated according to the in-memory size, after data is deserialized and decompressed.
When monitoring the activity progress, you can see that data is written to the Azure Data Explorer sink. When querying the Azure Data Explorer table, you see that data hasn't arrived. This is because there are two stages when copying to Azure Data Explorer.First stage reads the source data, splits it to 900-MB chunks, and uploads each chunk to an Azure Blob. The first stage is seen by the ADF activity progress view.The second stage begins once all the data is uploaded to Azure Blobs. The nodes of your cluster download the blobs and ingest the data into the sink table. The data is then seen in your Azure Data Explorer table.
When monitoring the activity progress, you can see that data is written to the Azure Data Explorer sink. When querying the Azure Data Explorer table, you see that data hasn't arrived. This is because there are two stages when copying to Azure Data Explorer.
First stage reads the source data, splits it to 900-MB chunks, and uploads each chunk to an Azure Blob. The first stage is seen by the ADF activity progress view.
The second stage begins once all the data is uploaded to Azure Blobs. The nodes of your cluster download the blobs and ingest the data into the sink table. The data is then seen in your Azure Data Explorer table.
Failure to ingest CSV files due to improper escaping
Azure Data Explorer expects CSV files to align withRFC 4180.
It expects:
Fields that contain characters that require escaping (such as " and new lines) should start and end with a"character, without whitespace. All"charactersinsidethe field are escaped by using a double"character (""). For example,"Hello, ""World"""is a valid CSV file with a single record having a single column or field with the contentHello, "World".
All records in the file must have the same number of columns and fields.
Azure Data Factory allows the backslash (escape) character. If you generate a CSV file with a backslash character using Azure Data Factory, ingestion of the file to Azure Data Explorer will fail.
The following text values:
Hello, "World"ABC   DEF"ABC\D"EF"ABC DEF
Should appear in a proper CSV file as follows:
"Hello, ""World""""ABC   DEF""""ABC\D""EF""""ABC DEF"
By using the default escape character (backslash), the following CSV won't work with Azure Data Explorer:
"Hello, "World"""ABC   DEF"""ABC\D"EF"""ABC DEF"
Nested JSON objects
When copying a JSON file to Azure Data Explorer, note that:
Arrays aren't supported.
If your JSON structure contains object data types, Azure Data Factory will flatten the object's child items, and try to map each child item to a different column in your Azure Data Explorer table. If you want the entire object item to be mapped to a single column in Azure Data Explorer:Ingest the entire JSON row into a single dynamic column in Azure Data Explorer.Manually edit the pipeline definition by using Azure Data Factory's JSON editor. InMappingsRemove the multiple mappings that were created for each child item, and add a single mapping that maps your object type to your table column.After the closing square bracket, add a comma followed by:"mapComplexValuesToString": true.
Ingest the entire JSON row into a single dynamic column in Azure Data Explorer.
Manually edit the pipeline definition by using Azure Data Factory's JSON editor. InMappingsRemove the multiple mappings that were created for each child item, and add a single mapping that maps your object type to your table column.After the closing square bracket, add a comma followed by:"mapComplexValuesToString": true.
Remove the multiple mappings that were created for each child item, and add a single mapping that maps your object type to your table column.
After the closing square bracket, add a comma followed by:"mapComplexValuesToString": true.
"mapComplexValuesToString": true
Specify Additional Properties when copying to Azure Data Explorer
You can add additionalingestion propertiesby specifying them in the copy activity in the pipeline.
In Azure Data Factory, select theAuthorpencil tool.
In Azure Data Factory, select theAuthorpencil tool.
UnderPipeline, select the pipeline where you want to add additional ingestion properties.
UnderPipeline, select the pipeline where you want to add additional ingestion properties.
In theActivitiescanvas, select theCopy dataactivity.
In theActivitiescanvas, select theCopy dataactivity.
In the activity details, selectSink, and then expandAdditional properties.
In the activity details, selectSink, and then expandAdditional properties.
SelectNew, select eitherAdd nodeorAdd arrayas required, and then specify the ingestion property name and value. Repeat this step to add more properties.
SelectNew, select eitherAdd nodeorAdd arrayas required, and then specify the ingestion property name and value. Repeat this step to add more properties.
Once complete save and publish your pipeline.
Once complete save and publish your pipeline.
Next step
Copy data to Azure Data Explorer by using Azure Data Factory.
Feedback
Was this page helpful?
Additional resources