Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Kernels for Jupyter Notebook on Apache Spark clusters in Azure HDInsight
Article
2024-06-15
11 contributors
In this article
HDInsight Spark clusters provide kernels that you can use with the Jupyter Notebook onApache Sparkfor testing your applications. A kernel is a program that runs and interprets your code. The three kernels are:
PySpark- for applications written in Python2. (Applicable only for Spark 2.4 version clusters)
PySpark3- for applications written in Python3.
Spark- for applications written in Scala.
In this article, you learn how to use these kernels and the benefits of using them.
Prerequisites
An Apache Spark cluster in HDInsight. For instructions, seeCreate Apache Spark clusters in Azure HDInsight.
Create a Jupyter Notebook on Spark HDInsight
From theAzure portal, select your Spark cluster.  SeeList and show clustersfor the instructions. TheOverviewview opens.
From theAzure portal, select your Spark cluster.  SeeList and show clustersfor the instructions. TheOverviewview opens.
From theOverviewview, in theCluster dashboardsbox, selectJupyter Notebook. If prompted, enter the admin credentials for the cluster.NoteYou may also reach the Jupyter Notebook on Spark cluster by opening the following URL in your browser. ReplaceCLUSTERNAMEwith the name of your cluster:https://CLUSTERNAME.azurehdinsight.net/jupyter
From theOverviewview, in theCluster dashboardsbox, selectJupyter Notebook. If prompted, enter the admin credentials for the cluster.

Note
You may also reach the Jupyter Notebook on Spark cluster by opening the following URL in your browser. ReplaceCLUSTERNAMEwith the name of your cluster:
https://CLUSTERNAME.azurehdinsight.net/jupyter
https://CLUSTERNAME.azurehdinsight.net/jupyter
SelectNew, and then select eitherPyspark,PySpark3, orSparkto create a notebook. Use the Spark kernel for Scala applications, PySpark kernel for Python2 applications, and PySpark3 kernel for Python3 applications.
SelectNew, and then select eitherPyspark,PySpark3, orSparkto create a notebook. Use the Spark kernel for Scala applications, PySpark kernel for Python2 applications, and PySpark3 kernel for Python3 applications.

Note
For Spark 3.1, onlyPySpark3, orSparkwill be available.

A notebook opens with the kernel you selected.
Benefits of using the kernels
Here are a few benefits of using the new kernels with Jupyter Notebook on Spark HDInsight clusters.
Preset contexts. WithPySpark,PySpark3, or theSparkkernels, you don't need to set the Spark or Hive contexts explicitly before you start working with your applications. These contexts are available by default. These contexts are:sc- for Spark contextsqlContext- for Hive contextSo, youdon'thave to run statements like the following to set the contexts:sc = SparkContext('yarn-client')
sqlContext = HiveContext(sc)Instead, you can directly use the preset contexts in your application.
Preset contexts. WithPySpark,PySpark3, or theSparkkernels, you don't need to set the Spark or Hive contexts explicitly before you start working with your applications. These contexts are available by default. These contexts are:
sc- for Spark context
sc- for Spark context
sqlContext- for Hive contextSo, youdon'thave to run statements like the following to set the contexts:sc = SparkContext('yarn-client')
sqlContext = HiveContext(sc)Instead, you can directly use the preset contexts in your application.
sqlContext- for Hive context
So, youdon'thave to run statements like the following to set the contexts:
sc = SparkContext('yarn-client')
sqlContext = HiveContext(sc)
sc = SparkContext('yarn-client')
sqlContext = HiveContext(sc)
Instead, you can directly use the preset contexts in your application.
Cell magics. The PySpark kernel provides some predefined "magics", which are special commands that you can call with%%(for example,%%MAGIC<args>). The magic command must be the first word in a code cell and allow for multiple lines of content. The magic word should be the first word in the cell. Adding anything before the magic, even comments, causes an error.     For more information on magics, seehere.The following table lists the different magics available through the kernels.MagicExampleDescriptionhelp%%helpGenerates a table of all the available magics with example and descriptioninfo%%infoOutputs session information for the current Livy endpointconfigure%%configure -f{"executorMemory": "1000M","executorCores": 4}Configures the parameters for creating a session. The force flag (-f) is mandatory if a session has already been created, which ensures that the session is dropped and recreated. Look atLivy's POST /sessions Request Bodyfor a list of valid parameters. Parameters must be passed in as a JSON string and must be on the next line after the magic, as shown in the example column.sql%%sql -o <variable name>SHOW TABLESExecutes a Hive query against the sqlContext. If the-oparameter is passed, the result of the query is persisted in the %%local Python context as aPandasdataframe.local%%locala=1All the code in later lines is executed locally. Code must be valid Python2 code no matter which kernel you're using. So, even if you selectedPySpark3orSparkkernels while creating the notebook, if you use the%%localmagic in a cell, that cell must only have valid Python2 code.logs%%logsOutputs the logs for the current Livy session.delete%%delete -f -s <session number>Deletes a specific session of the current Livy endpoint. You can't delete the session that is started for the kernel itself.cleanup%%cleanup -fDeletes all the sessions for the current Livy endpoint, including this notebook's session. The force flag -f is mandatory.NoteIn addition to the magics added by the PySpark kernel, you can also use thebuilt-in IPython magics, including%%sh. You can use the%%shmagic to run scripts and block of code on the cluster headnode.
Cell magics. The PySpark kernel provides some predefined "magics", which are special commands that you can call with%%(for example,%%MAGIC<args>). The magic command must be the first word in a code cell and allow for multiple lines of content. The magic word should be the first word in the cell. Adding anything before the magic, even comments, causes an error.     For more information on magics, seehere.
%%
%%MAGIC
<args>
The following table lists the different magics available through the kernels.
%%help
%%info
%%configure -f
{"executorMemory": "1000M"
"executorCores": 4
-f
%%sql -o <variable name>
SHOW TABLES
-o
%%local
a=1
%%local
%%logs
%%delete -f -s <session number>
%%cleanup -f
Note
In addition to the magics added by the PySpark kernel, you can also use thebuilt-in IPython magics, including%%sh. You can use the%%shmagic to run scripts and block of code on the cluster headnode.
%%sh
%%sh
Auto visualization. The Pyspark kernel automatically visualizes the output of Hive and SQL queries. You can choose between several different types of visualizations including Table, Pie, Line, Area, Bar.
Auto visualization. The Pyspark kernel automatically visualizes the output of Hive and SQL queries. You can choose between several different types of visualizations including Table, Pie, Line, Area, Bar.
Parameters supported with the %%sql magic
The%%sqlmagic supports different parameters that you can use to control the kind of output that you receive when you run queries. The following table lists the output.
%%sql
-o <VARIABLE NAME>
-q
-q -o <VARIABLE>
CREATE TABLE
-q
-o
-m <METHOD>
take
-r
-r <FRACTION>
sample
-m sample -r 0.01
-n <MAXROWS>
Example:
%%sql -q -m sample -r 0.1 -n 500 -o query2
SELECT * FROM hivesampletable
%%sql -q -m sample -r 0.1 -n 500 -o query2
SELECT * FROM hivesampletable
The statement above does the following actions:
Selects all records fromhivesampletable.
Because we use -q, it turns off autovisualization.
Because we use-m sample -r 0.1 -n 500, it randomly samples 10% of the rows in the hivesampletable and limits the size of the result set to 500 rows.
-m sample -r 0.1 -n 500
Finally, because we used-o query2it also saves the output into a dataframe calledquery2.
-o query2
Considerations while using the new kernels
Whichever kernel you use, leaving the notebooks running consumes the cluster resources.  With these kernels, because the contexts are preset, simply exiting the notebooks doesn't kill the context. And so the cluster resources continue to be in use. A good practice is to use theClose and Haltoption from the notebook'sFilemenu when you're finished using the notebook. The closure kills the context and then exits the notebook.
Where are the notebooks stored?
If your cluster uses Azure Storage as the default storage account, Jupyter Notebooks are saved to storage account under the/HdiNotebooksfolder.  Notebooks, text files, and folders that you create from within Jupyter are accessible from the storage account.  For example, if you use Jupyter to create a foldermyfolderand a notebookmyfolder/mynotebook.ipynb, you can access that notebook at/HdiNotebooks/myfolder/mynotebook.ipynbwithin the storage account.  The reverse is also true, that is, if you upload a notebook directly to your storage account at/HdiNotebooks/mynotebook1.ipynb, the notebook is visible from Jupyter as well.  Notebooks remain in the storage account even after the cluster is deleted.
myfolder
/HdiNotebooks/myfolder/mynotebook.ipynb
/HdiNotebooks/mynotebook1.ipynb
Note
HDInsight clusters with Azure Data Lake Storage as the default storage do not store notebooks in associated storage.
The way notebooks are saved to the storage account is compatible withApache Hadoop HDFS. If you SSH into the cluster you can use the file management commands:
hdfs dfs -ls /HdiNotebooks
hdfs dfs âcopyToLocal /HdiNotebooks
hdfs dfs âcopyFromLocal example.ipynb /HdiNotebooks
Whether the cluster uses Azure Storage or Azure Data Lake Storage as the default storage account, the notebooks are also saved on the cluster headnode at/var/lib/jupyter.
/var/lib/jupyter
Supported browser
Jupyter Notebooks on Spark HDInsight clusters are supported only on Google Chrome.
Suggestions
The new kernels are in evolving stage and will mature over time. So the APIs could change as these kernels mature. We would appreciate any feedback that you have while using these new kernels. The feedback is useful in shaping the final release of these kernels. You can leave your comments/feedback under theFeedbacksection at the bottom of this article.
Next steps
Overview: Apache Spark on Azure HDInsight
Use Apache Zeppelin notebooks with an Apache Spark cluster on HDInsight
Use external packages with Jupyter Notebooks
Install Jupyter on your computer and connect to an HDInsight Spark cluster
Feedback
Was this page helpful?
Additional resources