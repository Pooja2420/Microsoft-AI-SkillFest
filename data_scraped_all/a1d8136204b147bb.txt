Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Deploy models for batch inference and prediction
Article
2025-04-04
5 contributors
In this article
This article describes what Databricks recommends for batch inference.
For real-time model serving on Azure Databricks, seeDeploy models using Mosaic AI Model Serving.
AI Functions for batch inference
Important
This feature is inPublic Preview.
AI Functions are built-in functions that you can use to apply AI on your data that is stored on Databricks. You can run batch inference usingtask-specific AI functionsor the general purpose function,ai_query.
ai_query
The following is an example of batch inference using the task-specific AI function,ai_translate. If you want perform batch inference on an entire table, you can remove thelimit 500from your query.
ai_translate
limit 500
SELECT
writer_summary,
  ai_translate(writer_summary, "cn") as cn_translation
from user.batch.news_summaries
limit 500
;
SELECT
writer_summary,
  ai_translate(writer_summary, "cn") as cn_translation
from user.batch.news_summaries
limit 500
;
Alternatively, you can use the general purpose function,ai_queryto perform batch inference.
ai_query
See whichmodel types and the associated modelsthatai_querysupports.
ai_query
SeePerform batch LLM inference using AI Functions.
Batch inference using a Spark DataFrame
SeePerform batch inference using a Spark DataFramefor a step-by-step guide through the model inference workflow using Spark.
For deep learning model inference examples see the following articles:
Model inference using TensorFlow and TensorRT
Model inference using PyTorch
Feedback
Was this page helpful?
Additional resources