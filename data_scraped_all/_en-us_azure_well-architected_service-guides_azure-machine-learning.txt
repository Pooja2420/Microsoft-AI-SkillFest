Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Architecture best practices for Azure Machine Learning
Article
2024-03-18
8 contributors
In this article
Azure Machine Learning is a managed cloud service that you can use to train, deploy, and manage machine learning models. There are a wide range of choices and configurations for both training and deploying models, including compute SKUs and configurations. You can deploy Machine learning models to Machine Learning compute or to other Azure services such as Azure Kubernetes Service (AKS).
This article provides architectural recommendations for making informed decisions when you use Machine Learning to train, deploy, and manage machine learning models. The guidance is based on theAzure Well-Architected Framework pillars.
Important
How to use this guide
Each section has adesign checklistthat presents architectural areas of concern along with design strategies localized to the technology scope.
Also included arerecommendationson the technology capabilities that can help materialize those strategies. The recommendations don't represent an exhaustive list of all configurations available for Machine Learning and its dependencies. Instead, they list the key recommendations mapped to the design perspectives. Use the recommendations to build your proof-of-concept or to optimize your existing environments.
The foundational architecturebaseline OpenAI end-to-end chat reference architecturedemonstrates many of the key recommendations.
This review focuses on the interrelated decisions for these Azure resources:
Machine Learning
Machine Learning compute clusters
Machine Learning compute instances
The review doesn't address connected resources such as data stores or Azure Key Vault.
Reliability
The purpose of the Reliability pillar is to provide continued functionality bybuilding enough resilience and the ability to recover fast from failures.
TheReliability design principlesprovide a high-level design strategy applied for individual components, system flows, and the system as a whole.
Start your design strategy based on thedesign review checklist for Reliabilityand determine its relevance to your business requirements. Extend the strategy to include more approaches as needed.
Resiliency: Deploy models to environments that support availability zones, such as AKS. By ensuring deployments are distributed across availability zones, you're ensuring a deployment is available even in the event of a datacenter failure. For enhanced reliability and availability, consider a multi-region deployment topology.
Resiliency: Deploy models to environments that support availability zones, such as AKS. By ensuring deployments are distributed across availability zones, you're ensuring a deployment is available even in the event of a datacenter failure. For enhanced reliability and availability, consider a multi-region deployment topology.
Resiliency: Ensure you have sufficient compute for both training and inferencing. Through resource planning, make sure your compute SKU and scale settings meet the requirements of your workload.
Resiliency: Ensure you have sufficient compute for both training and inferencing. Through resource planning, make sure your compute SKU and scale settings meet the requirements of your workload.
Resiliency: Segregate Machine Learning workspaces used for exploratory work from those used for production.
Resiliency: Segregate Machine Learning workspaces used for exploratory work from those used for production.
Resiliency: When using managed online endpoints for inferencing, use a release strategy such as blue-green deployments to minimize downtime and reduce the risk associated with deploying new versions.
Resiliency: When using managed online endpoints for inferencing, use a release strategy such as blue-green deployments to minimize downtime and reduce the risk associated with deploying new versions.
Business requirements: Select your use of compute clusters, compute instances, and externalized inference hosts based on reliability needs, considering service-level agreements (SLAs) as a factor.
Business requirements: Select your use of compute clusters, compute instances, and externalized inference hosts based on reliability needs, considering service-level agreements (SLAs) as a factor.
Recovery: Ensure you have self-healing capabilities, such as checkpointing features supported by Machine Learning, when training large models.
Recovery: Ensure you have self-healing capabilities, such as checkpointing features supported by Machine Learning, when training large models.
Recovery: Ensure you have a recovery strategy defined. Machine Learning doesn't have automatic failover. Therefore, you must design a strategy that encompasses the workspace and all its dependencies, such as Key Vault, Azure Storage, and Azure Container Registry.
Recovery: Ensure you have a recovery strategy defined. Machine Learning doesn't have automatic failover. Therefore, you must design a strategy that encompasses the workspace and all its dependencies, such as Key Vault, Azure Storage, and Azure Container Registry.
Security
The purpose of the Security pillar is to provideconfidentiality, integrity, and availabilityguarantees to the workload.
TheSecurity design principlesprovide a high-level design strategy for achieving those goals by applying approaches to the technical design around Machine Learning.
Start your design strategy based on thedesign review checklist for Securityand identify vulnerabilities and controls to improve the security posture. Extend the strategy to include more approaches as needed.
Availability: Reduce the attack surface of the Machine Learning workspace by restricting access to the workspace to resources within the virtual network.
Availability: Reduce the attack surface of the Machine Learning workspace by restricting access to the workspace to resources within the virtual network.
Confidentiality: Guard against data exfiltration from the Machine Learning workspace by implementing network isolation. Ensure access to all external resources is explicitly approved and access to all other external resources isn't permitted.
Confidentiality: Guard against data exfiltration from the Machine Learning workspace by implementing network isolation. Ensure access to all external resources is explicitly approved and access to all other external resources isn't permitted.
Integrity: Implement access controls that authenticate and authorize the Machine Learning workspace for external resources based on the least privilege principle.
Integrity: Implement access controls that authenticate and authorize the Machine Learning workspace for external resources based on the least privilege principle.
Integrity: Implement use case segregation for Machine Learning workspaces by setting up workspaces based on specific use cases or projects. This approach adheres to the principle of least privilege by ensuring that workspaces are only accessible to individuals that require access to data and experimentation assets for the use case or project.
Integrity: Implement use case segregation for Machine Learning workspaces by setting up workspaces based on specific use cases or projects. This approach adheres to the principle of least privilege by ensuring that workspaces are only accessible to individuals that require access to data and experimentation assets for the use case or project.
Integrity: Regulate access to foundational models. Ensure only approved registries have access to models in the model registry.
Integrity: Regulate access to foundational models. Ensure only approved registries have access to models in the model registry.
Integrity: Regulate access to approved container registries. Ensure Machine Learning compute can only access approved registries.
Integrity: Regulate access to approved container registries. Ensure Machine Learning compute can only access approved registries.
Integrity: Regulate the Python packages that can be run on Machine Learning compute. Regulating the Python packages ensures only trusted packages are run.
Integrity: Regulate the Python packages that can be run on Machine Learning compute. Regulating the Python packages ensures only trusted packages are run.
Integrity: Require code used for training in Machine Learning compute environments to be signed. Requiring code signing ensures that the code running is from a trusted source and hasn't been tampered with.
Integrity: Require code used for training in Machine Learning compute environments to be signed. Requiring code signing ensures that the code running is from a trusted source and hasn't been tampered with.
Confidentiality: Adhere to the principle of least privilege for role-based access control (RBAC) to the Machine Learning workspace and related resources, such as the workspace storage account, to ensure individuals have only the necessary permissions for their role, thereby minimizing potential security risks.
Confidentiality: Adhere to the principle of least privilege for role-based access control (RBAC) to the Machine Learning workspace and related resources, such as the workspace storage account, to ensure individuals have only the necessary permissions for their role, thereby minimizing potential security risks.
Integrity: Establish trust and verified access by implementing encryption for data at rest and data in transit.
Integrity: Establish trust and verified access by implementing encryption for data at rest and data in transit.
Allow only approved outbound
remoteLoginPortPublicAccess
Disabled
false
The following are some examples of theAdvisorsecurity best practice recommendations for Machine Learning:
Workspaces should be encrypted with a customer-managed key (CMK).
Workspaces should use Azure Private Link.
Workspaces should disable public network access.
Compute should be in a virtual network.
Compute instances should be recreated to get the latest software updates.
The following are examples ofbuilt-in Azure Policy definitions for Machine Learningsecurity:
Configure allowed registries for specified Machine Learning computes.
Configure allowed Python packages for specified Machine Learning computes.
Machine Learning Workspaces should disable public network access.
Machine Learning compute instances should be recreated to get the latest software updates.
Machine Learning computes should be in a virtual network.
Machine Learning computes should have local authentication methods disabled.
Machine Learning workspaces should be encrypted with a CMK.
Machine Learning workspaces should use Private Link.
Machine Learning workspaces should use a user-assigned managed identity.
Require an approval endpoint called prior to jobs running for specified Machine Learning computes.
Require code signing for training code for computes.
Restrict model deployment to specific registries.
Cost Optimization
Cost Optimization focuses ondetecting spend patterns, prioritizing investments in critical areas, and optimizing in othersto meet the organization's budget while meeting business requirements.
Read theCost Optimization design principlesto understand the approaches to achieve those goals and the necessary tradeoffs in technical design choices related to training and deploying models in their environments.
Start your design strategy based on thedesign review checklist for Cost Optimizationfor investments and fine tune the design so that the workload is aligned with the budget allocated for the workload. Your design should use the right Azure capabilities, monitor investments, and find opportunities to optimize over time.
Usage optimization: Choose the appropriate resources to ensure that they align with your workload requirements. For example, choose between CPUs or GPUs, various SKUs, or low versus regular-priority VMs.
Usage optimization: Choose the appropriate resources to ensure that they align with your workload requirements. For example, choose between CPUs or GPUs, various SKUs, or low versus regular-priority VMs.
Usage optimization: Ensure compute resources that aren't being used are scaled down or shut down when idle to reduce waste.
Usage optimization: Ensure compute resources that aren't being used are scaled down or shut down when idle to reduce waste.
Usage optimization: Apply policies and configure quotas to comply with the design's upper and lower limits.
Usage optimization: Apply policies and configure quotas to comply with the design's upper and lower limits.
Usage optimization: Test parallelizing training workloads to determine if training requirements can be met on lower cost SKUs.
Usage optimization: Test parallelizing training workloads to determine if training requirements can be met on lower cost SKUs.
Rate optimization: Purchase Azure Reserved Virtual Machine Instances if you have a good estimate of usage over the next one to three years.
Rate optimization: Purchase Azure Reserved Virtual Machine Instances if you have a good estimate of usage over the next one to three years.
Monitor and optimize: Monitor your resource usage such as CPU and GPU usage when training models. If the resources aren't being fully used, modify your code to better use resources or scale down to smaller or cheaper VM sizes.
Monitor and optimize: Monitor your resource usage such as CPU and GPU usage when training models. If the resources aren't being fully used, modify your code to better use resources or scale down to smaller or cheaper VM sizes.
General Purpose â Balanced CPU to memory ratio, good for all purposes.
Compute Optimized â High CPU to memory ratio, good for math-heavy computations.
Memory Optimized â High memory to CPU, good for in-memory computations or database applications.
M Series â Very large machines that have huge amounts of memory and CPU.
GPU â Better for models with a high number of variables that can benefit from higher parallelism and specialized core instructions. Typical applications are deep learning, image or video processing, scientific simulations, data mining, and taking advantage of GPU development frameworks. Test with multiple families and document the results as your baseline. As your model and data evolve, the most adequate compute resource might change. Monitor execution times and reevaluate as needed.
Operational Excellence
Operational Excellence primarily focuses on procedures fordevelopment practices, observability, and release management.
TheOperational Excellence design principlesprovide a high-level design strategy for achieving those goals towards the operational requirements of the workload.
Start your design strategy based on thedesign review checklist for Operational Excellencefor defining processes for observability, testing, and deployment related to Machine Learning.
Development standards: Take advantage of Machine Learning model catalogs and registries to store, version, and share machine learning assets.
Development standards: Take advantage of Machine Learning model catalogs and registries to store, version, and share machine learning assets.
Automate for efficiency: Follow goodmachine learning operations (MLOps)practices. When possible, build end-to-end automated pipelines for data preparation, training, and scoring processes. In development, use scripts instead of notebooks for training models, as scripts are easier to integrate into automated pipelines.
Automate for efficiency: Follow goodmachine learning operations (MLOps)practices. When possible, build end-to-end automated pipelines for data preparation, training, and scoring processes. In development, use scripts instead of notebooks for training models, as scripts are easier to integrate into automated pipelines.
Deploy with confidence: Implement infrastructure as code (IaC) for Machine Learning workspaces, compute clusters, compute instances, and other deployment environments.
Deploy with confidence: Implement infrastructure as code (IaC) for Machine Learning workspaces, compute clusters, compute instances, and other deployment environments.
Observability: Monitor the performance of your deployed models including data drift.
Observability: Monitor the performance of your deployed models including data drift.
Observability: If your models are deployed to online endpoints,enable Application Insightstomonitor online endpoints and deployments. Monitor training infrastructure to ensure you're meeting your baseline requirements.
Observability: If your models are deployed to online endpoints,enable Application Insightstomonitor online endpoints and deployments. Monitor training infrastructure to ensure you're meeting your baseline requirements.
Simplicity: Use curated environments optimized for Machine Learning, when available.
Simplicity: Use curated environments optimized for Machine Learning, when available.
Performance Efficiency
Performance Efficiency is aboutmaintaining user experience even when there's an increase in loadby managing capacity. The strategy includes scaling resources, identifying and optimizing potential bottlenecks, and optimizing for peak performance.
ThePerformance Efficiency design principlesprovide a high-level design strategy for achieving those capacity goals against the expected usage.
Start your design strategy based on thedesign review checklist for Performance Efficiencyfor defining a baseline based on key performance indicators for Machine Learning workloads.
Performance targets: Determine the acceptable training time and retrain frequency for your model. Setting a clear target for training time, along with testing, helps you determine the compute resources, CPU versus GPU, and CPU SKUs required to meet the training time goal.
Performance targets: Determine the acceptable training time and retrain frequency for your model. Setting a clear target for training time, along with testing, helps you determine the compute resources, CPU versus GPU, and CPU SKUs required to meet the training time goal.
Performance targets: Define the acceptable performance targets for your deployed models including response time, requests per second, error rate, and uptime. Performance targets act as a benchmark for your deployed model's efficiency. Targets can help you make CPU versus GPU determinations, CPU SKU choices, and scaling requirements.
Performance targets: Define the acceptable performance targets for your deployed models including response time, requests per second, error rate, and uptime. Performance targets act as a benchmark for your deployed model's efficiency. Targets can help you make CPU versus GPU determinations, CPU SKU choices, and scaling requirements.
Meet capacity requirements: Choose the right compute resources for model training.
Meet capacity requirements: Choose the right compute resources for model training.
Meet capacity requirements: Choose the right compute resources for model deployments.
Meet capacity requirements: Choose the right compute resources for model deployments.
Meet capacity requirements: Choose deployment environments with autoscaling capabilities to add and remove capacity as demand fluctuates.
Meet capacity requirements: Choose deployment environments with autoscaling capabilities to add and remove capacity as demand fluctuates.
Achieve and sustain performance: Continuouslymonitor the performance of your deployed models, review results, and take appropriate actions.
Achieve and sustain performance: Continuouslymonitor the performance of your deployed models, review results, and take appropriate actions.
Achieve and sustain performance: Continuously monitor the performance of your infrastructure of deployed models, review results, and take appropriate actions. Monitor training infrastructure to ensure you're meeting your requirements for training time.
Achieve and sustain performance: Continuously monitor the performance of your infrastructure of deployed models, review results, and take appropriate actions. Monitor training infrastructure to ensure you're meeting your requirements for training time.
Azure policies
Azure provides an extensive set of built-in policies related to Machine Learning and its dependencies. Some of the preceding recommendations can be audited through Azure policies. Consider the following policies that are related to security:
Allowed registries for specified Machine Learning computes.
Configure allowed Python packages for specified Machine Learning computes.
Machine Learning computes should be in a virtual network.
Machine Learning computes should have local authentication methods disabled.
Machine Learning workspaces should disable public network access.
Machine Learning compute instances should be recreated to get the latest software updates.
Machine Learning workspaces should be encrypted with a customer-managed key.
Machine Learning workspaces should use private link.
Machine Learning workspaces should use user-assigned managed identity.
Require an approval endpoint called prior to jobs running for specified Machine Learning computes.
Require code signing for training code for computes.
Restrict model deployment to specific registries.
Consider the following policy that's related to cost optimization:
Machine Learning Compute instance should have idle shutdown.
Consider the following policies that are related to operational excellence:
Require log filter expressions and datastore to be used for full logs for specified Machine Learning computes.
Resource logs in Machine Learning workspaces should be enabled.
For comprehensive governance, review theAzure Policy built-in definitions for Machine Learning.
Advisor recommendations
Advisor is a personalized cloud consultant that helps you follow best practices to optimize your Azure deployments. Advisor recommendations can help you improve the reliability, security, cost effectiveness, performance, and operational excellence of Machine Learning.
Consider the followingAdvisorrecommendations for security:
Workspaces should be encrypted with a customer-managed key (CMK).
Workspaces should use private link.
Workspaces should disable public network access.
Compute should be in a virtual network.
Compute instances should be recreated to get the latest software updates.
Consider the followingAdvisorrecommendation for operational excellence:
Resource logs in Machine Learning workspaces should be enabled.
Next steps
Consider these articles as resources that demonstrate the recommendations highlighted in this article.
Use thebaseline OpenAI end-to-end chat reference architectureas an example of how these recommendations can be applied to a workload.
UseMachine Learningproduct documentation to build implementation expertise.
Feedback
Was this page helpful?
Additional resources