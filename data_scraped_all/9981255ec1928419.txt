Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Azure Event Hubs data connection
Article
2024-10-13
19 contributors
In this article
Azure Event Hubsis a big data streaming platform and event ingestion service. Azure Data Explorer offers continuous ingestion from customer-managed Event Hubs.
The Event Hubs ingestion pipeline transfers events to Azure Data Explorer in several steps. First you create an event hub in the Azure portal. Then create a target table in Azure Data Explorer into which thedata in a particular format, is ingested using the providedingestion properties. The Event Hubs connection needs to be aware ofevents routing. Data may be embedded with selected properties according to theevent system properties. Create a connection to Event Hubs tocreate an event hubandsend events. This process can be managed through theAzure portal, programmatically withC#orPython, or with theAzure Resource Manager template.
For general information about data ingestion in Azure Data Explorer, seeAzure Data Explorer data ingestion overview.
Azure Data Explorer data connection authentication options
Managed Identitybased data connection (recommended): Using a managed identity-based data connection is the most secure way to connect to data sources. It provides full control over the ability to fetch data from a data source.
Setup of a data connection using managed identity requires the following steps:Add a managed identity to your cluster.Grant permissions to the managed identity on the data source. To fetch data from Azure Event Hubs, the managed identity must haveAzure Event Hubs Data Receiverpermissions.Set amanaged identity policyon the target databases.Create a data connection using the managed identity authentication to fetch data.CautionIf the managed identity permissions are removed from the data source, the data connection will no longer work and will be unable to fetch data from the data source.
Managed Identitybased data connection (recommended): Using a managed identity-based data connection is the most secure way to connect to data sources. It provides full control over the ability to fetch data from a data source.
Setup of a data connection using managed identity requires the following steps:
Add a managed identity to your cluster.
Grant permissions to the managed identity on the data source. To fetch data from Azure Event Hubs, the managed identity must haveAzure Event Hubs Data Receiverpermissions.
Set amanaged identity policyon the target databases.
Create a data connection using the managed identity authentication to fetch data.
Caution
If the managed identity permissions are removed from the data source, the data connection will no longer work and will be unable to fetch data from the data source.
Key-based data connection: If a managed identity authentication is not specified for the data connection, the connection automatically defaults to key-based authentication. Key-based connections fetch data using a resource connection string, such as theAzure Event Hubs connection string. Azure Data Explorer gets the resource connection string for the specified resource and securely saves it. The connection string is then used to fetch data from the data source.CautionIf the key is rotated, the data connection will no longer work and will be unable to fetch data from the data source. To fix the issue, update or recreate the data connection.
Key-based data connection: If a managed identity authentication is not specified for the data connection, the connection automatically defaults to key-based authentication. Key-based connections fetch data using a resource connection string, such as theAzure Event Hubs connection string. Azure Data Explorer gets the resource connection string for the specified resource and securely saves it. The connection string is then used to fetch data from the data source.
Caution
If the key is rotated, the data connection will no longer work and will be unable to fetch data from the data source. To fix the issue, update or recreate the data connection.
Data format
Data is read from the event hub in form ofEventDataobjects.
Seesupported formats.
Note
Ingestion from Event Hubs doesn't support RAW format.
Azure Event Hubs Schema Registryand schema-less Avro are not supported.
Data can be compressed using thegzipcompression algorithm. You can specifyCompressiondynamically usingingestion properties, or in the static Data Connection settings.
gzip
Compression
Data compression isn't supported for binary formats (Avro, ApacheAvro, Parquet, ORC, and W3CLOGFILE).
Custom encoding and embeddedsystem propertiesaren't supported with binary formats and compressed data.
When using binary formats (Avro, ApacheAvro, Parquet, ORC, and W3CLOGFILE) andingestion mappings,
the order of the fields in the ingestion mapping definition must match the order of the corresponding columns in the table.
Event Hubs properties
Azure Data Explorer supports the following Event Hubs properties:
A closed set ofingestion properties, which helps to route the event to the relevant table.
A closed set ofevent system properties, which can be embedded in the data based on a given mapping.
Note
Ingesting Event Hubscustom properties, used to associate metadata with events, isn't supported. If you need to ingest custom properties, send them in the body of the event data. For more information, seeIngest custom properties.
Ingestion properties
Ingestion properties instruct the ingestion process, where to route the data, and how to process it. You can specifyingestion propertiesof the events ingestion using theEventData.Properties. You can set the following properties:
Note
Property names are case sensitive.
Table
Data Connection
Data format
Data Connection
Column mapping
Data Connection
None
gzip
Note
Only events enqueued after you create the data connection are ingested, unless a custom retrieval start date is provided. In any case, the lookback period cannot exceed the actual Event Hub retention period.
Events routing
When you create a data connection to your cluster, you can specify the routing for where to send ingested data. The default routing is to the target table specified in the connection string that is associated with the target database. The default routing for your data is also referred to asstatic routing. You can specify an alternative routing and processing options for your data by setting one or more of event data properties mentioned in the previous paragraph.
Note
Event Hubs data connection will attempt to process all the events it reads from the Event Hub, and every event it cannot process for whatever reason will be reported as an ingestion failure.
Read on how to monitor Azure Data Explorer ingestionhere.
Route event data to an alternate database
Routing data to an alternate database is off by default. To send the data to a different database, you must first set the connection as a multi-database connection. This feature can be enabled in the Azure portalAzure portal, withC#orPythonmanagement SDKs, or with anARM template. The user, group, service principal, or managed identity used to allow database routing must at least have thecontributorrole and write permissions on the cluster.
To specify an alternate database, set theDatabaseingestion property.
Warning
Specifying an alternate database without setting the connection as a multi-database data connection will cause the ingestion to fail.
Route event data to an alternate table
To specify an alternate table for each event, set theTable,Format,Compression, and mappingingestion properties. The connection dynamically routes the ingested data as specified in theEventData.Properties, overriding the static properties for this event.
The following example shows you how to set the event hub details and send weather metric data to alternate database (MetricsDB) and table (WeatherMetrics).
The data is in JSON format andmapping1is pre-defined on tableWeatherMetrics.
// This sample uses Azure.Messaging.EventHubs which is a .Net Framework library.
await using var producerClient = new EventHubProducerClient("<eventHubConnectionString>");
// Create the event and add optional "dynamic routing" properties
var eventData = new EventData(Encoding.UTF8.GetBytes(JsonConvert.SerializeObject(
    new { TimeStamp = DateTime.UtcNow, MetricName = "Temperature", Value = 32 }
)));
eventData.Properties.Add("Database", "MetricsDB");
eventData.Properties.Add("Table", "WeatherMetrics");
eventData.Properties.Add("Format", "json");
eventData.Properties.Add("IngestionMappingReference", "mapping1");
eventData.Properties.Add("Tags", "['myDataTag']");
var events = new[] { eventData };
// Send events
await producerClient.SendAsync(events);
// This sample uses Azure.Messaging.EventHubs which is a .Net Framework library.
await using var producerClient = new EventHubProducerClient("<eventHubConnectionString>");
// Create the event and add optional "dynamic routing" properties
var eventData = new EventData(Encoding.UTF8.GetBytes(JsonConvert.SerializeObject(
    new { TimeStamp = DateTime.UtcNow, MetricName = "Temperature", Value = 32 }
)));
eventData.Properties.Add("Database", "MetricsDB");
eventData.Properties.Add("Table", "WeatherMetrics");
eventData.Properties.Add("Format", "json");
eventData.Properties.Add("IngestionMappingReference", "mapping1");
eventData.Properties.Add("Tags", "['myDataTag']");
var events = new[] { eventData };
// Send events
await producerClient.SendAsync(events);
Event Hubs system properties mapping
System properties are fields set by the Event Hubs service, at the time the event is enqueued.
Azure Data Explorer Event Hubs data connection can embed a predefined set of system properties into the data ingested into a table based on a given mapping.
Note
Embedding system properties is supported for json and tabular formats (i.e.JSON,MultiJSON,CSV,TSV,PSV,SCsv,SOHsv,TSVE).
JSON
MultiJSON
CSV
TSV
PSV
SCsv
SOHsv
TSVE
When using a non-supported format (i.e. TXT or compressed formats likeParquet,Avroetc.) the data will still be ingested, but the properties will be ignored.
Parquet
Avro
Embedding system properties is not supported when a compression of Event Hub messages is set. In such scenarios, an appropriate error will be emitted and the data will not be ingested.
For tabular data, system properties are supported only for single-record event messages.
For json data, system properties are also supported for multiple-record event messages. In such cases, the system properties are added only to the first record of the event message.
ForCSVmapping, properties are added at the beginning of the record in the order listed in the creation of the data connection. Don't rely on the order of these properties, as it may change in the future.
CSV
ForJSONmapping, properties are added according to property names in theSystem propertiestable.
JSON
Event Hubs service exposes the following system properties:
datetime
long
string
string
string
When you work withIoT Centralevent hubs, you can also embed IoT Hub system properties in the payload. For the complete list, seeIoT Hub system properties.
If you selectedEvent system propertiesin theData Sourcesection of the table, you must include the properties in the table schema and mapping.
Schema mapping examples
Table schema mapping example
If your data includes three columns (TimeStamp,MetricName, andValue) and the properties you include arex-opt-enqueued-timeandx-opt-offset, create or alter the table schema by using this command:
TimeStamp
MetricName
Value
x-opt-enqueued-time
x-opt-offset
.create-merge table TestTable (TimeStamp: datetime, MetricName: string, Value: int, EventHubEnqueuedTime:datetime, EventHubOffset:string)
.create-merge table TestTable (TimeStamp: datetime, MetricName: string, Value: int, EventHubEnqueuedTime:datetime, EventHubOffset:string)
CSV mapping example
Run the following commands to add data to the beginning of the record. Note ordinal values.
.create table TestTable ingestion csv mapping "CsvMapping1"
    '['
    '   { "column" : "TimeStamp", "Properties":{"Ordinal":"2"}},'
    '   { "column" : "MetricName", "Properties":{"Ordinal":"3"}},'
    '   { "column" : "Value", "Properties":{"Ordinal":"4"}},'
    '   { "column" : "EventHubEnqueuedTime", "Properties":{"Ordinal":"0"}},'
    '   { "column" : "EventHubOffset", "Properties":{"Ordinal":"1"}}'
    ']'
.create table TestTable ingestion csv mapping "CsvMapping1"
    '['
    '   { "column" : "TimeStamp", "Properties":{"Ordinal":"2"}},'
    '   { "column" : "MetricName", "Properties":{"Ordinal":"3"}},'
    '   { "column" : "Value", "Properties":{"Ordinal":"4"}},'
    '   { "column" : "EventHubEnqueuedTime", "Properties":{"Ordinal":"0"}},'
    '   { "column" : "EventHubOffset", "Properties":{"Ordinal":"1"}}'
    ']'
JSON mapping example
Data is added by using the system properties mapping. Run these commands:
.create table TestTable ingestion json mapping "JsonMapping1"
    '['
    '    { "column" : "TimeStamp", "Properties":{"Path":"$.TimeStamp"}},'
    '    { "column" : "MetricName", "Properties":{"Path":"$.MetricName"}},'
    '    { "column" : "Value", "Properties":{"Path":"$.Value"}},'
    '    { "column" : "EventHubEnqueuedTime", "Properties":{"Path":"$.x-opt-enqueued-time"}},'
    '    { "column" : "EventHubOffset", "Properties":{"Path":"$.x-opt-offset"}}'
    ']'
.create table TestTable ingestion json mapping "JsonMapping1"
    '['
    '    { "column" : "TimeStamp", "Properties":{"Path":"$.TimeStamp"}},'
    '    { "column" : "MetricName", "Properties":{"Path":"$.MetricName"}},'
    '    { "column" : "Value", "Properties":{"Path":"$.Value"}},'
    '    { "column" : "EventHubEnqueuedTime", "Properties":{"Path":"$.x-opt-enqueued-time"}},'
    '    { "column" : "EventHubOffset", "Properties":{"Path":"$.x-opt-offset"}}'
    ']'
Schema mapping for Event Hubs Capture Avro files
One way to consume Event Hubs data is tocapture events through Azure Event Hubs in Azure Blob Storage or Azure Data Lake Storage. You can then ingest the capture files as they are written using anEvent Grid Data Connection in Azure Data Explorer.
The schema of the capture files is different from the schema of the original event sent to Event Hubs. You should design the destination table schema with this difference in mind.
Specifically, the event payload is represented in the capture file as a byte array, and this array isn't automatically decoded by the Event Grid Azure Data Explorer data connection. For more information on the file schema for Event Hubs Avro capture data, seeExploring captured Avro files in Azure Event Hubs.
To correctly decode the event payload:
Map theBodyfield of the captured event to a column of typedynamicin the destination table.
Body
dynamic
Apply anupdate policythat converts the byte array into a readable string using theunicode_codepoints_to_string()function.
Ingest custom properties
When ingesting events from Event Hubs, data is taken from thebodysection of the event data object. However, Event Hubscustom propertiesare defined in thepropertiessection of the object and are not ingested. To ingest customer properties, you must embed them into the data inbodysection of the object.
body
properties
body
The following example compares the events data object containing custom propertycustomPropertyasdefinedby Event Hubs (left) with theembeddedproperty required for ingestion (right).
customProperty
{
"body":{
"value": 42
},
"properties":{
"customProperty": "123456789"
}
}
{
"body":{
"value": 42
},
"properties":{
"customProperty": "123456789"
}
}
{
"body":{
"value": 42,
"customProperty": "123456789"
}
}
{
"body":{
"value": 42,
"customProperty": "123456789"
}
}
You can use one of the following methods to embed custom properties into the data inbodysection of the event data object:
body
In Event Hubs, when creating the event data object, embed the custom properties as part of the data in thebodysection of the object.
body
Use Azure Stream Analytics toprocess events from the event hub and embed the custom propertiesin the event data. From Azure Stream Analytics you can ingest the data natively using theAzure Data Explorer output connector, or route the data into another event hub and from there into your cluster.
UseAzure Functionsto add the custom properties and then ingest the data.
Create Event Hubs
If you don't already have one,Create an event hub. Connecting to event hub can be managed through theAzure portal, programmatically withC#orPython, or with theAzure Resource Manager template.
Note
The ability to dynamically add partitions after creating an event hub is only available with Event Hubs Premium and Dedicated tiers. Consider the long-term scale when setting partition count.
Consumer groupmustbe unique per consumer. Create a consumer group dedicated to Azure Data Explorer connection.
Cross-region Event Hubs data connection
For best performance, create the event hub in the same region as the cluster. If this is not possible, consider usingPremiumorDedicatedEvent Hubs tiers. For a comparison of tiers, seeCompare Azure Event Hubs tiers.
Send events
See thesample appthat generates data and sends it to an event hub.
Note
To enable efficient processing of events from Event Hubs to Azure Data Explorer, avoid an unbalanced distribution of events across partitions. Uneven mapping can cause a highdiscovery latency. For more information, seeMapping of events to partitions.
Set up Geo-disaster recovery solution
Event hub offers aGeo-disaster recoverysolution.
Azure Data Explorer doesn't supportAliasevent hub namespaces. To implement the Geo-disaster recovery in your solution, create two event hub data connections: one for the primary namespace and one for the secondary namespace. Azure Data Explorer listens to both event hub connections.
Alias
Note
It's the user's responsibility to implement a failover from the primary namespace to the secondary namespace.
Related content
Ingest data from event hub into Azure Data Explorer
Create an event hub data connection for Azure Data Explorer using C#
Create an event hub data connection for Azure Data Explorer using Python
Create an event hub data connection for Azure Data Explorer using Azure Resource Manager template
Manage Event Hubs data connections in your free cluster
Ingest and query Azure Monitor logs with Azure Data Explorer
Feedback
Was this page helpful?
Additional resources