Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Enable semantic caching for Azure OpenAI APIs in Azure API Management
Article
2025-04-08
7 contributors
In this article
APPLIES TO: All API Management tiers
Enable semantic caching of responses to Azure OpenAI API requests to reduce bandwidth and processing requirements imposed on the backend APIs and lower latency perceived by API consumers. With semantic caching, you can return cached responses for identical prompts and also for prompts that are similar in meaning, even if the text isn't the same. For background, seeTutorial: Use Azure Cache for Redis as a semantic cache.
Note
The configuration steps in this article enable semantic caching for Azure OpenAI APIs. These steps can be generalized to enable semantic caching for corresponding large language model (LLM) APIs available through theAzure AI Model Inference API.
Prerequisites
One or more Azure OpenAI Service APIs must be added to your API Management instance. For more information, seeAdd an Azure OpenAI Service API to Azure API Management.
The Azure OpenAI service must have deployments for the following:Chat Completion API - Deployment used for API consumer callsEmbeddings API - Deployment used for semantic caching
Chat Completion API - Deployment used for API consumer calls
Embeddings API - Deployment used for semantic caching
The API Management instance must be configured to use managed identity authentication to the Azure OpenAI APIs. For more information, seeAuthenticate and authorize access to Azure OpenAI APIs using Azure API Management.
AnAzure Cache for Redis EnterpriseorAzure Managed Redisinstance. TheRediSearchmodule must be enabled on the Redis cache.NoteYou can only enable theRediSearchmodule when creating a new Azure Redis Enterprise or Azure Managed Redis cache. You can't add a module to an existing cache.Learn more
Note
You can only enable theRediSearchmodule when creating a new Azure Redis Enterprise or Azure Managed Redis cache. You can't add a module to an existing cache.Learn more
External cache configured in the Azure API Management instance. For steps, seeUse an external Redis-compatible cache in Azure API Management.
Test Chat API deployment
First, test the Azure OpenAI deployment to ensure that the Chat Completion API or Chat API is working as expected. For steps, seeImport an Azure OpenAI API to Azure API Management.
For example, test the Azure OpenAI Chat API by sending a POST request to the API endpoint with a prompt in the request body. The response should include the completion of the prompt. Example request:
POST https://my-api-management.azure-api.net/my-api/openai/deployments/chat-deployment/chat/completions?api-version=2024-02-01
POST https://my-api-management.azure-api.net/my-api/openai/deployments/chat-deployment/chat/completions?api-version=2024-02-01
with request body:
{"messages":[{"role":"user","content":"Hello"}]}
{"messages":[{"role":"user","content":"Hello"}]}
When the request succeeds, the response includes a completion for the chat message.
Create a backend for embeddings API
Configure abackendresource for the embeddings API deployment with the following settings:
Name- A name of your choice, such asembeddings-backend. You use this name to reference the backend in policies.
embeddings-backend
Type- SelectCustom URL.
Runtime URL- The URL of the embeddings API deployment in the Azure OpenAI Service, similar to:https://my-aoai.openai.azure.com/openai/deployments/embeddings-deployment/embeddings
https://my-aoai.openai.azure.com/openai/deployments/embeddings-deployment/embeddings
Authorization credentials- Go toManaged Identitytab.Client identity- SelectSystem assigned identityor type in a User assigned managed identity client ID.Resource ID- Enterhttps://cognitiveservices.azure.com/for Azure OpenAI Service.
Client identity- SelectSystem assigned identityor type in a User assigned managed identity client ID.
Resource ID- Enterhttps://cognitiveservices.azure.com/for Azure OpenAI Service.
https://cognitiveservices.azure.com/
Test backend
To test the backend, create an API operation for your Azure OpenAI Service API:
On theDesigntab of your API, select+ Add operation.
Enter aDisplay nameand optionally aNamefor the operation.
In theFrontendsection, inURL, selectPOSTand enter the path/.
/
On theHeaderstab, add a required header with the nameContent-Typeand valueapplication/json.
Content-Type
application/json
SelectSave
Configure the following policies in theInbound processingsection of the API operation. In theset-backend-servicepolicy, substitute the name of the backend you created.
<policies>
    <inbound>
        <set-backend-service backend-id="embeddings-backend" />
        <authentication-managed-identity resource="https://cognitiveservices.azure.com/" />
        [...]
    </inbound>
    [...]
</policies>
<policies>
    <inbound>
        <set-backend-service backend-id="embeddings-backend" />
        <authentication-managed-identity resource="https://cognitiveservices.azure.com/" />
        [...]
    </inbound>
    [...]
</policies>
On theTesttab, test the operation by adding anapi-versionquery parameter with value such as2024-02-01. Provide a valid request body. For example:
api-version
2024-02-01
{"input":"Hello"}
{"input":"Hello"}
If the request is successful, the response includes a vector representation of the input text:
{
    "object": "list",
    "data": [{
        "object": "embedding",
        "index": 0,
        "embedding": [
            -0.021829502,
            -0.007157768,
            -0.028619017,
            [...]
        ]
    }]
}
{
    "object": "list",
    "data": [{
        "object": "embedding",
        "index": 0,
        "embedding": [
            -0.021829502,
            -0.007157768,
            -0.028619017,
            [...]
        ]
    }]
}
Configure semantic caching policies
To enable semantic caching for Azure OpenAI APIs in Azure API Management, apply the following policies: one to check the cache before sending requests (lookup) and another to store responses for future reuse (store):
In theInbound processingsection for the API, add theazure-openai-semantic-cache-lookuppolicy. In theembeddings-backend-idattribute, specify the Embeddings API backend you created.NoteWhen enabling semantic caching for other large language model APIs, use thellm-semantic-cache-lookuppolicy instead.Example:<azure-openai-semantic-cache-lookup
    score-threshold="0.8"
    embeddings-backend-id="embeddings-deployment"
    embeddings-backend-auth="system-assigned"
    ignore-system-messages="true"
    max-message-count="10">
    <vary-by>@(context.Subscription.Id)</vary-by>
</azure-openai-semantic-cache-lookup>
In theInbound processingsection for the API, add theazure-openai-semantic-cache-lookuppolicy. In theembeddings-backend-idattribute, specify the Embeddings API backend you created.
embeddings-backend-id
Note
When enabling semantic caching for other large language model APIs, use thellm-semantic-cache-lookuppolicy instead.
Example:
<azure-openai-semantic-cache-lookup
    score-threshold="0.8"
    embeddings-backend-id="embeddings-deployment"
    embeddings-backend-auth="system-assigned"
    ignore-system-messages="true"
    max-message-count="10">
    <vary-by>@(context.Subscription.Id)</vary-by>
</azure-openai-semantic-cache-lookup>
<azure-openai-semantic-cache-lookup
    score-threshold="0.8"
    embeddings-backend-id="embeddings-deployment"
    embeddings-backend-auth="system-assigned"
    ignore-system-messages="true"
    max-message-count="10">
    <vary-by>@(context.Subscription.Id)</vary-by>
</azure-openai-semantic-cache-lookup>
In theOutbound processingsection for the API, add theazure-openai-semantic-cache-storepolicy.NoteWhen enabling semantic caching for other large language model APIs, use thellm-semantic-cache-storepolicy instead.Example:<azure-openai-semantic-cache-store duration="60" />
In theOutbound processingsection for the API, add theazure-openai-semantic-cache-storepolicy.
Note
When enabling semantic caching for other large language model APIs, use thellm-semantic-cache-storepolicy instead.
Example:
<azure-openai-semantic-cache-store duration="60" />
<azure-openai-semantic-cache-store duration="60" />
Confirm caching
To confirm that semantic caching is working as expected, trace a test Completion or Chat Completion operation using the test console in the portal. Confirm that the cache was used on subsequent tries by inspecting the trace.Learn more about tracing API calls in Azure API Management.
For example, if the cache was used, theOutputsection includes entries similar to ones in the following screenshot:

Related content
Caching policies
Azure Cache for Redis
AI gateway capabilitiesin Azure API Management
Feedback
Was this page helpful?
Additional resources