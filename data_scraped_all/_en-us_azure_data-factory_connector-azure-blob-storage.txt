Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Copy and transform data in Azure Blob Storage by using Azure Data Factory or Azure Synapse Analytics
Article
2024-09-25
33 contributors
In this article
APPLIES TO:Azure Data FactoryAzure Synapse Analytics
Tip
Try outData Factory in Microsoft Fabric, an all-in-one analytics solution for enterprises.Microsoft Fabriccovers everything from data movement to data science, real-time analytics, business intelligence, and reporting. Learn how tostart a new trialfor free!
This article outlines how to use the Copy activity in Azure Data Factory and Azure Synapse pipelines to copy data from and to Azure Blob Storage. It also describes how to use the Data Flow activity to transform data in Azure Blob Storage. To learn more, read theAzure Data Factoryand theAzure Synapse Analyticsintroduction articles.
Tip
To learn about a migration scenario for a data lake or a data warehouse, see the articleMigrate data from your data lake or data warehouse to Azure.
Supported capabilities
This Azure Blob Storage connector is supported for the following capabilities:
â  Azure integration runtime â¡ Self-hosted integration runtime
For the Copy activity, this Blob storage connector supports:
Copying blobs to and from general-purpose Azure storage accounts and hot/cool blob storage.
Copying blobs by using an account key, a service shared access signature (SAS), a service principal, or managed identities for Azure resource authentications.
Copying blobs from block, append, or page blobs and copying data to only block blobs.
Copying blobs as is, or parsing or generating blobs withsupported file formats and compression codecs.
Preserving file metadata during copy.
Get started
To perform the Copy activity with a pipeline, you can use one of the following tools or SDKs:
The Copy Data tool
The Azure portal
The .NET SDK
The Python SDK
Azure PowerShell
The REST API
The Azure Resource Manager template
Create an Azure Blob Storage linked service using UI
Use the following steps to create an Azure Blob Storage linked service in the Azure portal UI.
Browse to the Manage tab in your Azure Data Factory or Synapse workspace and select Linked Services, then select New:Azure Data FactoryAzure Synapse
Browse to the Manage tab in your Azure Data Factory or Synapse workspace and select Linked Services, then select New:
Azure Data Factory
Azure Synapse


Search for blob and select the Azure Blob Storage connector.
Search for blob and select the Azure Blob Storage connector.

Configure the service details, test the connection, and create the new linked service.
Configure the service details, test the connection, and create the new linked service.

Connector configuration details
The following sections provide details about properties that are used to define Data Factory and Synapse pipeline entities specific to Blob storage.
Linked service properties
This Blob storage connector supports the following authentication types. See the corresponding sections for details.
Anonymous authentication
Account key authentication
Shared access signature authentication
Service principal authentication
System-assigned managed identity authentication
User-assigned managed identity authentication
Note
If want to use the public Azure integration runtime to connect to your Blob storage by leveraging theAllow trusted Microsoft services to access this storage accountoption enabled on Azure Storage firewall, you must usemanaged identity authentication. For more information about the Azure Storage firewalls settings, seeConfigure Azure Storage firewalls and virtual networks.
When you use PolyBase or COPY statement to load data into Azure Synapse Analytics, if your source or staging Blob storage is configured with an Azure Virtual Network endpoint, you must use managed identity authentication as required by Azure Synapse. See theManaged identity authenticationsection for more configuration prerequisites.
Note
Azure HDInsight and Azure Machine Learning activities only support authentication that uses Azure Blob Storage account keys.
Anonymous authentication
The following properties are supported for storage account key authentication in Azure Data Factory or Synapse pipelines:
type
AzureBlobStorage
AzureStorage
https://<AccountName>.blob.core.windows.net/<ContainerName>
Example:
{
    "name": "AzureBlobStorageAnonymous",
    "properties": {
        "annotations": [],
        "type": "AzureBlobStorage",
        "typeProperties": {
            "containerUri": "https:// <accountname>.blob.core.windows.net/ <containername>",
            "authenticationType": "Anonymous"
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
{
    "name": "AzureBlobStorageAnonymous",
    "properties": {
        "annotations": [],
        "type": "AzureBlobStorage",
        "typeProperties": {
            "containerUri": "https:// <accountname>.blob.core.windows.net/ <containername>",
            "authenticationType": "Anonymous"
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
Examples UI:
The UI experience is as described in the following image. This sample uses the Azure open dataset as the source. If you want to get the opendataset bing_covid-19_data.csv, you just need to chooseAuthentication typeasAnonymousand fill in Container URI withhttps://pandemicdatalake.blob.core.windows.net/public.
https://pandemicdatalake.blob.core.windows.net/public

Account key authentication
The following properties are supported for storage account key authentication in Azure Data Factory or Synapse pipelines:
type
AzureBlobStorage
AzureStorage
connectionString
accountKey
Note
A secondary Blob service endpoint isn't supported when you're using account key authentication. You can use other authentication types.
Note
If you're using theAzureStoragetype linked service, it's still supported as is. But we suggest that you use the newAzureBlobStoragelinked service type going forward.
AzureStorage
AzureBlobStorage
Example:
{
    "name": "AzureBlobStorageLinkedService",
    "properties": {
        "type": "AzureBlobStorage",
        "typeProperties": {
          "connectionString": "DefaultEndpointsProtocol=https;AccountName=<accountname>;AccountKey=<accountkey>"
        },
        "connectVia": {
          "referenceName": "<name of Integration Runtime>",
          "type": "IntegrationRuntimeReference"
        }
    }
}
{
    "name": "AzureBlobStorageLinkedService",
    "properties": {
        "type": "AzureBlobStorage",
        "typeProperties": {
          "connectionString": "DefaultEndpointsProtocol=https;AccountName=<accountname>;AccountKey=<accountkey>"
        },
        "connectVia": {
          "referenceName": "<name of Integration Runtime>",
          "type": "IntegrationRuntimeReference"
        }
    }
}
Example: store the account key in Azure Key Vault
{
    "name": "AzureBlobStorageLinkedService",
    "properties": {
        "type": "AzureBlobStorage",
        "typeProperties": {
            "connectionString": "DefaultEndpointsProtocol=https;AccountName=<accountname>;",
            "accountKey": {
                "type": "AzureKeyVaultSecret",
                "store": {
                    "referenceName": "<Azure Key Vault linked service name>",
                    "type": "LinkedServiceReference"
                },
                "secretName": "<secretName>"
            }
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
{
    "name": "AzureBlobStorageLinkedService",
    "properties": {
        "type": "AzureBlobStorage",
        "typeProperties": {
            "connectionString": "DefaultEndpointsProtocol=https;AccountName=<accountname>;",
            "accountKey": {
                "type": "AzureKeyVaultSecret",
                "store": {
                    "referenceName": "<Azure Key Vault linked service name>",
                    "type": "LinkedServiceReference"
                },
                "secretName": "<secretName>"
            }
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
Shared access signature authentication
A shared access signature provides delegated access to resources in your storage account. You can use a shared access signature to grant a client limited permissions to objects in your storage account for a specified time.
You don't have to share your account access keys. The shared access signature is a URI that encompasses in its query parameters all the information necessary for authenticated access to a storage resource. To access storage resources with the shared access signature, the client only needs to pass in the shared access signature to the appropriate constructor or method.
For more information about shared access signatures, seeShared access signatures: Understand the shared access signature model.
Note
The service now supports bothservice shared access signaturesandaccount shared access signatures. For more information about shared access signatures, seeGrant limited access to Azure Storage resources using shared access signatures.
In later dataset configurations, the folder path is the absolute path starting from the container level. You need to configure one aligned with the path in your SAS URI.
The following properties are supported for using shared access signature authentication:
type
AzureBlobStorage
AzureStorage
SecureString
Note
If you're using theAzureStoragetype linked service, it's still supported as is. But we suggest that you use the newAzureBlobStoragelinked service type going forward.
AzureStorage
AzureBlobStorage
Example:
{
    "name": "AzureBlobStorageLinkedService",
    "properties": {
        "type": "AzureBlobStorage",
        "typeProperties": {
            "sasUri": {
                "type": "SecureString",
                "value": "<SAS URI of the Azure Storage resource e.g. https://<accountname>.blob.core.windows.net/?sv=<storage version>&st=<start time>&se=<expire time>&sr=<resource>&sp=<permissions>&sip=<ip range>&spr=<protocol>&sig=<signature>>"
            }
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
{
    "name": "AzureBlobStorageLinkedService",
    "properties": {
        "type": "AzureBlobStorage",
        "typeProperties": {
            "sasUri": {
                "type": "SecureString",
                "value": "<SAS URI of the Azure Storage resource e.g. https://<accountname>.blob.core.windows.net/?sv=<storage version>&st=<start time>&se=<expire time>&sr=<resource>&sp=<permissions>&sip=<ip range>&spr=<protocol>&sig=<signature>>"
            }
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
Example: store the account key in Azure Key Vault
{
    "name": "AzureBlobStorageLinkedService",
    "properties": {
        "type": "AzureBlobStorage",
        "typeProperties": {
            "sasUri": {
                "type": "SecureString",
                "value": "<SAS URI of the Azure Storage resource without token e.g. https://<accountname>.blob.core.windows.net/>"
            },
            "sasToken": {
                "type": "AzureKeyVaultSecret",
                "store": {
                    "referenceName": "<Azure Key Vault linked service name>", 
                    "type": "LinkedServiceReference"
                },
                "secretName": "<secretName with value of SAS token e.g. ?sv=<storage version>&st=<start time>&se=<expire time>&sr=<resource>&sp=<permissions>&sip=<ip range>&spr=<protocol>&sig=<signature>>"
            }
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
{
    "name": "AzureBlobStorageLinkedService",
    "properties": {
        "type": "AzureBlobStorage",
        "typeProperties": {
            "sasUri": {
                "type": "SecureString",
                "value": "<SAS URI of the Azure Storage resource without token e.g. https://<accountname>.blob.core.windows.net/>"
            },
            "sasToken": {
                "type": "AzureKeyVaultSecret",
                "store": {
                    "referenceName": "<Azure Key Vault linked service name>", 
                    "type": "LinkedServiceReference"
                },
                "secretName": "<secretName with value of SAS token e.g. ?sv=<storage version>&st=<start time>&se=<expire time>&sr=<resource>&sp=<permissions>&sip=<ip range>&spr=<protocol>&sig=<signature>>"
            }
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
When you create a shared access signature URI, consider the following points:
Set appropriate read/write permissions on objects based on how the linked service (read, write, read/write) is used.
SetExpiry timeappropriately. Make sure that the access to Storage objects doesn't expire within the active period of the pipeline.
The URI should be created at the right container or blob based on the need. A shared access signature URI to a blob allows the data factory or Synapse pipeline to access that particular blob. A shared access signature URI to a Blob storage container allows the data factory or Synapse pipeline to iterate through blobs in that container. To provide access to more or fewer objects later, or to update the shared access signature URI, remember to update the linked service with the new URI.
Service principal authentication
For general information about Azure Storage service principal authentication, seeAuthenticate access to Azure Storage using Microsoft Entra ID.
To use service principal authentication, follow these steps:
Register an application with the Microsoft identity platform. To learn how, seeQuickstart: Register an application with the Microsoft identity platform. Make note of these values, which you use to define the linked service:Application IDApplication keyTenant ID
Register an application with the Microsoft identity platform. To learn how, seeQuickstart: Register an application with the Microsoft identity platform. Make note of these values, which you use to define the linked service:
Application ID
Application key
Tenant ID
Grant the service principal proper permission in Azure Blob Storage. For more information on the roles, seeUse the Azure portal to assign an Azure role for access to blob and queue data.As source, inAccess control (IAM), grant at least theStorage Blob Data Readerrole.As sink, inAccess control (IAM), grant at least theStorage Blob Data Contributorrole.
Grant the service principal proper permission in Azure Blob Storage. For more information on the roles, seeUse the Azure portal to assign an Azure role for access to blob and queue data.
As source, inAccess control (IAM), grant at least theStorage Blob Data Readerrole.
As sink, inAccess control (IAM), grant at least theStorage Blob Data Contributorrole.
These properties are supported for an Azure Blob Storage linked service:
https://<accountName>.blob.core.windows.net/
Note
If your blob account enablessoft delete, service principal authentication isn't supported in Data Flow.
If you access the blob storage through private endpoint using Data Flow, note when service principal authentication is used Data Flow connects to the ADLS Gen2 endpoint instead of Blob endpoint. Make sure you create the corresponding private endpoint in your data factory or Synapse workspace to enable access.
Note
Service principal authentication is supported only by the "AzureBlobStorage" type linked service, not the previous "AzureStorage" type linked service.
Example:
{
    "name": "AzureBlobStorageLinkedService",
    "properties": {
        "type": "AzureBlobStorage",
        "typeProperties": {            
            "serviceEndpoint": "https://<accountName>.blob.core.windows.net/",
            "accountKind": "StorageV2",
            "servicePrincipalId": "<service principal id>",
            "servicePrincipalKey": {
                "type": "SecureString",
                "value": "<service principal key>"
            },
            "tenant": "<tenant info, e.g. microsoft.onmicrosoft.com>" 
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
{
    "name": "AzureBlobStorageLinkedService",
    "properties": {
        "type": "AzureBlobStorage",
        "typeProperties": {            
            "serviceEndpoint": "https://<accountName>.blob.core.windows.net/",
            "accountKind": "StorageV2",
            "servicePrincipalId": "<service principal id>",
            "servicePrincipalKey": {
                "type": "SecureString",
                "value": "<service principal key>"
            },
            "tenant": "<tenant info, e.g. microsoft.onmicrosoft.com>" 
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
System-assigned managed identity authentication
A data factory or Synapse pipeline can be associated with asystem-assigned managed identity for Azure resources, which represents that resource for authentication to other Azure services. You can directly use this system-assigned managed identity for Blob storage authentication, which is similar to using your own service principal. It allows this designated resource to access and copy data from or to Blob storage. To learn more about managed identities for Azure resources, seeManaged identities for Azure resources
For general information about Azure Storage authentication, seeAuthenticate access to Azure Storage using Microsoft Entra ID. To use managed identities for Azure resource authentication, follow these steps:
Retrieve system-assigned managed identity informationby copying the value of the system-assigned managed identity object ID generated along with your factory or Synapse workspace.
Retrieve system-assigned managed identity informationby copying the value of the system-assigned managed identity object ID generated along with your factory or Synapse workspace.
Grant the managed identity permission in Azure Blob Storage. For more information on the roles, seeUse the Azure portal to assign an Azure role for access to blob and queue data.As source, inAccess control (IAM), grant at least theStorage Blob Data Readerrole.As sink, inAccess control (IAM), grant at least theStorage Blob Data Contributorrole.
Grant the managed identity permission in Azure Blob Storage. For more information on the roles, seeUse the Azure portal to assign an Azure role for access to blob and queue data.
As source, inAccess control (IAM), grant at least theStorage Blob Data Readerrole.
As sink, inAccess control (IAM), grant at least theStorage Blob Data Contributorrole.
These properties are supported for an Azure Blob Storage linked service:
https://<accountName>.blob.core.windows.net/
Example:
{
    "name": "AzureBlobStorageLinkedService",
    "properties": {
        "type": "AzureBlobStorage",
        "typeProperties": {            
            "serviceEndpoint": "https://<accountName>.blob.core.windows.net/",
            "accountKind": "StorageV2" 
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
{
    "name": "AzureBlobStorageLinkedService",
    "properties": {
        "type": "AzureBlobStorage",
        "typeProperties": {            
            "serviceEndpoint": "https://<accountName>.blob.core.windows.net/",
            "accountKind": "StorageV2" 
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
User-assigned managed identity authentication
A data factory can be assigned with one or multipleuser-assigned managed identities. You can use this user-assigned managed identity for Blob storage authentication, which allows to access and copy data from or to Blob storage. To learn more about managed identities for Azure resources, seeManaged identities for Azure resources
For general information about Azure storage authentication, seeAuthenticate access to Azure Storage using Microsoft Entra ID. To use user-assigned managed identity authentication, follow these steps:
Create one or multiple user-assigned managed identitiesand grant permission in Azure Blob Storage. For more information on the roles, seeUse the Azure portal to assign an Azure role for access to blob and queue data.As source, inAccess control (IAM), grant at least theStorage Blob Data Readerrole.As sink, inAccess control (IAM), grant at least theStorage Blob Data Contributorrole.
Create one or multiple user-assigned managed identitiesand grant permission in Azure Blob Storage. For more information on the roles, seeUse the Azure portal to assign an Azure role for access to blob and queue data.
As source, inAccess control (IAM), grant at least theStorage Blob Data Readerrole.
As sink, inAccess control (IAM), grant at least theStorage Blob Data Contributorrole.
Assign one or multiple user-assigned managed identities to your data factory andcreate credentialsfor each user-assigned managed identity.
Assign one or multiple user-assigned managed identities to your data factory andcreate credentialsfor each user-assigned managed identity.
These properties are supported for an Azure Blob Storage linked service:
https://<accountName>.blob.core.windows.net/
Example:
{
    "name": "AzureBlobStorageLinkedService",
    "properties": {
        "type": "AzureBlobStorage",
        "typeProperties": {            
            "serviceEndpoint": "https://<accountName>.blob.core.windows.net/",
            "accountKind": "StorageV2",
            "credential": {
                "referenceName": "credential1",
                "type": "CredentialReference"
            }
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
{
    "name": "AzureBlobStorageLinkedService",
    "properties": {
        "type": "AzureBlobStorage",
        "typeProperties": {            
            "serviceEndpoint": "https://<accountName>.blob.core.windows.net/",
            "accountKind": "StorageV2",
            "credential": {
                "referenceName": "credential1",
                "type": "CredentialReference"
            }
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
Important
If you use PolyBase or COPY statement to load data from Blob storage (as a source or as staging) into Azure Synapse Analytics, when you use managed identity authentication for Blob storage, make sure you also follow steps 1 to 3 inthis guidance. Those steps will register your server with Microsoft Entra ID and assign the Storage Blob Data Contributor role to your server. Data Factory handles the rest. If you configure Blob storage with an Azure Virtual Network endpoint, you also need to haveAllow trusted Microsoft services to access this storage accountturned on under Azure Storage accountFirewalls and Virtual networkssettings menu as required by Azure Synapse.
Note
If your blob account enablessoft delete, system-assigned/user-assigned managed identity authentication isn't supported in Data Flow.
If you access the blob storage through private endpoint using Data Flow, note when system-assigned/user-assigned managed identity authentication is used Data Flow connects to the ADLS Gen2 endpoint instead of Blob endpoint. Make sure you create the corresponding private endpoint in ADF to enable access.
Note
System-assigned/user-assigned managed identity authentication is supported only by the "AzureBlobStorage" type linked service, not the previous "AzureStorage" type linked service.
Dataset properties
For a full list of sections and properties available for defining datasets, see theDatasetsarticle.
Azure Data Factory supports the following file formats. Refer to each article for format-based settings.
Avro format
Binary format
Delimited text format
Excel format
JSON format
ORC format
Parquet format
XML format
The following properties are supported for Azure Blob Storage underlocationsettings in a format-based dataset:
location
Example:
{
    "name": "DelimitedTextDataset",
    "properties": {
        "type": "DelimitedText",
        "linkedServiceName": {
            "referenceName": "<Azure Blob Storage linked service name>",
            "type": "LinkedServiceReference"
        },
        "schema": [ < physical schema, optional, auto retrieved during authoring > ],
        "typeProperties": {
            "location": {
                "type": "AzureBlobStorageLocation",
                "container": "containername",
                "folderPath": "folder/subfolder"
            },
            "columnDelimiter": ",",
            "quoteChar": "\"",
            "firstRowAsHeader": true,
            "compressionCodec": "gzip"
        }
    }
}
{
    "name": "DelimitedTextDataset",
    "properties": {
        "type": "DelimitedText",
        "linkedServiceName": {
            "referenceName": "<Azure Blob Storage linked service name>",
            "type": "LinkedServiceReference"
        },
        "schema": [ < physical schema, optional, auto retrieved during authoring > ],
        "typeProperties": {
            "location": {
                "type": "AzureBlobStorageLocation",
                "container": "containername",
                "folderPath": "folder/subfolder"
            },
            "columnDelimiter": ",",
            "quoteChar": "\"",
            "firstRowAsHeader": true,
            "compressionCodec": "gzip"
        }
    }
}
Copy activity properties
For a full list of sections and properties available for defining activities, see thePipelinesarticle. This section provides a list of properties that the Blob storage source and sink support.
Blob storage as a source type
Azure Data Factory supports the following file formats. Refer to each article for format-based settings.
Avro format
Binary format
Delimited text format
Excel format
JSON format
ORC format
Parquet format
XML format
The following properties are supported for Azure Blob Storage understoreSettingssettings in a format-based copy source:
storeSettings
storeSettings
wildcardFileName
*
container_in_dataset/this_prefix
container/folder/subfolder/file.txt
folder/sub
subfolder/file.txt
*
?
^
*
?
^
fileListPath
modifiedDatetimeStart
modifiedDatetimeEnd
modifiedDatetimeStart
modifiedDatetimeEnd
modifiedDatetimeEnd
modifiedDatetimeStart
fileListPath
month
day
Note
For Parquet/delimited text format, theBlobSourcetype for the Copy activity source mentioned in the next section is still supported as is for backward compatibility. We suggest that you use the new model until the authoring UI has switched to generating these new types.
Example:
"activities":[
    {
        "name": "CopyFromBlob",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "<Delimited text input dataset name>",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "<output dataset name>",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "DelimitedTextSource",
                "formatSettings":{
                    "type": "DelimitedTextReadSettings",
                    "skipLineCount": 10
                },
                "storeSettings":{
                    "type": "AzureBlobStorageReadSettings",
                    "recursive": true,
                    "wildcardFolderPath": "myfolder*A",
                    "wildcardFileName": "*.csv"
                }
            },
            "sink": {
                "type": "<sink type>"
            }
        }
    }
]
"activities":[
    {
        "name": "CopyFromBlob",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "<Delimited text input dataset name>",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "<output dataset name>",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "DelimitedTextSource",
                "formatSettings":{
                    "type": "DelimitedTextReadSettings",
                    "skipLineCount": 10
                },
                "storeSettings":{
                    "type": "AzureBlobStorageReadSettings",
                    "recursive": true,
                    "wildcardFolderPath": "myfolder*A",
                    "wildcardFileName": "*.csv"
                }
            },
            "sink": {
                "type": "<sink type>"
            }
        }
    }
]
Note
The$logscontainer, which is automatically created when Storage Analytics is enabled for a storage account, isn't shown when a container listing operation is performed via the UI. The file path must be provided directly for your data factory or Synapse pipeline to consume files from the$logscontainer.
$logs
$logs
Blob storage as a sink type
Azure Data Factory supports the following file formats. Refer to each article for format-based settings.
Avro format
Binary format
Delimited text format
JSON format
ORC format
Parquet format
The following properties are supported for Azure Blob Storage understoreSettingssettings in a format-based copy sink:
storeSettings
type
storeSettings
AzureBlobStorageWriteSettings
blockSizeInMB*50000
metadata
name
value
$$LASTMODIFIED
Example:
"activities":[
    {
        "name": "CopyFromBlob",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "<input dataset name>",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "<Parquet output dataset name>",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "<source type>"
            },
            "sink": {
                "type": "ParquetSink",
                "storeSettings":{
                    "type": "AzureBlobStorageWriteSettings",
                    "copyBehavior": "PreserveHierarchy",
                    "metadata": [
                        {
                            "name": "testKey1",
                            "value": "value1"
                        },
                        {
                            "name": "testKey2",
                            "value": "value2"
                        },
                        {
                            "name": "lastModifiedKey",
                            "value": "$$LASTMODIFIED"
                        }
                    ]
                }
            }
        }
    }
]
"activities":[
    {
        "name": "CopyFromBlob",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "<input dataset name>",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "<Parquet output dataset name>",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "<source type>"
            },
            "sink": {
                "type": "ParquetSink",
                "storeSettings":{
                    "type": "AzureBlobStorageWriteSettings",
                    "copyBehavior": "PreserveHierarchy",
                    "metadata": [
                        {
                            "name": "testKey1",
                            "value": "value1"
                        },
                        {
                            "name": "testKey2",
                            "value": "value2"
                        },
                        {
                            "name": "lastModifiedKey",
                            "value": "$$LASTMODIFIED"
                        }
                    ]
                }
            }
        }
    }
]
Folder and file filter examples
This section describes the resulting behavior of the folder path and file name with wildcard filters.
container/Folder*
container/Folder*
container/Folder*
*.csv
container/Folder*
*.csv
File list examples
This section describes the resulting behavior of using a file list path in the Copy activity source.
Assume that you have the following source folder structure and want to copy the files in bold:
container
FolderA
container/Metadata/FileListToCopy.txt
Some recursive and copyBehavior examples
This section describes the resulting behavior of the Copy operation for different combinations ofrecursiveandcopyBehaviorvalues.
Preserving metadata during copy
When you copy files from Amazon S3, Azure Blob Storage, or Azure Data Lake Storage Gen2 to Azure Data Lake Storage Gen2 or Azure Blob Storage, you can choose to preserve the file metadata along with data. Learn more fromPreserve metadata.
Mapping data flow properties
When you're transforming data in mapping data flows, you can read and write files from Azure Blob Storage in the following formats:
Avro
Delimited text
Delta
Excel
JSON
Parquet
Format specific settings are located in the documentation for that format. For more information, seeSource transformation in mapping data flowandSink transformation in mapping data flow.
Source transformation
In source transformation, you can read from a container, folder, or individual file in Azure Blob Storage. Use theSource optionstab to manage how the files are read.

Wildcard paths:Using a wildcard pattern will instruct the service to loop through each matching folder and file in a single source transformation. This is an effective way to process multiple files within a single flow. Add multiple wildcard matching patterns with the plus sign that appears when you hover over your existing wildcard pattern.
From your source container, choose a series of files that match a pattern. Only a container can be specified in the dataset. Your wildcard path must therefore also include your folder path from the root folder.
Wildcard examples:
*Represents any set of characters.
*Represents any set of characters.
*
**Represents recursive directory nesting.
**Represents recursive directory nesting.
**
?Replaces one character.
?Replaces one character.
?
[]Matches one or more characters in the brackets.
[]Matches one or more characters in the brackets.
[]
/data/sales/**/*.csvGets all .csv files under /data/sales.
/data/sales/**/*.csvGets all .csv files under /data/sales.
/data/sales/**/*.csv
/data/sales/20??/**/Gets all files in the 20th century.
/data/sales/20??/**/Gets all files in the 20th century.
/data/sales/20??/**/
/data/sales/*/*/*.csvGets .csv files two levels under /data/sales.
/data/sales/*/*/*.csvGets .csv files two levels under /data/sales.
/data/sales/*/*/*.csv
/data/sales/2004/*/12/[XY]1?.csvGets all .csv files in December 2004 starting with X or Y prefixed by a two-digit number.
/data/sales/2004/*/12/[XY]1?.csvGets all .csv files in December 2004 starting with X or Y prefixed by a two-digit number.
/data/sales/2004/*/12/[XY]1?.csv
Partition root path:If you have partitioned folders in your file source with  akey=valueformat (for example,year=2019), then you can assign the top level of that partition folder tree to a column name in your data flow's data stream.
key=value
year=2019
First, set a wildcard to include all paths that are the partitioned folders plus the leaf files that you want to read.

Use thePartition root pathsetting to define what the top level of the folder structure is. When you view the contents of your data via a data preview, you'll see that the service adds the resolved partitions found in each of your folder levels.

List of files:This is a file set. Create a text file that includes a list of relative path files to process. Point to this text file.
Column to store file name:Store the name of the source file in a column in your data. Enter a new column name here to store the file name string.
After completion:Choose to do nothing with the source file after the data flow runs, delete the source file, or move the source file. The paths for the move are relative.
To move source files to another location post-processing, first select "Move" for file operation. Then, set the "from" directory. If you're not using any wildcards for your path, then the "from" setting will be the same folder as your source folder.
If you have a source path with wildcard, your syntax is as follows:
/data/sales/20??/**/*.csv
/data/sales/20??/**/*.csv
You can specify "from" as:
/data/sales
/data/sales
And you can specify "to" as:
/backup/priorSales
/backup/priorSales
In this case, all files that were sourced under/data/salesare moved to/backup/priorSales.
/data/sales
/backup/priorSales
Note
File operations run only when you start the data flow from a pipeline run (a pipeline debug or execution run) that uses the Execute Data Flow activity in a pipeline. File operationsdon'trun in Data Flow debug mode.
Filter by last modified:You can filter the files to be processed by specifying a date range of when they were last modified. All datetimes are in UTC.
Enable change data capture:If true, you'll get new or changed files only from the last run. Initial load of full snapshot data will always be gotten in the first run, followed by capturing new or changed files only in next runs.

Sink properties
In the sink transformation, you can write to either a container or a folder in Azure Blob Storage. Use theSettingstab to manage how the files get written.

Clear the folder:Determines whether or not the destination folder gets cleared before the data is written.
File name option:Determines how the destination files are named in the destination folder. The file name options are:
Default: Allow Spark to name files based on PART defaults.
Pattern: Enter a pattern that enumerates your output files per partition. For example,loans[n].csvcreatesloans1.csv,loans2.csv, and so on.
loans[n].csv
loans1.csv
loans2.csv
Per partition: Enter one file name per partition.
As data in column: Set the output file to the value of a column. The path is relative to the dataset container, not the destination folder. If you have a folder path in your dataset, it is overridden.
Output to a single file: Combine the partitioned output files into a single named file. The path is relative to the dataset folder. Be aware that the merge operation can possibly fail based on node size. We don't recommend this option for large datasets.
Quote all:Determines whether to enclose all values in quotation marks.
Lookup activity properties
To learn details about the properties, checkLookup activity.
GetMetadata activity properties
To learn details about the properties, checkGetMetadata activity.
Delete activity properties
To learn details about the properties, checkDelete activity.
Legacy models
Note
The following models are still supported as is for backward compatibility. We suggest that you use the new model mentioned earlier. The authoring UI has switched to generating the new model.
Legacy dataset model
type
AzureBlob
*
?
^
myblobcontainer/myblobfolder/
folderPath
*
?
"fileName": "*.csv"
"fileName": "???20180427.txt"
^
fileName
preserveHierarchy
[table name].[format].[compression if configured]
modifiedDatetimeStart
modifiedDatetimeEnd
NULL
modifiedDatetimeStart
modifiedDatetimeEnd
NULL
modifiedDatetimeEnd
modifiedDatetimeStart
NULL
modifiedDatetimeStart
modifiedDatetimeEnd
NULL
modifiedDatetimeStart
modifiedDatetimeEnd
NULL
modifiedDatetimeEnd
modifiedDatetimeStart
NULL
Tip
To copy all blobs under a folder, specifyfolderPathonly.To copy a single blob with a given name, specifyfolderPathfor the folder part andfileNamefor the file name.To copy a subset of blobs under a folder, specifyfolderPathfor the folder part andfileNamewith a wildcard filter.
Example:
{
    "name": "AzureBlobDataset",
    "properties": {
        "type": "AzureBlob",
        "linkedServiceName": {
            "referenceName": "<Azure Blob Storage linked service name>",
            "type": "LinkedServiceReference"
        },
        "typeProperties": {
            "folderPath": "mycontainer/myfolder",
            "fileName": "*",
            "modifiedDatetimeStart": "2018-12-01T05:00:00Z",
            "modifiedDatetimeEnd": "2018-12-01T06:00:00Z",
            "format": {
                "type": "TextFormat",
                "columnDelimiter": ",",
                "rowDelimiter": "\n"
            },
            "compression": {
                "type": "GZip",
                "level": "Optimal"
            }
        }
    }
}
{
    "name": "AzureBlobDataset",
    "properties": {
        "type": "AzureBlob",
        "linkedServiceName": {
            "referenceName": "<Azure Blob Storage linked service name>",
            "type": "LinkedServiceReference"
        },
        "typeProperties": {
            "folderPath": "mycontainer/myfolder",
            "fileName": "*",
            "modifiedDatetimeStart": "2018-12-01T05:00:00Z",
            "modifiedDatetimeEnd": "2018-12-01T06:00:00Z",
            "format": {
                "type": "TextFormat",
                "columnDelimiter": ",",
                "rowDelimiter": "\n"
            },
            "compression": {
                "type": "GZip",
                "level": "Optimal"
            }
        }
    }
}
Legacy source model for the Copy activity
type
BlobSource
recursive
true
true
false
Example:
"activities":[
    {
        "name": "CopyFromBlob",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "<Azure Blob input dataset name>",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "<output dataset name>",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "BlobSource",
                "recursive": true
            },
            "sink": {
                "type": "<sink type>"
            }
        }
    }
]
"activities":[
    {
        "name": "CopyFromBlob",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "<Azure Blob input dataset name>",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "<output dataset name>",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "BlobSource",
                "recursive": true
            },
            "sink": {
                "type": "<sink type>"
            }
        }
    }
]
Legacy sink model for the Copy activity
type
BlobSink
Example:
"activities":[
    {
        "name": "CopyToBlob",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "<input dataset name>",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "<Azure Blob output dataset name>",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "<source type>"
            },
            "sink": {
                "type": "BlobSink",
                "copyBehavior": "PreserveHierarchy"
            }
        }
    }
]
"activities":[
    {
        "name": "CopyToBlob",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "<input dataset name>",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "<Azure Blob output dataset name>",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "<source type>"
            },
            "sink": {
                "type": "BlobSink",
                "copyBehavior": "PreserveHierarchy"
            }
        }
    }
]
Change data capture
Azure Data Factory can get new or changed files only from Azure Blob Storage by enabling **Enable change data capture ** in the mapping data flow source transformation. With this connector option, you can read new or updated files only and apply transformations before loading transformed data into destination datasets of your choice. Please refer toChange Data Capturefor details.
Related content
For a list of data stores that the Copy activity supports as sources and sinks, seeSupported data stores.
Feedback
Was this page helpful?
Additional resources