Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Use Azure Toolkit for IntelliJ to create Apache Spark applications for HDInsight cluster
Article
2024-06-15
22 contributors
In this article
This article demonstrates how to develop Apache Spark applications on Azure HDInsight using theAzure Toolkitplug-in for the IntelliJ IDE.Azure HDInsightis a managed, open-source analytics service in the cloud. The service allows you to use open-source frameworks like Hadoop, Apache Spark, Apache Hive, and Apache Kafka.
You can use theAzure Toolkitplug-in in a few ways:
Develop and submit a Scala Spark application to an HDInsight Spark cluster.
Access your Azure HDInsight Spark cluster resources.
Develop and run a Scala Spark application locally.
In this article, you learn how to:
Use the Azure Toolkit for IntelliJ plug-in
Develop Apache Spark applications
Submit an application to Azure HDInsight cluster
Prerequisites
An Apache Spark cluster on HDInsight. For instructions, seeCreate Apache Spark clusters in Azure HDInsight. Only HDInsight clusters in public cloud are supported while other secure cloud types (e.g. government clouds) are not.
An Apache Spark cluster on HDInsight. For instructions, seeCreate Apache Spark clusters in Azure HDInsight. Only HDInsight clusters in public cloud are supported while other secure cloud types (e.g. government clouds) are not.
Oracle Java Development kit.  This article uses Java version 8.0.202.
Oracle Java Development kit.  This article uses Java version 8.0.202.
IntelliJ IDEA. This article usesIntelliJ IDEA Community 2018.3.4.
IntelliJ IDEA. This article usesIntelliJ IDEA Community 2018.3.4.
Azure Toolkit for IntelliJ.  SeeInstalling the Azure Toolkit for IntelliJ.
Azure Toolkit for IntelliJ.  SeeInstalling the Azure Toolkit for IntelliJ.
Install Scala plugin for IntelliJ IDEA
Steps to install the Scala plugin:
Open IntelliJ IDEA.
Open IntelliJ IDEA.
On the welcome screen, navigate toConfigure>Pluginsto open thePluginswindow.
On the welcome screen, navigate toConfigure>Pluginsto open thePluginswindow.

SelectInstallfor the Scala plugin that is featured in the new window.
SelectInstallfor the Scala plugin that is featured in the new window.

After the plugin installs successfully, you must restart the IDE.
After the plugin installs successfully, you must restart the IDE.
Create a Spark Scala application for an HDInsight Spark cluster
Start IntelliJ IDEA, and selectCreate New Projectto open theNew Projectwindow.
Start IntelliJ IDEA, and selectCreate New Projectto open theNew Projectwindow.
SelectAzure Spark/HDInsightfrom the left pane.
SelectAzure Spark/HDInsightfrom the left pane.
SelectSpark Project (Scala)from the main window.
SelectSpark Project (Scala)from the main window.
From theBuild tooldrop-down list, select one of the following options:Mavenfor Scala project-creation wizard support.SBTfor managing the dependencies and building for the Scala project.
From theBuild tooldrop-down list, select one of the following options:
Mavenfor Scala project-creation wizard support.
Mavenfor Scala project-creation wizard support.
SBTfor managing the dependencies and building for the Scala project.
SBTfor managing the dependencies and building for the Scala project.

SelectNext.
SelectNext.
In theNew Projectwindow, provide the following information:PropertyDescriptionProject nameEnter a name.  This article usesmyApp.ProjectÂ locationEnter the location to save your project.Project SDKThis field might be blank on your first use of IDEA.  SelectNew...and navigate to your JDK.Spark VersionThe creation wizard integrates the proper version for Spark SDK and Scala SDK. If the Spark cluster version is earlier than 2.0, selectSpark 1.x. Otherwise, selectSpark2.x. This example usesSpark 2.3.0 (Scala 2.11.8).
In theNew Projectwindow, provide the following information:
myApp

SelectFinish.  It may take a few minutes before the project becomes available.
SelectFinish.  It may take a few minutes before the project becomes available.
The Spark project automatically creates an artifact for you. To view the artifact, do the following steps:a. From the menu bar, navigate toFile>Project Structure....b. From theProject Structurewindow, selectArtifacts.c. SelectCancelafter viewing the artifact.
The Spark project automatically creates an artifact for you. To view the artifact, do the following steps:
a. From the menu bar, navigate toFile>Project Structure....
b. From theProject Structurewindow, selectArtifacts.
c. SelectCancelafter viewing the artifact.

Add your application source code by doing the following steps:a. From Project, navigate tomyApp>src>main>scala.b. Right-clickscala, and then navigate toNew>Scala Class.c. In theCreate New Scala Classdialog box, provide a name, selectObjectin theKinddrop-down list, and then selectOK.d. ThemyApp.scalafile then opens in the main view. Replace the default code with the code found below:import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

object myApp{
    def main (arg: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("myApp")
    val sc = new SparkContext(conf)

    val rdd = sc.textFile("wasbs:///HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv")

    //find the rows that have only one digit in the seventh column in the CSV file
    val rdd1 =  rdd.filter(s => s.split(",")(6).length() == 1)

    rdd1.saveAsTextFile("wasbs:///HVACOut")
    }

}The code reads the data from HVAC.csv (available on all HDInsight Spark clusters), retrieves the rows that have only one digit in the seventh column in the CSV file, and writes the output to/HVACOutunder the default storage container for the cluster.
Add your application source code by doing the following steps:
a. From Project, navigate tomyApp>src>main>scala.
b. Right-clickscala, and then navigate toNew>Scala Class.

c. In theCreate New Scala Classdialog box, provide a name, selectObjectin theKinddrop-down list, and then selectOK.

d. ThemyApp.scalafile then opens in the main view. Replace the default code with the code found below:
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

object myApp{
    def main (arg: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("myApp")
    val sc = new SparkContext(conf)

    val rdd = sc.textFile("wasbs:///HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv")

    //find the rows that have only one digit in the seventh column in the CSV file
    val rdd1 =  rdd.filter(s => s.split(",")(6).length() == 1)

    rdd1.saveAsTextFile("wasbs:///HVACOut")
    }

}
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

object myApp{
    def main (arg: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("myApp")
    val sc = new SparkContext(conf)

    val rdd = sc.textFile("wasbs:///HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv")

    //find the rows that have only one digit in the seventh column in the CSV file
    val rdd1 =  rdd.filter(s => s.split(",")(6).length() == 1)

    rdd1.saveAsTextFile("wasbs:///HVACOut")
    }

}
The code reads the data from HVAC.csv (available on all HDInsight Spark clusters), retrieves the rows that have only one digit in the seventh column in the CSV file, and writes the output to/HVACOutunder the default storage container for the cluster.
/HVACOut
Connect to your HDInsight cluster
User can eithersign in to your Azure subscription, orlink a HDInsight cluster. Use the Ambari username/password or domain joined credential to connect to your HDInsight cluster.
Sign in to your Azure subscription
From the menu bar, navigate toView>Tool Windows>Azure Explorer.
From the menu bar, navigate toView>Tool Windows>Azure Explorer.

From Azure Explorer, right-click theAzurenode, and then selectSign In.
From Azure Explorer, right-click theAzurenode, and then selectSign In.

In theAzure Sign Indialog box, chooseDevice Login, and then selectSign in.
In theAzure Sign Indialog box, chooseDevice Login, and then selectSign in.

In theAzure Device Logindialog box, clickCopy&Open.
In theAzure Device Logindialog box, clickCopy&Open.

In the browser interface, paste the code, and then clickNext.
In the browser interface, paste the code, and then clickNext.

Enter your Azure credentials, and then close the browser.
Enter your Azure credentials, and then close the browser.

After you're signed in, theSelect Subscriptionsdialog box lists all the Azure subscriptions that are associated with the credentials. Select your subscription and then select theSelectbutton.
After you're signed in, theSelect Subscriptionsdialog box lists all the Azure subscriptions that are associated with the credentials. Select your subscription and then select theSelectbutton.

FromAzure Explorer, expandHDInsightto view the HDInsight Spark clusters that are in your subscriptions.
FromAzure Explorer, expandHDInsightto view the HDInsight Spark clusters that are in your subscriptions.

To view the resources (for example, storage accounts) that are associated with the cluster, you can further expand a cluster-name node.
To view the resources (for example, storage accounts) that are associated with the cluster, you can further expand a cluster-name node.

Link a cluster
You can link an HDInsight cluster by using the Apache Ambari managed username. Similarly, for a domain-joined HDInsight cluster, you can link by using the domain and username, such asuser1@contoso.com. Also you can link Livy Service cluster.
user1@contoso.com
From the menu bar, navigate toView>Tool Windows>Azure Explorer.
From the menu bar, navigate toView>Tool Windows>Azure Explorer.
From Azure Explorer, right-click theHDInsightnode, and then selectLink A Cluster.
From Azure Explorer, right-click theHDInsightnode, and then selectLink A Cluster.

The available options in theLink A Clusterwindow will vary depending on which value you select from theLink Resource Typedrop-down list.  Enter your values and then selectOK.HDInsight ClusterPropertyValueLink Resource TypeSelectHDInsight Clusterfrom the drop-down list.Cluster Name/URLEnter cluster name.Authentication TypeLeave asBasic AuthenticationUser NameEnter cluster user name, default is admin.PasswordEnter password for user name.Livy ServicePropertyValueLink Resource TypeSelectLivy Servicefrom the drop-down list.Livy EndpointEnter Livy EndpointCluster NameEnter cluster name.Yarn EndpointOptional.Authentication TypeLeave asBasic AuthenticationUser NameEnter cluster user name, default is admin.PasswordEnter password for user name.
The available options in theLink A Clusterwindow will vary depending on which value you select from theLink Resource Typedrop-down list.  Enter your values and then selectOK.
HDInsight ClusterPropertyValueLink Resource TypeSelectHDInsight Clusterfrom the drop-down list.Cluster Name/URLEnter cluster name.Authentication TypeLeave asBasic AuthenticationUser NameEnter cluster user name, default is admin.PasswordEnter password for user name.
HDInsight Cluster

Livy ServicePropertyValueLink Resource TypeSelectLivy Servicefrom the drop-down list.Livy EndpointEnter Livy EndpointCluster NameEnter cluster name.Yarn EndpointOptional.Authentication TypeLeave asBasic AuthenticationUser NameEnter cluster user name, default is admin.PasswordEnter password for user name.
Livy Service

You can see your linked cluster from theHDInsightnode.
You can see your linked cluster from theHDInsightnode.

You also can unlink a cluster fromAzure Explorer.
You also can unlink a cluster fromAzure Explorer.

Run a Spark Scala application on an HDInsight Spark cluster
After creating a Scala application, you can submit it to the cluster.
From Project, navigate tomyApp>src>main>scala>myApp.  Right-clickmyApp, and selectSubmit Spark Application(It will likely be located at the bottom of the list).
From Project, navigate tomyApp>src>main>scala>myApp.  Right-clickmyApp, and selectSubmit Spark Application(It will likely be located at the bottom of the list).

In theSubmit Spark Applicationdialog window, select1. Spark on HDInsight.
In theSubmit Spark Applicationdialog window, select1. Spark on HDInsight.
In theEdit configurationwindow, provide the following values and then selectOK:PropertyValueSpark clusters (Linux only)Select the HDInsight Spark cluster on which you want to run your application.Select an Artifact to submitLeave default setting.Main class nameThe default value is the main class from the selected file. You can change the class by selecting the ellipsis(...)  and choosing another class.Job configurationsYou can change the default keys and, or values. For more information, seeApache Livy REST API.Command-line argumentsYou can enter arguments separated by space for the main class if needed.Referenced Jars and Referenced FilesYou can enter the paths for the referenced Jars and files if any. You can also browse files in the Azure virtual file system, which currently only supports ADLS Gen 2 cluster. For more information:Apache Spark Configuration.  See also,How to upload resources to cluster.Job Upload StorageExpand to reveal additional options.Storage TypeSelectUse Azure Blob to uploadfrom the drop-down list.Storage AccountEnter your storage account.Storage KeyEnter your storage key.Storage ContainerSelect your storage container from the drop-down list onceStorage AccountandStorage Keyhas been entered.
In theEdit configurationwindow, provide the following values and then selectOK:

SelectSparkJobRunto submit your project to the selected cluster. TheRemote Spark Job in Clustertab displays the job execution progress at the bottom. You can stop the application by clicking the red button.
SelectSparkJobRunto submit your project to the selected cluster. TheRemote Spark Job in Clustertab displays the job execution progress at the bottom. You can stop the application by clicking the red button.

Debug Apache Spark applications locally or remotely on an HDInsight cluster
We also recommend another way of submitting the Spark application to the cluster. You can do so by setting the parameters in theRun/Debug configurationsIDE. SeeDebug Apache Spark applications locally or remotely on an HDInsight cluster with Azure Toolkit for IntelliJ through SSH.
Access and manage HDInsight Spark clusters by using Azure Toolkit for IntelliJ
You can do various operations by using Azure Toolkit for IntelliJ.  Most of the operations are started fromAzure Explorer.  From the menu bar, navigate toView>Tool Windows>Azure Explorer.
Access the job view
From Azure Explorer, navigate toHDInsight> <Your Cluster> >Jobs.
From Azure Explorer, navigate toHDInsight> <Your Cluster> >Jobs.

In the right pane, theSpark Job Viewtab displays all the applications that were run on the cluster. Select the name of the application for which you want to see more details.
In the right pane, theSpark Job Viewtab displays all the applications that were run on the cluster. Select the name of the application for which you want to see more details.

To display basic running job information, hover over the job graph. To view the stages graph and information that every job generates, select a node on the job graph.
To display basic running job information, hover over the job graph. To view the stages graph and information that every job generates, select a node on the job graph.

To view frequently used logs, such asDriver Stderr,Driver Stdout, andDirectory Info, select theLogtab.
To view frequently used logs, such asDriver Stderr,Driver Stdout, andDirectory Info, select theLogtab.

You can view the Spark history UI and the YARN UI (at the application level). Select a link at the top of the window.
You can view the Spark history UI and the YARN UI (at the application level). Select a link at the top of the window.
Access the Spark history server
From Azure Explorer, expandHDInsight, right-click your Spark cluster name, and then selectOpen Spark History UI.
From Azure Explorer, expandHDInsight, right-click your Spark cluster name, and then selectOpen Spark History UI.
When you're prompted, enter the cluster's admin credentials, which you specified when you set up the cluster.
When you're prompted, enter the cluster's admin credentials, which you specified when you set up the cluster.
On the Spark history server dashboard, you can use the application name to look for the application that you just finished running. In the preceding code, you set the application name by usingval conf = new SparkConf().setAppName("myApp"). Your Spark application name ismyApp.
On the Spark history server dashboard, you can use the application name to look for the application that you just finished running. In the preceding code, you set the application name by usingval conf = new SparkConf().setAppName("myApp"). Your Spark application name ismyApp.
val conf = new SparkConf().setAppName("myApp")
Start the Ambari portal
From Azure Explorer, expandHDInsight, right-click your Spark cluster name, and then selectOpen Cluster Management Portal(Ambari).
From Azure Explorer, expandHDInsight, right-click your Spark cluster name, and then selectOpen Cluster Management Portal(Ambari).
When you're prompted, enter the admin credentials for the cluster. You specified these credentials during the cluster setup process.
When you're prompted, enter the admin credentials for the cluster. You specified these credentials during the cluster setup process.
Manage Azure subscriptions
By default, Azure Toolkit for IntelliJ lists the Spark clusters from all your Azure subscriptions. If necessary, you can specify the subscriptions that you want to access.
From Azure Explorer, right-click theAzureroot node, and then selectSelect Subscriptions.
From Azure Explorer, right-click theAzureroot node, and then selectSelect Subscriptions.
From theSelect Subscriptionswindow, clear the check boxes next to the subscriptions that you don't want to access, and then selectClose.
From theSelect Subscriptionswindow, clear the check boxes next to the subscriptions that you don't want to access, and then selectClose.
Spark Console
You can run Spark Local Console(Scala) or run Spark Livy Interactive Session Console(Scala).
Spark Local Console(Scala)
Ensure you've satisfied the WINUTILS.EXE prerequisite.
From the menu bar, navigate toRun>Edit Configurations....
From the menu bar, navigate toRun>Edit Configurations....
From theRun/Debug Configurationswindow, in the left pane, navigate toApache Spark on HDInsight>[Spark on HDInsight] myApp.
From theRun/Debug Configurationswindow, in the left pane, navigate toApache Spark on HDInsight>[Spark on HDInsight] myApp.
From the main window, select theLocally Runtab.
From the main window, select theLocally Runtab.
Locally Run
Provide the following values, and then selectOK:PropertyValueJob main classThe default value is the main class from the selected file. You can change the class by selecting the ellipsis(...)  and choosing another class.Environment variablesEnsure the value for HADOOP_HOME is correct.WINUTILS.exe locationEnsure the path is correct.
Provide the following values, and then selectOK:

From Project, navigate tomyApp>src>main>scala>myApp.
From Project, navigate tomyApp>src>main>scala>myApp.
From the menu bar, navigate toTools>Spark Console>Run Spark Local Console(Scala).
From the menu bar, navigate toTools>Spark Console>Run Spark Local Console(Scala).
Then two dialogs may be displayed to ask you if you want to auto fix dependencies. If so, selectAuto Fix.
Then two dialogs may be displayed to ask you if you want to auto fix dependencies. If so, selectAuto Fix.


The console should look similar to the picture below. In the console window typesc.appName, and then press ctrl+Enter.  The result will be shown. You can end the local console by clicking red button.
The console should look similar to the picture below. In the console window typesc.appName, and then press ctrl+Enter.  The result will be shown. You can end the local console by clicking red button.
sc.appName

Spark Livy Interactive Session Console(Scala)
From the menu bar, navigate toRun>Edit Configurations....
From the menu bar, navigate toRun>Edit Configurations....
From theRun/Debug Configurationswindow, in the left pane, navigate toApache Spark on HDInsight>[Spark on HDInsight] myApp.
From theRun/Debug Configurationswindow, in the left pane, navigate toApache Spark on HDInsight>[Spark on HDInsight] myApp.
From the main window, select theRemotely Run in Clustertab.
From the main window, select theRemotely Run in Clustertab.
Remotely Run in Cluster
Provide the following values, and then selectOK:PropertyValueSpark clusters (Linux only)Select the HDInsight Spark cluster on which you want to run your application.Main class nameThe default value is the main class from the selected file. You can change the class by selecting the ellipsis(...)  and choosing another class.
Provide the following values, and then selectOK:

From Project, navigate tomyApp>src>main>scala>myApp.
From Project, navigate tomyApp>src>main>scala>myApp.
From the menu bar, navigate toTools>Spark Console>Run Spark Livy Interactive Session Console(Scala).
From the menu bar, navigate toTools>Spark Console>Run Spark Livy Interactive Session Console(Scala).
The console should look similar to the picture below. In the console window typesc.appName, and then press ctrl+Enter.  The result will be shown. You can end the local console by clicking red button.
The console should look similar to the picture below. In the console window typesc.appName, and then press ctrl+Enter.  The result will be shown. You can end the local console by clicking red button.
sc.appName

Send Selection to Spark Console
It's convenient for you to foresee the script result by sending some code to the local console or Livy Interactive Session Console(Scala). You can highlight some code in the Scala file, then right-clickSend Selection To Spark Console. The selected code will be sent to the console. The result will be displayed after the code in the console. The console will check the errors if existing.

Integrate with HDInsight Identity Broker (HIB)
Connect to your HDInsight ESP cluster with ID Broker (HIB)
You can follow the normal steps to sign in to Azure subscription to connect to your HDInsight ESP cluster with ID Broker (HIB). After sign-in, you'll see the cluster list in Azure Explorer. For more instructions, seeConnect to your HDInsight cluster.
Run a Spark Scala application on an HDInsight ESP cluster with ID Broker (HIB)
You can follow the normal steps to submit job to HDInsight ESP cluster with ID Broker (HIB). Refer toRun a Spark Scala application on an HDInsight Spark clusterfor more instructions.
We upload the necessary files to a folder named with your sign-in account, and you can see the upload path in the configuration file.

Spark console on an HDInsight ESP cluster with ID Broker (HIB)
You can run Spark Local Console(Scala) or run Spark Livy Interactive Session Console(Scala) on an HDInsight ESP cluster with ID Broker (HIB). Refer toSpark Consolefor more instructions.
Note
For the HDInsight ESP cluster with Id Broker (HIB),link a clusteranddebug Apache Spark applications remotelyis not supported currently.
Reader-only role
When users submit job to a cluster with reader-only role permission, Ambari credentials is required.
Link cluster from context menu
Sign in with reader-only role account.
Sign in with reader-only role account.
FromAzure Explorer, expandHDInsightto view HDInsight clusters that are in your subscription. The clusters marked"Role:Reader"only have reader-only role permission.
FromAzure Explorer, expandHDInsightto view HDInsight clusters that are in your subscription. The clusters marked"Role:Reader"only have reader-only role permission.

Right-click the cluster with reader-only role permission. SelectLink this clusterfrom context menu to link cluster. Enter the Ambari username and Password.
Right-click the cluster with reader-only role permission. SelectLink this clusterfrom context menu to link cluster. Enter the Ambari username and Password.

If the cluster is linked successfully, HDInsight will be refreshed.
The stage of the cluster will become linked.
If the cluster is linked successfully, HDInsight will be refreshed.
The stage of the cluster will become linked.

Link cluster by expanding Jobs node
ClickJobsnode,Cluster Job Access Deniedwindow pops up.
ClickJobsnode,Cluster Job Access Deniedwindow pops up.
ClickLink this clusterto link cluster.
ClickLink this clusterto link cluster.

Link cluster from Run/Debug Configurations window
Create an HDInsight Configuration. Then selectRemotely Run in Cluster.
Create an HDInsight Configuration. Then selectRemotely Run in Cluster.
Select a cluster, which has reader-only role permission forSpark clusters(Linux only). Warning message shows out. You can ClickLink this clusterto link cluster.
Select a cluster, which has reader-only role permission forSpark clusters(Linux only). Warning message shows out. You can ClickLink this clusterto link cluster.

View Storage Accounts
For clusters with reader-only role permission, clickStorage Accountsnode,Storage Access Deniedwindow pops up. You can clickOpen Azure Storage Explorerto open Storage Explorer.
For clusters with reader-only role permission, clickStorage Accountsnode,Storage Access Deniedwindow pops up. You can clickOpen Azure Storage Explorerto open Storage Explorer.


For linked clusters, clickStorage Accountsnode,Storage Access Deniedwindow pops up. You can clickOpen Azure Storageto open Storage Explorer.
For linked clusters, clickStorage Accountsnode,Storage Access Deniedwindow pops up. You can clickOpen Azure Storageto open Storage Explorer.


Convert existing IntelliJ IDEA applications to use Azure Toolkit for IntelliJ
You can convert the existing Spark Scala applications that you created in IntelliJ IDEA to be compatible with Azure Toolkit for IntelliJ. You can then use the plug-in to submit the applications to an HDInsight Spark cluster.
For an existing Spark Scala application that was created through IntelliJ IDEA, open the associated.imlfile.
For an existing Spark Scala application that was created through IntelliJ IDEA, open the associated.imlfile.
.iml
At the root level, is amoduleelement like the following text:<module org.jetbrains.idea.maven.project.MavenProjectsManager.isMavenModule="true" type="JAVA_MODULE" version="4">Edit the element to addUniqueKey="HDInsightTool"so that themoduleelement looks like the following text:<module org.jetbrains.idea.maven.project.MavenProjectsManager.isMavenModule="true" type="JAVA_MODULE" version="4" UniqueKey="HDInsightTool">
At the root level, is amoduleelement like the following text:
<module org.jetbrains.idea.maven.project.MavenProjectsManager.isMavenModule="true" type="JAVA_MODULE" version="4">
<module org.jetbrains.idea.maven.project.MavenProjectsManager.isMavenModule="true" type="JAVA_MODULE" version="4">
Edit the element to addUniqueKey="HDInsightTool"so that themoduleelement looks like the following text:
UniqueKey="HDInsightTool"
<module org.jetbrains.idea.maven.project.MavenProjectsManager.isMavenModule="true" type="JAVA_MODULE" version="4" UniqueKey="HDInsightTool">
<module org.jetbrains.idea.maven.project.MavenProjectsManager.isMavenModule="true" type="JAVA_MODULE" version="4" UniqueKey="HDInsightTool">
Save the changes. Your application should now be compatible with Azure Toolkit for IntelliJ. You can test it by right-clicking the project name in Project. The pop-up menu now has the optionSubmit Spark Application to HDInsight.
Save the changes. Your application should now be compatible with Azure Toolkit for IntelliJ. You can test it by right-clicking the project name in Project. The pop-up menu now has the optionSubmit Spark Application to HDInsight.
Clean up resources
If you're not going to continue to use this application, delete the cluster that you created with the following steps:
Sign in to theAzure portal.
Sign in to theAzure portal.
In theSearchbox at the top, typeHDInsight.
In theSearchbox at the top, typeHDInsight.
SelectHDInsight clustersunderServices.
SelectHDInsight clustersunderServices.
In the list of HDInsight clusters that appears, select the...next to the cluster that you created for this article.
In the list of HDInsight clusters that appears, select the...next to the cluster that you created for this article.
SelectDelete. SelectYes.
SelectDelete. SelectYes.

Errors and solution
Unmark the src folder asSourcesif you get build failed errors as below:

Unmark the src folder asSourcesto solution this issue:
Navigate toFileand select theProject Structure.
Navigate toFileand select theProject Structure.
Select theModulesunder the Project Settings.
Select theModulesunder the Project Settings.
Select thesrcfile and unmark asSources.
Select thesrcfile and unmark asSources.
Click on Apply button and then click on OK button to close the dialog.
Click on Apply button and then click on OK button to close the dialog.

Next steps
In this article, you learned how to use the Azure Toolkit for IntelliJ plug-in to develop Apache Spark applications written inScala. Then submitted them to an HDInsight Spark cluster directly from the IntelliJ integrated development environment (IDE). Advance to the next article to see how the data you registered in Apache Spark can be pulled into a BI analytics tool such as Power BI.
Analyze Apache Spark data using Power BI
Feedback
Was this page helpful?
Additional resources