Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
.ingest into
Article
2025-01-08
3 contributors
In this article
Applies to: 창혵Microsoft Fabric창혵Azure Data Explorer
The.ingest intocommand ingests data into a table by "pulling" the data
from one or more cloud storage files.
For example, the command
can retrieve 1,000 CSV-formatted blobs from Azure Blob Storage, parse
them, and ingest them together into a single target table.
Data is appended to the table
without affecting existing records, and without modifying the table's schema.
.ingest into
Note
This ingestion method is intended for exploration and prototyping. Don't use it in production or high-volume scenarios.
Note
This ingestion method is intended for exploration and prototyping. Don't use it in production or high-volume scenarios. For more information about ingestion options, seeData ingestion overview.
Permissions
You must have at leastTable Ingestorpermissions to run this command.
Syntax
.ingest[async]intotableTableNameSourceDataLocator[with(IngestionPropertyName=IngestionPropertyValue[,...])]
.ingest
async
into
table
with
(
=
,
)
Learn more aboutsyntax conventions.
Parameters
async
string
OperationId
.show operation
string
string
Note
We recommend usingobfuscated string literalsfor theSourceDataLocators. The service will scrub credentials in internal traces and error messages.
Ingestion properties
Important
In queued ingestiondata is batched using Ingestion properties. The more distinct ingestion mapping properties used, such as different ConstValue values, the more fragmented the ingestion becomes, which can lead to performance degradation.
The following table lists and describes the supported properties, and provides examples:
ingestionMapping
format
with (format="json", ingestionMapping = "[{\"column\":\"rownumber\", \"Properties\":{\"Path\":\"$.RowNumber\"}}, {\"column\":\"rowguid\", \"Properties\":{\"Path\":\"$.RowGuid\"}}]")
avroMapping
csvMapping
jsonMapping
ingestionMappingReference
format
with (format="csv", ingestionMappingReference = "Mapping1")
avroMappingReference
csvMappingReference
jsonMappingReference
creationTime
now()
Lookback
with (creationTime="2017-02-13")
extend_schema
false
.append
.set-or-append
(a:string, b:int)
(a:string, b:int, c:datetime, d:string)
(a:string, c:datetime)
folder
with (folder="Tables/Temporary")
format
with (format="csv")
ingestIfNotExists
ingest-by:
with (ingestIfNotExists='["Part0001"]', tags='["ingest-by:Part0001"]')
ingest-by:Part0001
ignoreFirstRecord
true
CSV
false
with (ignoreFirstRecord=false)
policy_ingestiontime
true
with (policy_ingestiontime=false)
recreate_schema
.set-or-replace
extend_schema
with (recreate_schema=true)
tags
with (tags="['Tag1', 'Tag2']")
TreatGzAsUncompressed
true
.gz
with (treatGzAsUncompressed=true)
validationPolicy
with (validationPolicy='{"ValidationOptions":1, "ValidationImplications":1}')
zipPattern
with (zipPattern="*.csv")
Authentication and authorization
Each storage connection string indicates the authorization method to use for access to the storage. Depending on the authorization method, the principal might need to be granted permissions on the external storage to perform the ingestion.
The following table lists the supported authentication methods and the permissions needed for ingesting data from external storage.
Returns
The result of the command is a table with as many records as there are data shards ("extents") generated by the command.
If no data shards were generated, a single record is returned with an empty (zero-valued) extent ID.
guid
string
timespan
bool
guid
.show operation
Note
This command doesn't modify the schema of the table being ingested into. If necessary, the data is "coerced" into this schema during ingestion, not the other way around (extra columns are ignored, and missing columns are treated as null values).
Examples
Azure Blob Storage with shared access signature
The following example instructs your database to read two blobs from Azure Blob Storage as CSV files, and ingest their contents into tableT. The...represents an Azure Storage shared access signature (SAS) which gives read access to each blob. Obfuscated strings (thehin front of the string values) are used to ensure that the SAS is never recorded.
T
...
h
.ingest into table T (
    h'https://contoso.blob.core.windows.net/container/file1.csv?...',
    h'https://contoso.blob.core.windows.net/container/file2.csv?...'
)
.ingest into table T (
    h'https://contoso.blob.core.windows.net/container/file1.csv?...',
    h'https://contoso.blob.core.windows.net/container/file2.csv?...'
)
Azure Blob Storage with managed identity
The following example shows how to read a CSV file from Azure Blob Storage and ingest its contents into tableTusing managed identity authentication. Authentication uses the managed identity ID (object ID) assigned to the Azure Blob Storage in Azure. For more information, seeCreate a managed identity for storage containers.
T
.ingest into table T ('https://StorageAccount.blob.core.windows.net/Container/file.csv;managed_identity=802bada6-4d21-44b2-9d15-e66b29e4d63e')
.ingest into table T ('https://StorageAccount.blob.core.windows.net/Container/file.csv;managed_identity=802bada6-4d21-44b2-9d15-e66b29e4d63e')
Azure Data Lake Storage Gen 2
The following example is for ingesting data from Azure Data Lake Storage Gen 2
(ADLSv2). The credentials used here (...) are the storage account credentials
(shared key), and we use string obfuscation only for the secret part of the
connection string.
...
.ingest into table T (
  'abfss://myfilesystem@contoso.dfs.core.windows.net/path/to/file1.csv;...'
)
.ingest into table T (
  'abfss://myfilesystem@contoso.dfs.core.windows.net/path/to/file1.csv;...'
)
Azure Data Lake Storage
The following example ingests a single file from Azure Data Lake Storage (ADLS).
It uses the user's credentials to access ADLS (so there's no need to treat
the storage URI as containing a secret). It also shows how to specify ingestion
properties.
.ingest into table T ('adl://contoso.azuredatalakestore.net/Path/To/File/file1.ext;impersonate')
  with (format='csv')
.ingest into table T ('adl://contoso.azuredatalakestore.net/Path/To/File/file1.ext;impersonate')
  with (format='csv')
Amazon S3 with an access key
The following example ingests a single file from Amazon S3 using anaccess key ID and a secret access key.
.ingest into table T ('https://bucketname.s3.us-east-1.amazonaws.com/path/to/file.csv;AwsCredentials=EXAMPLEKEY')
  with (format='csv')
.ingest into table T ('https://bucketname.s3.us-east-1.amazonaws.com/path/to/file.csv;AwsCredentials=EXAMPLEKEY')
  with (format='csv')
Amazon S3 with a presigned URL
The following example ingests a single file from Amazon S3 using apreSigned URL.
.ingest into table T ('https://bucketname.s3.us-east-1.amazonaws.com/file.csv?<<pre signed string>>')
  with (format='csv')
.ingest into table T ('https://bucketname.s3.us-east-1.amazonaws.com/file.csv?<<pre signed string>>')
  with (format='csv')
Related content
Data formats supported for ingestion
.ingest inline
Ingest from query (.set, .append, .set-or-append, .set-or-replace)
.show ingestion failures command
.show ingestion mapping
Feedback
Was this page helpful?
Additional resources