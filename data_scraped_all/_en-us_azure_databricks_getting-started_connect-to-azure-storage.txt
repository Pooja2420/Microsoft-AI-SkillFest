Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Tutorial: Connect to Azure Data Lake Storage
Article
2024-12-16
2 contributors
In this article
Note
This article describes legacy patterns for configuring access to Azure Data Lake Storage. Databricks recommends using Unity Catalog. SeeCreate a Unity Catalog metastoreandConnect to cloud object storage and services using Unity Catalog.
This tutorial guides you through all the steps necessary to connect from Azure Databricks to Azure Data Lake Storage using OAuth 2.0 with a Microsoft Entra ID service principal.
Requirements
Complete these tasks before you begin this tutorial:
Create an Azure Databricks workspace. SeeQuickstart: Create an Azure Databricks workspace
Create an Azure Data Lake Storage storage account. SeeQuickstart: Create an Azure Data Lake Storage storage account.
Create an Azure Key Vault. SeeQuickstart: Create an Azure Key Vault
Step 1: Create a Microsoft Entra ID service principal
To use service principals to connect to Azure Data Lake Storage, an admin user must create a new Microsoft Entra ID application. If you already have a Microsoft Entra ID service principal available, skip ahead toStep 2: Create a client secret for your service principal.
To create a Microsoft Entra ID service principal, follow these instructions:
Sign in to theAzure portal.NoteThe portal to use is different depending on whether your Microsoft Entra ID application runs in the Azure public cloud or in a national or sovereign cloud. For more information, seeNational clouds.
Sign in to theAzure portal.
Note
The portal to use is different depending on whether your Microsoft Entra ID application runs in the Azure public cloud or in a national or sovereign cloud. For more information, seeNational clouds.
If you have access to multiple tenants, subscriptions, or directories, click theDirectories + subscriptions(directory with filter) icon in the top menu to switch to the directory in which you want to provision the service principal.
If you have access to multiple tenants, subscriptions, or directories, click theDirectories + subscriptions(directory with filter) icon in the top menu to switch to the directory in which you want to provision the service principal.
Search for and select<Microsoft Entra ID.
Search for and select<Microsoft Entra ID.
InManage, clickApp registrations > New registration.
InManage, clickApp registrations > New registration.
ForName, enter a name for the application.
ForName, enter a name for the application.
In theSupported account typessection, selectAccounts in this organizational directory only (Single tenant).
In theSupported account typessection, selectAccounts in this organizational directory only (Single tenant).
ClickRegister.
ClickRegister.
Step 2: Create a client secret for your service principal
InManage, clickCertificates & secrets.
InManage, clickCertificates & secrets.
On theClient secretstab, clickNew client secret.
On theClient secretstab, clickNew client secret.

In theAdd a client secretpane, forDescription, enter a description for the client secret.
In theAdd a client secretpane, forDescription, enter a description for the client secret.
ForExpires, select an expiry time period for the client secret, and then clickAdd.
ForExpires, select an expiry time period for the client secret, and then clickAdd.
Copy and store the client secretâsValuein a secure place, as this client secret is the password for your application.
Copy and store the client secretâsValuein a secure place, as this client secret is the password for your application.
On the application pageâsOverviewpage, in theEssentialssection, copy the following values:Application (client) IDDirectory (tenant) ID
On the application pageâsOverviewpage, in theEssentialssection, copy the following values:
Application (client) ID
Directory (tenant) ID
Step 3: Grant the service principal access to Azure Data Lake Storage
You grant access to storage resources by assigning roles to your service principal. In this tutorial, you assign theStorage Blob Data Contributorto the service principal on your Azure Data Lake Storage account. You may need to assign other roles depending on specific requirements.
In the Azure portal, go to theStorage accountsservice.
Select an Azure storage account to use.
ClickAccess Control (IAM).
Click+ Addand selectAdd role assignmentfrom the dropdown menu.
Set theSelectfield to the Microsoft Entra ID application name that you created in step 1 and setRoletoStorage Blob Data Contributor.
ClickSave.
Step 4: Add the client secret to Azure Key Vault
You can store the client secret from step 1 in Azure Key Vault.
In the Azure portal, go to theKey vaultservice.
Select an Azure Key Vault to use.
On the Key Vault settings pages, selectSecrets.
Click on+ Generate/Import.
InUpload options, selectManual.
ForName, enter a name for the secret. The secret name must be unique within a Key Vault.
ForValue, paste the Client Secret that you stored in Step 1.
ClickCreate.
Step 5: Configure your Azure key vault instance for Azure Databricks
In the Azure Portal, go to the Azure key vault instance.UnderSettings, select theAccess configurationtab.SetPermission modeltoVault access policy.NoteCreating an Azure Key Vault-backed secret scope role grants theGetandListpermissions to the application ID for the Azure Databricks service using key vault access policies. The Azure role-based access control permission model is not supported with Azure Databricks.UnderSettings, selectNetworking.InFirewalls and virtual networkssetAllow access from:toAllow public access from specific virtual networks and IP addresses.UnderException, checkAllow trusted Microsoft services to bypass this firewall.NoteYou can also setAllow access from:toAllow public access from all networks.
In the Azure Portal, go to the Azure key vault instance.
UnderSettings, select theAccess configurationtab.
UnderSettings, select theAccess configurationtab.
SetPermission modeltoVault access policy.NoteCreating an Azure Key Vault-backed secret scope role grants theGetandListpermissions to the application ID for the Azure Databricks service using key vault access policies. The Azure role-based access control permission model is not supported with Azure Databricks.
SetPermission modeltoVault access policy.
Note
Creating an Azure Key Vault-backed secret scope role grants theGetandListpermissions to the application ID for the Azure Databricks service using key vault access policies. The Azure role-based access control permission model is not supported with Azure Databricks.
UnderSettings, selectNetworking.
UnderSettings, selectNetworking.
InFirewalls and virtual networkssetAllow access from:toAllow public access from specific virtual networks and IP addresses.UnderException, checkAllow trusted Microsoft services to bypass this firewall.NoteYou can also setAllow access from:toAllow public access from all networks.
InFirewalls and virtual networkssetAllow access from:toAllow public access from specific virtual networks and IP addresses.
UnderException, checkAllow trusted Microsoft services to bypass this firewall.
Note
You can also setAllow access from:toAllow public access from all networks.
Step 6: Create Azure Key Vault-backed secret scope in your Azure Databricks workspace
To reference the client secret stored in an Azure Key Vault, you can create a secret scope backed by Azure Key Vault in Azure Databricks.
Go tohttps://<databricks-instance>#secrets/createScope. This URL is case sensitive; scope increateScopemust be uppercase.
Go tohttps://<databricks-instance>#secrets/createScope. This URL is case sensitive; scope increateScopemust be uppercase.
https://<databricks-instance>#secrets/createScope
createScope

Enter the name of the secret scope. Secret scope names are case insensitive.
Enter the name of the secret scope. Secret scope names are case insensitive.
Use theManage Principaldropdown menu to specify whetherAll UsershaveMANAGEpermission for this secret scope or only theCreatorof the secret scope (that is to say, you).
Use theManage Principaldropdown menu to specify whetherAll UsershaveMANAGEpermission for this secret scope or only theCreatorof the secret scope (that is to say, you).
MANAGE
Enter theDNS Name(for example,https://databrickskv.vault.azure.net/) andResource ID, for example:/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourcegroups/databricks-rg/providers/Microsoft.KeyVault/vaults/databricksKVThese properties are available from the*Settings > Propertiestab of an Azure Key Vault in your Azure portal.
Enter theDNS Name(for example,https://databrickskv.vault.azure.net/) andResource ID, for example:
https://databrickskv.vault.azure.net/
/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourcegroups/databricks-rg/providers/Microsoft.KeyVault/vaults/databricksKV
/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx/resourcegroups/databricks-rg/providers/Microsoft.KeyVault/vaults/databricksKV
These properties are available from the*Settings > Propertiestab of an Azure Key Vault in your Azure portal.
Click theCreatebutton.
Click theCreatebutton.
Step 7: Connect to Azure Data Lake Storage using python
You can now securely access data in the Azure storage account using OAuth 2.0 with your Microsoft Entra ID application service principal for authentication from an Azure Databricks notebook.
Navigate to your Azure Databricks workspace and create a new python notebook.
Navigate to your Azure Databricks workspace and create a new python notebook.
Run the following python code, with the replacements below, to connect to Azure Data Lake Storage.service_credential = dbutils.secrets.get(scope="<scope>",key="<service-credential-key>")

spark.conf.set("fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net", "OAuth")
spark.conf.set("fs.azure.account.oauth.provider.type.<storage-account>.dfs.core.windows.net", "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider")
spark.conf.set("fs.azure.account.oauth2.client.id.<storage-account>.dfs.core.windows.net", "<application-id>")
spark.conf.set("fs.azure.account.oauth2.client.secret.<storage-account>.dfs.core.windows.net", service_credential)
spark.conf.set("fs.azure.account.oauth2.client.endpoint.<storage-account>.dfs.core.windows.net", "https://login.microsoftonline.com/<directory-id>/oauth2/token")Replace<scope>with the secret scope name from step 5.<service-credential-key>with the name of the key containing the client secret.<storage-account>with the name of the Azure storage account.<application-id>with theApplication (client) IDfor the Microsoft Entra ID application.<directory-id>with theDirectory (tenant) IDfor the Microsoft Entra ID application.You have now successfully connected your Azure Databricks workspace to your Azure Data Lake Storage account.
Run the following python code, with the replacements below, to connect to Azure Data Lake Storage.
service_credential = dbutils.secrets.get(scope="<scope>",key="<service-credential-key>")

spark.conf.set("fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net", "OAuth")
spark.conf.set("fs.azure.account.oauth.provider.type.<storage-account>.dfs.core.windows.net", "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider")
spark.conf.set("fs.azure.account.oauth2.client.id.<storage-account>.dfs.core.windows.net", "<application-id>")
spark.conf.set("fs.azure.account.oauth2.client.secret.<storage-account>.dfs.core.windows.net", service_credential)
spark.conf.set("fs.azure.account.oauth2.client.endpoint.<storage-account>.dfs.core.windows.net", "https://login.microsoftonline.com/<directory-id>/oauth2/token")
service_credential = dbutils.secrets.get(scope="<scope>",key="<service-credential-key>")

spark.conf.set("fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net", "OAuth")
spark.conf.set("fs.azure.account.oauth.provider.type.<storage-account>.dfs.core.windows.net", "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider")
spark.conf.set("fs.azure.account.oauth2.client.id.<storage-account>.dfs.core.windows.net", "<application-id>")
spark.conf.set("fs.azure.account.oauth2.client.secret.<storage-account>.dfs.core.windows.net", service_credential)
spark.conf.set("fs.azure.account.oauth2.client.endpoint.<storage-account>.dfs.core.windows.net", "https://login.microsoftonline.com/<directory-id>/oauth2/token")
Replace
<scope>with the secret scope name from step 5.
<scope>
<service-credential-key>with the name of the key containing the client secret.
<service-credential-key>
<storage-account>with the name of the Azure storage account.
<storage-account>
<application-id>with theApplication (client) IDfor the Microsoft Entra ID application.
<application-id>
<directory-id>with theDirectory (tenant) IDfor the Microsoft Entra ID application.
<directory-id>
You have now successfully connected your Azure Databricks workspace to your Azure Data Lake Storage account.
Grant your Azure Databricks workspace access to Azure Data Lake Storage
If you configure a firewall on Azure Data Lake Storage, you must configure network settings to allow your Azure Databricks workspace to connect to Azure Data Lake Storage. First, ensure that your Azure Databricks workspace is deployed in your own virtual network followingDeploy Azure Databricks in your Azure virtual network (VNet injection). You can then configure eitherprivate endpointsoraccess from your virtual networkto allow connections from your subnets to your Azure Data Lake Storage account.
If you are using serverless compute like serverless SQL warehouses, you must grant access from the serverless compute plane to Azure Data Lake Storage. SeeServerless compute plane networking.
Grant access using private endpoints
You can useprivate endpointsfor your Azure Data Lake Storage account to allow your Azure Databricks workspace to securely access data over aprivate link.
To create a private endpoint by using the Azure Portal, seeTutorial: Connect to a storage account using an Azure Private Endpoint. Ensure to create the private endpoint in the same virtual network that your Azure Databricks workspace is deployed in.
Grant access from your virtual network
Virtual Network service endpointsallow you to secure your critical Azure service resources to only your virtual networks. You can enable a service endpoint for Azure Storage within the VNet that you used for your Azure Databricks workspace.
For more information, including Azure CLI and PowerShell instructions, seeGrant access from a virtual network.
Log in to the Azure Portal, as a user with the Storage Account Contributor role on your Azure Data Lake Storage account.
Navigate to your Azure Storage account, and go to theNetworkingtab.
Check that youâve selected to allow access fromSelected virtual networks and IP addresses.
UnderVirtual networks, selectAdd existing virtual network.
In the side panel, underSubscription, select the subscription that your virtual network is in.
UnderVirtual networks, select the virtual network that your Azure Databricks workspace is deployed in.
UnderSubnets, pickSelect all.
ClickEnable.
SelectSaveto apply your changes.
Troubleshooting
Error: IllegalArgumentException: Secret does not exist with scope: KeyVaultScope and key
This error probably means:
The Databricks-backed scope that is referred in the code is not valid.
Review the name of your secret from step 4 in this article.
Error: com.databricks.common.client.DatabricksServiceHttpClientException: INVALID_STATE: Databricks could not access keyvault
This error probably means:
The Databricks-backed scope that is referred to in the code is not valid. or the secret stored in the Key Vault has expired.
Review step 3 to ensure your Azure Key Vault secret is valid. Review the name of your secret from step 4 in this article.
Error: ADAuthenticator$HttpException: HTTP Error 401: token failed for getting token from AzureAD response
This error probably means:
The service principalâs client secret key has expired.
Create a new client secret following step 2 in this article and update the secret in your Azure Key Vault.
Resources
Access storage using a service principal & Microsoft Entra ID(Azure Active Directory)
Connect to Azure Data Lake Storage and Blob Storage
Manage secret scopes
Get started: Query and visualize data from a notebook
Feedback
Was this page helpful?
Additional resources