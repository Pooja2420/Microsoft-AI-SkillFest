Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Load data into Azure Data Lake Storage Gen2 with Azure Data Factory
Article
2025-02-13
14 contributors
In this article
APPLIES TO:Azure Data FactoryAzure Synapse Analytics
Tip
Try outData Factory in Microsoft Fabric, an all-in-one analytics solution for enterprises.Microsoft Fabriccovers everything from data movement to data science, real-time analytics, business intelligence, and reporting. Learn how tostart a new trialfor free!
Azure Data Lake Storage Gen2 is a set of capabilities dedicated to big data analytics, built intoAzure Blob storage. It allows you to interface with your data using both file system and object storage paradigms.
Azure Data Factory (ADF) is a fully managed cloud-based data integration service. You can use the service to populate the lake with data from a rich set of on-premises and cloud-based data stores and save time when building your analytics solutions. For a detailed list of supported connectors, see the table ofSupported data stores.
Azure Data Factory offers a scale-out, managed data movement solution. Due to the scale-out architecture of ADF, it can ingest data at a high throughput. For details, seeCopy activity performance.
This article shows you how to use the Data Factory Copy Data tool to load data fromAmazon Web Services S3 serviceintoAzure Data Lake Storage Gen2. You can follow similar steps to copy data from other types of data stores.
Tip
For copying data from Azure Data Lake Storage Gen1 into Gen2, refer tothis specific walkthrough.
Prerequisites
Azure subscription: If you don't have an Azure subscription, create afree accountbefore you begin.
Azure Storage account with Data Lake Storage Gen2 enabled: If you don't have a Storage account,create an account.
AWS account with an S3 bucket that contains data: This article shows how to copy data from Amazon S3. You can use other data stores by following similar steps.
Create a data factory
If you have not created your data factory yet, follow the steps inQuickstart: Create a data factory by using the Azure portal and Azure Data Factory Studioto create one.  After creating it, browse to the data factory in the Azure portal.
If you have not created your data factory yet, follow the steps inQuickstart: Create a data factory by using the Azure portal and Azure Data Factory Studioto create one.  After creating it, browse to the data factory in the Azure portal.

SelectOpenon theOpen Azure Data Factory Studiotile to launch the Data Integration application in a separate tab.
SelectOpenon theOpen Azure Data Factory Studiotile to launch the Data Integration application in a separate tab.
Load data into Azure Data Lake Storage Gen2
In the home page of Azure Data Factory, select theIngesttile to launch the Copy Data tool.
In the home page of Azure Data Factory, select theIngesttile to launch the Copy Data tool.
In thePropertiespage, chooseBuilt-in copy taskunderTask type, and chooseRun once nowunderTask cadence or task schedule, then selectNext.
In thePropertiespage, chooseBuilt-in copy taskunderTask type, and chooseRun once nowunderTask cadence or task schedule, then selectNext.

In theSource data storepage, complete the following steps:Select+ New connection. SelectAmazon S3from the connector gallery, and selectContinue.In theNew connection (Amazon S3)page, do the following steps:Specify theAccess Key IDvalue.Specify theSecret Access Keyvalue.SelectTest connectionto validate the settings, then selectCreate.In theSource data storepage, ensure that the newly created Amazon S3 connection is selected in theConnectionblock.In theFile or foldersection, browse to the folder and file that you want to copy over. Select the folder/file, and then selectOK.Specify the copy behavior by checking theRecursivelyandBinary copyoptions. SelectNext.
In theSource data storepage, complete the following steps:
Select+ New connection. SelectAmazon S3from the connector gallery, and selectContinue.
Select+ New connection. SelectAmazon S3from the connector gallery, and selectContinue.

In theNew connection (Amazon S3)page, do the following steps:Specify theAccess Key IDvalue.Specify theSecret Access Keyvalue.SelectTest connectionto validate the settings, then selectCreate.
In theNew connection (Amazon S3)page, do the following steps:
Specify theAccess Key IDvalue.
Specify theSecret Access Keyvalue.
SelectTest connectionto validate the settings, then selectCreate.

In theSource data storepage, ensure that the newly created Amazon S3 connection is selected in theConnectionblock.
In theSource data storepage, ensure that the newly created Amazon S3 connection is selected in theConnectionblock.
In theFile or foldersection, browse to the folder and file that you want to copy over. Select the folder/file, and then selectOK.
In theFile or foldersection, browse to the folder and file that you want to copy over. Select the folder/file, and then selectOK.
Specify the copy behavior by checking theRecursivelyandBinary copyoptions. SelectNext.
Specify the copy behavior by checking theRecursivelyandBinary copyoptions. SelectNext.

In theDestination data storepage, complete the following steps.Select+ New connection, and then selectAzure Data Lake Storage Gen2, and selectContinue.In theNew connection (Azure Data Lake Storage Gen2)page, select your Data Lake Storage Gen2 capable account from the "Storage account name" drop-down list, and selectCreateto create the connection.In theDestination data storepage, select the newly created connection in theConnectionblock. Then underFolder path, entercopyfroms3as the output folder name, and selectNext. ADF will create the corresponding ADLS Gen2 file system and subfolders during copy if it doesn't exist.
In theDestination data storepage, complete the following steps.
Select+ New connection, and then selectAzure Data Lake Storage Gen2, and selectContinue.
Select+ New connection, and then selectAzure Data Lake Storage Gen2, and selectContinue.

In theNew connection (Azure Data Lake Storage Gen2)page, select your Data Lake Storage Gen2 capable account from the "Storage account name" drop-down list, and selectCreateto create the connection.
In theNew connection (Azure Data Lake Storage Gen2)page, select your Data Lake Storage Gen2 capable account from the "Storage account name" drop-down list, and selectCreateto create the connection.

In theDestination data storepage, select the newly created connection in theConnectionblock. Then underFolder path, entercopyfroms3as the output folder name, and selectNext. ADF will create the corresponding ADLS Gen2 file system and subfolders during copy if it doesn't exist.
In theDestination data storepage, select the newly created connection in theConnectionblock. Then underFolder path, entercopyfroms3as the output folder name, and selectNext. ADF will create the corresponding ADLS Gen2 file system and subfolders during copy if it doesn't exist.

In theSettingspage, specifyCopyFromAmazonS3ToADLSfor theTask namefield, and selectNextto use the default settings.
In theSettingspage, specifyCopyFromAmazonS3ToADLSfor theTask namefield, and selectNextto use the default settings.

In theSummarypage, review the settings, and selectNext.
In theSummarypage, review the settings, and selectNext.

On theDeployment page, selectMonitorto monitor the pipeline (task).
On theDeployment page, selectMonitorto monitor the pipeline (task).
When the pipeline run completes successfully, you see a pipeline run that is triggered by a manual trigger. You can use links under thePipeline namecolumn to view activity details and to rerun the pipeline.
When the pipeline run completes successfully, you see a pipeline run that is triggered by a manual trigger. You can use links under thePipeline namecolumn to view activity details and to rerun the pipeline.

To see activity runs associated with the pipeline run, select theCopyFromAmazonS3ToADLSlink under thePipeline namecolumn. For details about the copy operation, select theDetailslink (eyeglasses icon) under theActivity namecolumn. You can monitor details like the volume of data copied from the source to the sink, data throughput, execution steps with corresponding duration, and used configuration.
To see activity runs associated with the pipeline run, select theCopyFromAmazonS3ToADLSlink under thePipeline namecolumn. For details about the copy operation, select theDetailslink (eyeglasses icon) under theActivity namecolumn. You can monitor details like the volume of data copied from the source to the sink, data throughput, execution steps with corresponding duration, and used configuration.


To refresh the view, selectRefresh. SelectAll pipeline runsat the top to go back to the "Pipeline runs" view.
To refresh the view, selectRefresh. SelectAll pipeline runsat the top to go back to the "Pipeline runs" view.
Verify that the data is copied into your Data Lake Storage Gen2 account.
Verify that the data is copied into your Data Lake Storage Gen2 account.
Related content
Copy activity overview
Azure Data Lake Storage Gen2 connector
Feedback
Was this page helpful?
Additional resources