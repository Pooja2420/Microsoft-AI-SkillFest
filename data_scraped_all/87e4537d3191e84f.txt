Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Scrape Prometheus metrics at scale in Azure Monitor
Article
2025-02-28
3 contributors
In this article
This article provides guidance on performance that can be expected when collection metrics at high scale forAzure Monitor managed service for Prometheus.
CPU and memory
The CPU and memory usage is correlated with the number of bytes of each sample and the number of samples scraped. These benchmarks are based on thedefault targets scraped, volume of custom metrics scraped, and number of nodes, pods, and containers. These numbers are meant as a reference since usage can still vary significantly depending on the number of time series and bytes per metric.
The upper volume limit per pod is currently about 3-3.5 million samples per minute, depending on the number of bytes per sample.
The agent consists of a deployment with two replicas by default (which will be automatically configured by HPA based on memory utilization) and DaemonSet for scraping metrics. The DaemonSet scrapes any node-level targets such as cAdvisor, kubelet, and node exporter. You can also configure it to scrape any custom targets at the node level with static configs. The replica set scrapes everything else such as kube-state-metrics or custom scrape jobs that utilize service discovery.
Comparison between small and large cluster for replica
Comparison between small and large cluster for DaemonSets
For more custom metrics, the single pod behaves the same as the replica pod depending on the volume of custom metrics.
Schedule ama-metrics replica pod on a node pool with more resources
A large volume of metrics per pod needs a node with enough CPU and memory. If theama-metricsreplica pods aren't scheduled on nodes or node pools with enough resources, they might get OOMKilled and go into CrashLoopBackoff. To fix this, you can add the labelazuremonitor/metrics.replica.preferred=trueto nodes or node pools on your cluster with higher resources (insystem node pool). This ensures the replica pods get scheduled on those nodes. You can also create extra system pools with larger nodes and add the same label. It's better to label node pools rather than individual nodes so new nodes in the pool can also be used for scheduling.
azuremonitor/metrics.replica.preferred=true
kubectl label nodes <node-name> azuremonitor/metrics.replica.preferred="true"
kubectl label nodes <node-name> azuremonitor/metrics.replica.preferred="true"
Next steps
Troubleshoot issues with Prometheus data collection.
Feedback
Was this page helpful?
Additional resources