Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Azure NetApp Files large volume performance benchmarks for Linux
Article
2025-03-25
3 contributors
In this article
This article describes the tested performance capabilities of a singleAzure NetApp Files large volumesas it pertains to Linux use cases. The tests explored scenarios for both scale-out and scale-up read and write workloads, involving one and many virtual machines (VMs). Knowing the performance envelope of large volumes helps you facilitate volume sizing.
Testing summary
The Azure NetApp Files large volumes feature offersfour service levels, each with throughput limits. The service levels can be scaled up or down nondisruptively as your performance needs change.Standard, Preimium, and Ultra service levels: 12,800 MiB/sFlexible service: The Flexible service level enables you to adjust throughput and size limits independently for capacity pools using manual QoSThe Ultra service level was used in these tests.
The Azure NetApp Files large volumes feature offersfour service levels, each with throughput limits. The service levels can be scaled up or down nondisruptively as your performance needs change.
Standard, Preimium, and Ultra service levels: 12,800 MiB/s
Flexible service: The Flexible service level enables you to adjust throughput and size limits independently for capacity pools using manual QoS
The Ultra service level was used in these tests.
Sequential writes: 100% sequential writes maxed out at ~8,500 MiB/second in these benchmarks. (A single large volumeâs maximum throughput is capped at 12,800 MiB/second by the service, so more potential throughput is possible.)
Sequential writes: 100% sequential writes maxed out at ~8,500 MiB/second in these benchmarks. (A single large volumeâs maximum throughput is capped at 12,800 MiB/second by the service, so more potential throughput is possible.)
Sequential reads: 100% sequential reads maxed out at ~12,761 MiB/second in these benchmarks. (A single large volume's throughput is capped at 12,800 MiB/second. This result is near the maximum achievable throughput at this time.)
Sequential reads: 100% sequential reads maxed out at ~12,761 MiB/second in these benchmarks. (A single large volume's throughput is capped at 12,800 MiB/second. This result is near the maximum achievable throughput at this time.)
Random I/O: The same single large volume delivers over 700,000 operations per second.
Random I/O: The same single large volume delivers over 700,000 operations per second.
Metadata-heavy workloads are advantageous for Azure NetApp File large volumes due to the large volumeâs increased parallelism. Performance benefits are noticeable in workloads heavy in file creation, unlink, and file renames as typical with VCS applications, and EDA workloads where there are high file counts present. For more information on performance of high metadata workloads, seeBenefits of using Azure NetApp Files for electronic design automation.
Metadata-heavy workloads are advantageous for Azure NetApp File large volumes due to the large volumeâs increased parallelism. Performance benefits are noticeable in workloads heavy in file creation, unlink, and file renames as typical with VCS applications, and EDA workloads where there are high file counts present. For more information on performance of high metadata workloads, seeBenefits of using Azure NetApp Files for electronic design automation.
FIO, a synthetic workload generator designed as a storage stress test, was used to drive these test results. There are fundamentally two models of storage performance testing:Scale-out compute, which refers to using multiple VMs to generate the maximum load possible on a single Azure NetApp Files volume.Scale-up compute, which refers to using a large VM to test the upper boundaries of a single client on a single Azure NetApp Files volume.
FIO, a synthetic workload generator designed as a storage stress test, was used to drive these test results. There are fundamentally two models of storage performance testing:
Scale-out compute, which refers to using multiple VMs to generate the maximum load possible on a single Azure NetApp Files volume.
Scale-up compute, which refers to using a large VM to test the upper boundaries of a single client on a single Azure NetApp Files volume.
Linux scale-out test
Tests observed performance thresholds of a single large volume on scale-out and were conducted with the following configuration:
hard,rsize=65536,wsize=65536,vers=3
256-KiB sequential workloads (MiB/s)
The graph represents a 256-KiB sequential workload using 12 virtual machines reading and writing to a single large volume using a 1-TiB working set. The graph shows that a single Azure NetApp Files large volume can handle between approximately 8,518 MiB/s pure sequential writes and 12,761 MiB/s pure sequential reads.

8-KiB random workload (IOPS)
The graph represents an 8-KiB random workload and a 1 TiB working set. The graph shows that an Azure NetApp Files large volume can handle between approximately 474,000 pure random writes and approximately 709,000 pure random reads.

Linux scale-up tests
Whereas scale-out tests are designed to find the limits of a single large volume, scale-up tests are designed to find the upper limits of a single instance against said large volume. Azure places network egress limits on its VMs; for network attached storage that means that the write bandwidth is capped per VM. These scale-up tests demonstrate capabilities given the large available bandwidth cap and with sufficient processors to drive said workload.
The tests in this section were run with the following configuration:
hard,rsize=65536,wsize=65536,vers=3
The graphs in this section show the results for the client-side mount option ofnconnectwith NFSv3. For more information, seeLinux NFS mount options best practices for Azure NetApp File.
nconnect
The following graphs compare the advantages ofnconnectwith an NFS-mounted volume withoutnconnect. In the tests, FIO generated the workload from a single E104id-v5 instance in the East US Azure region using a 64-KiB sequential workload; a 256 I/0 size was used, which is the largest I/O size recommended by Azure NetApp Files resulted in comparable performance numbers. For more information, seeâ¯rsizeâ¯andâ¯wsize.
nconnect
nconnect
rsize
wsize
Linux read throughput
The following graphs show 256-KiB sequential reads of approximately 10,000M iB/s withâ¯nconnect, which is roughly ten times the throughput achieved withoutnconnect.
nconnect
nconnect
Note that 10,000 MiB/s is roughly the line rate of the 100 Gbps network interface card attached to the E104id_v5.

Linux write throughput
The following graphs show sequential writes. Usingnconnectprovides observable benefits for sequential writes at 6,600 MiB/s, roughly four times that of mounts withoutnconnect.
nconnect
nconnect

Linux read IOPS
The following graphs show 8-KiB random reads of ~426,000 read IOPS withnconnect, roughly seven times what is observed withoutnconnect.
nconnect
nconnect

Linux write IOPS
The following graphs show 8-KiB random writes of ~405,000 write IOPS withnconnect, roughly 7.2 times that what is observed withoutnconnect.
nconnect
nconnect

Next steps
Requirements and considerations for large volumes
Linux NFS mount options best practices for Azure NetApp Files
Benefits of using Azure NetApp Files for electronic design automation
Feedback
Was this page helpful?
Additional resources