Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Metrics in Application Insights
Article
2025-04-16
4 contributors
In this article
Application Insights supports three different types of metrics: standard (preaggregated), log-based, and custom metrics. Each one brings a unique value in monitoring application health, diagnostics, and analytics. Developers who are instrumenting applications can decide which type of metric is best suited to a particular scenario. Decisions are based on the size of the application, expected volume of telemetry, and business requirements for metrics precision and alerting. This article explains the difference between all supported metrics types.
Standard metrics in Application Insights are predefined metrics which are automatically collected and monitored by the service. These metrics cover a wide range of performance and usage indicators, such as CPU usage, memory consumption, request rates, and response times. Standard metrics provide a comprehensive overview of your application's health and performance without requiring any additional configuration. Standard metricsare preaggregatedduring collection and stored as a time series in a specialized repository with only key dimensions, which gives them better performance at query time. This makes standard metrics the best choice for near real time alerting on dimensions of metrics and more responsivedashboards.
Log-based metrics in Application Insights are a query-time concept, represented as a time series on top of the log data of your application. The underlying logsaren't preaggregatedat the collection or storage time and retain all properties of each log entry. This retention makes it possible to use log properties as dimensions on log-based metrics at query time formetric chart filteringandmetric splitting, giving log-based metrics superior analytical and diagnostic value. However, telemetry volume reduction techniques such assamplingandtelemetry filtering, commonly used with monitoring applications generating large volumes of telemetry, impacts the quantity of the collected log entries and therefore reduce the accuracy of log-based metrics.
Custom metrics in Application Insights allow you to define and track specific measurements that are unique to your application. These metrics can be created by instrumenting your code to send custom telemetry data to Application Insights. Custom metrics provide the flexibility to monitor any aspect of your application that isn't covered by standard metrics, enabling you to gain deeper insights into your application's behavior and performance.
For more information, seeCustom metrics in Azure Monitor (preview).
Note
Application Insights also provides a feature calledLive Metrics stream, which allows for near real-time monitoring of your web applications and doesn't store any telemetry data.
Metrics comparison
Metrics preaggregation
OpenTelemetry SDKs and newer Application Insights SDKs (Classic API) preaggregate metrics during collection to reduce the volume of data sent from the SDK to the telemetry channel endpoint. This process applies to standard metrics sent by default, so the accuracy isn't affected by sampling or filtering. It also applies to custom metrics sent using theOpenTelemetry APIorGetMetric and TrackValue, which results in less data ingestion and lower cost. If your version of the Application Insights SDK supports GetMetric and TrackValue, it's the preferred method of sending custom metrics.
For SDKs that don't implement preaggregation (that is, older versions of Application Insights SDKs or for browser instrumentation), the Application Insights back end still populates the new metrics by aggregating the events received by the Application Insights telemetry channel endpoint. For custom metrics, you can use thetrackMetricmethod. Although you don't benefit from the reduced volume of data transmitted over the wire, you can still use the preaggregated metrics and experience better performance and support of the near real time dimensional alerting with SDKs that don't preaggregate metrics during collection.
The telemetry channel endpoint preaggregates events before ingestion sampling. For this reason, ingestion sampling never affects the accuracy of preaggregated metrics, regardless of the SDK version you use with your application.
The following tables list where preaggregation are preaggregated.
Metrics preaggregation with Azure Monitor OpenTelemetry Distro
Metrics preaggregation with Application Insights SDK (Classic API)
Caution
The Application Insights Java 2.x SDK is no longer recommended. Use theOpenTelemetry-based Java offeringinstead.
The Application Insights Java 2.x SDK is no longer recommended. Use theOpenTelemetry-based Java offeringinstead.
TheOpenCensus Python SDK is retired. We recommend theOpenTelemetry-based Python offeringand providemigration guidance.
TheOpenCensus Python SDK is retired. We recommend theOpenTelemetry-based Python offeringand providemigration guidance.
Metrics preaggregation with autoinstrumentation
With autoinstrumentation, the SDK is automatically added to your application code and can't be customized. For custom metrics, manual instrumentation is required.
Footnotes
1ASP.NET Core autoinstrumentation on App Serviceemits standard metrics without dimensions. Manual instrumentation is required for all dimensions.
2ASP.NET autoinstrumentation on virtual machines/virtual machine scale setsandon-premisesemits standard metrics without dimensions. The same is true for Azure App Service, but the collection level must be set to recommended. Manual instrumentation is required for all dimensions.
3The Java agent used with autoinstrumentation captures metrics emitted by popular libraries and sends them to Application Insights as custom metrics.
Custom metrics dimensions and preaggregation
All metrics that you send usingOpenTelemetry,trackMetric, orGetMetric and TrackValueAPI calls are automatically stored in both the metrics store and logs. These metrics can be found in the customMetrics table in Application Insights and in Metrics Explorer under the Custom Metric Namespace calledazure.applicationinsights. Although the log-based version of your custom metric always retains all dimensions, the preaggregated version of the metric is stored by default with no dimensions. Retaining dimensions of custom metrics is a Preview feature that can be turned on from theUsage and estimated costtab by selectingWith dimensionsunderSend custom metrics to Azure Metric Store.

Quotas
Preaggregated metrics are stored as time series in Azure Monitor.Azure Monitor quotas on custom metricsapply.
Note
Going over the quota might have unintended consequences. Azure Monitor might become unreliable in your subscription or region. To learn how to avoid exceeding the quota, seeDesign limitations and considerations.
Why is collection of custom metrics dimensions turned off by default?
The collection of custom metrics dimensions is turned off by default because in the future, storing custom metrics with dimensions will be billed separately from Application Insights. Storing the nondimensional custom metrics remain free (up to a quota). You can learn about the upcoming pricing model changes on our officialpricing page.
Create charts and explore metrics
UseAzure Monitor metrics explorerto plot charts from preaggregated, log-based, and custom metrics, and to author dashboards with charts. After you select the Application Insights resource you want, use the namespace picker to switch between metrics.

Pricing models for Application Insights metrics
Ingesting metrics into Application Insights, whether log-based or preaggregated, generates costs based on the size of the ingested data. For more information, seeAzure Monitor Logs pricing details. Your custom metrics, including all its dimensions, are always stored in the Application Insights log store. Also, a preaggregated version of your custom metrics with no dimensions is forwarded to the metrics store by default.
Selecting theEnable alerting on custom metric dimensionsoption to store all dimensions of the preaggregated metrics in the metric store can generateextra costsbased oncustom metrics pricing.
Available metrics
Standard
Log-based
The following sections list metrics with supported aggregations and dimensions. The details about log-based metrics include the underlying Kusto query statements.
Important
Time Series Limit:Each metric can only have up to5,000time series within24 hours. Once this limit is reached, all dimension values of that metric point are replaced with the constantMaximum values reached.
Time Series Limit:Each metric can only have up to5,000time series within24 hours. Once this limit is reached, all dimension values of that metric point are replaced with the constantMaximum values reached.
Maximum values reached
Cardinality limit:Each dimension can only have a certain number of unique values within7 days. Once this limit is reached, the dimension value is replaced with the constantOther values. The cardinality limit for each dimension is listed in the tables below.
Cardinality limit:Each dimension can only have a certain number of unique values within7 days. Once this limit is reached, the dimension value is replaced with the constantOther values. The cardinality limit for each dimension is listed in the tables below.
Other values
The following sections list metrics with supported aggregations and dimensions. The details about log-based metrics include the underlying Kusto query statements. For convenience, each query uses defaults for time granularity, chart type, and sometimes splitting dimension which simplifies using the query in Log Analytics without any need for modification.
When you plot the same metric inmetrics explorer, there are no defaults - the query is dynamically adjusted based on your chart settings:
The selectedTime rangeis translated into an additionalwhere timestamp...clause to only pick the events from selected time range. For example, a chart showing data for the most recent 24 hours, the query includes| where timestamp > ago(24 h).
The selectedTime rangeis translated into an additionalwhere timestamp...clause to only pick the events from selected time range. For example, a chart showing data for the most recent 24 hours, the query includes| where timestamp > ago(24 h).
where timestamp...
| where timestamp > ago(24 h)
The selectedTime granularityis put into the finalsummarize ... by bin(timestamp, [time grain])clause.
The selectedTime granularityis put into the finalsummarize ... by bin(timestamp, [time grain])clause.
summarize ... by bin(timestamp, [time grain])
Any selectedFilterdimensions are translated into additionalwhereclauses.
Any selectedFilterdimensions are translated into additionalwhereclauses.
where
The selectedSplit chartdimension is translated into an extra summarize property. For example, if you split your chart bylocation, and plot using a 5-minute time granularity, thesummarizeclause is summarized... by bin(timestamp, 5 m), location.
The selectedSplit chartdimension is translated into an extra summarize property. For example, if you split your chart bylocation, and plot using a 5-minute time granularity, thesummarizeclause is summarized... by bin(timestamp, 5 m), location.
location
summarize
... by bin(timestamp, 5 m), location
Note
If you're new to the Kusto query language, you start by copying and pasting Kusto statements into the Log Analytics query pane without making any modifications. ClickRunto see basic chart. As you begin to understand the syntax of query language, you can start making small modifications and see the impact of your change. Exploring your own data is a great way to start realizing the full power ofLog AnalyticsandAzure Monitor.
Important
For the following log-based metrics, if multiple aggregations are supported, the aggregation initalicis used in the Kusto query example.
Availability metrics
Metrics in the Availability category enable you to see the health of your web application as observed from points around the world.Configure the availability teststo start using any metrics from this category.
Standard
Log-based
TheAvailabilitymetric shows the percentage of the web test runs that didn't detect any issues. The lowest possible value is 0, which indicates that all of the web test runs have failed. The value of 100 means that all of the web test runs passed the validation criteria.
Run location
availabilityResult/location
Test name
availabilityResult/name
TheAvailability test durationmetric shows how much time it took for the web test to run. For themulti-step web tests, the metric reflects the total execution time of all steps.
Run location
availabilityResult/location
Test name
availabilityResult/name
Test result
availabilityResult/success
TheAvailability testsmetric reflects the count of the web tests runs by Azure Monitor.
Run location
availabilityResult/location
Test name
availabilityResult/name
Test result
availabilityResult/success
TheAvailabilitymetric shows the percentage of the web test runs that didn't detect any issues. The lowest possible value is 0, which indicates that all of the web test runs have failed. The value of 100 means that all of the web test runs passed the validation criteria.
availabilityResults
| summarize ['availabilityResults/availabilityPercentage_avg'] = sum(todouble(success == 1) * 100) / count() by bin(timestamp, 15m)
| render timechart
availabilityResults
| summarize ['availabilityResults/availabilityPercentage_avg'] = sum(todouble(success == 1) * 100) / count() by bin(timestamp, 15m)
| render timechart
TheAvailability testsmetric reflects the count of the web tests runs by Azure Monitor.
availabilityResults
| summarize ['availabilityResults/count_sum'] = sum(itemCount) by bin(timestamp, 15m)
| render barchart
availabilityResults
| summarize ['availabilityResults/count_sum'] = sum(itemCount) by bin(timestamp, 15m)
| render barchart
TheAvailability test durationmetric shows how much time it took for the web test to run. For themulti-step web tests, the metric reflects the total execution time of all steps.
availabilityResults
| where notempty(duration)
| extend availabilityResult_duration = iif(itemType == 'availabilityResult', duration, todouble(''))
| summarize ['availabilityResults/duration_avg'] = sum(availabilityResult_duration) / sum(itemCount) by bin(timestamp, 5m)
| render timechart
availabilityResults
| where notempty(duration)
| extend availabilityResult_duration = iif(itemType == 'availabilityResult', duration, todouble(''))
| summarize ['availabilityResults/duration_avg'] = sum(availabilityResult_duration) / sum(itemCount) by bin(timestamp, 5m)
| render timechart
Browser metrics
Browser metrics are collected by the Application Insights JavaScript SDK from real end-user browsers. They provide great insights into your users' experience with your web app. Browser metrics are typically not sampled, which means that they provide higher precision of the usage numbers compared to server-side metrics which might be skewed by sampling.
Note
To collect browser metrics, your application must be instrumented with theApplication Insights JavaScript SDK.
Standard
Log-based
Time from user request until DOM, stylesheets, scripts, and images are loaded.
browserTimings
| where notempty(totalDuration)
| extend _sum = totalDuration
| extend _count = itemCount
| extend _sum = _sum * _count
| summarize ['browserTimings/totalDuration_avg'] = sum(_sum) / sum(_count) by bin(timestamp, 15m)
| render timechart
browserTimings
| where notempty(totalDuration)
| extend _sum = totalDuration
| extend _count = itemCount
| extend _sum = _sum * _count
| summarize ['browserTimings/totalDuration_avg'] = sum(_sum) / sum(_count) by bin(timestamp, 15m)
| render timechart
Time between receiving the last byte of a document until the DOM is loaded. Async requests may still be processing.
browserTimings
| where notempty(processingDuration)
| extend _sum = processingDuration
| extend _count = itemCount
| extend _sum = _sum * _count
| summarize ['browserTimings/processingDuration_avg'] = sum(_sum) / sum(_count) by bin(timestamp, 15m)
| render timechart
browserTimings
| where notempty(processingDuration)
| extend _sum = processingDuration
| extend _count = itemCount
| extend _sum = _sum * _count
| summarize ['browserTimings/processingDuration_avg'] = sum(_sum) / sum(_count) by bin(timestamp, 15m)
| render timechart
Time between user request and network connection. Includes DNS lookup and transport connection.
browserTimings
| where notempty(networkDuration)
| extend _sum = networkDuration
| extend _count = itemCount
| extend _sum = _sum * _count
| summarize ['browserTimings/networkDuration_avg'] = sum(_sum) / sum(_count) by bin(timestamp, 15m)
| render timechart
browserTimings
| where notempty(networkDuration)
| extend _sum = networkDuration
| extend _count = itemCount
| extend _sum = _sum * _count
| summarize ['browserTimings/networkDuration_avg'] = sum(_sum) / sum(_count) by bin(timestamp, 15m)
| render timechart
Time between the first and last bytes, or until disconnection.
browserTimings
| where notempty(receiveDuration)
| extend _sum = receiveDuration
| extend _count = itemCount
| extend _sum = _sum * _count
| summarize ['browserTimings/receiveDuration_avg'] = sum(_sum) / sum(_count) by bin(timestamp, 15m)
| render timechart
browserTimings
| where notempty(receiveDuration)
| extend _sum = receiveDuration
| extend _count = itemCount
| extend _sum = _sum * _count
| summarize ['browserTimings/receiveDuration_avg'] = sum(_sum) / sum(_count) by bin(timestamp, 15m)
| render timechart
Time between network connection and receiving the first byte.
browserTimings
| where notempty(sendDuration)
| extend _sum = sendDuration
| extend _count = itemCount
| extend _sum = _sum * _count
| summarize ['browserTimings/sendDuration_avg'] = sum(_sum) / sum(_count) by bin(timestamp, 15m)
| render timechart
browserTimings
| where notempty(sendDuration)
| extend _sum = sendDuration
| extend _count = itemCount
| extend _sum = _sum * _count
| summarize ['browserTimings/sendDuration_avg'] = sum(_sum) / sum(_count) by bin(timestamp, 15m)
| render timechart
Failure metrics
The metrics inFailuresshow problems with processing requests, dependency calls, and thrown exceptions.
Standard
Log-based
This metric reflects the number of thrown exceptions from your application code running in browser. Only exceptions that are tracked with atrackException()Application Insights API call are included in the metric.
trackException()
Cloud role name
cloud/roleName
The number of failed dependency calls.
Cloud role instance
cloud/roleInstance
Cloud role name
cloud/roleName
Dependency performance
dependency/performanceBucket
Dependency type
dependency/type
Is traffic synthetic
operation/synthetic
Result code
dependency/resultCode
Target of dependency call
dependency/target
Each time when you log an exception to Application Insights, there's a call to thetrackException() methodof the SDK. The Exceptions metric shows the number of logged exceptions.
Cloud role instance
cloud/roleInstance
Cloud role name
cloud/roleName
Device type
client/type
The count of tracked server requests that were marked asfailed. By default, the Application Insights SDK automatically marks each server request that returned HTTP response code 5xx or 4xx (except for 401) as a failed request. You can customize this logic by modifyingsuccessproperty of request telemetry item in acustom telemetry initializer. For more information about various response codes, seeApplication Insights telemetry data model.
Cloud role instance
cloud/roleInstance
Cloud role name
cloud/roleName
Is synthetic traffic
operation/synthetic
Request performance
request/performanceBucket
Result code
request/resultCode
This metric shows the number of server exceptions.
Cloud role instance
cloud/roleInstance
Cloud role name
cloud/roleName
This metric reflects the number of thrown exceptions from your application code running in browser. Only exceptions that are tracked with atrackException()Application Insights API call are included in the metric.
trackException()
Note
When using sampling, the itemCount indicates how many telemetry items a single log record represents. For example, with 25% sampling, each log record kept represents 4 items (1 kept + 3 sampled out). Log-based queries sum up all itemCount values to ensure the metric reflects the total number of actual events, not just the number of stored log records.
exceptions
| where client_Type == 'Browser'
| summarize ['exceptions/browser_sum'] = sum(itemCount) by bin(timestamp, 15m)
| render barchart
exceptions
| where client_Type == 'Browser'
| summarize ['exceptions/browser_sum'] = sum(itemCount) by bin(timestamp, 15m)
| render barchart
The number of failed dependency calls.
dependencies
| where success == 'False'
| summarize ['dependencies/failed_sum'] = sum(itemCount) by bin(timestamp, 15m)
| render barchart
dependencies
| where success == 'False'
| summarize ['dependencies/failed_sum'] = sum(itemCount) by bin(timestamp, 15m)
| render barchart
Each time when you log an exception to Application Insights, there's a call to thetrackException() methodof the SDK. The Exceptions metric shows the number of logged exceptions.
exceptions
| summarize ['exceptions/count_sum'] = sum(itemCount) by bin(timestamp, 15m)
| render barchart
exceptions
| summarize ['exceptions/count_sum'] = sum(itemCount) by bin(timestamp, 15m)
| render barchart
The count of tracked server requests that were marked asfailed. By default, the Application Insights SDK automatically marks each server request that returned HTTP response code 5xx or 4xx as a failed request. You can customize this logic by modifyingsuccessproperty of request telemetry item in acustom telemetry initializer.
requests
| where success == 'False'
| summarize ['requests/failed_sum'] = sum(itemCount) by bin(timestamp, 15m)
| render barchart
requests
| where success == 'False'
| summarize ['requests/failed_sum'] = sum(itemCount) by bin(timestamp, 15m)
| render barchart
This metric shows the number of server exceptions.
exceptions
| where client_Type != 'Browser'
| summarize ['exceptions/server_sum'] = sum(itemCount) by bin(timestamp, 15m)
| render barchart
exceptions
| where client_Type != 'Browser'
| summarize ['exceptions/server_sum'] = sum(itemCount) by bin(timestamp, 15m)
| render barchart
Performance counters
Use metrics in thePerformance counterscategory to accesssystem performance counters collected by Application Insights.
Standard
Log-based
Cloud role instance
cloud/roleInstance
Cloud role instance
cloud/roleInstance
Cloud role instance
cloud/roleInstance
Cloud role instance
cloud/roleInstance
Cloud role instance
cloud/roleInstance
The metric shows how much of the total processor capacity is consumed by the process that is hosting your monitored app.
Cloud role instance
cloud/roleInstance
Note
The range of the metric is between 0 and 100 * n, where n is the number of available CPU cores. For example, the metric value of 200% could represent full utilization of two CPU core or half utilization of 4 CPU cores and so on. TheProcess CPU Normalizedis an alternative metric collected by many SDKs which represents the same value but divides it by the number of available CPU cores. Thus, the range ofProcess CPU Normalizedmetric is 0 through 100.
Cloud role instance
cloud/roleInstance
Amount of nonshared memory that the monitored process allocated for its data.
Cloud role instance
cloud/roleInstance
CPU consumption byallprocesses running on the monitored server instance.
Cloud role instance
cloud/roleInstance
Note
The processor time metric is not available for the applications hosted in Azure App Services. Use theProcess CPUmetric to track CPU utilization of the web applications hosted in App Services.
performanceCounters
| where ((category == "ASP.NET Applications" and counter == "Request Execution Time") or name == "requestExecutionTime")
| extend performanceCounter_value = iif(itemType == 'performanceCounter', value, todouble(''))
| summarize ['performanceCounters/requestExecutionTime_avg'] = sum(performanceCounter_value) / count() by bin(timestamp, 15m)
| render barchart
performanceCounters
| where ((category == "ASP.NET Applications" and counter == "Request Execution Time") or name == "requestExecutionTime")
| extend performanceCounter_value = iif(itemType == 'performanceCounter', value, todouble(''))
| summarize ['performanceCounters/requestExecutionTime_avg'] = sum(performanceCounter_value) / count() by bin(timestamp, 15m)
| render barchart
performanceCounters
| where ((category == "ASP.NET Applications" and counter == "Requests/Sec") or name == "requestsPerSecond")
| extend performanceCounter_value = iif(itemType == 'performanceCounter', value, todouble(''))
| summarize ['performanceCounters/requestsPerSecond_avg'] = sum(performanceCounter_value) / count() by bin(timestamp, 15m)
| render barchart
performanceCounters
| where ((category == "ASP.NET Applications" and counter == "Requests/Sec") or name == "requestsPerSecond")
| extend performanceCounter_value = iif(itemType == 'performanceCounter', value, todouble(''))
| summarize ['performanceCounters/requestsPerSecond_avg'] = sum(performanceCounter_value) / count() by bin(timestamp, 15m)
| render barchart
performanceCounters
| where ((category == "ASP.NET Applications" and counter == "Requests In Application Queue") or name == "requestsInQueue")
| extend performanceCounter_value = iif(itemType == 'performanceCounter', value, todouble(''))
| summarize ['performanceCounters/requestsInQueue_avg'] = sum(performanceCounter_value) / count() by bin(timestamp, 15m)
| render barchart
performanceCounters
| where ((category == "ASP.NET Applications" and counter == "Requests In Application Queue") or name == "requestsInQueue")
| extend performanceCounter_value = iif(itemType == 'performanceCounter', value, todouble(''))
| summarize ['performanceCounters/requestsInQueue_avg'] = sum(performanceCounter_value) / count() by bin(timestamp, 15m)
| render barchart
performanceCounters
| where ((category == "Memory" and counter == "Available Bytes") or name == "memoryAvailableBytes")
| extend performanceCounter_value = iif(itemType == 'performanceCounter', value, todouble(''))
| summarize ['performanceCounters/memoryAvailableBytes_avg'] = sum(performanceCounter_value) / count() by bin(timestamp, 15m)
| render timechart
performanceCounters
| where ((category == "Memory" and counter == "Available Bytes") or name == "memoryAvailableBytes")
| extend performanceCounter_value = iif(itemType == 'performanceCounter', value, todouble(''))
| summarize ['performanceCounters/memoryAvailableBytes_avg'] = sum(performanceCounter_value) / count() by bin(timestamp, 15m)
| render timechart
performanceCounters
| where ((category == ".NET CLR Exceptions" and counter == "# of Exceps Thrown / sec") or name == "exceptionsPerSecond")
| extend performanceCounter_value = iif(itemType == 'performanceCounter', value, todouble(''))
| summarize ['performanceCounters/exceptionsPerSecond_avg'] = sum(performanceCounter_value) / count() by bin(timestamp, 15m)
| render timechart
performanceCounters
| where ((category == ".NET CLR Exceptions" and counter == "# of Exceps Thrown / sec") or name == "exceptionsPerSecond")
| extend performanceCounter_value = iif(itemType == 'performanceCounter', value, todouble(''))
| summarize ['performanceCounters/exceptionsPerSecond_avg'] = sum(performanceCounter_value) / count() by bin(timestamp, 15m)
| render timechart
performanceCounters
| where name == "GC Total Count"
| extend performanceCounter_value = iif(itemType == 'performanceCounter', value, todouble(''))
| summarize ['performanceCounters/GC Total Count_avg'] = sum(performanceCounter_value) / count() by bin(timestamp, 15m)
| render timechart
performanceCounters
| where name == "GC Total Count"
| extend performanceCounter_value = iif(itemType == 'performanceCounter', value, todouble(''))
| summarize ['performanceCounters/GC Total Count_avg'] = sum(performanceCounter_value) / count() by bin(timestamp, 15m)
| render timechart
performanceCounters
| where name == "GC Total Time"
| extend performanceCounter_value = iif(itemType == 'performanceCounter', value, todouble(''))
| summarize ['performanceCounters/GC Total Time_avg'] = sum(performanceCounter_value) / count() by bin(timestamp, 15m)
| render timechart
performanceCounters
| where name == "GC Total Time"
| extend performanceCounter_value = iif(itemType == 'performanceCounter', value, todouble(''))
| summarize ['performanceCounters/GC Total Time_avg'] = sum(performanceCounter_value) / count() by bin(timestamp, 15m)
| render timechart
performanceCounters
| where name == "Heap Memory Used (MB)"
| extend performanceCounter_value = iif(itemType == 'performanceCounter', value, todouble(''))
| summarize ['performanceCounters/Heap Memory Used (MB)_avg'] = sum(performanceCounter_value) / count() by bin(timestamp, 15m)
| render timechart
performanceCounters
| where name == "Heap Memory Used (MB)"
| extend performanceCounter_value = iif(itemType == 'performanceCounter', value, todouble(''))
| summarize ['performanceCounters/Heap Memory Used (MB)_avg'] = sum(performanceCounter_value) / count() by bin(timestamp, 15m)
| render timechart
performanceCounters
| where ((category == "ASP.NET Applications" and counter == "Request Execution Time") or name == "requestExecutionTime")
| extend performanceCounter_value = iif(itemType == "performanceCounter", value, todouble(''))
| summarize sum(performanceCounter_value) / count() by bin(timestamp, 1m)
| render timechart
performanceCounters
| where ((category == "ASP.NET Applications" and counter == "Request Execution Time") or name == "requestExecutionTime")
| extend performanceCounter_value = iif(itemType == "performanceCounter", value, todouble(''))
| summarize sum(performanceCounter_value) / count() by bin(timestamp, 1m)
| render timechart
performanceCounters
| where ((category == "ASP.NET Applications" and counter == "Requests/Sec") or name == "requestsPerSecond")
| extend performanceCounter_value = iif(itemType == "performanceCounter", value, todouble(''))
| summarize sum(performanceCounter_value) / count() by bin(timestamp, 1m)
| render timechart
performanceCounters
| where ((category == "ASP.NET Applications" and counter == "Requests/Sec") or name == "requestsPerSecond")
| extend performanceCounter_value = iif(itemType == "performanceCounter", value, todouble(''))
| summarize sum(performanceCounter_value) / count() by bin(timestamp, 1m)
| render timechart
performanceCounters
| where ((category == "ASP.NET Applications" and counter == "Requests In Application Queue") or name == "requestsInQueue")
| extend performanceCounter_value = iif(itemType == "performanceCounter", value, todouble(''))
| summarize sum(performanceCounter_value) / count() by bin(timestamp, 1m)
| render timechart
performanceCounters
| where ((category == "ASP.NET Applications" and counter == "Requests In Application Queue") or name == "requestsInQueue")
| extend performanceCounter_value = iif(itemType == "performanceCounter", value, todouble(''))
| summarize sum(performanceCounter_value) / count() by bin(timestamp, 1m)
| render timechart
The metric shows how much of the total processor capacity is consumed by the process that is hosting your monitored app.
performanceCounters
| where ((category == "Process" and counter == "% Processor Time Normalized") or name == "processCpuPercentage")
| extend performanceCounter_value = iif(itemType == 'performanceCounter', value, todouble(''))
| summarize ['performanceCounters/processCpuPercentage_avg'] = sum(performanceCounter_value) / count() by bin(timestamp, 15m)
| render timechart
performanceCounters
| where ((category == "Process" and counter == "% Processor Time Normalized") or name == "processCpuPercentage")
| extend performanceCounter_value = iif(itemType == 'performanceCounter', value, todouble(''))
| summarize ['performanceCounters/processCpuPercentage_avg'] = sum(performanceCounter_value) / count() by bin(timestamp, 15m)
| render timechart
Note
The range of the metric is between 0 and 100 * n, where n is the number of available CPU cores. For example, the metric value of 200% could represent full utilization of two CPU core or half utilization of 4 CPU cores and so on. TheProcess CPU Normalizedis an alternative metric collected by many SDKs which represents the same value but divides it by the number of available CPU cores. Thus, the range ofProcess CPU Normalizedmetric is 0 through 100.
performanceCounters
| where ((category == "Process" and counter == "% Processor Time") or name == "processCpuPercentageTotal")
| extend performanceCounter_value = iif(itemType == 'performanceCounter', value, todouble(''))
| summarize ['performanceCounters/processCpuPercentageTotal_avg'] = sum(performanceCounter_value) / count() by bin(timestamp, 15m)
| render timechart
performanceCounters
| where ((category == "Process" and counter == "% Processor Time") or name == "processCpuPercentageTotal")
| extend performanceCounter_value = iif(itemType == 'performanceCounter', value, todouble(''))
| summarize ['performanceCounters/processCpuPercentageTotal_avg'] = sum(performanceCounter_value) / count() by bin(timestamp, 15m)
| render timechart
performanceCounters
| where ((category == "Process" and counter == "IO Data Bytes/sec") or name == "processIOBytesPerSecond")
| extend performanceCounter_value = iif(itemType == 'performanceCounter', value, todouble(''))
| summarize ['performanceCounters/processIOBytesPerSecond_avg'] = sum(performanceCounter_value) / count() by bin(timestamp, 15m)
| render timechart
performanceCounters
| where ((category == "Process" and counter == "IO Data Bytes/sec") or name == "processIOBytesPerSecond")
| extend performanceCounter_value = iif(itemType == 'performanceCounter', value, todouble(''))
| summarize ['performanceCounters/processIOBytesPerSecond_avg'] = sum(performanceCounter_value) / count() by bin(timestamp, 15m)
| render timechart
Amount of nonshared memory that the monitored process allocated for its data.
performanceCounters
| where ((category == "Process" and counter == "Private Bytes") or name == "processPrivateBytes")
| extend performanceCounter_value = iif(itemType == 'performanceCounter', value, todouble(''))
| summarize ['performanceCounters/processPrivateBytes_avg'] = sum(performanceCounter_value) / count() by bin(timestamp, 15m)
| render timechart
performanceCounters
| where ((category == "Process" and counter == "Private Bytes") or name == "processPrivateBytes")
| extend performanceCounter_value = iif(itemType == 'performanceCounter', value, todouble(''))
| summarize ['performanceCounters/processPrivateBytes_avg'] = sum(performanceCounter_value) / count() by bin(timestamp, 15m)
| render timechart
CPU consumption byallprocesses running on the monitored server instance.
Note
The processor time metric is not available for the applications hosted in Azure App Services. Use theProcess CPUmetric to track CPU utilization of the web applications hosted in App Services.
performanceCounters
| where ((category == "Processor" and counter == "% Processor Time") or name == "processorCpuPercentage")
| extend performanceCounter_value = iif(itemType == 'performanceCounter', value, todouble(''))
| summarize ['performanceCounters/processorCpuPercentage_avg'] = sum(performanceCounter_value) / count() by bin(timestamp, 15m)
| render timechart
performanceCounters
| where ((category == "Processor" and counter == "% Processor Time") or name == "processorCpuPercentage")
| extend performanceCounter_value = iif(itemType == 'performanceCounter', value, todouble(''))
| summarize ['performanceCounters/processorCpuPercentage_avg'] = sum(performanceCounter_value) / count() by bin(timestamp, 15m)
| render timechart
Suspected Deadlock Threads (performanceCounters/Suspected Deadlocked Threads)
performanceCounters
| where name == "Suspected Deadlocked Threads"
| extend performanceCounter_value = iif(itemType == 'performanceCounter', value, todouble(''))
| summarize ['performanceCounters/Suspected Deadlocked Threads_avg'] = sum(performanceCounter_value) / count() by bin(timestamp, 15m)
| render timechart
performanceCounters
| where name == "Suspected Deadlocked Threads"
| extend performanceCounter_value = iif(itemType == 'performanceCounter', value, todouble(''))
| summarize ['performanceCounters/Suspected Deadlocked Threads_avg'] = sum(performanceCounter_value) / count() by bin(timestamp, 15m)
| render timechart
Server metrics
Standard
Log-based
This metric is in relation to the number of dependency calls.
Cloud role instance
cloud/roleInstance
Cloud role name
cloud/roleName
Dependency performance
dependency/performanceBucket
Dependency type
dependency/type
Is traffic synthetic
operation/synthetic
Result code
request/resultCode
Successful call
dependency/success
Target of a dependency call
dependency/target
This metric refers to duration of dependency calls.
Cloud role instance
cloud/roleInstance
Cloud role name
cloud/roleName
Dependency performance
dependency/performanceBucket
Dependency type
dependency/type
Is traffic synthetic
operation/synthetic
Result code
request/resultCode
Successful call
dependency/success
Target of a dependency call
dependency/target
This metric reflects the number of incoming server requests that were received by your web application.
Cloud role instance
cloud/roleInstance
Cloud role name
cloud/roleName
Is traffic synthetic
operation/synthetic
Request performance
request/performanceBucket
Result code
request/resultCode
Successful call
dependency/success
Cloud role instance
cloud/roleInstance
Cloud role name
cloud/roleName
Is traffic synthetic
operation/synthetic
Request performance
request/performanceBucket
Result code
request/resultCode
Successful call
dependency/success
This metric reflects the time it took for the servers to process incoming requests.
Cloud role instance
cloud/roleInstance
Cloud role name
cloud/roleName
Is traffic synthetic
operation/synthetic
Request performance
request/performanceBucket
Result code
request/resultCode
Successful call
dependency/success
This metric is in relation to the number of dependency calls.
dependencies
| summarize ['dependencies/count_sum'] = sum(itemCount) by bin(timestamp, 15m)
| render barchart
dependencies
| summarize ['dependencies/count_sum'] = sum(itemCount) by bin(timestamp, 15m)
| render barchart
This metric refers to duration of dependency calls.
dependencies
| where notempty(duration)
| extend dependency_duration = iif(itemType == 'dependency', duration, todouble(''))
| extend _sum = dependency_duration
| extend _count = itemCount
| extend _sum = _sum * _count
| summarize ['dependencies/duration_avg'] = sum(_sum) / sum(_count) by bin(timestamp, 15m)
| render timechart
dependencies
| where notempty(duration)
| extend dependency_duration = iif(itemType == 'dependency', duration, todouble(''))
| extend _sum = dependency_duration
| extend _count = itemCount
| extend _sum = _sum * _count
| summarize ['dependencies/duration_avg'] = sum(_sum) / sum(_count) by bin(timestamp, 15m)
| render timechart
This metric reflects the number of incoming server requests that were received by your web application.
requests
| summarize ['requests/count_sum'] = sum(itemCount) by bin(timestamp, 15m)
| render barchart
requests
| summarize ['requests/count_sum'] = sum(itemCount) by bin(timestamp, 15m)
| render barchart
This metric reflects the time it took for the servers to process incoming requests.
requests
| where notempty(duration)
| extend request_duration = iif(itemType == 'request', duration, todouble(''))
| extend _sum = request_duration
| extend _count = itemCount
| extend _sum = _sum * _count
| summarize ['requests/duration_avg'] = sum(_sum) / sum(_count) by bin(timestamp, 15m)
| render timechart
requests
| where notempty(duration)
| extend request_duration = iif(itemType == 'request', duration, todouble(''))
| extend _sum = request_duration
| extend _count = itemCount
| extend _sum = _sum * _count
| summarize ['requests/duration_avg'] = sum(_sum) / sum(_count) by bin(timestamp, 15m)
| render timechart
Usage metrics
Standard
Log-based
This metric refers to the amount of time it took for PageView events to load.
Cloud role name
cloud/roleName
Is traffic synthetic
operation/synthetic
The count of PageView events logged with the TrackPageView() Application Insights API.
Cloud role name
cloud/roleName
Is traffic synthetic
operation/synthetic
The count of trace statements logged with the TrackTrace() Application Insights API call.
Cloud role instance
cloud/roleInstance
Cloud role name
cloud/roleName
Is traffic synthetic
operation/synthetic
Severity level
trace/severityLevel
customEvents
| summarize ['customEvents/count_sum'] = sum(itemCount) by bin(timestamp, 15m)
| render barchart
customEvents
| summarize ['customEvents/count_sum'] = sum(itemCount) by bin(timestamp, 15m)
| render barchart
This metric refers to the amount of time it took for PageView events to load.
pageViews
| where notempty(duration)
| extend pageView_duration = iif(itemType == 'pageView', duration, todouble(''))
| extend _sum = pageView_duration
| extend _count = itemCount
| extend _sum = _sum * _count
| summarize ['pageViews/duration_avg'] = sum(_sum) / sum(_count) by bin(timestamp, 15m)
| render barchart
pageViews
| where notempty(duration)
| extend pageView_duration = iif(itemType == 'pageView', duration, todouble(''))
| extend _sum = pageView_duration
| extend _count = itemCount
| extend _sum = _sum * _count
| summarize ['pageViews/duration_avg'] = sum(_sum) / sum(_count) by bin(timestamp, 15m)
| render barchart
The count of PageView events logged with the TrackPageView() Application Insights API.
pageViews
| summarize ['pageViews/count_sum'] = sum(itemCount) by bin(timestamp, 15m)
| render barchart
pageViews
| summarize ['pageViews/count_sum'] = sum(itemCount) by bin(timestamp, 15m)
| render barchart
This metric refers to the count of distinct session IDs.
union traces, requests, pageViews, dependencies, customEvents, availabilityResults, exceptions, customMetrics, browserTimings
| where notempty(session_Id)
| summarize ['sessions/count_unique'] = dcount(session_Id) by bin(timestamp, 15m)
| render barchart
union traces, requests, pageViews, dependencies, customEvents, availabilityResults, exceptions, customMetrics, browserTimings
| where notempty(session_Id)
| summarize ['sessions/count_unique'] = dcount(session_Id) by bin(timestamp, 15m)
| render barchart
The count of trace statements logged with the TrackTrace() Application Insights API call.
traces
| summarize ['traces/count_sum'] = sum(itemCount) by bin(timestamp, 15m)
| render barchart
traces
| summarize ['traces/count_sum'] = sum(itemCount) by bin(timestamp, 15m)
| render barchart
The number of distinct users who accessed your application. The accuracy of this metric may be  significantly impacted by using telemetry sampling and filtering.
union traces, requests, pageViews, dependencies, customEvents, availabilityResults, exceptions, customMetrics, browserTimings
| where notempty(user_Id)
| summarize ['users/count_unique'] = dcount(user_Id) by bin(timestamp, 15m)
| render barchart
union traces, requests, pageViews, dependencies, customEvents, availabilityResults, exceptions, customMetrics, browserTimings
| where notempty(user_Id)
| summarize ['users/count_unique'] = dcount(user_Id) by bin(timestamp, 15m)
| render barchart
The number of distinct users who authenticated into your application.
union traces, requests, pageViews, dependencies, customEvents, availabilityResults, exceptions, customMetrics, browserTimings
| where notempty(user_AuthenticatedId)
| summarize ['users/authenticated_unique'] = dcount(user_AuthenticatedId) by bin(timestamp, 15m)
| render barchart
union traces, requests, pageViews, dependencies, customEvents, availabilityResults, exceptions, customMetrics, browserTimings
| where notempty(user_AuthenticatedId)
| summarize ['users/authenticated_unique'] = dcount(user_AuthenticatedId) by bin(timestamp, 15m)
| render barchart
Custom metrics
Standard
Log-based
Not applicable to standard metrics.
Custom metrics are stored in both the metrics store and logs, making it possible to retrieve them using Kusto queries.
For example, if you instrument your application with_telemetryClient.GetMetric("Sales Amount").TrackValue(saleAmount);usingGetMetricandTrackValueto track the custom metricSales Amount, you can use the following Kusto queries for each available aggregation.
_telemetryClient.GetMetric("Sales Amount").TrackValue(saleAmount);
customMetrics
| where name == "Sales Amount"
| extend
    customMetric_valueSum = iif(itemType == 'customMetric', valueSum, todouble('')),
    customMetric_valueCount = iif(itemType == 'customMetric', valueCount, toint(''))
| summarize ['customMetrics/Sales Amount_avg'] = sum(customMetric_valueSum) / sum(customMetric_valueCount) by bin(timestamp, 15m)
| order by timestamp desc
| render timechart
customMetrics
| where name == "Sales Amount"
| extend
    customMetric_valueSum = iif(itemType == 'customMetric', valueSum, todouble('')),
    customMetric_valueCount = iif(itemType == 'customMetric', valueCount, toint(''))
| summarize ['customMetrics/Sales Amount_avg'] = sum(customMetric_valueSum) / sum(customMetric_valueCount) by bin(timestamp, 15m)
| order by timestamp desc
| render timechart
customMetrics
| where name == "Sales Amount"
| extend customMetric_valueMin = iif(itemType == 'customMetric', valueMin, todouble(''))
| summarize ['customMetrics/Sales Amount_min'] = min(customMetric_valueMin) by bin(timestamp, 15m)
| render timechart
customMetrics
| where name == "Sales Amount"
| extend customMetric_valueMin = iif(itemType == 'customMetric', valueMin, todouble(''))
| summarize ['customMetrics/Sales Amount_min'] = min(customMetric_valueMin) by bin(timestamp, 15m)
| render timechart
customMetrics
| where name == "Sales Amount"
| extend customMetric_valueMax = iif(itemType == 'customMetric', valueMax, todouble(''))
| summarize ['customMetrics/Sales Amount_max'] = max(customMetric_valueMax) by bin(timestamp, 15m)
| render timechart
customMetrics
| where name == "Sales Amount"
| extend customMetric_valueMax = iif(itemType == 'customMetric', valueMax, todouble(''))
| summarize ['customMetrics/Sales Amount_max'] = max(customMetric_valueMax) by bin(timestamp, 15m)
| render timechart
customMetrics
| where name == "Sales Amount"
| extend customMetric_valueSum = iif(itemType == 'customMetric', valueSum, todouble(''))
| summarize ['customMetrics/Sales Amount_sum'] = sum(customMetric_valueSum) by bin(timestamp, 15m)
| render barchart
customMetrics
| where name == "Sales Amount"
| extend customMetric_valueSum = iif(itemType == 'customMetric', valueSum, todouble(''))
| summarize ['customMetrics/Sales Amount_sum'] = sum(customMetric_valueSum) by bin(timestamp, 15m)
| render barchart
customMetrics
| where name == "Sales Amount"
| extend customMetric_valueCount = iif(itemType == 'customMetric', valueCount, toint(''))
| summarize ['customMetrics/Sales Amount_count'] = sum(customMetric_valueCount) by bin(timestamp, 15m)
| render barchart
customMetrics
| where name == "Sales Amount"
| extend customMetric_valueCount = iif(itemType == 'customMetric', valueCount, toint(''))
| summarize ['customMetrics/Sales Amount_count'] = sum(customMetric_valueCount) by bin(timestamp, 15m)
| render barchart
customMetrics
| where name == "Sales Amount"
| extend customMetric_value = iif(itemType == 'customMetric', value, todouble(''))
| summarize ['customMetrics/Sales Amount_unique'] = dcount(customMetric_value) by bin(timestamp, 15m)
| render barchart
customMetrics
| where name == "Sales Amount"
| extend customMetric_value = iif(itemType == 'customMetric', value, todouble(''))
| summarize ['customMetrics/Sales Amount_unique'] = dcount(customMetric_value) by bin(timestamp, 15m)
| render barchart
Access log-based metrics directly with the Application Insights REST API
The Application Insights REST API enables programmatic retrieval of log-based metrics. It also features an optional parameterai.include-query-payloadthat when added to a query string, prompts the API to return not only the time series data, but also the Kusto Query Language (KQL) statement used to fetch it. This parameter can be particularly beneficial for users aiming to comprehend the connection between raw events in Log Analytics and the resulting log-based metric.
ai.include-query-payload
To access your data directly, pass the parameterai.include-query-payloadto the Application Insights API in a query using KQL.
ai.include-query-payload
Note
To retrieve the underlying logs query,DEMO_APPandDEMO_KEYdon'thave to be replaced. If you just want to retrieve the KQL statement and not the time series data of your own application, you can copy and paste it directly into your browser search bar.
DEMO_APP
DEMO_KEY
api.applicationinsights.io/v1/apps/DEMO_APP/metrics/users/authenticated?api_key=DEMO_KEY&prefer=ai.include-query-payload
api.applicationinsights.io/v1/apps/DEMO_APP/metrics/users/authenticated?api_key=DEMO_KEY&prefer=ai.include-query-payload
The following is an example of a return KQL statement for the metricAuthenticated Users. (In this example,"users/authenticated"is the metric ID.)
Authenticated Users
"users/authenticated"
output
{
    "value": {
        "start": "2024-06-21T09:14:25.450Z",
        "end": "2024-06-21T21:14:25.450Z",
        "users/authenticated": {
            "unique": 0
        }
    },
    "@ai.query": "union (traces | where timestamp >= datetime(2024-06-21T09:14:25.450Z) and timestamp < datetime(2024-06-21T21:14:25.450Z)), (requests | where timestamp >= datetime(2024-06-21T09:14:25.450Z) and timestamp < datetime(2024-06-21T21:14:25.450Z)), (pageViews | where timestamp >= datetime(2024-06-21T09:14:25.450Z) and timestamp < datetime(2024-06-21T21:14:25.450Z)), (dependencies | where timestamp >= datetime(2024-06-21T09:14:25.450Z) and timestamp < datetime(2024-06-21T21:14:25.450Z)), (customEvents | where timestamp >= datetime(2024-06-21T09:14:25.450Z) and timestamp < datetime(2024-06-21T21:14:25.450Z)), (availabilityResults | where timestamp >= datetime(2024-06-21T09:14:25.450Z) and timestamp < datetime(2024-06-21T21:14:25.450Z)), (exceptions | where timestamp >= datetime(2024-06-21T09:14:25.450Z) and timestamp < datetime(2024-06-21T21:14:25.450Z)), (customMetrics | where timestamp >= datetime(2024-06-21T09:14:25.450Z) and timestamp < datetime(2024-06-21T21:14:25.450Z)), (browserTimings | where timestamp >= datetime(2024-06-21T09:14:25.450Z) and timestamp < datetime(2024-06-21T21:14:25.450Z)) | where notempty(user_AuthenticatedId) | summarize ['users/authenticated_unique'] = dcount(user_AuthenticatedId)"
}
output
{
    "value": {
        "start": "2024-06-21T09:14:25.450Z",
        "end": "2024-06-21T21:14:25.450Z",
        "users/authenticated": {
            "unique": 0
        }
    },
    "@ai.query": "union (traces | where timestamp >= datetime(2024-06-21T09:14:25.450Z) and timestamp < datetime(2024-06-21T21:14:25.450Z)), (requests | where timestamp >= datetime(2024-06-21T09:14:25.450Z) and timestamp < datetime(2024-06-21T21:14:25.450Z)), (pageViews | where timestamp >= datetime(2024-06-21T09:14:25.450Z) and timestamp < datetime(2024-06-21T21:14:25.450Z)), (dependencies | where timestamp >= datetime(2024-06-21T09:14:25.450Z) and timestamp < datetime(2024-06-21T21:14:25.450Z)), (customEvents | where timestamp >= datetime(2024-06-21T09:14:25.450Z) and timestamp < datetime(2024-06-21T21:14:25.450Z)), (availabilityResults | where timestamp >= datetime(2024-06-21T09:14:25.450Z) and timestamp < datetime(2024-06-21T21:14:25.450Z)), (exceptions | where timestamp >= datetime(2024-06-21T09:14:25.450Z) and timestamp < datetime(2024-06-21T21:14:25.450Z)), (customMetrics | where timestamp >= datetime(2024-06-21T09:14:25.450Z) and timestamp < datetime(2024-06-21T21:14:25.450Z)), (browserTimings | where timestamp >= datetime(2024-06-21T09:14:25.450Z) and timestamp < datetime(2024-06-21T21:14:25.450Z)) | where notempty(user_AuthenticatedId) | summarize ['users/authenticated_unique'] = dcount(user_AuthenticatedId)"
}
Next steps
Metrics - Get - REST API
Application Insights API for custom events and metrics
GetMetric and TrackValue
Feedback
Was this page helpful?
Additional resources