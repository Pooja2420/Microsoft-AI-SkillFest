Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Tutorial: Load and transform data using Apache Spark DataFrames
Article
2024-08-29
2 contributors
In this article
This tutorial shows you how to load and transform data using the Apache Spark Python (PySpark) DataFrame API, the Apache Spark Scala DataFrame API, and the SparkR SparkDataFrame API in Azure Databricks.
By the end of this tutorial, you will understand what a DataFrame is and be familiar with the following tasks:
Python
Define variables and copy public data into a Unity Catalog volume
Create a DataFrame with Python
Load data into a DataFrame from CSV file
View and interact with a DataFrame
Save the DataFrame
Run SQL queries in PySpark
See alsoApache Spark PySpark API reference.
Scala
Define variables and copy public data into a Unity Catalog volume
Create a DataFrame with Scala
Load data into a DataFrame from CSV file
View and interacting with a DataFrame
Save the DataFrame
Run SQL queries in Apache Spark
See alsoApache Spark Scala API reference.
R
Define variables and copy public data into a Unity Catalog volume
Create a SparkR SparkDataFrames
Load data into a DataFrame from CSV file
View and interact with a DataFrame
Save the DataFrame
Run SQL queries in SparkR
See alsoApache SparkR API reference.
What is a DataFrame?
A DataFrame is a two-dimensional labeled data structure with columns of potentially different types. You can think of a DataFrame like a spreadsheet, a SQL table, or a dictionary of series objects. Apache Spark DataFrames provide a rich set of functions (select columns, filter, join, aggregate) that allow you to solve common data analysis problems efficiently.
Apache Spark DataFrames are an abstraction built on top of Resilient Distributed Datasets (RDDs). Spark DataFrames and Spark SQL use a unified planning and optimization engine, allowing you to get nearly identical performance across all supported languages on Azure Databricks (Python, SQL, Scala, and R).
Requirements
To complete the following tutorial, you must meet the following requirements:
To use the examples in this tutorial, your workspace must haveUnity Catalogenabled.
To use the examples in this tutorial, your workspace must haveUnity Catalogenabled.
The examples in this tutorial use a Unity Catalogvolumeto store sample data. To use these examples, create a volume and use that volumeâs catalog, schema, and volume names to set the volume path used by the examples.
The examples in this tutorial use a Unity Catalogvolumeto store sample data. To use these examples, create a volume and use that volumeâs catalog, schema, and volume names to set the volume path used by the examples.
You must have the following permissions in Unity Catalog:READ VOLUMEandWRITE VOLUME, orALL PRIVILEGESfor the volume used for this tutorial.USE SCHEMAorALL PRIVILEGESfor the schema used for this tutorial.USE CATALOGorALL PRIVILEGESfor the catalog used for this tutorial.To set these permissions, see your Databricks administrator orUnity Catalog privileges and securable objects.
You must have the following permissions in Unity Catalog:
READ VOLUMEandWRITE VOLUME, orALL PRIVILEGESfor the volume used for this tutorial.
READ VOLUME
WRITE VOLUME
ALL PRIVILEGES
USE SCHEMAorALL PRIVILEGESfor the schema used for this tutorial.
USE SCHEMA
ALL PRIVILEGES
USE CATALOGorALL PRIVILEGESfor the catalog used for this tutorial.
USE CATALOG
ALL PRIVILEGES
To set these permissions, see your Databricks administrator orUnity Catalog privileges and securable objects.
Tip
For a completed notebook for this article, seeDataFrame tutorial notebooks.
Step 1: Define variables and load CSV file
This step defines variables for use in this tutorial and then loads a CSV file containing baby name data fromhealth.data.ny.govinto your Unity Catalog volume.
Open a new notebook by clicking theicon. To learn how to navigate Azure Databricks notebooks, seeCustomize notebook appearance.
Open a new notebook by clicking theicon. To learn how to navigate Azure Databricks notebooks, seeCustomize notebook appearance.
Copy and paste the following code into the new empty notebook cell. Replace<catalog-name>,<schema-name>, and<volume-name>with the catalog, schema, and volume names for a Unity Catalog volume. Replace<table_name>with a table name of your choice. You will load baby name data into this table later in this tutorial.Pythoncatalog = "<catalog_name>"
schema = "<schema_name>"
volume = "<volume_name>"
download_url = "https://health.data.ny.gov/api/views/jxy9-yhdk/rows.csv"
file_name = "rows.csv"
table_name = "<table_name>"
path_volume = "/Volumes/" + catalog + "/" + schema + "/" + volume
path_table = catalog + "." + schema
print(path_table) # Show the complete path
print(path_volume) # Show the complete pathScalaval catalog = "<catalog_name>"
val schema = "<schema_name>"
val volume = "<volume_name>"
val downloadUrl = "https://health.data.ny.gov/api/views/jxy9-yhdk/rows.csv"
val fileName = "rows.csv"
val tableName = "<table_name>"
val pathVolume = s"/Volumes/$catalog/$schema/$volume"
val pathTable = s"$catalog.$schema"
print(pathVolume) // Show the complete path
print(pathTable) // Show the complete pathRcatalog <- "<catalog_name>"
schema <- "<schema_name>"
volume <- "<volume_name>"
download_url <- "https://health.data.ny.gov/api/views/jxy9-yhdk/rows.csv"
file_name <- "rows.csv"
table_name <- "<table_name>"
path_volume <- paste("/Volumes/", catalog, "/", schema, "/", volume, sep = "")
path_table <- paste(catalog, ".", schema, sep = "")
print(path_volume) # Show the complete path
print(path_table) # Show the complete path
Copy and paste the following code into the new empty notebook cell. Replace<catalog-name>,<schema-name>, and<volume-name>with the catalog, schema, and volume names for a Unity Catalog volume. Replace<table_name>with a table name of your choice. You will load baby name data into this table later in this tutorial.
<catalog-name>
<schema-name>
<volume-name>
<table_name>
Python
catalog = "<catalog_name>"
schema = "<schema_name>"
volume = "<volume_name>"
download_url = "https://health.data.ny.gov/api/views/jxy9-yhdk/rows.csv"
file_name = "rows.csv"
table_name = "<table_name>"
path_volume = "/Volumes/" + catalog + "/" + schema + "/" + volume
path_table = catalog + "." + schema
print(path_table) # Show the complete path
print(path_volume) # Show the complete path
catalog = "<catalog_name>"
schema = "<schema_name>"
volume = "<volume_name>"
download_url = "https://health.data.ny.gov/api/views/jxy9-yhdk/rows.csv"
file_name = "rows.csv"
table_name = "<table_name>"
path_volume = "/Volumes/" + catalog + "/" + schema + "/" + volume
path_table = catalog + "." + schema
print(path_table) # Show the complete path
print(path_volume) # Show the complete path
Scala
val catalog = "<catalog_name>"
val schema = "<schema_name>"
val volume = "<volume_name>"
val downloadUrl = "https://health.data.ny.gov/api/views/jxy9-yhdk/rows.csv"
val fileName = "rows.csv"
val tableName = "<table_name>"
val pathVolume = s"/Volumes/$catalog/$schema/$volume"
val pathTable = s"$catalog.$schema"
print(pathVolume) // Show the complete path
print(pathTable) // Show the complete path
val catalog = "<catalog_name>"
val schema = "<schema_name>"
val volume = "<volume_name>"
val downloadUrl = "https://health.data.ny.gov/api/views/jxy9-yhdk/rows.csv"
val fileName = "rows.csv"
val tableName = "<table_name>"
val pathVolume = s"/Volumes/$catalog/$schema/$volume"
val pathTable = s"$catalog.$schema"
print(pathVolume) // Show the complete path
print(pathTable) // Show the complete path
R
catalog <- "<catalog_name>"
schema <- "<schema_name>"
volume <- "<volume_name>"
download_url <- "https://health.data.ny.gov/api/views/jxy9-yhdk/rows.csv"
file_name <- "rows.csv"
table_name <- "<table_name>"
path_volume <- paste("/Volumes/", catalog, "/", schema, "/", volume, sep = "")
path_table <- paste(catalog, ".", schema, sep = "")
print(path_volume) # Show the complete path
print(path_table) # Show the complete path
catalog <- "<catalog_name>"
schema <- "<schema_name>"
volume <- "<volume_name>"
download_url <- "https://health.data.ny.gov/api/views/jxy9-yhdk/rows.csv"
file_name <- "rows.csv"
table_name <- "<table_name>"
path_volume <- paste("/Volumes/", catalog, "/", schema, "/", volume, sep = "")
path_table <- paste(catalog, ".", schema, sep = "")
print(path_volume) # Show the complete path
print(path_table) # Show the complete path
PressShift+Enterto run the cell and create a new blank cell.
PressShift+Enterto run the cell and create a new blank cell.
Shift+Enter
Copy and paste the following code into the new empty notebook cell. This code copies therows.csvfile fromhealth.data.ny.govinto your Unity Catalog volume using theDatabricks dbutuilscommand.Pythondbutils.fs.cp(f"{download_url}", f"{path_volume}/{file_name}")Scaladbutils.fs.cp(downloadUrl, s"$pathVolume/$fileName")Rdbutils.fs.cp(download_url, paste(path_volume, "/", file_name, sep = ""))
Copy and paste the following code into the new empty notebook cell. This code copies therows.csvfile fromhealth.data.ny.govinto your Unity Catalog volume using theDatabricks dbutuilscommand.
rows.csv
Python
dbutils.fs.cp(f"{download_url}", f"{path_volume}/{file_name}")
dbutils.fs.cp(f"{download_url}", f"{path_volume}/{file_name}")
Scala
dbutils.fs.cp(downloadUrl, s"$pathVolume/$fileName")
dbutils.fs.cp(downloadUrl, s"$pathVolume/$fileName")
R
dbutils.fs.cp(download_url, paste(path_volume, "/", file_name, sep = ""))
dbutils.fs.cp(download_url, paste(path_volume, "/", file_name, sep = ""))
PressShift+Enterto run the cell and then move to the next cell.
PressShift+Enterto run the cell and then move to the next cell.
Shift+Enter
Step 2: Create a DataFrame
This step creates a DataFrame nameddf1with test data and then displays its contents.
df1
Copy and paste the following code into the new empty notebook cell. This code creates the DataFrame with test data, and then displays the contents and the schema of the DataFrame.Pythondata = [[2021, "test", "Albany", "M", 42]]
columns = ["Year", "First_Name", "County", "Sex", "Count"]

df1 = spark.createDataFrame(data, schema="Year int, First_Name STRING, County STRING, Sex STRING, Count int")
display(df1) # The display() method is specific to Databricks notebooks and provides a richer visualization.
# df1.show() The show() method is a part of the Apache Spark DataFrame API and provides basic visualization.Scalaval data = Seq((2021, "test", "Albany", "M", 42))
val columns = Seq("Year", "First_Name", "County", "Sex", "Count")

val df1 = data.toDF(columns: _*)
display(df1) // The display() method is specific to Databricks notebooks and provides a richer visualization.
// df1.show() The show() method is a part of the Apache Spark DataFrame API and provides basic visualization.R# Load the SparkR package that is already preinstalled on the cluster.
library(SparkR)

data <- data.frame(
  Year = as.integer(c(2021)),
  First_Name = c("test"),
  County = c("Albany"),
  Sex = c("M"),
  Count = as.integer(c(42))
)

df1 <- createDataFrame(data)
display(df1) # The display() method is specific to Databricks notebooks and provides a richer visualization.
# head(df1) The head() method is a part of the Apache SparkR DataFrame API and provides basic visualization.
Copy and paste the following code into the new empty notebook cell. This code creates the DataFrame with test data, and then displays the contents and the schema of the DataFrame.
Python
data = [[2021, "test", "Albany", "M", 42]]
columns = ["Year", "First_Name", "County", "Sex", "Count"]

df1 = spark.createDataFrame(data, schema="Year int, First_Name STRING, County STRING, Sex STRING, Count int")
display(df1) # The display() method is specific to Databricks notebooks and provides a richer visualization.
# df1.show() The show() method is a part of the Apache Spark DataFrame API and provides basic visualization.
data = [[2021, "test", "Albany", "M", 42]]
columns = ["Year", "First_Name", "County", "Sex", "Count"]

df1 = spark.createDataFrame(data, schema="Year int, First_Name STRING, County STRING, Sex STRING, Count int")
display(df1) # The display() method is specific to Databricks notebooks and provides a richer visualization.
# df1.show() The show() method is a part of the Apache Spark DataFrame API and provides basic visualization.
Scala
val data = Seq((2021, "test", "Albany", "M", 42))
val columns = Seq("Year", "First_Name", "County", "Sex", "Count")

val df1 = data.toDF(columns: _*)
display(df1) // The display() method is specific to Databricks notebooks and provides a richer visualization.
// df1.show() The show() method is a part of the Apache Spark DataFrame API and provides basic visualization.
val data = Seq((2021, "test", "Albany", "M", 42))
val columns = Seq("Year", "First_Name", "County", "Sex", "Count")

val df1 = data.toDF(columns: _*)
display(df1) // The display() method is specific to Databricks notebooks and provides a richer visualization.
// df1.show() The show() method is a part of the Apache Spark DataFrame API and provides basic visualization.
R
# Load the SparkR package that is already preinstalled on the cluster.
library(SparkR)

data <- data.frame(
  Year = as.integer(c(2021)),
  First_Name = c("test"),
  County = c("Albany"),
  Sex = c("M"),
  Count = as.integer(c(42))
)

df1 <- createDataFrame(data)
display(df1) # The display() method is specific to Databricks notebooks and provides a richer visualization.
# head(df1) The head() method is a part of the Apache SparkR DataFrame API and provides basic visualization.
# Load the SparkR package that is already preinstalled on the cluster.
library(SparkR)

data <- data.frame(
  Year = as.integer(c(2021)),
  First_Name = c("test"),
  County = c("Albany"),
  Sex = c("M"),
  Count = as.integer(c(42))
)

df1 <- createDataFrame(data)
display(df1) # The display() method is specific to Databricks notebooks and provides a richer visualization.
# head(df1) The head() method is a part of the Apache SparkR DataFrame API and provides basic visualization.
PressShift+Enterto run the cell and then move to the next cell.
PressShift+Enterto run the cell and then move to the next cell.
Shift+Enter
Step 3: Load data into a DataFrame from CSV file
This step creates a DataFrame nameddf_csvfrom the CSV file that you previously loaded into your Unity Catalog volume. Seespark.read.csv.
df_csv
Copy and paste the following code into the new empty notebook cell. This code loads baby name data into DataFramedf_csvfrom the CSV file and then displays the contents of the DataFrame.Pythondf_csv = spark.read.csv(f"{path_volume}/{file_name}",
    header=True,
    inferSchema=True,
    sep=",")
display(df_csv)Scalaval dfCsv = spark.read
    .option("header", "true")
    .option("inferSchema", "true")
    .option("delimiter", ",")
    .csv(s"$pathVolume/$fileName")

display(dfCsv)Rdf_csv <- read.df(paste(path_volume, "/", file_name, sep=""),
    source="csv",
    header = TRUE,
    inferSchema = TRUE,
    delimiter = ",")

display(df_csv)
Copy and paste the following code into the new empty notebook cell. This code loads baby name data into DataFramedf_csvfrom the CSV file and then displays the contents of the DataFrame.
df_csv
Python
df_csv = spark.read.csv(f"{path_volume}/{file_name}",
    header=True,
    inferSchema=True,
    sep=",")
display(df_csv)
df_csv = spark.read.csv(f"{path_volume}/{file_name}",
    header=True,
    inferSchema=True,
    sep=",")
display(df_csv)
Scala
val dfCsv = spark.read
    .option("header", "true")
    .option("inferSchema", "true")
    .option("delimiter", ",")
    .csv(s"$pathVolume/$fileName")

display(dfCsv)
val dfCsv = spark.read
    .option("header", "true")
    .option("inferSchema", "true")
    .option("delimiter", ",")
    .csv(s"$pathVolume/$fileName")

display(dfCsv)
R
df_csv <- read.df(paste(path_volume, "/", file_name, sep=""),
    source="csv",
    header = TRUE,
    inferSchema = TRUE,
    delimiter = ",")

display(df_csv)
df_csv <- read.df(paste(path_volume, "/", file_name, sep=""),
    source="csv",
    header = TRUE,
    inferSchema = TRUE,
    delimiter = ",")

display(df_csv)
PressShift+Enterto run the cell and then move to the next cell.
PressShift+Enterto run the cell and then move to the next cell.
Shift+Enter
You can load data from manysupported file formats.
Step 4: View and interact with your DataFrame
View and interact with your baby names DataFrames using the following methods.
Print the DataFrame schema
Learn how to display the schema of an Apache Spark DataFrame. Apache Spark uses the termschemato refer to the names and data types of the columns in the DataFrame.
Note
Azure Databricks also uses the term schema to describe a collection of tables registered to a catalog.
Copy and paste the following code into an empty notebook cell. This code shows the schema of your DataFrames with the.printSchema()method to view the schemas of the two DataFrames - to prepare to union the two DataFrames.Pythondf_csv.printSchema()
df1.printSchema()ScaladfCsv.printSchema()
df1.printSchema()RprintSchema(df_csv)
printSchema(df1)
Copy and paste the following code into an empty notebook cell. This code shows the schema of your DataFrames with the.printSchema()method to view the schemas of the two DataFrames - to prepare to union the two DataFrames.
.printSchema()
df_csv.printSchema()
df1.printSchema()
df_csv.printSchema()
df1.printSchema()
dfCsv.printSchema()
df1.printSchema()
dfCsv.printSchema()
df1.printSchema()
printSchema(df_csv)
printSchema(df1)
printSchema(df_csv)
printSchema(df1)
PressShift+Enterto run the cell and then move to the next cell.
PressShift+Enterto run the cell and then move to the next cell.
Shift+Enter
Rename column in the DataFrame
Learn how to rename a column in a DataFrame.
Copy and paste the following code into an empty notebook cell. This code renames a column in thedf1_csvDataFrame to match the respective column in thedf1DataFrame. This code uses the Apache SparkwithColumnRenamed()method.Pythondf_csv = df_csv.withColumnRenamed("First Name", "First_Name")
df_csv.printSchemaScalaval dfCsvRenamed = dfCsv.withColumnRenamed("First Name", "First_Name")
// when modifying a DataFrame in Scala, you must assign it to a new variable
dfCsvRenamed.printSchema()Rdf_csv <- withColumnRenamed(df_csv, "First Name", "First_Name")
printSchema(df_csv)
Copy and paste the following code into an empty notebook cell. This code renames a column in thedf1_csvDataFrame to match the respective column in thedf1DataFrame. This code uses the Apache SparkwithColumnRenamed()method.
df1_csv
df1
withColumnRenamed()
df_csv = df_csv.withColumnRenamed("First Name", "First_Name")
df_csv.printSchema
df_csv = df_csv.withColumnRenamed("First Name", "First_Name")
df_csv.printSchema
val dfCsvRenamed = dfCsv.withColumnRenamed("First Name", "First_Name")
// when modifying a DataFrame in Scala, you must assign it to a new variable
dfCsvRenamed.printSchema()
val dfCsvRenamed = dfCsv.withColumnRenamed("First Name", "First_Name")
// when modifying a DataFrame in Scala, you must assign it to a new variable
dfCsvRenamed.printSchema()
df_csv <- withColumnRenamed(df_csv, "First Name", "First_Name")
printSchema(df_csv)
df_csv <- withColumnRenamed(df_csv, "First Name", "First_Name")
printSchema(df_csv)
PressShift+Enterto run the cell and then move to the next cell.
PressShift+Enterto run the cell and then move to the next cell.
Shift+Enter
Combine DataFrames
Learn how to create a new DataFrame that adds the rows of one DataFrame to another.
Copy and paste the following code into an empty notebook cell. This code uses the Apache Sparkunion()method to combine the contents of your first DataFramedfwith DataFramedf_csvcontaining the baby names data loaded from the CSV file.Pythondf = df1.union(df_csv)
display(df)Scalaval df = df1.union(dfCsvRenamed)
display(df)Rdisplay(df <- union(df1, df_csv))
Copy and paste the following code into an empty notebook cell. This code uses the Apache Sparkunion()method to combine the contents of your first DataFramedfwith DataFramedf_csvcontaining the baby names data loaded from the CSV file.
union()
df
df_csv
df = df1.union(df_csv)
display(df)
df = df1.union(df_csv)
display(df)
val df = df1.union(dfCsvRenamed)
display(df)
val df = df1.union(dfCsvRenamed)
display(df)
display(df <- union(df1, df_csv))
display(df <- union(df1, df_csv))
PressShift+Enterto run the cell and then move to the next cell.
PressShift+Enterto run the cell and then move to the next cell.
Shift+Enter
Filter rows in a DataFrame
Discover the most popular baby names in your data set by filtering rows, using the Apache Spark.filter()or.where()methods. Use filtering to select a subset of rows to return or modify in a DataFrame. There is no difference in performance or syntax, as seen in the following examples.
.filter()
.where()
Copy and paste the following code into an empty notebook cell. This code uses the the Apache Spark.filter()method to display those rows in the DataFrame with a count of more than 50.Pythondisplay(df.filter(df["Count"] > 50))Scaladisplay(df.filter(df("Count") > 50))Rdisplay(filteredDF <- filter(df, df$Count > 50))
Copy and paste the following code into an empty notebook cell. This code uses the the Apache Spark.filter()method to display those rows in the DataFrame with a count of more than 50.
.filter()
display(df.filter(df["Count"] > 50))
display(df.filter(df["Count"] > 50))
display(df.filter(df("Count") > 50))
display(df.filter(df("Count") > 50))
display(filteredDF <- filter(df, df$Count > 50))
display(filteredDF <- filter(df, df$Count > 50))
PressShift+Enterto run the cell and then move to the next cell.
PressShift+Enterto run the cell and then move to the next cell.
Shift+Enter
Copy and paste the following code into an empty notebook cell. This code uses the the Apache Spark.where()method to display those rows in the DataFrame with a count of more than 50.Pythondisplay(df.where(df["Count"] > 50))Scaladisplay(df.where(df("Count") > 50))Rdisplay(filtered_df <- where(df, df$Count > 50))
Copy and paste the following code into an empty notebook cell. This code uses the the Apache Spark.where()method to display those rows in the DataFrame with a count of more than 50.
.where()
display(df.where(df["Count"] > 50))
display(df.where(df["Count"] > 50))
display(df.where(df("Count") > 50))
display(df.where(df("Count") > 50))
display(filtered_df <- where(df, df$Count > 50))
display(filtered_df <- where(df, df$Count > 50))
PressShift+Enterto run the cell and then move to the next cell.
PressShift+Enterto run the cell and then move to the next cell.
Shift+Enter
Select columns from a DataFrame and order by frequency
Learn about which baby name frequency with theselect()method to specify the columns from the DataFrame to return. Use the Apache Sparkorderbyanddescfunctions to order the results.
select()
orderby
desc
The pyspark.sql module for Apache Spark provides support for SQL functions. Among these functions that we use in this tutorial are the the Apache SparkorderBy(),desc(), andexpr()functions. You enable the use of these functions by importing them into your session as needed.
orderBy()
desc()
expr()
Copy and paste the following code into an empty notebook cell. This code imports thedesc()function and then uses the Apache Sparkselect()method and Apache SparkorderBy()anddesc()functions to display the most common names and their counts in descending order.Pythonfrom pyspark.sql.functions import desc
display(df.select("First_Name", "Count").orderBy(desc("Count")))Scalaimport org.apache.spark.sql.functions.desc
display(df.select("First_Name", "Count").orderBy(desc("Count")))Rdisplay(arrange(select(df, df$First_Name, df$Count), desc(df$Count)))
Copy and paste the following code into an empty notebook cell. This code imports thedesc()function and then uses the Apache Sparkselect()method and Apache SparkorderBy()anddesc()functions to display the most common names and their counts in descending order.
desc()
select()
orderBy()
desc()
from pyspark.sql.functions import desc
display(df.select("First_Name", "Count").orderBy(desc("Count")))
from pyspark.sql.functions import desc
display(df.select("First_Name", "Count").orderBy(desc("Count")))
import org.apache.spark.sql.functions.desc
display(df.select("First_Name", "Count").orderBy(desc("Count")))
import org.apache.spark.sql.functions.desc
display(df.select("First_Name", "Count").orderBy(desc("Count")))
display(arrange(select(df, df$First_Name, df$Count), desc(df$Count)))
display(arrange(select(df, df$First_Name, df$Count), desc(df$Count)))
PressShift+Enterto run the cell and then move to the next cell.
PressShift+Enterto run the cell and then move to the next cell.
Shift+Enter
Create a subset DataFrame
Learn how to create a subset DataFrame from an existing DataFrame.
Copy and paste the following code into an empty notebook cell. This code uses the Apache Sparkfiltermethod to create a new DataFrame restricting the data by year, count, and sex. It uses the Apache Sparkselect()method to limit the columns. It also uses the Apache SparkorderBy()anddesc()functions to sort the new DataFrame by count.PythonsubsetDF = df.filter((df["Year"] == 2009) & (df["Count"] > 100) & (df["Sex"] == "F")).select("First_Name", "County", "Count").orderBy(desc("Count"))
display(subsetDF)Scalaval subsetDF = df.filter((df("Year") === 2009) && (df("Count") > 100) && (df("Sex") === "F")).select("First_Name", "County", "Count").orderBy(desc("Count"))

display(subsetDF)RsubsetDF <- select(filter(df, (df$Count > 100) & (df$year == 2009) & df["Sex"] == "F")), "First_Name", "County", "Count")
display(subsetDF)
Copy and paste the following code into an empty notebook cell. This code uses the Apache Sparkfiltermethod to create a new DataFrame restricting the data by year, count, and sex. It uses the Apache Sparkselect()method to limit the columns. It also uses the Apache SparkorderBy()anddesc()functions to sort the new DataFrame by count.
filter
select()
orderBy()
desc()
subsetDF = df.filter((df["Year"] == 2009) & (df["Count"] > 100) & (df["Sex"] == "F")).select("First_Name", "County", "Count").orderBy(desc("Count"))
display(subsetDF)
subsetDF = df.filter((df["Year"] == 2009) & (df["Count"] > 100) & (df["Sex"] == "F")).select("First_Name", "County", "Count").orderBy(desc("Count"))
display(subsetDF)
val subsetDF = df.filter((df("Year") === 2009) && (df("Count") > 100) && (df("Sex") === "F")).select("First_Name", "County", "Count").orderBy(desc("Count"))

display(subsetDF)
val subsetDF = df.filter((df("Year") === 2009) && (df("Count") > 100) && (df("Sex") === "F")).select("First_Name", "County", "Count").orderBy(desc("Count"))

display(subsetDF)
subsetDF <- select(filter(df, (df$Count > 100) & (df$year == 2009) & df["Sex"] == "F")), "First_Name", "County", "Count")
display(subsetDF)
subsetDF <- select(filter(df, (df$Count > 100) & (df$year == 2009) & df["Sex"] == "F")), "First_Name", "County", "Count")
display(subsetDF)
PressShift+Enterto run the cell and then move to the next cell.
PressShift+Enterto run the cell and then move to the next cell.
Shift+Enter
Step 5: Save the DataFrame
Learn how to save a DataFrame,. You can either save your DataFrame to a table or write the DataFrame to a file or multiple files.
Save the DataFrame to a table
Azure Databricks uses the Delta Lake format for all tables by default. To save your DataFrame, you must haveCREATEtable privileges on the catalog and schema.
CREATE
Copy and paste the following code into an empty notebook cell. This code saves the contents of the DataFrame to a table using the variable you defined at the start of this tutorial.Pythondf.write.mode("overwrite").saveAsTable(f"{path_table}.{table_name}")Scaladf.write.mode("overwrite").saveAsTable(s"$pathTable" + "." + s"$tableName")RsaveAsTable(df, paste(path_table, ".", table_name), mode = "overwrite")
Copy and paste the following code into an empty notebook cell. This code saves the contents of the DataFrame to a table using the variable you defined at the start of this tutorial.
df.write.mode("overwrite").saveAsTable(f"{path_table}.{table_name}")
df.write.mode("overwrite").saveAsTable(f"{path_table}.{table_name}")
df.write.mode("overwrite").saveAsTable(s"$pathTable" + "." + s"$tableName")
df.write.mode("overwrite").saveAsTable(s"$pathTable" + "." + s"$tableName")
saveAsTable(df, paste(path_table, ".", table_name), mode = "overwrite")
saveAsTable(df, paste(path_table, ".", table_name), mode = "overwrite")
PressShift+Enterto run the cell and then move to the next cell.
PressShift+Enterto run the cell and then move to the next cell.
Shift+Enter
Most Apache Spark applications work on large data sets and in a distributed fashion. Apache Spark writes out a directory of files rather than a single file. Delta Lake splits the Parquet folders and files. Many data systems can read these directories of files. Azure Databricks recommends using tables over file paths for most applications.
Save the DataFrame to JSON files
Copy and paste the following code into an empty notebook cell. This code saves the DataFrame to a directory of JSON files.Pythondf.write.format("json").mode("overwrite").save("/tmp/json_data")Scaladf.write.format("json").mode("overwrite").save("/tmp/json_data")Rwrite.df(df, path = "/tmp/json_data", source = "json", mode = "overwrite")
Copy and paste the following code into an empty notebook cell. This code saves the DataFrame to a directory of JSON files.
df.write.format("json").mode("overwrite").save("/tmp/json_data")
df.write.format("json").mode("overwrite").save("/tmp/json_data")
df.write.format("json").mode("overwrite").save("/tmp/json_data")
df.write.format("json").mode("overwrite").save("/tmp/json_data")
write.df(df, path = "/tmp/json_data", source = "json", mode = "overwrite")
write.df(df, path = "/tmp/json_data", source = "json", mode = "overwrite")
PressShift+Enterto run the cell and then move to the next cell.
PressShift+Enterto run the cell and then move to the next cell.
Shift+Enter
Read the DataFrame from a JSON file
Learn how to use the Apache Sparkspark.read.format()method to read JSON data from a directory into a DataFrame.
spark.read.format()
Copy and paste the following code into an empty notebook cell. This code displays the JSON files you saved in the previous example.Pythondisplay(spark.read.format("json").json("/tmp/json_data"))Scaladisplay(spark.read.format("json").json("/tmp/json_data"))Rdisplay(read.json("/tmp/json_data"))
Copy and paste the following code into an empty notebook cell. This code displays the JSON files you saved in the previous example.
display(spark.read.format("json").json("/tmp/json_data"))
display(spark.read.format("json").json("/tmp/json_data"))
display(spark.read.format("json").json("/tmp/json_data"))
display(spark.read.format("json").json("/tmp/json_data"))
display(read.json("/tmp/json_data"))
display(read.json("/tmp/json_data"))
PressShift+Enterto run the cell and then move to the next cell.
PressShift+Enterto run the cell and then move to the next cell.
Shift+Enter
Additional tasks: Run SQL queries in PySpark, Scala, and R
Apache Spark DataFrames provide the following options to combine SQL with PySpark, Scala, and R. You can run the following code in the same notebook that you created for this tutorial.
Specify a column as a SQL query
Learn how to use the Apache SparkselectExpr()method. This is a variant of theselect()method that accepts SQL expressions and return an updated DataFrame. This method allows you to use a SQL expression, such asupper.
selectExpr()
select()
upper
Copy and paste the following code into an empty notebook cell. This code uses the Apache SparkselectExpr()method and the SQLupperexpression to convert a string column to upper case (and rename the column).Pythondisplay(df.selectExpr("Count", "upper(County) as big_name"))Scaladisplay(df.selectExpr("Count", "upper(County) as big_name"))Rdisplay(df_selected <- selectExpr(df, "Count", "upper(County) as big_name"))
Copy and paste the following code into an empty notebook cell. This code uses the Apache SparkselectExpr()method and the SQLupperexpression to convert a string column to upper case (and rename the column).
selectExpr()
upper
display(df.selectExpr("Count", "upper(County) as big_name"))
display(df.selectExpr("Count", "upper(County) as big_name"))
display(df.selectExpr("Count", "upper(County) as big_name"))
display(df.selectExpr("Count", "upper(County) as big_name"))
display(df_selected <- selectExpr(df, "Count", "upper(County) as big_name"))
display(df_selected <- selectExpr(df, "Count", "upper(County) as big_name"))
PressShift+Enterto run the cell and then move to the next cell.
PressShift+Enterto run the cell and then move to the next cell.
Shift+Enter
Useexpr()to use SQL syntax for a column
expr()
Learn how to import and use the Apache Sparkexpr()function to use SQL syntax anywhere a column would be specified.
expr()
Copy and paste the following code into an empty notebook cell. This code imports theexpr()function and then uses the Apache Sparkexpr()function and the SQLlowerexpression to convert a string column to lower case (and rename the column).Pythonfrom pyspark.sql.functions import expr
display(df.select("Count", expr("lower(County) as little_name")))Scalaimport org.apache.spark.sql.functions.{col, expr}
// Scala requires us to import the col() function as well as the expr() function

display(df.select(col("Count"), expr("lower(County) as little_name")))Rdisplay(df_selected <- selectExpr(df, "Count", "lower(County) as little_name"))
# expr() function is not supported in R, selectExpr in SparkR replicates this functionality
Copy and paste the following code into an empty notebook cell. This code imports theexpr()function and then uses the Apache Sparkexpr()function and the SQLlowerexpression to convert a string column to lower case (and rename the column).
expr()
expr()
lower
from pyspark.sql.functions import expr
display(df.select("Count", expr("lower(County) as little_name")))
from pyspark.sql.functions import expr
display(df.select("Count", expr("lower(County) as little_name")))
import org.apache.spark.sql.functions.{col, expr}
// Scala requires us to import the col() function as well as the expr() function

display(df.select(col("Count"), expr("lower(County) as little_name")))
import org.apache.spark.sql.functions.{col, expr}
// Scala requires us to import the col() function as well as the expr() function

display(df.select(col("Count"), expr("lower(County) as little_name")))
display(df_selected <- selectExpr(df, "Count", "lower(County) as little_name"))
# expr() function is not supported in R, selectExpr in SparkR replicates this functionality
display(df_selected <- selectExpr(df, "Count", "lower(County) as little_name"))
# expr() function is not supported in R, selectExpr in SparkR replicates this functionality
PressShift+Enterto run the cell and then move to the next cell.
PressShift+Enterto run the cell and then move to the next cell.
Shift+Enter
Run an arbitrary SQL query using spark.sql() function
Learn how to use the Apache Sparkspark.sql()function to run arbitrary SQL queries.
spark.sql()
Copy and paste the following code into an empty notebook cell. This code uses the Apache Sparkspark.sql()function to query a SQL table using SQL syntax.Pythondisplay(spark.sql(f"SELECT * FROM {path_table}.{table_name}"))Scaladisplay(spark.sql(s"SELECT * FROM $pathTable.$tableName"))Rdisplay(sql(paste("SELECT * FROM", path_table, ".", table_name)))
Copy and paste the following code into an empty notebook cell. This code uses the Apache Sparkspark.sql()function to query a SQL table using SQL syntax.
spark.sql()
display(spark.sql(f"SELECT * FROM {path_table}.{table_name}"))
display(spark.sql(f"SELECT * FROM {path_table}.{table_name}"))
display(spark.sql(s"SELECT * FROM $pathTable.$tableName"))
display(spark.sql(s"SELECT * FROM $pathTable.$tableName"))
display(sql(paste("SELECT * FROM", path_table, ".", table_name)))
display(sql(paste("SELECT * FROM", path_table, ".", table_name)))
PressShift+Enterto run the cell and then move to the next cell.
PressShift+Enterto run the cell and then move to the next cell.
Shift+Enter
DataFrame tutorial notebooks
The following notebooks include the examples queries from this tutorial.
Python
Get notebook
Scala
Get notebook
R
Get notebook
Additional resources
PySpark on Azure Databricks
Reference for Apache Spark APIs
Convert between PySpark and pandas DataFrames
Pandas API on Spark
Feedback
Was this page helpful?
Additional resources