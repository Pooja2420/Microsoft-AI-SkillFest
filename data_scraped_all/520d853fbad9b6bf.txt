Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Linux NFS mount options best practices for Azure NetApp Files
Article
2024-03-07
6 contributors
In this article
This article helps you understand mount options and the best practices for using them with Azure NetApp Files.
Nconnect
Nconnect
Using thenconnectmount option allows you to specify the number of connections (network flows) that should be established between the NFS client and NFS endpoint up to a limit of 16. Traditionally, an NFS client uses a single connection between itself and the endpoint. Increasing the number of network flows increases the upper limits of I/O and throughput significantly. Testing has foundnconnect=8to be the most performant.
nconnect
nconnect=8
When preparing a multi-node SAS GRID environment for production, you might notice a repeatable 30% reduction in run time going from 8 hours to 5.5 hours:
nconnect
nconnect=8
Both sets of tests used the same E32-8_v4 virtual machine and RHEL8.3, with read-ahead set to 15 MiB.
When you usenconnect, keep the following rules in mind:
nconnect
nconnectis supported by Azure NetApp Files on all major Linux distributions but only on newer releases:Linux releaseNFSv3 (minimum release)NFSv4.1 (minimum release)Red Hat Enterprise LinuxRHEL8.3RHEL8.3SUSESLES12SP4 or SLES15SP1SLES15SP2UbuntuUbuntu18.04NoteSLES15SP2 is the minimum SUSE release in whichnconnectis supported by Azure NetApp Files for NFSv4.1. All other releases as specified are the first releases that introduced thenconnectfeature.
nconnectis supported by Azure NetApp Files on all major Linux distributions but only on newer releases:
nconnect
Note
SLES15SP2 is the minimum SUSE release in whichnconnectis supported by Azure NetApp Files for NFSv4.1. All other releases as specified are the first releases that introduced thenconnectfeature.
nconnect
nconnect
All mounts from a single endpoint inherit thenconnectsetting of the first export mounted, as shown in the following scenarios:Scenario 1:nconnectis used by the first mount. Therefore, all mounts against the same endpoint usenconnect=8.mount 10.10.10.10:/volume1 /mnt/volume1 -o nconnect=8mount 10.10.10.10:/volume2 /mnt/volume2mount 10.10.10.10:/volume3 /mnt/volume3Scenario 2:nconnectisn't used by the first mount. Therefore, no mounts against the same endpoint usenconnecteven thoughnconnectmay be specified thereon.mount 10.10.10.10:/volume1 /mnt/volume1mount 10.10.10.10:/volume2 /mnt/volume2 -o nconnect=8mount 10.10.10.10:/volume3 /mnt/volume3 -o nconnect=8Scenario 3:nconnectsettings aren't propagated across separate storage endpoints.nconnectis used by the mount coming from10.10.10.10but not by the mount coming from10.12.12.12.mount 10.10.10.10:/volume1 /mnt/volume1 -o nconnect=8mount 10.12.12.12:/volume2 /mnt/volume2
All mounts from a single endpoint inherit thenconnectsetting of the first export mounted, as shown in the following scenarios:
nconnect
Scenario 1:nconnectis used by the first mount. Therefore, all mounts against the same endpoint usenconnect=8.
nconnect
nconnect=8
mount 10.10.10.10:/volume1 /mnt/volume1 -o nconnect=8
mount 10.10.10.10:/volume1 /mnt/volume1 -o nconnect=8
mount 10.10.10.10:/volume2 /mnt/volume2
mount 10.10.10.10:/volume2 /mnt/volume2
mount 10.10.10.10:/volume3 /mnt/volume3
mount 10.10.10.10:/volume3 /mnt/volume3
Scenario 2:nconnectisn't used by the first mount. Therefore, no mounts against the same endpoint usenconnecteven thoughnconnectmay be specified thereon.
nconnect
nconnect
nconnect
mount 10.10.10.10:/volume1 /mnt/volume1
mount 10.10.10.10:/volume1 /mnt/volume1
mount 10.10.10.10:/volume2 /mnt/volume2 -o nconnect=8
mount 10.10.10.10:/volume2 /mnt/volume2 -o nconnect=8
mount 10.10.10.10:/volume3 /mnt/volume3 -o nconnect=8
mount 10.10.10.10:/volume3 /mnt/volume3 -o nconnect=8
Scenario 3:nconnectsettings aren't propagated across separate storage endpoints.nconnectis used by the mount coming from10.10.10.10but not by the mount coming from10.12.12.12.
nconnect
nconnect
10.10.10.10
10.12.12.12
mount 10.10.10.10:/volume1 /mnt/volume1 -o nconnect=8
mount 10.10.10.10:/volume1 /mnt/volume1 -o nconnect=8
mount 10.12.12.12:/volume2 /mnt/volume2
mount 10.12.12.12:/volume2 /mnt/volume2
nconnectmay be used to increase storage concurrency from any given client.
nconnectmay be used to increase storage concurrency from any given client.
nconnect
For details, seeLinux concurrency best practices for Azure NetApp Files.
Nconnectconsiderations
Nconnect
It's not recommended to usenconnectandsec=krb5*mount options together. Using these options together can cause performance degradation.
nconnect
sec=krb5*
The Generic Security Standard Application Programming Interface (GSS-API) provides a way for applications to protect data sent to peer applications. This data might be sent from a client on one machine to a server on another machine.â¯
Whennconnectis used in Linux, the GSS security context is shared between all thenconnectconnections to a particular server. TCP is a reliable transport that supports out-of-order packet delivery to deal with out-of-order packets in a GSS stream, using a sliding window of sequence numbers.â¯When packets not in the sequence window are received, the security context is discarded, andâ¯a new security context is negotiated. All messages sent with in the now-discarded context are no longer valid, thus requiring the messages to be sent again. Larger number of packets in annconnectsetup cause frequent out-of-window packets, triggering the described behavior. No specific degradation percentages can be stated with this behavior.
nconnect
nconnect
nconnect
RsizeandWsize
Rsize
Wsize
Examples in this section provide information about how to approach performance tuning. You might need to make adjustments to suit your specific application needs.
Thersizeandwsizeflags set the maximum transfer size of an NFS operation. Ifrsizeorwsizearen't specified on mount, the client and server negotiate the largest size supported by the two. Currently, both Azure NetApp Files and modern Linux distributions support read and write sizes as large as 1,048,576 Bytes (1 MiB). However, for best overall throughput and latency, Azure NetApp Files recommends setting bothrsizeandwsizeno larger than 262,144 Bytes (256 K). You might observe that both increased latency and decreased throughput when usingrsizeandwsizelarger than 256 KiB.
rsize
wsize
rsize
wsize
rsize
wsize
rsize
wsize
For example,Deploy a SAP HANA scale-out system with standby node on Azure VMs by using Azure NetApp Files on SUSE Linux Enterprise Servershows the 256-KiBrsizeandwsizemaximum as follows:
rsize
wsize
sudo vi /etc/fstab
# Add the following entries
10.23.1.5:/HN1-data-mnt00001 /hana/data/HN1/mnt00001  nfs rw,vers=4,minorversion=1,hard,timeo=600,rsize=262144,wsize=262144,noatime,_netdev,sec=sys  0  0
10.23.1.6:/HN1-data-mnt00002 /hana/data/HN1/mnt00002  nfs   rw,vers=4,minorversion=1,hard,timeo=600,rsize=262144,wsize=262144,noatime,_netdev,sec=sys  0  0
10.23.1.4:/HN1-log-mnt00001 /hana/log/HN1/mnt00001  nfs   rw,vers=4,minorversion=1,hard,timeo=600,rsize=262144,wsize=262144,noatime,_netdev,sec=sys  0  0
10.23.1.6:/HN1-log-mnt00002 /hana/log/HN1/mnt00002  nfs   rw,vers=4,minorversion=1,hard,timeo=600,rsize=262144,wsize=262144,noatime,_netdev,sec=sys  0  0
10.23.1.4:/HN1-shared/shared /hana/shared  nfs   rw,vers=4,minorversion=1,hard,timeo=600,rsize=262144,wsize=262144,noatime,_netdev,sec=sys  0  0
sudo vi /etc/fstab
# Add the following entries
10.23.1.5:/HN1-data-mnt00001 /hana/data/HN1/mnt00001  nfs rw,vers=4,minorversion=1,hard,timeo=600,rsize=262144,wsize=262144,noatime,_netdev,sec=sys  0  0
10.23.1.6:/HN1-data-mnt00002 /hana/data/HN1/mnt00002  nfs   rw,vers=4,minorversion=1,hard,timeo=600,rsize=262144,wsize=262144,noatime,_netdev,sec=sys  0  0
10.23.1.4:/HN1-log-mnt00001 /hana/log/HN1/mnt00001  nfs   rw,vers=4,minorversion=1,hard,timeo=600,rsize=262144,wsize=262144,noatime,_netdev,sec=sys  0  0
10.23.1.6:/HN1-log-mnt00002 /hana/log/HN1/mnt00002  nfs   rw,vers=4,minorversion=1,hard,timeo=600,rsize=262144,wsize=262144,noatime,_netdev,sec=sys  0  0
10.23.1.4:/HN1-shared/shared /hana/shared  nfs   rw,vers=4,minorversion=1,hard,timeo=600,rsize=262144,wsize=262144,noatime,_netdev,sec=sys  0  0
For example, SAS Viya recommends a 256-KiB read and write sizes, andSAS GRIDlimits ther/wsizeto 64 KiB while augmenting read performance with increased read-ahead for the NFS mounts. SeeNFS read-ahead best practices for Azure NetApp Filesfor details.
r/wsize
The following considerations apply to the use ofrsizeandwsize:
rsize
wsize
Random I/O operation sizes are often smaller than thersizeandwsizemount options. As such, they aren't constraints.
rsize
wsize
When using the filesystem cache, sequential I/O will occur at the size predicated by thersizeandwsizemount options, unless the file size is smaller thanrsizeandwsize.
rsize
wsize
rsize
wsize
Operations bypassing the filesystem cache, although still constrained by thersizeandwsizemount options, aren't as large as the maximum specified byrsizeorwsize. This consideration is important when you use workload generators that have thedirectiooption.
rsize
wsize
rsize
wsize
directio
As a best practice with Azure NetApp Files, for best overall throughput and latency, setrsizeandwsizeno larger than 262,144 Bytes.
rsize
wsize
Close-to-open consistency and cache attribute timers
NFS uses a loose consistency model. The consistency is loose because the application doesn't have to go to shared storage and fetch data every time to use it, a scenario that would have a tremendous impact to application performance. There are two mechanisms that manage this process: cache attribute timers and close-to-open consistency.
If the client has complete ownership of data, that is, it's not shared between multiple nodes or systems, there is guaranteed consistency.In that case, you can reduce thegetattraccess operations to storage and speed up the application by turning off close-to-open (cto) consistency (noctoas a mount option) and by turning up the timeouts for the attribute cache management (actimeo=600as a mount option changes the timer to 10m versus the defaultsacregmin=3,acregmax=30,acdirmin=30,acdirmax=60). In some testing,noctoreduces approximately 65-70% of thegetattraccess calls, and adjustingactimeoreduces these calls another 20-25%.
getattr
cto
nocto
actimeo=600
acregmin=3,acregmax=30,acdirmin=30,acdirmax=60
nocto
getattr
actimeo
How attribute cache timers work
The attributesacregmin,acregmax,acdirmin, andacdirmaxcontrol the coherency of the cache. The former two attributes control how long the attributes of files are trusted. The latter two attributes control how long the attributes of the directory file itself are trusted (directory size, directory ownership, directory permissions). Theminandmaxattributes define minimum and maximum duration over which attributes of a directory, attributes of a file, and cache content of a file are deemed trustworthy, respectively. Betweenminandmax, an algorithm is used to define the amount of time over which a cached entry is trusted.
acregmin
acregmax
acdirmin
acdirmax
min
max
min
max
For example, consider the defaultacregminandacregmaxvalues, 3 and 30 seconds, respectively. For instance, the attributes are repeatedly evaluated for the files in a directory. After 3 seconds, the NFS service is queried for freshness. If the attributes are deemed valid, the client doubles the trusted time to 6 seconds, 12 seconds, 24 seconds, then as the maximum is set to 30, 30 seconds. From that point on, until the cached attributes are deemed out of date (at which point the cycle starts over), trustworthiness is defined as 30 seconds being the value specified byacregmax.
acregmin
acregmax
acregmax
There are other cases that can benefit from a similar set of mount options, even when there's no complete ownership by the clients, for example, if the clients use the data as read only and data update is managed through another path. For applications that use grids of clients like EDA, web hosting and movie rendering and have relatively static data sets (EDA tools or libraries, web content, texture data), the typical behavior is that the data set is largely cached on the clients. There are few reads and no writes. There are manygetattr/access calls coming back to storage. These data sets are typically updated through another client mounting the file systems and periodically pushing content updates.
getattr
In these cases, there's a known lag in picking up new content and the application still works with potentially out-of-date data. In these cases,noctoandactimeocan be used to control the period where out-of-data date can be managed. For example, in EDA tools and libraries,actimeo=600works well because this data is typically updated infrequently. For small web hosting where clients need to see their data updates timely as they're editing their sites,actimeo=10might be acceptable. For large-scale web sites where there's content pushed to multiple file systems,actimeo=60might be acceptable.
nocto
actimeo
actimeo=600
actimeo=10
actimeo=60
Using these mount options significantly reduces the workload to storage in these cases. (For example, a recent EDA experience reduced IOPs to the tool volume from >150 K to ~6 K.) Applications can run significantly faster because they can trust the data in memory. (Memory access time is nanoseconds vs. hundreds of microseconds forgetattr/access on a fast network.)
getattr
Close-to-open consistency
Close-to-open consistency (thectomount option) ensures that no matter the state of the cache, on open the most recent data for a file is always presented to the application.
cto
When a directory is crawled (ls,ls -lfor example) a certain set of RPCs (remote procedure calls) are issued.
The NFS server shares its view of the filesystem. As long asctois used by all NFS clients accessing a given NFS export, all clients see the same list of files and directories therein. The freshness of the attributes of the files in the directory is controlled by theattribute cache timers. In other words, as long asctois used, files appear to remote clients as soon as the file is created and the file lands on the storage.
ls
ls -l
cto
cto
When a file is opened, the content of the file is guaranteed fresh from the perspective of the NFS server.
If there's a race condition where the content hasn't finished flushing from Machine 1 when a file is opened on Machine 2, Machine 2 only receives the data present on the server at the time of the open. In this case, Machine 2 doesn't retrieve more data from the file until theacregtimer is reached, and Machine 2 checks its cache coherency from the server again. This scenario can be observed using a tail-ffrom Machine 2 when the file is still being written to from Machine 1.
acreg
-f
No close-to-open consistency
When no close-to-open consistency (nocto) is used, the client trusts the freshness of its current view of the file and directory until the cache attribute timers have been breached.
nocto
When a directory is crawled (ls,ls -lfor example) a certain set of RPCs (remote procedure calls) are issued.
The client only issues a call to the server for a current listing of files when theacdircache timer value has been breached. In this case, recently created files and directories don't appear. Recently removed files and directories do appear.
When a directory is crawled (ls,ls -lfor example) a certain set of RPCs (remote procedure calls) are issued.
The client only issues a call to the server for a current listing of files when theacdircache timer value has been breached. In this case, recently created files and directories don't appear. Recently removed files and directories do appear.
ls
ls -l
acdir
When a file is opened, as long as the file is still in the cache, its cached content (if any) is returned without validating consistency with the NFS server.
When a file is opened, as long as the file is still in the cache, its cached content (if any) is returned without validating consistency with the NFS server.
Next steps
Linux direct I/O best practices for Azure NetApp Files
Linux filesystem cache best practices for Azure NetApp Files
Linux concurrency best practices for Azure NetApp Files
Linux NFS read-ahead best practices
Azure virtual machine SKUs best practices
Performance benchmarks for Linux
Feedback
Was this page helpful?
Additional resources