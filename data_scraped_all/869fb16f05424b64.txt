Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Load data for machine learning and deep learning
Article
2024-12-17
4 contributors
In this article
This section covers information about loading data specifically for ML and DL applications. For general information about loading data, seeIngest data into an Azure Databricks lakehouse.
Store files for data loading and model checkpointing
Machine learning applications may need to use shared storage for data loading and model checkpointing. This is particularly important for distributed deep learning.
Azure Databricks providesUnity Catalog, a unified governance solution for data and AI assets. You can use Unity Catalog for accessing data on a cluster using both Spark and local file APIs.
Load tabular data
You can load tabular machine learning data fromtablesor files (for example, seeRead CSV files). You can convert Apache Spark DataFrames into pandas DataFrames using thePySpark methodtoPandas(), and then optionally convert to NumPy format using thePySpark methodto_numpy().
toPandas()
to_numpy()
Prepare data to fine tune large language models
You can prepare your data for fine-tuning open source large language models withHugging Face TransformersandHugging Face Datasets.
Prepare data for fine tuning Hugging Face models
Prepare data for distributed deep learning training
This section covers preparing data fordistributed deep learning trainingusing Mosaic Streaming and TFRecords.
Feedback
Was this page helpful?
Additional resources