Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Develop code in Databricks notebooks
Article
2025-04-18
4 contributors
In this article
This page describes how to develop code in Databricks notebooks, including autocomplete, automatic formatting for Python and SQL, combining Python and SQL in a notebook, and tracking the notebook version history.
For more details about advanced functionality available with the editor, such as autocomplete, variable selection, multi-cursor support, and side-by-side diffs, seeNavigate the Databricks notebook and file editor.
When you use the notebook or the file editor, Databricks Assistant is available to help you generate, explain, and debug code. SeeUse Databricks Assistantfor more information.
Databricks notebooks also include a built-in interactive debugger for Python notebooks. SeeDebug notebooks.
Modularize your code
With Databricks Runtime 11.3 LTS and above, you can create and manage source code files in the Azure Databricks workspace, and then import these files into your notebooks as needed.
For more information on working with source code files, seeShare code between Databricks notebooksandWork with Python and R modules.

Format code cells
Azure Databricks provides tools that allow you to format Python and SQL code in notebook cells quickly and easily. These tools reduce the effort to keep your code formatted and help to enforce the same coding standards across your notebooks.
Python black formatter library
Important
This feature is inPublic Preview.
Azure Databricks supports Python code formatting usingblackwithin the notebook. The notebook must be attached to a cluster withblackandtokenize-rtPython packages installed.
black
tokenize-rt
On Databricks Runtime 11.3 LTS and above, Azure Databricks preinstallsblackandtokenize-rt. You can use the formatter directly without needing to install these libraries.
black
tokenize-rt
On Databricks Runtime 10.4 LTS and below, you must installblack==22.3.0andtokenize-rt==4.2.1from PyPI on your notebook or cluster to use the Python formatter. You can run the following command in your notebook:
black==22.3.0
tokenize-rt==4.2.1
%pip install black==22.3.0 tokenize-rt==4.2.1
%pip install black==22.3.0 tokenize-rt==4.2.1
orinstall the library on your cluster.
For more details about installing libraries, seePython environment management.
For files and notebooks in Databricks Git folders, you can configure the Python formatter based on thepyproject.tomlfile. To use this feature, create apyproject.tomlfile in the Git folder root directory and configure it according to theBlack configuration format. Edit the [tool.black] section in the file. The configuration is applied when you format any file and notebook in that Git folder.
pyproject.toml
pyproject.toml
How to format Python and SQL cells
You must haveCAN EDIT permissionon the notebook to format code.
Azure Databricks uses theGethue/sql-formatterlibrary to format SQL and theblackcode formatter for Python.
You can trigger the formatter in the following ways:
Format a single cellKeyboard shortcut: PressCmd+Shift+F.Command context menu:Format SQL cell: SelectFormat SQLin the command context dropdown menu of a SQL cell. This menu item is visible only in SQL notebook cells or those with a%sqllanguage magic.Format Python cell: SelectFormat Pythonin the command context dropdown menu of a Python cell. This menu item is visible only in Python notebook cells or those with a%pythonlanguage magic.NotebookEditmenu: Select a Python or SQL cell, and then selectEdit > Format Cell(s).
Format a single cell
Keyboard shortcut: PressCmd+Shift+F.
Command context menu:Format SQL cell: SelectFormat SQLin the command context dropdown menu of a SQL cell. This menu item is visible only in SQL notebook cells or those with a%sqllanguage magic.Format Python cell: SelectFormat Pythonin the command context dropdown menu of a Python cell. This menu item is visible only in Python notebook cells or those with a%pythonlanguage magic.
Format SQL cell: SelectFormat SQLin the command context dropdown menu of a SQL cell. This menu item is visible only in SQL notebook cells or those with a%sqllanguage magic.
%sql
Format Python cell: SelectFormat Pythonin the command context dropdown menu of a Python cell. This menu item is visible only in Python notebook cells or those with a%pythonlanguage magic.
%python
NotebookEditmenu: Select a Python or SQL cell, and then selectEdit > Format Cell(s).
Format multiple cellsSelect multiple cellsand then selectEdit > Format Cell(s). If you select cells of more than one language, only SQL and Python cells are formatted. This includes those that use%sqland%python.
Format multiple cells
Select multiple cellsand then selectEdit > Format Cell(s). If you select cells of more than one language, only SQL and Python cells are formatted. This includes those that use%sqland%python.
%sql
%python
Format all Python and SQL cells in the notebookSelectEdit > Format Notebook. If your notebook contains more than one language, only SQL and Python cells are formatted. This includes those that use%sqland%python.
Format all Python and SQL cells in the notebook
SelectEdit > Format Notebook. If your notebook contains more than one language, only SQL and Python cells are formatted. This includes those that use%sqland%python.
%sql
%python
To customize how your SQL queries are formatted, seeCustom format SQL statements.
Limitations of code formatting
Black enforcesPEP 8standards for 4-space indentation. Indentation is not configurable.
Formatting embedded Python strings inside a SQL UDF is not supported. Similarly, formatting SQL strings inside a Python UDF is not supported.
Code languages in notebooks
Set default language
The default language for the notebook appears below the notebook name.

To change the default language, click the language button and select the new language from the dropdown menu. To ensure that existing commands continue to work, commands of the previous default language are automatically prefixed with a language magic command.
Mix languages
By default, cells use the default language of the notebook. You can override the default language in a cell by clicking the language button and selecting a language from the dropdown menu.

Alternately, you can use the language magic command%<language>at the beginning of a cell. The supported magic commands are:%python,%r,%scala, and%sql.
%<language>
%python
%r
%scala
%sql
Note
When you invoke a language magic command, the command is dispatched to the REPL in the execution context for the notebook. Variables defined in one language (and hence in the REPL for that language) are not available in the REPL of another language. REPLs can share state only through external resources such as files in DBFS or objects in object storage.
Notebooks also support a few auxiliary magic commands:
%sh: Allows you to run shell code in your notebook. To fail the cell if the shell command has a non-zero exit status, add the-eoption. This command runs only on the Apache Spark driver, and not the workers. To run a shell command on all nodes, use aninit script.
%sh
-e
%fs: Allows you to usedbutilsfilesystem commands. For example, to run thedbutils.fs.lscommand to list files, you can specify%fs lsinstead. For more information, seeWork with files on Azure Databricks.
%fs
dbutils
dbutils.fs.ls
%fs ls
%md: Allows you to include various types of documentation, including text, images, and mathematical formulas and equations. See the next section.
%md
SQL syntax highlighting and autocomplete in Python commands
Syntax highlighting and SQLautocompleteare available when you use SQL inside a Python command, such as in aspark.sqlcommand.
spark.sql
Explore SQL cell results
In a Databricks notebook, results from a SQL language cell are automatically made available as an implicit DataFrame assigned to the variable_sqldf. You can then use this variable in any Python and SQL cells you run afterward, regardless of their position in the notebook.
_sqldf
Note
This feature has the following limitations:
The_sqldfvariable is not available in notebooks that use aSQL warehousefor compute.
_sqldf
Using_sqldfin subsequent Python cells is supported in Databricks Runtime 13.3 and above.
_sqldf
Using_sqldfin subsequent SQL cells is only supported on Databricks Runtime 14.3 and above.
_sqldf
If the query uses the keywordsCACHE TABLEorUNCACHE TABLE, the_sqldfvariable is not available.
CACHE TABLE
UNCACHE TABLE
_sqldf
The screenshot below shows how_sqldfcan be used in subsequent Python and SQL cells:
_sqldf

Important
The variable_sqldfis reassigned each time a SQL cell is run. To avoid losing reference to a specific DataFrame result, assign it to a new variable name before you run the next SQL cell:
_sqldf
new_dataframe_name = _sqldf
new_dataframe_name = _sqldf
ALTER VIEW _sqldf RENAME TO new_dataframe_name
ALTER VIEW _sqldf RENAME TO new_dataframe_name
Execute SQL cells in parallel
While a command is running and your notebook is attached to an interactive cluster, you can run a SQL cell simultaneously with the current command. The SQL cell is executed in a new, parallel session.
To execute a cell in parallel:
Run the cell.
Run the cell.
ClickRun now. The cell is immediately executed.
ClickRun now. The cell is immediately executed.

Because the cell is run in a new session, temporary views, UDFs, and theimplicit Python DataFrame(_sqldf) are not supported for cells that are executed in parallel. In addition, the default catalog and database names are used during parallel execution. If your code refers to a table in a different catalog or database, you must specify the table name using three-level namespace (catalog.schema.table).
_sqldf
catalog
schema
table
Execute SQL cells on a SQL warehouse
You can run SQL commands in a Databricks notebook on aSQL warehouse, a type of compute that is optimized for SQL analytics. SeeUse a notebook with a SQL warehouse.
Run code in Assistant on serverless compute
You can run code in the Assistant on Databricks on serverless compute, which is the default compute on Databricks. For pages where there is already a compute selected (for example, notebooks or SQL editor), Databricks defaults to using it. For information about compute types, seeTypes of compute.
If you don't have access to serverless compute, you must have a compute instance available to run code in the Assistant panel.
Feedback
Was this page helpful?
Additional resources