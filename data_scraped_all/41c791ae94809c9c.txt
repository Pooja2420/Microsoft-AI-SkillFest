Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Databricks Autologging
Article
2024-09-27
4 contributors
In this article
This page covers how to customizeDatabricks Autologging, which automatically captures model parameters, metrics, files, and lineage information when you train models from a variety of popular machine learning libraries. Training sessions are recorded asMLflow tracking runs. Model files are also tracked so you can easily log them to theMLflow Model Registry.
Note
To enable trace logging for generative AI workloads, MLflow supportsOpenAI autologging.
The following video shows Databricks Autologging with a scikit-learn model training session in an
interactive Python notebook. Tracking information is automatically captured and displayed in the
Experiment Runs sidebar and in the MLflow UI.

Requirements
Databricks Autologging is generally available in all regions with Databricks Runtime 10.4 LTS ML or above.
Databricks Autologging is available in select preview regions with Databricks Runtime 9.1 LTS ML or above.
How it works
When you attach an interactive Python notebook to an Azure Databricks cluster, Databricks Autologging
callsmlflow.autolog()to set up tracking for your model training sessions. When you train models in the notebook,
model training information is automatically tracked withMLflow Tracking. For information about how this model training
information is secured and managed, seeSecurity and data management.
The default configuration for themlflow.autolog()call is:
mlflow.autolog(
    log_input_examples=False,
    log_model_signatures=True,
    log_models=True,
    disable=False,
    exclusive=False,
    disable_for_unsupported_versions=True,
    silent=False
)
mlflow.autolog(
    log_input_examples=False,
    log_model_signatures=True,
    log_models=True,
    disable=False,
    exclusive=False,
    disable_for_unsupported_versions=True,
    silent=False
)
You cancustomize the autologging configuration.
Usage
To use Databricks Autologging, train a machine learning model in asupported frameworkusing an
interactive Azure Databricks Python notebook. Databricks Autologging automatically records model lineage
information, parameters, and metrics toMLflow Tracking. You
can alsocustomize the behavior of Databricks Autologging.
Note
Databricks Autologging is not applied to runs created using theMLflow fluent APIwithmlflow.start_run(). In these cases, you must callmlflow.autolog()to save autologged content
to the MLflow run. SeeTrack additional content.
mlflow.start_run()
mlflow.autolog()
Customize logging behavior
To customize logging, usemlflow.autolog().
This function provides configuration parameters to enable model logging (log_models), log datasets (log_datasets),
collect input examples (log_input_examples), log model signatures (log_model_signatures), configure warnings (silent), and more.
log_models
log_datasets
log_input_examples
log_model_signatures
silent
Track additional content
To track additional metrics, parameters, files, and metadata with MLflow runs created by
Databricks Autologging, follow these steps in an Azure Databricks interactive Python notebook:
Callmlflow.autolog()withexclusive=False.
exclusive=False
Start an MLflow run usingmlflow.start_run().
You can wrap this call inwith mlflow.start_run(); when you do this, the run is ended automatically after it completes.
with mlflow.start_run()
UseMLflow Tracking methods, such asmlflow.log_param(),
to track pre-training content.
Train one or more machine learning models in a framework supported by Databricks Autologging.
UseMLflow Tracking methods, such asmlflow.log_metric(),
to track post-training content.
If you did not usewith mlflow.start_run()in Step 2, end the MLflow run usingmlflow.end_run().
with mlflow.start_run()
For example:
import mlflow
mlflow.autolog(exclusive=False)

with mlflow.start_run():
  mlflow.log_param("example_param", "example_value")
  # <your model training code here>
  mlflow.log_metric("example_metric", 5)
import mlflow
mlflow.autolog(exclusive=False)

with mlflow.start_run():
  mlflow.log_param("example_param", "example_value")
  # <your model training code here>
  mlflow.log_metric("example_metric", 5)
Disable Databricks Autologging
To disable Databricks Autologging in an Azure Databricks interactive Python notebook, callmlflow.autolog()withdisable=True:
disable=True
import mlflow
mlflow.autolog(disable=True)
import mlflow
mlflow.autolog(disable=True)
Administrators can also disable Databricks Autologging for all clusters in a workspace from
theAdvancedtab of theadmin settings page. Clusters
must be restarted for this change to take effect.
Supported environments and frameworks
Databricks Autologging is supported in interactive Python notebooks and is available for the
following ML frameworks:
scikit-learn
Apache Spark MLlib
TensorFlow
Keras
PyTorch Lightning
XGBoost
LightGBM
Gluon
Fast.ai
statsmodels
PaddlePaddle
OpenAI
LangChain
For more information about each of the supported frameworks,
seeMLflow automatic logging.
MLflow Tracing enablement
MLflow Tracing utilizes theautologfeature within respective model framework integrations to control
the enabling or disabling of tracing support for integrations that support tracing.
autolog
For example, to enable tracing when using a LlamaIndex model, utilizemlflow.llama_index.autolog()withlog_traces=True:
log_traces=True
import mlflow
mlflow.llama_index.autolog(log_traces=True)
import mlflow
mlflow.llama_index.autolog(log_traces=True)
The supported integrations that have trace enablement within their autolog implementations are:
OpenAI
LangChain
LangGraph
LlamaIndex
AutoGen
Security and data management
All model training information tracked with Databricks Autologging is stored in MLflow Tracking and
is secured byMLflow Experiment permissions.
You can share, modify, or delete model training information using theMLflow TrackingAPI or UI.
Administration
Administrators can enable or disable Databricks Autologging for all interactive notebook sessions
across their workspace in theAdvancedtab of theadmin settings page.
Changes do not take effect until the cluster is restarted.
Limitations
Databricks Autologging is enabled only on the driver node of your Azure Databricks cluster. To use
autologging from worker nodes, you must explicitly callmlflow.autolog()from
within the code executing on each worker.
The XGBoost scikit-learn integration is not supported.
Apache Spark MLlib, Hyperopt, and automated MLflow tracking
Databricks Autologging does not change the behavior of existing automated MLflow tracking
integrations forApache Spark MLlibandHyperopt.
Note
In Databricks Runtime 10.1 ML, disabling the automated MLflow tracking integration for Apache Spark MLlibCrossValidatorandTrainValidationSplitmodels also disables the Databricks Autologging feature for all Apache Spark MLlib models.
CrossValidator
TrainValidationSplit
Feedback
Was this page helpful?
Additional resources