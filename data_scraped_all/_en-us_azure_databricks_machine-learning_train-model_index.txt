Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Train AI and ML models
Article
2025-01-15
4 contributors
In this article
This section shows you how to train machine learning and AI models on Mosaic AI.
Mosaic AI Model Training streamlines and unifies the process of training and deploying traditional ML models through AutoML and Foundation Model Fine-tuning workloads.
AutoML
AutoMLsimplifies the process of applying machine learning to your datasets by automatically finding the best algorithm and hyperparameter configuration for you. AutoML offers a no-code UI as well as a Python API.
Foundation Model Fine-tuning
Foundation Model Fine-tuning(now part of Mosaic AI Model Training) on Databricks lets you customize large language models (LLMs) using your own data. This process involves fine-tuning the training of a pre-existing foundation model, significantly reducing the data, time, and compute resources required compared to training a model from scratch. Key features include:
Instruction fine-tuning:Adapt your model to new tasks by training on structured prompt-response data.
Continued pre-training:Enhance your model with additional text data to add new knowledge or focus on a specific domain.
Chat completion:Train your model on chat logs to improve conversational abilities.
Open source library examples
Seemachine learning training examplesfrom a wide variety of open source machine learning libraries, including hyperparameter tuning examples using Optuna and Hyperopt.
Deep learning
See examples and best practices fordistributed deep learning trainingso you can develop and fine-tune deep learning models on Azure Databricks.
Recommenders
Learn how to traindeep-learning-based recommendation modelson Azure Databricks. Compared to traditional recommendation models, deep learning models can achieve higher quality results and scale to larger amounts of data.
Feedback
Was this page helpful?
Additional resources