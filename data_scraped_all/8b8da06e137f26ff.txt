Use Azure Synapse Analytics to design an enterprise BI solution
This article describes how to transfer data from an on-premises data warehouse to a cloud environment and then use a business intelligence (BI) model to serve the data. You can use this approach as an end goal or a first step toward full modernization with cloud-based components.
This guidance builds on theAzure Synapse Analytics end-to-end scenario. This process uses Azure Synapse Analytics pipelines to ingest data from a SQL database into SQL pools. Then it performs data transformation for analysis. This article focuses on Azure Synapse Analytics pipelines, but you can also use Azure Data Factory pipelines or Fabric Data Factory pipelines to perform these tasks.
When to use this architecture
You can use various methods to meet business requirements for enterprise BI. Various aspects define business requirements, such as current technology investments, human skills, the timeline for modernization, future goals, and whether you have a preference for platform as a service (PaaS) or software as a service (SaaS).
Consider the following design approaches:
A lakehouse in Microsoft Fabric
A lakehouse in Microsoft Fabric
Fabric and Azure Databricksfor customers that have existing investment in Azure Databricks and Power BI and want to modernize with Fabric
Fabric and Azure Databricksfor customers that have existing investment in Azure Databricks and Power BI and want to modernize with Fabric
Enterprise BI for small and medium businesses that use anAzure SQL ecosystem and Fabric
Enterprise BI for small and medium businesses that use anAzure SQL ecosystem and Fabric
Data warehousing completely on Fabric for customers that prefer SaaS
Data warehousing completely on Fabric for customers that prefer SaaS
The architecture in this article assumes that you use Azure Synapse Analytics data warehouse as the persistent layer of the enterprise semantic model and you use Power BI for business intelligence. This PaaS approach has the flexibility to accommodate various business requirements and preferences.
Architecture

The diagram shows types of input, like data streams, databases, data services, unstructured data, and structured data. Components in the Ingest phase receive the data input. The Ingest phase components are Azure Event Hubs, Azure IoT Hub, Azure Synapse Analytics, and pipelines. Azure Synapse Analytics is also in the Store phase and the Process phase. The next step in the dataflow is the Store phase, which contains Azure Data Lake Storage. Then the data goes to the Process phase, which contains Azure Stream Analytics, Azure Data Explorer pools, Apache Spark pools, and serverless and dedicated SQL pools. Some of the machine learning model data goes to the Enrich phase, which contains Azure AI services and Azure Machine Learning. The other data goes to the Serve phase, which contains Power BI premium, Azure Cosmos DB, Azure AI Search, and Azure Data Share. The data outputs to business users, analytics, applications, and shared datasets.

Download aVisio fileof this architecture.
Workflow
A SQL Server database in Azure contains the source data. To simulate the on-premises environment, deployment scripts for this scenario configure an Azure SQL database. TheAdventureWorks sample databaseis used as the source data schema and sample data. For more information, seeCopy and transform data to and from SQL Server.
Azure Data Lake Storageis a temporary staging area during data ingestion. You can usePolyBase to copy data into an Azure Synapse Analytics dedicated SQL pool.
Azure Data Lake Storageis a temporary staging area during data ingestion. You can usePolyBase to copy data into an Azure Synapse Analytics dedicated SQL pool.
Azure Synapse Analyticsis a distributed system that performs analytics on large data. It supports massive parallel processing, so it can run high-performance analytics. The Azure Synapse Analytics dedicated SQL pool is a target for ongoing ingestion from the on-premises environment. The SQL pool can serve data toPower BIvia DirectQuery and perform further processing.
Azure Synapse Analyticsis a distributed system that performs analytics on large data. It supports massive parallel processing, so it can run high-performance analytics. The Azure Synapse Analytics dedicated SQL pool is a target for ongoing ingestion from the on-premises environment. The SQL pool can serve data toPower BIvia DirectQuery and perform further processing.
Azure Synapse Analytics pipelinesorchestrate data ingestion and transformation within the Azure Synapse Analytics workspace.
Azure Synapse Analytics pipelinesorchestrate data ingestion and transformation within the Azure Synapse Analytics workspace.
The data-modeling approach in this scenario combines theenterprise modeland theBI semantic model. TheAzure Synapse Analytics dedicated SQL poolcontains the enterprise model.Power BI Premium capacity F64contains the BI semantic model. Power BI accesses the data via DirectQuery.
Components
This scenario uses the following components:
Azure SQL Databaseis an Azure-hosted PaaS SQL server. This architecture uses SQL Database to demonstrate the flow of data for the migration scenario.
Azure SQL Databaseis an Azure-hosted PaaS SQL server. This architecture uses SQL Database to demonstrate the flow of data for the migration scenario.
Data Lake Storageprovides flexible cloud storage for unstructured data that's used for persisting intermediate migration results.
Data Lake Storageprovides flexible cloud storage for unstructured data that's used for persisting intermediate migration results.
Azure Synapse Analyticsis an enterprise analytics service for data warehousing and big data systems. Azure Synapse Analytics serves as main compute and persistent storage in enterprise semantic modeling and servicing.
Azure Synapse Analyticsis an enterprise analytics service for data warehousing and big data systems. Azure Synapse Analytics serves as main compute and persistent storage in enterprise semantic modeling and servicing.
Power BI Premiumis a BI tool that presents and visualizes data in this scenario.
Power BI Premiumis a BI tool that presents and visualizes data in this scenario.
Microsoft Entra IDis a multicloud identity and network solution suite that supports the authentication and authorization flow.
Microsoft Entra IDis a multicloud identity and network solution suite that supports the authentication and authorization flow.
Simplified architecture

The diagram shows a dataflow where Azure Synapse Analytics pipelines ingest relational databases. Azure Synapse Analytics dedicated SQL pools store the data. Power BI Premium serves the data. The data outputs to business users and analytics.

Scenario details
In this scenario, an organization has a SQL database that contains a large on-premises data warehouse. The organization wants to use Azure Synapse Analytics to perform analysis, then deliver these insights via Power BI to users and analytics.
Authentication
Microsoft Entra ID authenticates users who connect to Power BI dashboards and apps. Single sign-on connects users to the data source in an Azure Synapse Analytics provisioned pool. Authorization occurs on the source.
Incremental loading
When you run an automated extract, transform, load (ETL) or extract, load, transform (ELT) process, you should load only the data that changed since the previous run. This process is called anincremental load. Conversely, a full load loads all the data. To perform an incremental load, determine how to identify the changed data. You can use ahigh water markvalue approach, which tracks the latest value of a date-time column or a unique integer column in the source table.
You can usetemporal tablesin SQL Server. Temporal tables are system-versioned tables that store data change history. The database engine automatically records the history of every change in a separate history table. To query the historical data, you can add aFOR SYSTEM_TIMEclause to a query. Internally, the database engine queries the history table, but it's transparent to the application.
FOR SYSTEM_TIME
Temporal tables support dimension data, which can change over time. Fact tables usually represent an immutable transaction such as a sale, in which case keeping the system version history doesn't make sense. Instead, transactions usually have a column that represents the transaction date. The column can be used as the watermark value. For example, in the AdventureWorks data warehouse, theSalesLT.*tables have aLastModifiedfield.
SalesLT.*
LastModified
Here's the general flow for the ELT pipeline:
For each table in the source database, track the cutoff time when the last ELT job ran. Store this information in the data warehouse. On initial setup, all times are set to1-1-1900.
For each table in the source database, track the cutoff time when the last ELT job ran. Store this information in the data warehouse. On initial setup, all times are set to1-1-1900.
1-1-1900
During the data export step, the cutoff time is passed as a parameter to a set of stored procedures in the source database. These stored procedures query any records that are changed or created after the cutoff time. For all tables in the example, you can use theModifiedDatecolumn.
During the data export step, the cutoff time is passed as a parameter to a set of stored procedures in the source database. These stored procedures query any records that are changed or created after the cutoff time. For all tables in the example, you can use theModifiedDatecolumn.
ModifiedDate
When the data migration is complete, update the table that stores the cutoff times.
When the data migration is complete, update the table that stores the cutoff times.
Data pipeline
This scenario uses theAdventureWorks sample databaseas a data source. The incremental data load pattern ensures that only data that's modified or added after the most recent pipeline run is loaded.
Metadata-driven copy tool
The built-inmetadata-driven copy toolwithin Azure Synapse Analytics pipelines incrementally loads all tables that are contained in the relational database.
Use a wizard interface to connect the Copy Data tool to the source database.
Use a wizard interface to connect the Copy Data tool to the source database.
After it connects, configure incremental loading or full loading for each table.
After it connects, configure incremental loading or full loading for each table.
The Copy Data tool creates the pipelines and SQL scripts needed to generate the control table. This table stores data, such as the high watermark value or column for each table, for the incremental loading process.
The Copy Data tool creates the pipelines and SQL scripts needed to generate the control table. This table stores data, such as the high watermark value or column for each table, for the incremental loading process.
After these scripts run, the pipeline loads all source data warehouse tables into the Azure Synapse Analytics dedicated pool.
After these scripts run, the pipeline loads all source data warehouse tables into the Azure Synapse Analytics dedicated pool.

Before the tool loads the data, it creates three pipelines to iterate over the tables in the database.
The pipelines do the following tasks:
Count the number of objects, such as tables, to be copied in the pipeline run.
Count the number of objects, such as tables, to be copied in the pipeline run.
Iterate over each object to be loaded or copied.
Iterate over each object to be loaded or copied.
After a pipeline iterates over each object, it does the following tasks:Checks whether a delta load is required. Otherwise, the pipeline completes a normal full load.Retrieves the high watermark value from the control table.Copies data from the source tables into the staging account in Data Lake Storage.Loads data into the dedicated SQL pool via the selected copy method, such as the PolyBase or Copy command.Updates the high watermark value in the control table.
After a pipeline iterates over each object, it does the following tasks:
Checks whether a delta load is required. Otherwise, the pipeline completes a normal full load.
Checks whether a delta load is required. Otherwise, the pipeline completes a normal full load.
Retrieves the high watermark value from the control table.
Retrieves the high watermark value from the control table.
Copies data from the source tables into the staging account in Data Lake Storage.
Copies data from the source tables into the staging account in Data Lake Storage.
Loads data into the dedicated SQL pool via the selected copy method, such as the PolyBase or Copy command.
Loads data into the dedicated SQL pool via the selected copy method, such as the PolyBase or Copy command.
Updates the high watermark value in the control table.
Updates the high watermark value in the control table.
Load data into an Azure Synapse Analytics SQL pool
Thecopy activitycopies data from the SQL database into the Azure Synapse Analytics SQL pool. This example's SQL database is in Azure, so it uses the Azure integration runtime to read data from the SQL database and write the data into the specified staging environment.
The copy statement then loads data from the staging environment into the Azure Synapse Analytics dedicated pool.
Use Azure Synapse Analytics pipelines
Pipelines in Azure Synapse Analytics define an ordered set of activities to complete an incremental load pattern. Manual or automatic triggers start the pipeline.
Transform the data
The sample database in this reference architecture is small, so replicated tables that have no partitions are created. For production workloads, distributed tables can improve query performance. For more information, seeGuidance for designing distributed tables in Azure Synapse Analytics. The example scripts run the queries via a staticresource class.
In a production environment, consider creating staging tables that have round-robin distribution. Then transform and move the data into production tables that have clustered columnstore indexes, which offer the best overall query performance. Columnstore indexes are optimized for queries that scan many records.
Columnstore indexes don't perform optimally for singleton lookups, or looking up a single row. If you need to perform frequent singleton lookups, you can add a nonclustered index to a table, which increases speed. However, singleton lookups are typically less common in data warehouse scenarios than online transaction processing workloads. For more information, seeIndex tables in Azure Synapse Analytics.
Note
Clustered columnstore tables don't supportvarchar(max),nvarchar(max), orvarbinary(max)data types. If you use those data types, consider a heap or clustered index. You might also consider putting these columns into a separate table.
varchar(max)
nvarchar(max)
varbinary(max)
Use Power BI Premium to access, model, and visualize data
Power BI Premium supports several options to connect to data sources on Azure. You can use Azure Synapse Analytics provisioned pools to do the following tasks:
Import: The data is imported into the Power BI model.
DirectQuery: Data is pulled directly from relational storage.
Composite model: CombineImportfor some tables andDirectQueryfor others.
This scenario uses the DirectQuery dashboard because it has a small amount of data and low model complexity. DirectQuery delegates the query to the powerful compute engine underneath and uses extensive security capabilities on the source. DirectQuery ensures that results are always consistent with the latest source data.
Import mode provides the fastest query response time. Consider import mode if:
The model fits entirely within the memory of Power BI.
The data latency between refreshes is acceptable.
You require complex transformations between the source system and the final model.
In this case, the end users want full access to the most recent data with no delays in Power BI refreshing, and they want all historical data, which exceeds the Power BI dataset capacity. A Power BI dataset can handle 25-400 GB, depending on the capacity size. The data model in the dedicated SQL pool is already in a star schema and doesn't require transformation, so DirectQuery is an appropriate choice.

UsePower BI Premiumto manage large models, paginated reports, and deployment pipelines. Take advantage of the built-in Azure Analysis Services endpoint. You can also have dedicatedcapacitywith unique value proposition.
When the BI model grows or dashboard complexity increases, you can switch to composite models and import parts of lookup tables viahybrid tables, and import preaggregated data. You can enablequery cachingwithin Power BI for imported datasets and usedual tablesfor the storage mode property.
Within the composite model, datasets serve as a virtual pass-through layer. When users interact with visualizations, Power BI generates SQL queries to Azure Synapse Analytics SQL pools. Power BI determines whether to use in-memory or DirectQuery storage based on efficiency. The engine decides when to switch from in-memory to DirectQuery and pushes the logic to the Azure Synapse Analytics SQL pool. Depending on the context of the query tables, they can act as either cached (imported) or non-cached composite models. You can choose which table to cache into memory, combine data from one or more DirectQuery sources, or combine DirectQuery source data and imported data.
When you use DirectQuery with an Azure Synapse Analytics provisioned pool:
Use Azure Synapse Analyticsresult set cachingto cache query results in the user database for repetitive use. This approach improves query performance to milliseconds and reduces compute resource usage. Queries that use cached results sets don't consume any concurrency slots in Azure Synapse Analytics, so they don't count against existing concurrency limits.
Use Azure Synapse Analyticsresult set cachingto cache query results in the user database for repetitive use. This approach improves query performance to milliseconds and reduces compute resource usage. Queries that use cached results sets don't consume any concurrency slots in Azure Synapse Analytics, so they don't count against existing concurrency limits.
Use Azure Synapse Analyticsmaterialized viewsto precompute, store, and maintain data like a table. Queries that use all data or a subset of the data in materialized views can achieve faster performance without needing to directly reference the defined materialized view to use it.
Use Azure Synapse Analyticsmaterialized viewsto precompute, store, and maintain data like a table. Queries that use all data or a subset of the data in materialized views can achieve faster performance without needing to directly reference the defined materialized view to use it.
Considerations
These considerations implement the pillars of the Azure Well-Architected Framework, which is a set of guiding tenets that you can use to improve the quality of a workload. For more information, seeWell-Architected Framework.
Security
Security provides assurances against deliberate attacks and the misuse of your valuable data and systems. For more information, seeDesign review checklist for Security.
Cloud modernization introduces security concerns, such as data breaches, malware infections, and malicious code injection. You need a cloud provider or service solution that can address your concerns because inadequate security measures can create major problems.
This scenario addresses the most demanding security concerns by using a combination of layered security controls: network, identity, privacy, and authorization controls. An Azure Synapse Analytics provisioned pool stores most of the data. Power BI accesses the data via DirectQuery through single sign-on. You can use Microsoft Entra ID for authentication. There are also extensive security controls for data authorization within the provisioned pools.
Some common security questions include:
Define who can see what data.Ensure that your data complies with federal, local, and company guidelines to mitigate data breach risks. Azure Synapse Analytics provides multipledata protection capabilitiesto achieve compliance.
Define who can see what data.
Ensure that your data complies with federal, local, and company guidelines to mitigate data breach risks. Azure Synapse Analytics provides multipledata protection capabilitiesto achieve compliance.
Determine how to verify a user's identity.Use Azure Synapse Analytics to control who can access what data viaaccess controlandauthentication.
Determine how to verify a user's identity.
Use Azure Synapse Analytics to control who can access what data viaaccess controlandauthentication.
Choose a network security technology to protect the integrity, confidentiality, and access of your networks and data.Help secure Azure Synapse Analytics by usingnetwork securityoptions.
Choose a network security technology to protect the integrity, confidentiality, and access of your networks and data.
Help secure Azure Synapse Analytics by usingnetwork securityoptions.
Choose tools to detect and notify you of threats.Use Azure Synapse Analyticsthreat detectioncapabilities, such as SQL auditing, SQL threat detection, and vulnerability assessment to audit, protect, and monitor databases.
Choose tools to detect and notify you of threats.
Use Azure Synapse Analyticsthreat detectioncapabilities, such as SQL auditing, SQL threat detection, and vulnerability assessment to audit, protect, and monitor databases.
Determine how to protect data in your storage account.Use Azure Storage accounts for workloads that require fast and consistent response times or that have a high number of input/output operations (IOPs) per second. Storage accounts can store all your data objects and have severalstorage account security options.
Determine how to protect data in your storage account.
Use Azure Storage accounts for workloads that require fast and consistent response times or that have a high number of input/output operations (IOPs) per second. Storage accounts can store all your data objects and have severalstorage account security options.
Cost Optimization
Cost Optimization focuses on ways to reduce unnecessary expenses and improve operational efficiencies. For more information, seeDesign review checklist for Cost Optimization.
This section provides information about pricing for different services involved in this solution, and mentions decisions made for this scenario with a sample dataset. Use this starting configuration in theAzure pricing calculator, and adjust it to fit your scenario.
Azure Synapse Analytics is a serverless architecture that you can use to scale your compute and storage levels independently. Compute resources incur costs based on usage. You can scale or pause these resources on demand. Storage resources incur costs per terabyte, so your costs increase as you ingest data.
Three main components influence the price of a pipeline:
Data pipeline activities and integration runtime hours
Data flows cluster size and implementation
Operation charges
For pricing details, see theData Integrationtab onAzure Synapse Analytics pricing.
The price varies depending on components or activities, frequency, and the number of integration runtime units.
For the sample dataset, which uses the standard Azure-hosted integration runtime,copy data activityserves as the core of the pipeline. It runs on a daily schedule for all the entities (tables) in the source database. The scenario doesn't contain data flows. And it doesn't incur operational costs because the pipelines run less than one million operations per month.
For the sample dataset, you can provision 500 data warehouse units (DWUs) to provide a smooth experience for analytical loads. You can maintain compute during business hours for reporting purposes. If the solution moves to production, use reserved data warehouse capacity as a cost-efficient strategy. Use various techniques to maximize cost and performance metrics.
For pricing details for an Azure Synapse Analytics dedicated pool, see theData Warehousingtab onAzure Synapse Analytics pricing. Under the dedicated consumption model, customers incur costs for each provisioned DWU, per hour of uptime. Also consider data storage costs, including the size of your data at rest, snapshots, and geo-redundancy.
Consider using the Azure Storage reserved capacity to reduce storage costs. With this model, you get a discount if you reserve fixed storage capacity for one or three years. For more information, seeOptimize costs for blob storage with reserved capacity. This scenario doesn't use persistent storage.
This scenario usesPower BI Premium workspaceswith built-in performance enhancements to accommodate demanding analytical needs.
For more information, seePower BI pricing.
Operational Excellence
Operational Excellence covers the operations processes that deploy an application and keep it running in production. For more information, seeDesign review checklist for Operational Excellence.
Use an Azure DevOps release pipeline and GitHub Actions to automate the deployment of an Azure Synapse Analytics workspace across multiple environments. For more information, seeContinuous integration and continuous delivery for an Azure Synapse Analytics workspace.
Use an Azure DevOps release pipeline and GitHub Actions to automate the deployment of an Azure Synapse Analytics workspace across multiple environments. For more information, seeContinuous integration and continuous delivery for an Azure Synapse Analytics workspace.
Put each workload in a separate deployment template, and store the resources in source control systems. You can deploy the templates together or individually as part of a continuous integration and continuous delivery (CI/CD) process. This approach simplifies the automation process. This architecture has four main workloads:The data warehouse server and related resourcesAzure Synapse Analytics pipelinesPower BI assets, including dashboards, apps, and datasetsAn on-premises to cloud simulated scenario
Put each workload in a separate deployment template, and store the resources in source control systems. You can deploy the templates together or individually as part of a continuous integration and continuous delivery (CI/CD) process. This approach simplifies the automation process. This architecture has four main workloads:
The data warehouse server and related resources
Azure Synapse Analytics pipelines
Power BI assets, including dashboards, apps, and datasets
An on-premises to cloud simulated scenario
Consider staging your workloads where practical. Deploy your workload to various stages. Run validation checks at each stage before you move to the next stage. This approach pushes updates to your production environments in a controlled way and minimizes unanticipated deployment problems. Useblue-green deploymentandcanary releasestrategies to update live production environments.
Consider staging your workloads where practical. Deploy your workload to various stages. Run validation checks at each stage before you move to the next stage. This approach pushes updates to your production environments in a controlled way and minimizes unanticipated deployment problems. Useblue-green deploymentandcanary releasestrategies to update live production environments.
Use a rollback strategy to handle failed deployments. For example, you can automatically redeploy an earlier, successful deployment from your deployment history. Use the--rollback-on-errorflag in the Azure CLI.
Use a rollback strategy to handle failed deployments. For example, you can automatically redeploy an earlier, successful deployment from your deployment history. Use the--rollback-on-errorflag in the Azure CLI.
--rollback-on-error
UseAzure Monitorto analyze the performance of your data warehouse and the entire Azure analytics platform for an integrated monitoring experience.Azure Synapse Analyticsprovides a monitoring experience within the Azure portal to show insights about your data warehouse workload. Use the Azure portal to monitor your data warehouse. It provides configurable retention periods, alerts, recommendations, and customizable charts and dashboards for metrics and logs.
UseAzure Monitorto analyze the performance of your data warehouse and the entire Azure analytics platform for an integrated monitoring experience.Azure Synapse Analyticsprovides a monitoring experience within the Azure portal to show insights about your data warehouse workload. Use the Azure portal to monitor your data warehouse. It provides configurable retention periods, alerts, recommendations, and customizable charts and dashboards for metrics and logs.
For more information, see the following resources:
Tutorial: Get started with Azure Synapse Analytics
Create an Azure Synapse Analytics workspace by using the Azure CLI
Performance Efficiency
Performance Efficiency refers to your workload's ability to scale to meet user demands efficiently. For more information, seeDesign review checklist for Performance Efficiency.
This section provides details about sizing decisions to accommodate this dataset.
You can use variousdata warehouse configurations.
-- TO --
To see the performance benefits of scaling out, especially for larger DWUs, use at least a 1-TB dataset. To find the best number of DWUs for your dedicated SQL pool, try scaling up and down. Run queries that have different numbers of DWUs after you load your data. Scaling is quick, so you can easily experiment with various performance levels.
For a dedicated SQL pool in development, select a small number of DWUs as a starting point, such asDW400corDW200c. Monitor your application performance for each number of DWUs. Assume a linear scale, and determine how much you need to increase or decrease the DWUs. Continue making adjustments until you reach an optimum performance level for your business requirements.
For scalability and performance optimization features of pipelines in Azure Synapse Analytics and of the copy activity that you use, seeCopy activity performance and scalability guide.
For more information, see the following resources:
Scale compute for an Azure Synapse Analytics SQL pool with the Azure portal
Scale compute for a dedicated SQL pool with Azure PowerShell
Scale compute for a dedicated SQL pool in Azure Synapse Analytics by using T-SQL
Manage compute for a dedicated SQL pool
This article uses thePower BI Premium F64 capacityto demonstrate BI capabilities. Dedicated Power BI capacities in Fabric range from F64 (8 vCores) to F1024 (128 vCores).
To determine how much capacity you need:
Evaluate the loadon your capacity.
Install the Fabriccapacity metrics appfor ongoing monitoring.
Consider using workload-relatedcapacity optimization techniques.
Contributors
Microsoft maintains this article. The following contributors wrote this article.
Principal authors:
Galina Polyakova| Senior Cloud Solution Architect
Noah Costar| Cloud Solution Architect
George Stevens| Cloud Solution Architect
Other contributors:
Jim McLeod| Cloud Solution Architect
Miguel Myers| Senior Program Manager
To see nonpublic LinkedIn profiles, sign in to LinkedIn.
Next steps
What is Power BI Premium?
What is Microsoft Entra ID?
Access Data Lake Storage and Azure Blob Storage with Azure Databricks
What is Azure Synapse Analytics?
Pipelines and activities in Azure Data Factory and Azure Synapse Analytics
What is Azure SQL?
Related resources
Automated enterprise BI
Analytics end-to-end with Azure Synapse Analytics