Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Azure Kubernetes Service (AKS) node auto-repair
Article
2025-04-02
18 contributors
In this article
Azure Kubernetes Service (AKS) continuously monitors the health state of worker nodes and performs automatic node repair if they become unhealthy. The Azure virtual machine (VM) platformperforms maintenance on VMsexperiencing issues. AKS and Azure VMs work together to minimize service disruptions for clusters.
In this article, you learn how the automatic node repair functionality behaves for Windows and Linux nodes.
How AKS checks for NotReady nodes
AKS uses the following rules to determine if a node is unhealthy and needs repair:
The node reports theNotReadystatus on consecutive checks within a 10-minute time frame.
The node doesn't report any status within 10 minutes.
You can manually check the health state of your nodes with thekubectl get nodescommand.
kubectl get nodes
How automatic repair works
Note
AKS initiates repair operations with the user accountaks-remediator.
If AKS identifies an unhealthy node that remains unhealthy for at leastfiveminutes, AKS performs the following actions:
AKS reboots the node.
If the node remains unhealthy after reboot, AKS reimages the node.
If the node remains unhealthy after reimage and it's a Linux node, AKS redeploys the node.
AKS retries the restart, reimage, and redeploy sequence up to three times if the node remains unhealthy. The overall auto repair process can take up to an hour to complete.
Limitations
AKS node auto-repair is a best effort service and we don't guarantee that the node is restored back to healthy status. If your node persists in an unhealthy state, we highly encourage that you perform manual investigation of the node. Learn more abouttroubleshooting node NotReady status.
There are cases where AKS doesn't perform automatic repair. Failure to automatically repair the node can occur either by design or if Azure can't detect that an issue exists. Examples of when auto-repair isn't performed include:
A node status isn't being reported due to error in network configuration.
A node failed to initially register as a healthy node.
If either of the following taints are present on the node:node.cloudprovider.kubernetes.io/shutdown,ToBeDeletedByClusterAutoscaler.
node.cloudprovider.kubernetes.io/shutdown
ToBeDeletedByClusterAutoscaler
A node is in the process of being upgraded, resulting in the following annotation on the node"cluster-autoscaler.kubernetes.io/scale-down-disabled": "true"and"kubernetes.azure.com/azure-cluster-autoscaler-scale-down-disabled-reason": "upgrade"
"cluster-autoscaler.kubernetes.io/scale-down-disabled": "true"
"kubernetes.azure.com/azure-cluster-autoscaler-scale-down-disabled-reason": "upgrade"
Monitor node auto-repair using Kubernetes events
When AKS performs node auto-repair on your cluster, AKS emits Kubernetes events from the aks-auto-repair source for visibility. The following events appear on a node object when auto-repair happens.
To learn more about accessing, storing, and configuring alerts on Kubernetes events, seeUse Kubernetes events for troubleshooting in Azure Kubernetes Service.
If any errors occur during the node auto-repair process, the following events are emitted with the verbatim error message. Learn more abouttroubleshooting common node auto-repair errors.
Note
Error codein the following event messages varies depending on the error reported.
Next steps
By default, you can access Kubernetes events and logs on your AKS cluster from the past 1 hour. To store and query events and logs from the past 90 days, enableContainer Insightsfor deeper troubleshooting on your AKS cluster.
Azure Kubernetes Service

Additional resources