Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Add and configure models to Azure AI model inference
Article
2025-01-28
3 contributors
In this article
Important
Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, seeSupplemental Terms of Use for Microsoft Azure Previews.
You can decide and configure which models are available for inference in the inference endpoint. When a given model is configured, you can then generate predictions from it by indicating its model name or deployment name on your requests. No further changes are required in your code to use it.
In this article, you'll learn how to add a new model to Azure AI model inference in Azure AI Foundry.
Prerequisites
To complete this article, you need:
An Azure subscription. If you're usingGitHub Models, you can upgrade your experience and create an Azure subscription in the process. ReadUpgrade from GitHub Models to Azure AI model inferenceif that's your case.
An Azure subscription. If you're usingGitHub Models, you can upgrade your experience and create an Azure subscription in the process. ReadUpgrade from GitHub Models to Azure AI model inferenceif that's your case.
An Azure AI services resource.
An Azure AI services resource.
An AI project connected to your Azure AI Services resource with the featureDeploy models to Azure AI model inference serviceon.You can follow the steps atConfigure Azure AI model inference service in my projectin Azure AI Foundry.
An AI project connected to your Azure AI Services resource with the featureDeploy models to Azure AI model inference serviceon.
You can follow the steps atConfigure Azure AI model inference service in my projectin Azure AI Foundry.
Add a model
You can add models to the Azure AI model inference endpoint using the following steps:
Go toModel catalogsection inAzure AI Foundry portal.
Go toModel catalogsection inAzure AI Foundry portal.
Scroll to the model you're interested in and select it.
Scroll to the model you're interested in and select it.

You can review the details of the model in the model card.
You can review the details of the model in the model card.
SelectDeploy.
SelectDeploy.
For model providers that require more terms of contract, you'll be asked to accept those terms. This is the case for Mistral models for instance. Accept the terms on those cases by selectingSubscribe and deploy.
For model providers that require more terms of contract, you'll be asked to accept those terms. This is the case for Mistral models for instance. Accept the terms on those cases by selectingSubscribe and deploy.

You can configure the deployment settings at this time. By default, the deployment receives the name of the model you're deploying. The deployment name is used in themodelparameter for request to route to this particular model deployment. This allows you to also configure specific names for your models when you attach specific configurations. For instanceo1-preview-safefor a model with a strict content safety content filter.TipEach model can support different deployments types, providing different data residency or throughput guarantees. Seedeployment typesfor more details.
You can configure the deployment settings at this time. By default, the deployment receives the name of the model you're deploying. The deployment name is used in themodelparameter for request to route to this particular model deployment. This allows you to also configure specific names for your models when you attach specific configurations. For instanceo1-preview-safefor a model with a strict content safety content filter.
model
o1-preview-safe
Tip
Each model can support different deployments types, providing different data residency or throughput guarantees. Seedeployment typesfor more details.
We automatically select an Azure AI Services connection depending on your project. Use theCustomizeoption to change the connection based on your needs. If you're deploying under theStandarddeployment type, the models need to be available in the region of the Azure AI Services resource.TipIf the desired resource isn't listed, you might need to create a connection to it. SeeConfigure Azure AI model inference service in my projectin Azure AI Foundry portal.
We automatically select an Azure AI Services connection depending on your project. Use theCustomizeoption to change the connection based on your needs. If you're deploying under theStandarddeployment type, the models need to be available in the region of the Azure AI Services resource.

Tip
If the desired resource isn't listed, you might need to create a connection to it. SeeConfigure Azure AI model inference service in my projectin Azure AI Foundry portal.
SelectDeploy.
SelectDeploy.
Once the deployment completes, the new model is listed in the page and it's ready to be used.
Once the deployment completes, the new model is listed in the page and it's ready to be used.
Manage models
You can manage the existing model deployments in the resource using Azure AI Foundry portal.
Go toModels + Endpointssection inAzure AI Foundry portal.
Go toModels + Endpointssection inAzure AI Foundry portal.
Scroll to the connection to your Azure AI Services resource. Model deployments are grouped and displayed per connection.
Scroll to the connection to your Azure AI Services resource. Model deployments are grouped and displayed per connection.

You see a list of models available under each connection. Select the model deployment you're interested in.
You see a list of models available under each connection. Select the model deployment you're interested in.
EditorDeletethe deployment as needed.
EditorDeletethe deployment as needed.
Test the deployment in the playground
You can interact with the new model in Azure AI Foundry portal using the playground:
Note
Playground is only available when working with AI projects in Azure AI Foundry. Create an AI project to get full access to all the capabilities in Azure AI Foundry.
Go toPlaygroundssection inAzure AI Foundry portal.
Go toPlaygroundssection inAzure AI Foundry portal.
Depending on the type of model you deployed, select the playground needed. In this case we selectChat playground.
Depending on the type of model you deployed, select the playground needed. In this case we selectChat playground.
In theDeploymentdrop down, underSetupselect the name of the model deployment you have created.
In theDeploymentdrop down, underSetupselect the name of the model deployment you have created.

Type your prompt and see the outputs.
Type your prompt and see the outputs.
Additionally, you can useView codeso see details about how to access the model deployment programmatically.
Additionally, you can useView codeso see details about how to access the model deployment programmatically.
Important
Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, seeSupplemental Terms of Use for Microsoft Azure Previews.
You can decide and configure which models are available for inference in the inference endpoint. When a given model is configured, you can then generate predictions from it by indicating its model name or deployment name on your requests. No further changes are required in your code to use it.
In this article, you'll learn how to add a new model to Azure AI model inference in Azure AI Foundry.
Prerequisites
To complete this article, you need:
An Azure subscription. If you're usingGitHub Models, you can upgrade your experience and create an Azure subscription in the process. ReadUpgrade from GitHub Models to Azure AI model inferenceif that's your case.
An Azure subscription. If you're usingGitHub Models, you can upgrade your experience and create an Azure subscription in the process. ReadUpgrade from GitHub Models to Azure AI model inferenceif that's your case.
An Azure AI services resource.
An Azure AI services resource.
Install theAzure CLIand thecognitiveservicesextension for Azure AI Services:az extension add -n cognitiveservices
Install theAzure CLIand thecognitiveservicesextension for Azure AI Services:
cognitiveservices
az extension add -n cognitiveservices
az extension add -n cognitiveservices
Some of the commands in this tutorial use thejqtool, which might not be installed in your system. For installation instructions, seeDownloadjq.
Some of the commands in this tutorial use thejqtool, which might not be installed in your system. For installation instructions, seeDownloadjq.
jq
jq
Identify the following information:Your Azure subscription ID.Your Azure AI Services resource name.The resource group where the Azure AI Services resource is deployed.
Identify the following information:
Your Azure subscription ID.
Your Azure subscription ID.
Your Azure AI Services resource name.
Your Azure AI Services resource name.
The resource group where the Azure AI Services resource is deployed.
The resource group where the Azure AI Services resource is deployed.
Add models
To add a model, you first need to identify the model that you want to deploy. You can query the available models as follows:
Log in into your Azure subscription:az login
Log in into your Azure subscription:
az login
az login
If you have more than 1 subscription, select the subscription where your resource is located:az account set --subscription $subscriptionId
If you have more than 1 subscription, select the subscription where your resource is located:
az account set --subscription $subscriptionId
az account set --subscription $subscriptionId
Set the following environment variables with the name of the Azure AI Services resource you plan to use and resource group.accountName="<ai-services-resource-name>"
resourceGroupName="<resource-group>"
location="eastus2"
Set the following environment variables with the name of the Azure AI Services resource you plan to use and resource group.
accountName="<ai-services-resource-name>"
resourceGroupName="<resource-group>"
location="eastus2"
accountName="<ai-services-resource-name>"
resourceGroupName="<resource-group>"
location="eastus2"
If you don't have an Azure AI Services account create yet, you can create one as follows:az cognitiveservices account create -n $accountName -g $resourceGroupName --custom-domain $accountName --location $location --kind AIServices --sku S0
If you don't have an Azure AI Services account create yet, you can create one as follows:
az cognitiveservices account create -n $accountName -g $resourceGroupName --custom-domain $accountName --location $location --kind AIServices --sku S0
az cognitiveservices account create -n $accountName -g $resourceGroupName --custom-domain $accountName --location $location --kind AIServices --sku S0
Let's see first which models are available to you and under which SKU. SKUs, also known asdeployment types, define how Azure infrastructure is used to process requests. Models may offer different deployment types. The following command list all the model definitions available:az cognitiveservices account list-models \
    -n $accountName \
    -g $resourceGroupName \
| jq '.[] | { name: .name, format: .format, version: .version, sku: .skus[0].name, capacity: .skus[0].capacity.default }'
Let's see first which models are available to you and under which SKU. SKUs, also known asdeployment types, define how Azure infrastructure is used to process requests. Models may offer different deployment types. The following command list all the model definitions available:
az cognitiveservices account list-models \
    -n $accountName \
    -g $resourceGroupName \
| jq '.[] | { name: .name, format: .format, version: .version, sku: .skus[0].name, capacity: .skus[0].capacity.default }'
az cognitiveservices account list-models \
    -n $accountName \
    -g $resourceGroupName \
| jq '.[] | { name: .name, format: .format, version: .version, sku: .skus[0].name, capacity: .skus[0].capacity.default }'
Outputs look as follows:{
  "name": "Phi-3.5-vision-instruct",
  "format": "Microsoft",
  "version": "2",
  "sku": "GlobalStandard",
  "capacity": 1
}
Outputs look as follows:
{
  "name": "Phi-3.5-vision-instruct",
  "format": "Microsoft",
  "version": "2",
  "sku": "GlobalStandard",
  "capacity": 1
}
{
  "name": "Phi-3.5-vision-instruct",
  "format": "Microsoft",
  "version": "2",
  "sku": "GlobalStandard",
  "capacity": 1
}
Identify the model you want to deploy. You need the propertiesname,format,version, andsku. The propertyformatindicates the provider offering the model. Capacity might also be needed depending on the type of deployment.
Identify the model you want to deploy. You need the propertiesname,format,version, andsku. The propertyformatindicates the provider offering the model. Capacity might also be needed depending on the type of deployment.
name
format
version
sku
format
Add the model deployment to the resource. The following example addsPhi-3.5-vision-instruct:az cognitiveservices account deployment create \
    -n $accountName \
    -g $resourceGroupName \
    --deployment-name Phi-3.5-vision-instruct \
    --model-name Phi-3.5-vision-instruct \
    --model-version 2 \
    --model-format Microsoft \
    --sku-capacity 1 \
    --sku-name GlobalStandard
Add the model deployment to the resource. The following example addsPhi-3.5-vision-instruct:
Phi-3.5-vision-instruct
az cognitiveservices account deployment create \
    -n $accountName \
    -g $resourceGroupName \
    --deployment-name Phi-3.5-vision-instruct \
    --model-name Phi-3.5-vision-instruct \
    --model-version 2 \
    --model-format Microsoft \
    --sku-capacity 1 \
    --sku-name GlobalStandard
az cognitiveservices account deployment create \
    -n $accountName \
    -g $resourceGroupName \
    --deployment-name Phi-3.5-vision-instruct \
    --model-name Phi-3.5-vision-instruct \
    --model-version 2 \
    --model-format Microsoft \
    --sku-capacity 1 \
    --sku-name GlobalStandard
The model is ready to be consumed.
The model is ready to be consumed.
You can deploy the same model multiple times if needed as long as it's under a different deployment name. This capability might be useful in case you want to test different configurations for a given model, including content safety.
Use the model
Deployed models in Azure AI model inference can be consumed using theAzure AI model's inference endpointfor the resource. When constructing your request, indicate the parametermodeland insert the model deployment name you have created. You can programmatically get the URI for the inference endpoint using the following code:
model
Inference endpoint
az cognitiveservices account show  -n $accountName -g $resourceGroupName | jq '.properties.endpoints["Azure AI Model Inference API"]'
az cognitiveservices account show  -n $accountName -g $resourceGroupName | jq '.properties.endpoints["Azure AI Model Inference API"]'
To make requests to the Azure AI model inference endpoint, append the routemodels, for examplehttps://<resource>.services.ai.azure.com/models. You can see the API reference for the endpoint atAzure AI model inference API reference page.
models
https://<resource>.services.ai.azure.com/models
Inference keys
az cognitiveservices account keys list  -n $accountName -g $resourceGroupName
az cognitiveservices account keys list  -n $accountName -g $resourceGroupName
Manage deployments
You can see all the deployments available using the CLI:
Run the following command to see all the active deployments:az cognitiveservices account deployment list -n $accountName -g $resourceGroupName
Run the following command to see all the active deployments:
az cognitiveservices account deployment list -n $accountName -g $resourceGroupName
az cognitiveservices account deployment list -n $accountName -g $resourceGroupName
You can see the details of a given deployment:az cognitiveservices account deployment show \
    --deployment-name "Phi-3.5-vision-instruct" \
    -n $accountName \
    -g $resourceGroupName
You can see the details of a given deployment:
az cognitiveservices account deployment show \
    --deployment-name "Phi-3.5-vision-instruct" \
    -n $accountName \
    -g $resourceGroupName
az cognitiveservices account deployment show \
    --deployment-name "Phi-3.5-vision-instruct" \
    -n $accountName \
    -g $resourceGroupName
You can delete a given deployment as follows:az cognitiveservices account deployment delete \
    --deployment-name "Phi-3.5-vision-instruct" \
    -n $accountName \
    -g $resourceGroupName
You can delete a given deployment as follows:
az cognitiveservices account deployment delete \
    --deployment-name "Phi-3.5-vision-instruct" \
    -n $accountName \
    -g $resourceGroupName
az cognitiveservices account deployment delete \
    --deployment-name "Phi-3.5-vision-instruct" \
    -n $accountName \
    -g $resourceGroupName
Important
Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, seeSupplemental Terms of Use for Microsoft Azure Previews.
You can decide and configure which models are available for inference in the inference endpoint. When a given model is configured, you can then generate predictions from it by indicating its model name or deployment name on your requests. No further changes are required in your code to use it.
In this article, you'll learn how to add a new model to Azure AI model inference in Azure AI Foundry.
Prerequisites
To complete this article, you need:
An Azure subscription. If you're usingGitHub Models, you can upgrade your experience and create an Azure subscription in the process. ReadUpgrade from GitHub Models to Azure AI model inferenceif that's your case.
An Azure subscription. If you're usingGitHub Models, you can upgrade your experience and create an Azure subscription in the process. ReadUpgrade from GitHub Models to Azure AI model inferenceif that's your case.
An Azure AI services resource.
An Azure AI services resource.
Install theAzure CLI.
Install theAzure CLI.
Identify the following information:Your Azure subscription ID.Your Azure AI Services resource name.The resource group where the Azure AI Services resource is deployed.The model name, provider, version, and SKU you would like to deploy. You can use the Azure AI Foundry portal or the Azure CLI to identify it. In this example we deploy the following model:Model name::Phi-3.5-vision-instructProvider:MicrosoftVersion:2Deployment type: Global standard
Identify the following information:
Your Azure subscription ID.
Your Azure subscription ID.
Your Azure AI Services resource name.
Your Azure AI Services resource name.
The resource group where the Azure AI Services resource is deployed.
The resource group where the Azure AI Services resource is deployed.
The model name, provider, version, and SKU you would like to deploy. You can use the Azure AI Foundry portal or the Azure CLI to identify it. In this example we deploy the following model:Model name::Phi-3.5-vision-instructProvider:MicrosoftVersion:2Deployment type: Global standard
The model name, provider, version, and SKU you would like to deploy. You can use the Azure AI Foundry portal or the Azure CLI to identify it. In this example we deploy the following model:
Model name::Phi-3.5-vision-instruct
Phi-3.5-vision-instruct
Provider:Microsoft
Microsoft
Version:2
2
Deployment type: Global standard
About this tutorial
The example in this article is based on code samples contained in theAzure-Samples/azureai-model-inference-biceprepository. To run the commands locally without having to copy or paste file content, use the following commands to clone the repository and go to the folder for your coding language:
git clone https://github.com/Azure-Samples/azureai-model-inference-bicep
git clone https://github.com/Azure-Samples/azureai-model-inference-bicep
The files for this example are in:
cd azureai-model-inference-bicep/infra
cd azureai-model-inference-bicep/infra
Add the model
Use the templateai-services-deployment-template.bicepto describe model deployments:ai-services-deployment-template.bicep@description('Name of the Azure AI services account')
param accountName string

@description('Name of the model to deploy')
param modelName string

@description('Version of the model to deploy')
param modelVersion string

@allowed([
  'AI21 Labs'
  'Cohere'
  'Core42'
  'DeepSeek'
  'Meta'
  'Microsoft'
  'Mistral AI'
  'OpenAI'
])
@description('Model provider')
param modelPublisherFormat string

@allowed([
    'GlobalStandard'
    'Standard'
    'GlobalProvisioned'
    'Provisioned'
])
@description('Model deployment SKU name')
param skuName string = 'GlobalStandard'

@description('Content filter policy name')
param contentFilterPolicyName string = 'Microsoft.DefaultV2'

@description('Model deployment capacity')
param capacity int = 1

resource modelDeployment 'Microsoft.CognitiveServices/accounts/deployments@2024-04-01-preview' = {
  name: '${accountName}/${modelName}'
  sku: {
    name: skuName
    capacity: capacity
  }
  properties: {
    model: {
      format: modelPublisherFormat
      name: modelName
      version: modelVersion
    }
    raiPolicyName: contentFilterPolicyName == null ? 'Microsoft.Nill' : contentFilterPolicyName
  }
}
Use the templateai-services-deployment-template.bicepto describe model deployments:
ai-services-deployment-template.bicep
ai-services-deployment-template.bicep
@description('Name of the Azure AI services account')
param accountName string

@description('Name of the model to deploy')
param modelName string

@description('Version of the model to deploy')
param modelVersion string

@allowed([
  'AI21 Labs'
  'Cohere'
  'Core42'
  'DeepSeek'
  'Meta'
  'Microsoft'
  'Mistral AI'
  'OpenAI'
])
@description('Model provider')
param modelPublisherFormat string

@allowed([
    'GlobalStandard'
    'Standard'
    'GlobalProvisioned'
    'Provisioned'
])
@description('Model deployment SKU name')
param skuName string = 'GlobalStandard'

@description('Content filter policy name')
param contentFilterPolicyName string = 'Microsoft.DefaultV2'

@description('Model deployment capacity')
param capacity int = 1

resource modelDeployment 'Microsoft.CognitiveServices/accounts/deployments@2024-04-01-preview' = {
  name: '${accountName}/${modelName}'
  sku: {
    name: skuName
    capacity: capacity
  }
  properties: {
    model: {
      format: modelPublisherFormat
      name: modelName
      version: modelVersion
    }
    raiPolicyName: contentFilterPolicyName == null ? 'Microsoft.Nill' : contentFilterPolicyName
  }
}
@description('Name of the Azure AI services account')
param accountName string

@description('Name of the model to deploy')
param modelName string

@description('Version of the model to deploy')
param modelVersion string

@allowed([
  'AI21 Labs'
  'Cohere'
  'Core42'
  'DeepSeek'
  'Meta'
  'Microsoft'
  'Mistral AI'
  'OpenAI'
])
@description('Model provider')
param modelPublisherFormat string

@allowed([
    'GlobalStandard'
    'Standard'
    'GlobalProvisioned'
    'Provisioned'
])
@description('Model deployment SKU name')
param skuName string = 'GlobalStandard'

@description('Content filter policy name')
param contentFilterPolicyName string = 'Microsoft.DefaultV2'

@description('Model deployment capacity')
param capacity int = 1

resource modelDeployment 'Microsoft.CognitiveServices/accounts/deployments@2024-04-01-preview' = {
  name: '${accountName}/${modelName}'
  sku: {
    name: skuName
    capacity: capacity
  }
  properties: {
    model: {
      format: modelPublisherFormat
      name: modelName
      version: modelVersion
    }
    raiPolicyName: contentFilterPolicyName == null ? 'Microsoft.Nill' : contentFilterPolicyName
  }
}
Run the deployment:RESOURCE_GROUP="<resource-group-name>"
ACCOUNT_NAME="<azure-ai-model-inference-name>" 
MODEL_NAME="Phi-3.5-vision-instruct"
PROVIDER="Microsoft"
VERSION=2

az deployment group create \
    --resource-group $RESOURCE_GROUP \
    --template-file ai-services-deployment-template.bicep \
    --parameters accountName=$ACCOUNT_NAME modelName=$MODEL_NAME modelVersion=$VERSION modelPublisherFormat=$PROVIDER
Run the deployment:
RESOURCE_GROUP="<resource-group-name>"
ACCOUNT_NAME="<azure-ai-model-inference-name>" 
MODEL_NAME="Phi-3.5-vision-instruct"
PROVIDER="Microsoft"
VERSION=2

az deployment group create \
    --resource-group $RESOURCE_GROUP \
    --template-file ai-services-deployment-template.bicep \
    --parameters accountName=$ACCOUNT_NAME modelName=$MODEL_NAME modelVersion=$VERSION modelPublisherFormat=$PROVIDER
RESOURCE_GROUP="<resource-group-name>"
ACCOUNT_NAME="<azure-ai-model-inference-name>" 
MODEL_NAME="Phi-3.5-vision-instruct"
PROVIDER="Microsoft"
VERSION=2

az deployment group create \
    --resource-group $RESOURCE_GROUP \
    --template-file ai-services-deployment-template.bicep \
    --parameters accountName=$ACCOUNT_NAME modelName=$MODEL_NAME modelVersion=$VERSION modelPublisherFormat=$PROVIDER
Use the model
Deployed models in Azure AI model inference can be consumed using theAzure AI model's inference endpointfor the resource. When constructing your request, indicate the parametermodeland insert the model deployment name you have created.
model
Next steps
Develop applications using Azure AI model inference service in Azure AI services
Feedback
Was this page helpful?
Additional resources