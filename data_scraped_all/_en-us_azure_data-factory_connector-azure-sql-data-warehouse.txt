Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Copy and transform data in Azure Synapse Analytics by using Azure Data Factory or Synapse pipelines
Article
2025-02-13
36 contributors
In this article
APPLIES TO:Azure Data FactoryAzure Synapse Analytics
Tip
Try outData Factory in Microsoft Fabric, an all-in-one analytics solution for enterprises.Microsoft Fabriccovers everything from data movement to data science, real-time analytics, business intelligence, and reporting. Learn how tostart a new trialfor free!
This article outlines how to use Copy Activity in Azure Data Factory or Synapse pipelines to copy data from and to Azure Synapse Analytics, and use Data Flow to transform data in Azure Data Lake Storage Gen2. To learn about Azure Data Factory, read theintroductory article.
Supported capabilities
This Azure Synapse Analytics connector is supported for the following capabilities:
â  Azure integration runtime â¡ Self-hosted integration runtime
For Copy activity, this Azure Synapse Analytics connector supports these functions:
Copy data by using SQL authentication and Microsoft Entra Application token authentication with a service principal or managed identities for Azure resources.
As a source, retrieve data by using a SQL query or stored procedure. You can also choose to parallel copy from an Azure Synapse Analytics source, see theParallel copy from Azure Synapse Analyticssection for details.
As a sink, load data by usingCOPY statementorPolyBaseor bulk insert. We recommend COPY statement or PolyBase for better copy performance. The connector also supports automatically creating destination table with DISTRIBUTION = ROUND_ROBIN if not exists based on the source schema.
Important
If you copy data by using an Azure Integration Runtime, configure aserver-level firewall ruleso that Azure services can access thelogical SQL server.
If you copy data by using a self-hosted integration runtime, configure the firewall to allow the appropriate IP range. This range includes the machine's IP that is used to connect to Azure Synapse Analytics.
Get started
Tip
To achieve best performance, use PolyBase or COPY statement to load data into Azure Synapse Analytics. TheUse PolyBase to load data into Azure Synapse AnalyticsandUse COPY statement to load data into Azure Synapse Analyticssections have details. For a walkthrough with a use case, seeLoad 1 TB into Azure Synapse Analytics under 15 minutes with Azure Data Factory.
To perform the Copy activity with a pipeline, you can use one of the following tools or SDKs:
The Copy Data tool
The Azure portal
The .NET SDK
The Python SDK
Azure PowerShell
The REST API
The Azure Resource Manager template
Create an Azure Synapse Analytics linked service using UI
Use the following steps to create an Azure Synapse Analytics linked service in the Azure portal UI.
Browse to the Manage tab in your Azure Data Factory or Synapse workspace and select Linked Services, then click New:Azure Data FactoryAzure Synapse
Browse to the Manage tab in your Azure Data Factory or Synapse workspace and select Linked Services, then click New:
Azure Data Factory
Azure Synapse


Search for Synapse and select the Azure Synapse Analytics connector.
Search for Synapse and select the Azure Synapse Analytics connector.

Configure the service details, test the connection, and create the new linked service.
Configure the service details, test the connection, and create the new linked service.

Connector configuration details
The following sections provide details about properties that define Data Factory and Synapse pipeline entities specific to an Azure Synapse Analytics connector.
Linked service properties
The Azure Synapse Analytics connectorRecommendedversion supports TLS 1.3. Refer to thissectionto upgrade your Azure Synapse Analytics connector version fromLegacyone. For the property details, see the corresponding sections.
Recommended version
Legacy version
Tip
When creating linked service for aserverlessSQL pool in Azure Synapse from the Azure portal:
ForAccount Selection Method, chooseEnter manually.
Paste thefully qualified domain nameof the serverless endpoint. You can find this in the Azure portal Overview page for your Synapse workspace, in the properties underServerless SQL endpoint. For example,myserver-ondemand.sql-azuresynapse.net.
myserver-ondemand.sql-azuresynapse.net
ForDatabase name, provide the database name in the serverless SQL pool.
Tip
If you hit error with error code as "UserErrorFailedToConnectToSqlServer" and message like "The session limit for the database is XXX and has been reached.", addPooling=falseto your connection string and try again.
Pooling=false
Recommended version
These generic properties are supported for an Azure Synapse Analytics linked service when you applyRecommendedversion:
For additional connection properties, see the table below:
ReadOnly
ReadWrite
true
false
false
true
true
false
true
false
true
false
true
true
false
true
false
To use SQL authentication, in addition to the generic properties that are described in the preceding section, specify the following properties:
Example: using SQL authentication
{
    "name": "AzureSqlDWLinkedService",
    "properties": {
        "type": "AzureSqlDW",
        "typeProperties": {
            "server": "<name or network address of the SQL server instance>",
            "database": "<database name>",
            "encrypt": "<encrypt>",
            "trustServerCertificate": false,
            "authenticationType": "SQL",
            "userName": "<user name>",
            "password": {
                "type": "SecureString",
                "value": "<password>"
            }
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
{
    "name": "AzureSqlDWLinkedService",
    "properties": {
        "type": "AzureSqlDW",
        "typeProperties": {
            "server": "<name or network address of the SQL server instance>",
            "database": "<database name>",
            "encrypt": "<encrypt>",
            "trustServerCertificate": false,
            "authenticationType": "SQL",
            "userName": "<user name>",
            "password": {
                "type": "SecureString",
                "value": "<password>"
            }
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
Example: password in Azure Key Vault
{
    "name": "AzureSqlDWLinkedService",
    "properties": {
        "type": "AzureSqlDW",
        "typeProperties": {
            "server": "<name or network address of the SQL server instance>",
            "database": "<database name>",
            "encrypt": "<encrypt>",
            "trustServerCertificate": false,
            "authenticationType": "SQL",
            "userName": "<user name>",
            "password": {
                "type": "AzureKeyVaultSecret",
                "store": {
                    "referenceName": "<Azure Key Vault linked service name>",
                    "type": "LinkedServiceReference"
                },
                "secretName": "<secretName>"
            }
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
{
    "name": "AzureSqlDWLinkedService",
    "properties": {
        "type": "AzureSqlDW",
        "typeProperties": {
            "server": "<name or network address of the SQL server instance>",
            "database": "<database name>",
            "encrypt": "<encrypt>",
            "trustServerCertificate": false,
            "authenticationType": "SQL",
            "userName": "<user name>",
            "password": {
                "type": "AzureKeyVaultSecret",
                "store": {
                    "referenceName": "<Azure Key Vault linked service name>",
                    "type": "LinkedServiceReference"
                },
                "secretName": "<secretName>"
            }
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
To use service principal authentication, in addition to the generic properties that are described in the preceding section, specify the following properties:
AzurePublic
AzureChina
AzureUsGovernment
AzureGermany
You also need to follow the steps below:
Create a Microsoft Entra applicationfrom the Azure portal. Make note of the application name and the following values that define the linked service:Application IDApplication keyTenant ID
Create a Microsoft Entra applicationfrom the Azure portal. Make note of the application name and the following values that define the linked service:
Application ID
Application key
Tenant ID
Provision a Microsoft Entra administratorfor your server in the Azure portal if you haven't already done so. The Microsoft Entra administrator can be a Microsoft Entra user or Microsoft Entra group. If you grant the group with managed identity an admin role, skip steps 3 and 4. The administrator will have full access to the database.
Provision a Microsoft Entra administratorfor your server in the Azure portal if you haven't already done so. The Microsoft Entra administrator can be a Microsoft Entra user or Microsoft Entra group. If you grant the group with managed identity an admin role, skip steps 3 and 4. The administrator will have full access to the database.
Create contained database usersfor the service principal. Connect to the data warehouse from or to which you want to copy data by using tools like SSMS, with a Microsoft Entra identity that has at least ALTER ANY USER permission. Run the following T-SQL:CREATE USER [your_application_name] FROM EXTERNAL PROVIDER;
Create contained database usersfor the service principal. Connect to the data warehouse from or to which you want to copy data by using tools like SSMS, with a Microsoft Entra identity that has at least ALTER ANY USER permission. Run the following T-SQL:
CREATE USER [your_application_name] FROM EXTERNAL PROVIDER;
CREATE USER [your_application_name] FROM EXTERNAL PROVIDER;
Grant the service principal needed permissionsas you normally do for SQL users or others. Run the following code, or refer to more optionshere. If you want to use PolyBase to load the data, learn therequired database permission.EXEC sp_addrolemember db_owner, [your application name];
Grant the service principal needed permissionsas you normally do for SQL users or others. Run the following code, or refer to more optionshere. If you want to use PolyBase to load the data, learn therequired database permission.
EXEC sp_addrolemember db_owner, [your application name];
EXEC sp_addrolemember db_owner, [your application name];
Configure an Azure Synapse Analytics linked servicein an Azure Data Factory or Synapse workspace.
Configure an Azure Synapse Analytics linked servicein an Azure Data Factory or Synapse workspace.
{
    "name": "AzureSqlDWLinkedService",
    "properties": {
        "type": "AzureSqlDW",
        "typeProperties": {
            "connectionString": "Server=tcp:<servername>.database.windows.net,1433;Database=<databasename>;Connection Timeout=30",
            "servicePrincipalId": "<service principal id>",
            "servicePrincipalCredential": {
                "type": "SecureString",
                "value": "<application key>"
            },
            "tenant": "<tenant info, e.g. microsoft.onmicrosoft.com>"
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
{
    "name": "AzureSqlDWLinkedService",
    "properties": {
        "type": "AzureSqlDW",
        "typeProperties": {
            "connectionString": "Server=tcp:<servername>.database.windows.net,1433;Database=<databasename>;Connection Timeout=30",
            "servicePrincipalId": "<service principal id>",
            "servicePrincipalCredential": {
                "type": "SecureString",
                "value": "<application key>"
            },
            "tenant": "<tenant info, e.g. microsoft.onmicrosoft.com>"
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
A data factory or Synapse workspace can be associated with asystem-assigned managed identity for Azure resourcesthat represents the resource. You can use this managed identity for Azure Synapse Analytics authentication. The designated resource can access and copy data from or to your data warehouse by using this identity.
To use system-assigned managed identity authentication, specify the generic properties that are described in the preceding section, and follow these steps.
Provision a Microsoft Entra administratorfor your server on the Azure portal if you haven't already done so. The Microsoft Entra administrator can be a Microsoft Entra user or Microsoft Entra group. If you grant the group with system-assigned managed identity an admin role, skip steps 3 and 4. The administrator will have full access to the database.
Provision a Microsoft Entra administratorfor your server on the Azure portal if you haven't already done so. The Microsoft Entra administrator can be a Microsoft Entra user or Microsoft Entra group. If you grant the group with system-assigned managed identity an admin role, skip steps 3 and 4. The administrator will have full access to the database.
Create contained database usersfor the system-assigned managed identity. Connect to the data warehouse from or to which you want to copy data by using tools like SSMS, with a Microsoft Entra identity that has at least ALTER ANY USER permission. Run the following T-SQL.CREATE USER [your_resource_name] FROM EXTERNAL PROVIDER;
Create contained database usersfor the system-assigned managed identity. Connect to the data warehouse from or to which you want to copy data by using tools like SSMS, with a Microsoft Entra identity that has at least ALTER ANY USER permission. Run the following T-SQL.
CREATE USER [your_resource_name] FROM EXTERNAL PROVIDER;
CREATE USER [your_resource_name] FROM EXTERNAL PROVIDER;
Grant the system-assigned managed identity needed permissionsas you normally do for SQL users and others. Run the following code, or refer to more optionshere. If you want to use PolyBase to load the data, learn therequired database permission.EXEC sp_addrolemember db_owner, [your_resource_name];
Grant the system-assigned managed identity needed permissionsas you normally do for SQL users and others. Run the following code, or refer to more optionshere. If you want to use PolyBase to load the data, learn therequired database permission.
EXEC sp_addrolemember db_owner, [your_resource_name];
EXEC sp_addrolemember db_owner, [your_resource_name];
Configure an Azure Synapse Analytics linked service.
Configure an Azure Synapse Analytics linked service.
Example:
{
    "name": "AzureSqlDWLinkedService",
    "properties": {
        "type": "AzureSqlDW",
        "typeProperties": {
            "server": "<name or network address of the SQL server instance>",
            "database": "<database name>",
            "encrypt": "<encrypt>",
            "trustServerCertificate": false,
            "authenticationType": "SystemAssignedManagedIdentity"
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
{
    "name": "AzureSqlDWLinkedService",
    "properties": {
        "type": "AzureSqlDW",
        "typeProperties": {
            "server": "<name or network address of the SQL server instance>",
            "database": "<database name>",
            "encrypt": "<encrypt>",
            "trustServerCertificate": false,
            "authenticationType": "SystemAssignedManagedIdentity"
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
A data factory or Synapse workspace can be associated with auser-assigned managed identitiesthat represents the resource. You can use this managed identity for Azure Synapse Analytics authentication. The designated resource can access and copy data from or to your data warehouse by using this identity.
To use user-assigned managed identity authentication, in addition to the generic properties that are described in the preceding section, specify the following properties:
You also need to follow the steps below:
Provision a Microsoft Entra administratorfor your server on the Azure portal if you haven't already done so. The Microsoft Entra administrator can be a Microsoft Entra user or Microsoft Entra group. If you grant the group with user-assigned managed identity an admin role, skip steps 3. The administrator will have full access to the database.
Provision a Microsoft Entra administratorfor your server on the Azure portal if you haven't already done so. The Microsoft Entra administrator can be a Microsoft Entra user or Microsoft Entra group. If you grant the group with user-assigned managed identity an admin role, skip steps 3. The administrator will have full access to the database.
Create contained database usersfor the user-assigned managed identity. Connect to the data warehouse from or to which you want to copy data by using tools like SSMS, with a Microsoft Entra identity that has at least ALTER ANY USER permission. Run the following T-SQL.CREATE USER [your_resource_name] FROM EXTERNAL PROVIDER;
Create contained database usersfor the user-assigned managed identity. Connect to the data warehouse from or to which you want to copy data by using tools like SSMS, with a Microsoft Entra identity that has at least ALTER ANY USER permission. Run the following T-SQL.
CREATE USER [your_resource_name] FROM EXTERNAL PROVIDER;
CREATE USER [your_resource_name] FROM EXTERNAL PROVIDER;
Create one or multiple user-assigned managed identitiesandgrant the user-assigned managed identity needed permissionsas you normally do for SQL users and others. Run the following code, or refer to more optionshere. If you want to use PolyBase to load the data, learn therequired database permission.EXEC sp_addrolemember db_owner, [your_resource_name];
Create one or multiple user-assigned managed identitiesandgrant the user-assigned managed identity needed permissionsas you normally do for SQL users and others. Run the following code, or refer to more optionshere. If you want to use PolyBase to load the data, learn therequired database permission.
EXEC sp_addrolemember db_owner, [your_resource_name];
EXEC sp_addrolemember db_owner, [your_resource_name];
Assign one or multiple user-assigned managed identities to your data factory andcreate credentialsfor each user-assigned managed identity.
Assign one or multiple user-assigned managed identities to your data factory andcreate credentialsfor each user-assigned managed identity.
Configure an Azure Synapse Analytics linked service.
Configure an Azure Synapse Analytics linked service.
Example
{
    "name": "AzureSqlDWLinkedService",
    "properties": {
        "type": "AzureSqlDW",
        "typeProperties": {
            "server": "<name or network address of the SQL server instance>",
            "database": "<database name>",
            "encrypt": "<encrypt>",
            "trustServerCertificate": false,
            "authenticationType": "UserAssignedManagedIdentity",
            "credential": {
                "referenceName": "credential1",
                "type": "CredentialReference"
            }
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
{
    "name": "AzureSqlDWLinkedService",
    "properties": {
        "type": "AzureSqlDW",
        "typeProperties": {
            "server": "<name or network address of the SQL server instance>",
            "database": "<database name>",
            "encrypt": "<encrypt>",
            "trustServerCertificate": false,
            "authenticationType": "UserAssignedManagedIdentity",
            "credential": {
                "referenceName": "credential1",
                "type": "CredentialReference"
            }
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
Legacy version
These generic properties are supported for an Azure Synapse Analytics linked service when you applyLegacyversion:
password
For different authentication types, refer to the following sections on specific properties and prerequisites respectively:
SQL authentication for the legacy version
Service principal authentication for the legacy version
System-assigned managed identity authentication for the legacy version
User-assigned managed identity authentication for the legacy version
To use SQL authentication, specify the generic properties that are described in the preceding section.
To use service principal authentication, in addition to the generic properties that are described in the preceding section, specify the following properties:
You also need to follow the steps inService principal authenticationto grant the corresponding permission.
To use system-assigned managed identity authentication, follow the same step for the recommended version inSystem-assigned managed identity authentication.
To use user-assigned managed identity authentication, follow the same step for the recommended version inUser-assigned managed identity authentication.
Dataset properties
For a full list of sections and properties available for defining datasets, see theDatasetsarticle.
The following properties are supported for Azure Synapse Analytics dataset:
schema
table
Dataset properties example
{
    "name": "AzureSQLDWDataset",
    "properties":
    {
        "type": "AzureSqlDWTable",
        "linkedServiceName": {
            "referenceName": "<Azure Synapse Analytics linked service name>",
            "type": "LinkedServiceReference"
        },
        "schema": [ < physical schema, optional, retrievable during authoring > ],
        "typeProperties": {
            "schema": "<schema_name>",
            "table": "<table_name>"
        }
    }
}
{
    "name": "AzureSQLDWDataset",
    "properties":
    {
        "type": "AzureSqlDWTable",
        "linkedServiceName": {
            "referenceName": "<Azure Synapse Analytics linked service name>",
            "type": "LinkedServiceReference"
        },
        "schema": [ < physical schema, optional, retrievable during authoring > ],
        "typeProperties": {
            "schema": "<schema_name>",
            "table": "<table_name>"
        }
    }
}
Copy Activity properties
For a full list of sections and properties available for defining activities, see thePipelinesarticle. This section provides a list of properties supported by the Azure Synapse Analytics source and sink.
Azure Synapse Analytics as the source
Tip
To load data from Azure Synapse Analytics efficiently by using data partitioning, learn more fromParallel copy from Azure Synapse Analytics.
To copy data from Azure Synapse Analytics, set thetypeproperty in the Copy Activity source toSqlDWSource. The following properties are supported in the Copy Activitysourcesection:
select * from MyTable
None
parallelCopies
None
partitionSettings
int
smallint
bigint
date
smalldatetime
datetime
datetime2
datetimeoffset
DynamicRange
?DfDynamicRangePartitionCondition
DynamicRange
DynamicRange
Note the following point:
When using stored procedure in source to retrieve data, note if your stored procedure is designed as returning different schema when different parameter value is passed in, you may encounter failure or see unexpected result when importing schema from UI or when copying data to SQL database with auto table creation.
"activities":[
    {
        "name": "CopyFromAzureSQLDW",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "<Azure Synapse Analytics input dataset name>",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "<output dataset name>",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "SqlDWSource",
                "sqlReaderQuery": "SELECT * FROM MyTable"
            },
            "sink": {
                "type": "<sink type>"
            }
        }
    }
]
"activities":[
    {
        "name": "CopyFromAzureSQLDW",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "<Azure Synapse Analytics input dataset name>",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "<output dataset name>",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "SqlDWSource",
                "sqlReaderQuery": "SELECT * FROM MyTable"
            },
            "sink": {
                "type": "<sink type>"
            }
        }
    }
]
"activities":[
    {
        "name": "CopyFromAzureSQLDW",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "<Azure Synapse Analytics input dataset name>",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "<output dataset name>",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "SqlDWSource",
                "sqlReaderStoredProcedureName": "CopyTestSrcStoredProcedureWithParameters",
                "storedProcedureParameters": {
                    "stringData": { "value": "str3" },
                    "identifier": { "value": "$$Text.Format('{0:yyyy}', <datetime parameter>)", "type": "Int"}
                }
            },
            "sink": {
                "type": "<sink type>"
            }
        }
    }
]
"activities":[
    {
        "name": "CopyFromAzureSQLDW",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "<Azure Synapse Analytics input dataset name>",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "<output dataset name>",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "SqlDWSource",
                "sqlReaderStoredProcedureName": "CopyTestSrcStoredProcedureWithParameters",
                "storedProcedureParameters": {
                    "stringData": { "value": "str3" },
                    "identifier": { "value": "$$Text.Format('{0:yyyy}', <datetime parameter>)", "type": "Int"}
                }
            },
            "sink": {
                "type": "<sink type>"
            }
        }
    }
]
CREATE PROCEDURE CopyTestSrcStoredProcedureWithParameters
(
    @stringData varchar(20),
    @identifier int
)
AS
SET NOCOUNT ON;
BEGIN
    select *
    from dbo.UnitTestSrcTable
    where dbo.UnitTestSrcTable.stringData != stringData
    and dbo.UnitTestSrcTable.identifier != identifier
END
GO
CREATE PROCEDURE CopyTestSrcStoredProcedureWithParameters
(
    @stringData varchar(20),
    @identifier int
)
AS
SET NOCOUNT ON;
BEGIN
    select *
    from dbo.UnitTestSrcTable
    where dbo.UnitTestSrcTable.stringData != stringData
    and dbo.UnitTestSrcTable.identifier != identifier
END
GO
Azure Synapse Analytics as sink
Azure Data Factory and Synapse pipelines support three ways to load data into Azure Synapse Analytics.
Use COPY statement
Use PolyBase
Use bulk insert
The fastest and most scalable way to load data is through theCOPY statementor thePolyBase.
To copy data to Azure Synapse Analytics, set the sink type in Copy Activity toSqlDWSink. The following properties are supported in the Copy Activitysinksection:
allowCopyCommand
allowPolyBase
allowPolybase
allowCopyCommand
allowPolyBase
allowCopyCommand
none
autoCreate
true
false
Upsert
upsertSettings
"sink": {
    "type": "SqlDWSink",
    "allowPolyBase": true,
    "polyBaseSettings":
    {
        "rejectType": "percentage",
        "rejectValue": 10.0,
        "rejectSampleValue": 100,
        "useTypeDefault": true
    }
}
"sink": {
    "type": "SqlDWSink",
    "allowPolyBase": true,
    "polyBaseSettings":
    {
        "rejectType": "percentage",
        "rejectValue": 10.0,
        "rejectSampleValue": 100,
        "useTypeDefault": true
    }
}
"sink": {
    "type": "SqlDWSink",
    "writeBehavior": "Upsert",
    "upsertSettings": {
        "keys": [
             "<column name>"
        ],
        "interimSchemaName": "<interim schema name>"
    },
}
"sink": {
    "type": "SqlDWSink",
    "writeBehavior": "Upsert",
    "upsertSettings": {
        "keys": [
             "<column name>"
        ],
        "interimSchemaName": "<interim schema name>"
    },
}
Parallel copy from Azure Synapse Analytics
The Azure Synapse Analytics connector in copy activity provides built-in data partitioning to copy data in parallel. You can find data partitioning options on theSourcetab of the copy activity.

When you enable partitioned copy, copy activity runs parallel queries against your Azure Synapse Analytics source to load data by partitions. The parallel degree is controlled by theparallelCopiessetting on the copy activity. For example, if you setparallelCopiesto four, the service concurrently generates and runs four queries based on your specified partition option and settings, and each query retrieves a portion of data from your Azure Synapse Analytics.
parallelCopies
parallelCopies
You are suggested to enable parallel copy with data partitioning especially when you load large amount of data from your Azure Synapse Analytics. The following are suggested configurations for different scenarios. When copying data into file-based data store, it's recommended to write to a folder as multiple files (only specify folder name), in which case the performance is better than writing to a single file.
SELECT * FROM <TableName> WHERE ?DfDynamicRangePartitionCondition AND <your_additional_where_clause>
SELECT * FROM <TableName> WHERE ?DfDynamicRangePartitionCondition
SELECT <column_list> FROM <TableName> WHERE ?DfDynamicRangePartitionCondition AND <your_additional_where_clause>
SELECT <column_list> FROM (<your_sub_query>) AS T WHERE ?DfDynamicRangePartitionCondition AND <your_additional_where_clause>
SELECT <column_list> FROM (SELECT <your_sub_query_column_list> FROM <TableName> WHERE ?DfDynamicRangePartitionCondition) AS T
Best practices to load data with partition option:
Choose distinctive column as partition column (like primary key or unique key) to avoid data skew.
If the table has built-in partition, use partition option "Physical partitions of table" to get better performance.
If you use Azure Integration Runtime to copy data, you can set larger "Data Integration Units (DIU)" (>4) to utilize more computing resource. Check the applicable scenarios there.
"Degree of copy parallelism" control the partition numbers, setting this number too large sometime hurts the performance, recommend setting this number as (DIU or number of Self-hosted IR nodes) * (2 to 4).
Note Azure Synapse Analytics can execute a maximum of 32 queries at a moment, setting "Degree of copy parallelism" too large may cause a Synapse throttling issue.
Example: full load from large table with physical partitions
"source": {
    "type": "SqlDWSource",
    "partitionOption": "PhysicalPartitionsOfTable"
}
"source": {
    "type": "SqlDWSource",
    "partitionOption": "PhysicalPartitionsOfTable"
}
Example: query with dynamic range partition
"source": {
    "type": "SqlDWSource",
    "query":â¯"SELECT * FROM <TableName> WHERE ?DfDynamicRangePartitionCondition AND <your_additional_where_clause>",
    "partitionOption": "DynamicRange",
    "partitionSettings": {
        "partitionColumnName": "<partition_column_name>",
        "partitionUpperBound": "<upper_value_of_partition_column (optional) to decide the partition stride, not as data filter>",
        "partitionLowerBound": "<lower_value_of_partition_column (optional) to decide the partition stride, not as data filter>"
    }
}
"source": {
    "type": "SqlDWSource",
    "query":â¯"SELECT * FROM <TableName> WHERE ?DfDynamicRangePartitionCondition AND <your_additional_where_clause>",
    "partitionOption": "DynamicRange",
    "partitionSettings": {
        "partitionColumnName": "<partition_column_name>",
        "partitionUpperBound": "<upper_value_of_partition_column (optional) to decide the partition stride, not as data filter>",
        "partitionLowerBound": "<lower_value_of_partition_column (optional) to decide the partition stride, not as data filter>"
    }
}
Sample query to check physical partition
SELECT DISTINCT s.name AS SchemaName, t.name AS TableName, c.name AS ColumnName, CASE WHEN c.name IS NULL THEN 'no' ELSE 'yes' END AS HasPartition
FROM sys.tables AS t
LEFT JOIN sys.objects AS o ON t.object_id = o.object_id
LEFT JOIN sys.schemas AS s ON o.schema_id = s.schema_id
LEFT JOIN sys.indexes AS i ON t.object_id = i.object_id
LEFT JOIN sys.index_columns AS ic ON ic.partition_ordinal > 0 AND ic.index_id = i.index_id AND ic.object_id = t.object_id
LEFT JOIN sys.columns AS c ON c.object_id = ic.object_id AND c.column_id = ic.column_id
LEFT JOIN sys.types AS y ON c.system_type_id = y.system_type_id
WHERE s.name='[your schema]' AND t.name = '[your table name]'
SELECT DISTINCT s.name AS SchemaName, t.name AS TableName, c.name AS ColumnName, CASE WHEN c.name IS NULL THEN 'no' ELSE 'yes' END AS HasPartition
FROM sys.tables AS t
LEFT JOIN sys.objects AS o ON t.object_id = o.object_id
LEFT JOIN sys.schemas AS s ON o.schema_id = s.schema_id
LEFT JOIN sys.indexes AS i ON t.object_id = i.object_id
LEFT JOIN sys.index_columns AS ic ON ic.partition_ordinal > 0 AND ic.index_id = i.index_id AND ic.object_id = t.object_id
LEFT JOIN sys.columns AS c ON c.object_id = ic.object_id AND c.column_id = ic.column_id
LEFT JOIN sys.types AS y ON c.system_type_id = y.system_type_id
WHERE s.name='[your schema]' AND t.name = '[your table name]'
If the table has physical partition, you would see "HasPartition" as "yes".
Use COPY statement to load data into Azure Synapse Analytics
UsingCOPY statementis a simple and flexible way to load data into Azure Synapse Analytics with high throughput. To learn more details, checkBulk load data using the COPY statement
If your source data is inAzure Blob or Azure Data Lake Storage Gen2, and theformat is COPY statement compatible, you can use copy activity to directly invoke COPY statement to let Azure Synapse Analytics pull the data from source. For details, seeDirect copy by using COPY statement.
If your source data store and format isn't originally supported by COPY statement, use theStaged copy by using COPY statementfeature instead. The staged copy feature also provides you better throughput. It automatically converts the data into COPY statement compatible format, stores the data in Azure Blob storage, then calls COPY statement to load data into Azure Synapse Analytics.
Tip
When using COPY statement with Azure Integration Runtime, effectiveData Integration Units (DIU)is always 2. Tuning the DIU doesn't impact the performance, as loading data from storage is powered by the Azure Synapse engine.
Direct copy by using COPY statement
Azure Synapse Analytics COPY statement directly supports Azure Blob and Azure Data Lake Storage Gen2. If your source data meets the criteria described in this section, use COPY statement to copy directly from the source data store to Azure Synapse Analytics. Otherwise, useStaged copy by using COPY statement. The service checks the settings and fails the copy activity run if the criteria is not met.
Thesource linked service and formatare with the following types and authentication methods:Supported source data store typeSupported formatSupported source authentication typeAzure BlobDelimited textAccount key authentication, shared access signature authentication, service principal authentication (using ServicePrincipalKey), system-assigned managed identity authenticationParquetAccount key authentication, shared access signature authenticationORCAccount key authentication, shared access signature authenticationAzure Data Lake Storage Gen2Delimited textParquetORCAccount key authentication, service principal authentication (using ServicePrincipalKey), shared access signature authentication, system-assigned managed identity authenticationImportantWhen you use managed identity authentication for your storage linked service, learn the needed configurations forAzure BlobandAzure Data Lake Storage Gen2respectively.If your Azure Storage is configured with VNet service endpoint, you must use managed identity authentication with "allow trusted Microsoft service" enabled on storage account, refer toImpact of using VNet Service Endpoints with Azure storage.
Thesource linked service and formatare with the following types and authentication methods:
Important
When you use managed identity authentication for your storage linked service, learn the needed configurations forAzure BlobandAzure Data Lake Storage Gen2respectively.
If your Azure Storage is configured with VNet service endpoint, you must use managed identity authentication with "allow trusted Microsoft service" enabled on storage account, refer toImpact of using VNet Service Endpoints with Azure storage.
Format settings are with the following:ForParquet:compressioncan beno compression,Snappy, orGZip.ForORC:compressioncan beno compression,zlib, orSnappy.ForDelimited text:rowDelimiteris explicitly set assingle characteror "\r\n", the default value is not supported.nullValueis left as default or set toempty string("").encodingNameis left as default or set toutf-8 or utf-16.escapeCharmust be same asquoteChar, and is not empty.skipLineCountis left as default or set to 0.compressioncan beno compressionorGZip.
Format settings are with the following:
ForParquet:compressioncan beno compression,Snappy, orGZip.
compression
GZip
ForORC:compressioncan beno compression,zlib, orSnappy.
compression
zlib
ForDelimited text:rowDelimiteris explicitly set assingle characteror "\r\n", the default value is not supported.nullValueis left as default or set toempty string("").encodingNameis left as default or set toutf-8 or utf-16.escapeCharmust be same asquoteChar, and is not empty.skipLineCountis left as default or set to 0.compressioncan beno compressionorGZip.
rowDelimiteris explicitly set assingle characteror "\r\n", the default value is not supported.
rowDelimiter
nullValueis left as default or set toempty string("").
nullValue
encodingNameis left as default or set toutf-8 or utf-16.
encodingName
escapeCharmust be same asquoteChar, and is not empty.
escapeChar
quoteChar
skipLineCountis left as default or set to 0.
skipLineCount
compressioncan beno compressionorGZip.
compression
GZip
If your source is a folder,recursivein copy activity must be set to true, andwildcardFilenameneed to be*or*.*.
If your source is a folder,recursivein copy activity must be set to true, andwildcardFilenameneed to be*or*.*.
recursive
wildcardFilename
*
*.*
wildcardFolderPath,wildcardFilename(other than*or*.*),modifiedDateTimeStart,modifiedDateTimeEnd,prefix,enablePartitionDiscoveryandadditionalColumnsare not specified.
wildcardFolderPath,wildcardFilename(other than*or*.*),modifiedDateTimeStart,modifiedDateTimeEnd,prefix,enablePartitionDiscoveryandadditionalColumnsare not specified.
wildcardFolderPath
wildcardFilename
*
*.*
modifiedDateTimeStart
modifiedDateTimeEnd
prefix
enablePartitionDiscovery
additionalColumns
The following COPY statement settings are supported underallowCopyCommandin copy activity:
allowCopyCommand
"activities":[
    {
        "name": "CopyFromAzureBlobToSQLDataWarehouseViaCOPY",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "ParquetDataset",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "AzureSQLDWDataset",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "ParquetSource",
                "storeSettings":{
                    "type": "AzureBlobStorageReadSettings",
                    "recursive": true
                }
            },
            "sink": {
                "type": "SqlDWSink",
                "allowCopyCommand":â¯true,
                "copyCommandSettings":â¯{
                    "defaultValues":â¯[
                        {
                            "columnName":â¯"col_string",
                            "defaultValue":â¯"DefaultStringValue"
                        }
                    ],
                    "additionalOptions":â¯{
                        "MAXERRORS":â¯"10000",
                        "DATEFORMAT":â¯"'ymd'"
                    }
                }
            },
            "enableSkipIncompatibleRow": true
        }
    }
]
"activities":[
    {
        "name": "CopyFromAzureBlobToSQLDataWarehouseViaCOPY",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "ParquetDataset",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "AzureSQLDWDataset",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "ParquetSource",
                "storeSettings":{
                    "type": "AzureBlobStorageReadSettings",
                    "recursive": true
                }
            },
            "sink": {
                "type": "SqlDWSink",
                "allowCopyCommand":â¯true,
                "copyCommandSettings":â¯{
                    "defaultValues":â¯[
                        {
                            "columnName":â¯"col_string",
                            "defaultValue":â¯"DefaultStringValue"
                        }
                    ],
                    "additionalOptions":â¯{
                        "MAXERRORS":â¯"10000",
                        "DATEFORMAT":â¯"'ymd'"
                    }
                }
            },
            "enableSkipIncompatibleRow": true
        }
    }
]
Staged copy by using COPY statement
When your source data is not natively compatible with COPY statement, enable data copying via an interim staging Azure Blob or Azure Data Lake Storage Gen2 (it can't be Azure Premium Storage). In this case, the service automatically converts the data to meet the data format requirements of COPY statement. Then it invokes COPY statement to load data into Azure Synapse Analytics. Finally, it cleans up your temporary data from the storage. SeeStaged copyfor details about copying data via a staging.
To use this feature, create anAzure Blob Storage linked serviceorAzure Data Lake Storage Gen2 linked servicewithaccount key or system-managed identity authenticationthat refers to the Azure storage account as the interim storage.
Important
When you use managed identity authentication for your staging linked service, learn the needed configurations forAzure BlobandAzure Data Lake Storage Gen2respectively. You also need to grant permissions to your Azure Synapse Analytics workspace managed identity in your staging Azure Blob Storage or Azure Data Lake Storage Gen2 account. To learn how to grant this permission, seeGrant permissions to workspace managed identity.
If your staging Azure Storage is configured with VNet service endpoint, you must use managed identity authentication with "allow trusted Microsoft service" enabled on storage account, refer toImpact of using VNet Service Endpoints with Azure storage.
Important
If your staging Azure Storage is configured with Managed Private Endpoint and has the storage firewall enabled, you must use managed identity authentication and grant Storage Blob Data Reader permissions to the Synapse SQL Server to ensure it can access the staged files during the COPY statement load.
"activities":[
    {
        "name": "CopyFromSQLServerToSQLDataWarehouseViaCOPYstatement",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "SQLServerDataset",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "AzureSQLDWDataset",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "SqlSource",
            },
            "sink": {
                "type": "SqlDWSink",
                "allowCopyCommand": true
            },
            "stagingSettings": {
                "linkedServiceName": {
                    "referenceName": "MyStagingStorage",
                    "type": "LinkedServiceReference"
                }
            }
        }
    }
]
"activities":[
    {
        "name": "CopyFromSQLServerToSQLDataWarehouseViaCOPYstatement",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "SQLServerDataset",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "AzureSQLDWDataset",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "SqlSource",
            },
            "sink": {
                "type": "SqlDWSink",
                "allowCopyCommand": true
            },
            "stagingSettings": {
                "linkedServiceName": {
                    "referenceName": "MyStagingStorage",
                    "type": "LinkedServiceReference"
                }
            }
        }
    }
]
Use PolyBase to load data into Azure Synapse Analytics
UsingPolyBaseis an efficient way to load a large amount of data into Azure Synapse Analytics with high throughput. You'll see a large gain in the throughput by using PolyBase instead of the default BULKINSERT mechanism.
If your source data is inAzure Blob or Azure Data Lake Storage Gen2, and theformat is PolyBase compatible, you can use copy activity to directly invoke PolyBase to let Azure Synapse Analytics pull the data from source. For details, seeDirect copy by using PolyBase.
If your source data store and format isn't originally supported by PolyBase, use theStaged copy by using PolyBasefeature instead. The staged copy feature also provides you better throughput. It automatically converts the data into PolyBase-compatible format, stores the data in Azure Blob storage, then calls PolyBase to load data into Azure Synapse Analytics.
Tip
Learn more onBest practices for using PolyBase. When using PolyBase with Azure Integration Runtime, effectiveData Integration Units (DIU)for direct or staged storage-to-Synapse is always 2. Tuning the DIU doesn't impact the performance, as loading data from storage is powered by Synapse engine.
The following PolyBase settings are supported underpolyBaseSettingsin copy activity:
polyBaseSettings
Direct copy by using PolyBase
Azure Synapse Analytics PolyBase directly supports Azure Blob and Azure Data Lake Storage Gen2. If your source data meets the criteria described in this section, use PolyBase to copy directly from the source data store to Azure Synapse Analytics. Otherwise, useStaged copy by using PolyBase.
Tip
To copy data efficiently to Azure Synapse Analytics, learn more fromAzure Data Factory makes it even easier and convenient to uncover insights from data when using Data Lake Store with Azure Synapse Analytics.
If the requirements aren't met, the service checks the settings and automatically falls back to the BULKINSERT mechanism for the data movement.
Thesource linked serviceis with the following types and authentication methods:Supported source data store typeSupported source authentication typeAzure BlobAccount key authentication, system-assigned managed identity authenticationAzure Data Lake Storage Gen2Account key authentication, system-assigned managed identity authenticationImportantWhen you use managed identity authentication for your storage linked service, learn the needed configurations forAzure BlobandAzure Data Lake Storage Gen2respectively.If your Azure Storage is configured with VNet service endpoint, you must use managed identity authentication with "allow trusted Microsoft service" enabled on storage account, refer toImpact of using VNet Service Endpoints with Azure storage.
Thesource linked serviceis with the following types and authentication methods:
Important
When you use managed identity authentication for your storage linked service, learn the needed configurations forAzure BlobandAzure Data Lake Storage Gen2respectively.
If your Azure Storage is configured with VNet service endpoint, you must use managed identity authentication with "allow trusted Microsoft service" enabled on storage account, refer toImpact of using VNet Service Endpoints with Azure storage.
Thesource data formatis ofParquet,ORC, orDelimited text, with the following configurations:Folder path doesn't contain wildcard filter.File name is empty, or points to a single file. If you specify wildcard file name in copy activity, it can only be*or*.*.rowDelimiterisdefault,\n,\r\n, or\r.nullValueis left as default or set toempty string(""), andtreatEmptyAsNullis left as default or set to true.encodingNameis left as default or set toutf-8.quoteChar,escapeChar, andskipLineCountaren't specified. PolyBase support skip header row, which can be configured asfirstRowAsHeader.compressioncan beno compression,GZip, orDeflate.
Thesource data formatis ofParquet,ORC, orDelimited text, with the following configurations:
Folder path doesn't contain wildcard filter.
File name is empty, or points to a single file. If you specify wildcard file name in copy activity, it can only be*or*.*.
*
*.*
rowDelimiterisdefault,\n,\r\n, or\r.
rowDelimiter
nullValueis left as default or set toempty string(""), andtreatEmptyAsNullis left as default or set to true.
nullValue
treatEmptyAsNull
encodingNameis left as default or set toutf-8.
encodingName
quoteChar,escapeChar, andskipLineCountaren't specified. PolyBase support skip header row, which can be configured asfirstRowAsHeader.
quoteChar
escapeChar
skipLineCount
firstRowAsHeader
compressioncan beno compression,GZip, orDeflate.
compression
GZip
If your source is a folder,recursivein copy activity must be set to true.
If your source is a folder,recursivein copy activity must be set to true.
recursive
wildcardFolderPath,wildcardFilename,modifiedDateTimeStart,modifiedDateTimeEnd,prefix,enablePartitionDiscovery, andadditionalColumnsare not specified.
wildcardFolderPath,wildcardFilename,modifiedDateTimeStart,modifiedDateTimeEnd,prefix,enablePartitionDiscovery, andadditionalColumnsare not specified.
wildcardFolderPath
wildcardFilename
modifiedDateTimeStart
modifiedDateTimeEnd
prefix
enablePartitionDiscovery
additionalColumns
Note
If your source is a folder, note PolyBase retrieves files from the folder and all of its subfolders, and it doesn't retrieve data from files for which the file name begins with an underline (_) or a period (.), as documentedhere - LOCATION argument.
"activities":[
    {
        "name": "CopyFromAzureBlobToSQLDataWarehouseViaPolyBase",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "ParquetDataset",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "AzureSQLDWDataset",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "ParquetSource",
                "storeSettings":{
                    "type": "AzureBlobStorageReadSettings",
                    "recursive": true
                }
            },
            "sink": {
                "type": "SqlDWSink",
                "allowPolyBase": true
            }
        }
    }
]
"activities":[
    {
        "name": "CopyFromAzureBlobToSQLDataWarehouseViaPolyBase",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "ParquetDataset",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "AzureSQLDWDataset",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "ParquetSource",
                "storeSettings":{
                    "type": "AzureBlobStorageReadSettings",
                    "recursive": true
                }
            },
            "sink": {
                "type": "SqlDWSink",
                "allowPolyBase": true
            }
        }
    }
]
Staged copy by using PolyBase
When your source data is not natively compatible with PolyBase, enable data copying via an interim staging Azure Blob or Azure Data Lake Storage Gen2 (it can't be Azure Premium Storage). In this case, the service automatically converts the data to meet the data format requirements of PolyBase. Then it invokes PolyBase to load data into Azure Synapse Analytics. Finally, it cleans up your temporary data from the storage. SeeStaged copyfor details about copying data via a staging.
To use this feature, create anAzure Blob Storage linked serviceorAzure Data Lake Storage Gen2 linked servicewithaccount key or managed identity authenticationthat refers to the Azure storage account as the interim storage.
Important
When you use managed identity authentication for your staging linked service, learn the needed configurations forAzure BlobandAzure Data Lake Storage Gen2respectively. You also need to grant permissions to your Azure Synapse Analytics workspace managed identity in your staging Azure Blob Storage or Azure Data Lake Storage Gen2 account. To learn how to grant this permission, seeGrant permissions to workspace managed identity.
If your staging Azure Storage is configured with VNet service endpoint, you must use managed identity authentication with "allow trusted Microsoft service" enabled on storage account, refer toImpact of using VNet Service Endpoints with Azure storage.
Important
If your staging Azure Storage is configured with Managed Private Endpoint and has the storage firewall enabled, you must use managed identity authentication and grant Storage Blob Data Reader permissions to the Synapse SQL Server to ensure it can access the staged files during the PolyBase load.
"activities":[
    {
        "name": "CopyFromSQLServerToSQLDataWarehouseViaPolyBase",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "SQLServerDataset",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "AzureSQLDWDataset",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "SqlSource",
            },
            "sink": {
                "type": "SqlDWSink",
                "allowPolyBase": true
            },
            "enableStaging": true,
            "stagingSettings": {
                "linkedServiceName": {
                    "referenceName": "MyStagingStorage",
                    "type": "LinkedServiceReference"
                }
            }
        }
    }
]
"activities":[
    {
        "name": "CopyFromSQLServerToSQLDataWarehouseViaPolyBase",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "SQLServerDataset",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "AzureSQLDWDataset",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "SqlSource",
            },
            "sink": {
                "type": "SqlDWSink",
                "allowPolyBase": true
            },
            "enableStaging": true,
            "stagingSettings": {
                "linkedServiceName": {
                    "referenceName": "MyStagingStorage",
                    "type": "LinkedServiceReference"
                }
            }
        }
    }
]
Best practices for using PolyBase
The following sections provide best practices in addition to those practices mentioned inBest practices for Azure Synapse Analytics.
To use PolyBase, the user that loads data into Azure Synapse Analytics must have"CONTROL" permissionon the target database. One way to achieve that is to add the user as a member of thedb_ownerrole. Learn how to do that in theAzure Synapse Analytics overview.
PolyBase loads are limited to rows smaller than 1 MB. It cannot be used to load to VARCHR(MAX), NVARCHAR(MAX), or VARBINARY(MAX). For more information, seeAzure Synapse Analytics service capacity limits.
When your source data has rows greater than 1 MB, you might want to vertically split the source tables into several small ones. Make sure that the largest size of each row doesn't exceed the limit. The smaller tables can then be loaded by using PolyBase and merged together in Azure Synapse Analytics.
Alternatively, for data with such wide columns, you can use non-PolyBase to load the data by turning off "allow PolyBase" setting.
To achieve the best possible throughput, assign a larger resource class to the user that loads data into Azure Synapse Analytics via PolyBase.
If your source data is in text format or other non-PolyBase compatible stores (using staged copy and PolyBase), and it contains empty value to be loaded into Azure Synapse Analytics Decimal column, you may get the following error:
ErrorCode=FailedDbOperation, ......HadoopSqlException: Error converting data type VARCHAR to DECIMAL.....Detailed Message=Empty string can't be converted to DECIMAL.....
ErrorCode=FailedDbOperation, ......HadoopSqlException: Error converting data type VARCHAR to DECIMAL.....Detailed Message=Empty string can't be converted to DECIMAL.....
The solution is to unselect "Use type default" option (as false) in copy activity sink -> PolyBase settings. "USE_TYPE_DEFAULT" is a PolyBase native configuration, which specifies how to handle missing values in delimited text files when PolyBase retrieves data from the text file.
The following table gives examples of how to specify thetableNameproperty in the JSON dataset. It shows several combinations of schema and table names.
If you see the following error, the problem might be the value you specified for thetableNameproperty. See the preceding table for the correct way to specify values for thetableNameJSON property.
Type=System.Data.SqlClient.SqlException,Message=Invalid object name 'stg.Account_test'.,Source=.Net SqlClient Data Provider
Type=System.Data.SqlClient.SqlException,Message=Invalid object name 'stg.Account_test'.,Source=.Net SqlClient Data Provider
Currently, the PolyBase feature accepts only the same number of columns as in the target table. An example is a table with four columns where one of them is defined with a default value. The input data still needs to have four columns. A three-column input dataset yields an error similar to the following message:
All columns of the table must be specified in the INSERT BULK statement.
All columns of the table must be specified in the INSERT BULK statement.
The NULL value is a special form of the default value. If the column is nullable, the input data in the blob for that column might be empty. But it can't be missing from the input dataset. PolyBase inserts NULL for missing values in Azure Synapse Analytics.
If you receive the following error, ensure that you are using managed identity authentication and have granted Storage Blob Data Reader permissions to the Azure Synapse workspace's managed identity.
Job failed due to reason: at Sink '[SinkName]': shaded.msdataflow.com.microsoft.sqlserver.jdbc.SQLServerException: External file access failed due to internal error: 'Error occurred while accessing HDFS: Java exception raised on call to HdfsBridge_IsDirExist. Java exception message:\r\nHdfsBridge::isDirExist
Job failed due to reason: at Sink '[SinkName]': shaded.msdataflow.com.microsoft.sqlserver.jdbc.SQLServerException: External file access failed due to internal error: 'Error occurred while accessing HDFS: Java exception raised on call to HdfsBridge_IsDirExist. Java exception message:\r\nHdfsBridge::isDirExist
For more information, seeGrant permissions to managed identity after workspace creation.
Mapping data flow properties
When transforming data in mapping data flow, you can read and write to tables from Azure Synapse Analytics. For more information, see thesource transformationandsink transformationin mapping data flows.
Source transformation
Settings specific to Azure Synapse Analytics are available in theSource Optionstab of the source transformation.
InputSelect whether you point your source at a table (equivalent ofSelect * from <table-name>) or enter a custom SQL query.
Select * from <table-name>
Enable StagingIt is highly recommended that you use this option in production workloads with Azure Synapse Analytics sources. When you execute adata flow activitywith Azure Synapse Analytics sources from a pipeline, you will be prompted for a staging location storage account and will use that for staged data loading. It is the fastest mechanism to load data from Azure Synapse Analytics.
When you use managed identity authentication for your storage linked service, learn the needed configurations forAzure BlobandAzure Data Lake Storage Gen2respectively.
If your Azure Storage is configured with VNet service endpoint, you must use managed identity authentication with "allow trusted Microsoft service" enabled on storage account, refer toImpact of using VNet Service Endpoints with Azure storage.
When you use Azure SynapseserverlessSQL pool as source, enable staging is not supported.
Query: If you select Query in the input field, enter a SQL query for your source. This setting overrides any table that you've chosen in the dataset.Order Byclauses aren't supported here, but you can set a full SELECT FROM statement. You can also use user-defined table functions.select * from udfGetData()is a UDF in SQL that returns a table. This query will produce a source table that you can use in your data flow. Using queries is also a great way to reduce rows for testing or for lookups.
SQL Example:Select * from MyTable where customerId > 1000 and customerId < 2000
Select * from MyTable where customerId > 1000 and customerId < 2000
Batch size: Enter a batch size to chunk large data into reads. In data flows, this setting will be used to set Spark columnar caching. This is an option field, which will use Spark defaults if it is left blank.
Isolation Level: The default for SQL sources in mapping data flow is read uncommitted. You can change the isolation level here to one of these values:
Read Committed
Read Uncommitted
Repeatable Read
Serializable
None (ignore isolation level)

Sink transformation
Settings specific to Azure Synapse Analytics are available in theSettingstab of the sink transformation.
Update method:Determines what operations are allowed on your database destination. The default is to only allow inserts. To update, upsert, or delete rows, an alter-row transformation is required to tag rows for those actions. For updates, upserts and deletes, a key column or columns must be set to determine which row to alter.
Table action:Determines whether to recreate or remove all rows from the destination table prior to writing.
None: No action will be done to the table.
Recreate: The table will get dropped and recreated. Required if creating a new table dynamically.
Truncate: All rows from the target table will get removed.
Enable staging:This enables loading into Azure Synapse Analytics SQL Pools using the copy command and is recommended for most Synapse sinks. The staging storage is configured inExecute Data Flow activity.
When you use managed identity authentication for your storage linked service, learn the needed configurations forAzure BlobandAzure Data Lake Storage Gen2respectively.
If your Azure Storage is configured with VNet service endpoint, you must use managed identity authentication with "allow trusted Microsoft service" enabled on storage account, refer toImpact of using VNet Service Endpoints with Azure storage.
Batch size: Controls how many rows are being written in each bucket. Larger batch sizes improve compression and memory optimization, but risk out of memory exceptions when caching data.
Use sink schema: By default, a temporary table will be created under the sink schema as staging. You can alternatively uncheck theUse sink schemaoption and instead, inSelect user DB schema, specify a schema name under which Data Factory will create a staging table to load upstream data and automatically clean them up upon completion. Make sure you have create table permission in the database and alter permission on the schema.

Pre and Post SQL scripts: Enter multi-line SQL scripts that will execute before (pre-processing) and after (post-processing) data is written to your Sink database

Tip
It's recommended to break single batch scripts with multiple commands into multiple batches.
Only Data Definition Language (DDL) and Data Manipulation Language (DML) statements that return a simple update count can be run as part of a batch. Learn more fromPerforming batch operations
Error row handling
When writing to Azure Synapse Analytics, certain rows of data may fail due to constraints set by the destination. Some common errors include:
String or binary data would be truncated in table
Cannot insert the value NULL into column
Conversion failed when converting the value to data type
By default, a data flow run will fail on the first error it gets. You can choose toContinue on errorthat allows your data flow to complete even if individual rows have errors. The service provides different options for you to handle these error rows.
Transaction Commit:Choose whether your data gets written in a single transaction or in batches. Single transaction will provide better performance and no data written will be visible to others until the transaction completes. Batch transactions have worse performance but can work for large datasets.
Output rejected data:If enabled, you can output the error rows into a csv file in Azure Blob Storage or an Azure Data Lake Storage Gen2 account of your choosing. This will write the error rows with three additional columns: the SQL operation like INSERT or UPDATE, the data flow error code, and the error message on the row.
Report success on error:If enabled, the data flow will be marked as a success even if error rows are found.

Lookup activity properties
To learn details about the properties, checkLookup activity.
GetMetadata activity properties
To learn details about the properties, checkGetMetadata activity
Data type mapping for Azure Synapse Analytics
When you copy data from or to Azure Synapse Analytics, the following mappings are used from Azure Synapse Analytics data types to Azure Data Factory interim data types. These mappings are also used when copying data from or to Azure Synapse Analytics using Synapse pipelines, since pipelines also implement Azure Data Factory within Azure Synapse. Seeschema and data type mappingsto learn how Copy Activity maps the source schema and data type to the sink.
Tip
Refer toTable data types in Azure Synapse Analyticsarticle on Azure Synapse Analytics supported data types and the workarounds for unsupported ones.
Upgrade the Azure Synapse Analytics version
To upgrade the Azure Synapse Analytics version, inEdit linked servicepage, selectRecommendedunderVersionand configure the linked service by referring toLinked service properties for the recommended version.
Differences between the recommended and the legacy version
The table below shows the differences between Azure Synapse Analytics using the recommended and the legacy version.
encrypt
strict
Related content
For a list of data stores supported as sources and sinks by Copy Activity, seesupported data stores and formats.
Feedback
Was this page helpful?
Additional resources