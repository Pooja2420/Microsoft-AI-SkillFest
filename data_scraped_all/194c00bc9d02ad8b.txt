Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
What is Azure AI model inference?
Article
2025-04-22
4 contributors
In this article
Azure AI model inference provides access to the most powerful models available in the Azure AI model catalog. The models come from key model providers in the industry, including OpenAI, Microsoft, Meta, Mistral, Cohere, G42, and AI21 Labs. These models can be integrated with software solutions to deliver a wide range of tasks that include content generation, summarization, image understanding, semantic search, and code generation.
Azure AI model inference provides a way toconsume models as APIs without hosting them on your infrastructure. Models are hosted in a Microsoft-managed infrastructure, which enables API-based access to the model provider's model. API-based access can dramatically reduce the cost of accessing a model and simplify the provisioning experience.
Azure AI model inference is part of Azure AI Services, and users can access the service throughREST APIs,SDKs in several languagessuch as Python, C#, JavaScript, and Java. You can also use the Azure AI model inference fromAzure AI Foundry by configuring a connection.
Models
You can get access to the key model providers in the industry including OpenAI, Microsoft, Meta, Mistral, Cohere, G42, and AI21 Labs. Model providers define the license terms and set the price for use of their models.
Explore the following model families available:
AI21 Labs
Azure OpenAI
Cohere
Core42
DeepSeek
Meta
Microsoft
Mistral AI
NTT Data
To see details for each model including language, types, and capabilities, seeModelsarticle.
Pricing
For models from non-Microsoft providers (for example, Meta AI and Mistral models), billing is through Azure Marketplace. For such models, you're required to subscribe to the particular model offering in accordance with theMicrosoft Commercial Marketplace Terms of Use. Users accept license terms for use of the models. Pricing information for consumption is provided during deployment.
For Microsoft models (for example, Phi-3 models and Azure OpenAI models) billing is via Azure meters as First Party Consumption Services. As described in theProduct Terms, you purchase First Party Consumption Services by using Azure meters, but they aren't subject to Azure service terms.
Tip
Learn how tomonitor and manage costin Azure AI model inference.
Responsible AI
At Microsoft, we're committed to the advancement of AI driven by principles that put people first. Generative models such as the ones available in Azure AI models have significant potential benefits, but without careful design and thoughtful mitigations, such models have the potential to generate incorrect or even harmful content.
Microsoft helps guard against abuse and unintended harm by taking the following actions:
Incorporating Microsoft'sprinciples for responsible AI use
Adopting acode of conductfor use of the service
Buildingcontent filtersto support customers
Providing responsible AIinformation and guidancethat customers should consider when using Azure OpenAI.
Getting started
Azure AI model inference is a new feature offering on Azure AI Services resources. You can get started with it the same way as any other Azure product where youcreate and configure your resource for Azure AI model inference, or instance of the service, in your Azure Subscription. You can create as many resources as needed and configure them independently in case you have multiple teams with different requirements.
Once you create an Azure AI Services resource, you must deploy a model before you can start making API calls. By default, no models are available on it, so you can control which ones to start from. See the tutorialCreate your first model deployment in Azure AI model inference.
Next steps
Create your first model deployment in Azure AI model inference
Feedback
Was this page helpful?
Additional resources