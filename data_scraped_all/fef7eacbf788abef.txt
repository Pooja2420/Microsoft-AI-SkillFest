Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Limit large language model API token usage
Article
2025-04-15
1 contributor
In this article
APPLIES TO: Developer | Basic | Basic v2 | Standard | Standard v2 | Premium | Premium v2
Thellm-token-limitpolicy prevents large language model (LLM) API usage spikes on a per key basis by limiting consumption of language model tokens to either a specified rate (number per minute), a quota over a specified period, or both. When a specified token rate limit is exceeded, the caller receives a429 Too Many Requestsresponse status code. When a specified quota is exceeded, the caller receives a403 Forbiddenresponse status code.
llm-token-limit
429 Too Many Requests
403 Forbidden
By relying on token usage metrics returned from the LLM endpoint, the policy can accurately monitor and enforce limits in real time. The policy also enables precalculation of prompt tokens by API Management, minimizing unnecessary requests to the LLM backend if the limit is already exceeded.
Note
Currently, this policy is in preview.
Note
Set the policy's elements and child elements in the order provided in the policy statement. Learn more abouthow to set or edit API Management policies.
Supported models
Use the policy with LLM APIs added to Azure API Management that are available through theAzure AI Model Inference API.
Policy statement
<llm-token-limit counter-key="key value"
        tokens-per-minute="number"
        token-quota="number"
        token-quota-period="Hourly | Daily | Weekly | Monthly | Yearly"
        estimate-prompt-tokens="true | false"    
        retry-after-header-name="custom header name, replaces default 'Retry-After'" 
        retry-after-variable-name="policy expression variable name"
        remaining-quota-tokens-header-name="header name"  
        remaining-quota-tokens-variable-name="policy expression variable name"
        remaining-tokens-header-name="header name"  
        remaining-tokens-variable-name="policy expression variable name"
        tokens-consumed-header-name="header name"
        tokens-consumed-variable-name="policy expression variable name" />
<llm-token-limit counter-key="key value"
        tokens-per-minute="number"
        token-quota="number"
        token-quota-period="Hourly | Daily | Weekly | Monthly | Yearly"
        estimate-prompt-tokens="true | false"    
        retry-after-header-name="custom header name, replaces default 'Retry-After'" 
        retry-after-variable-name="policy expression variable name"
        remaining-quota-tokens-header-name="header name"  
        remaining-quota-tokens-variable-name="policy expression variable name"
        remaining-tokens-header-name="header name"  
        remaining-tokens-variable-name="policy expression variable name"
        tokens-consumed-header-name="header name"
        tokens-consumed-variable-name="policy expression variable name" />
Attributes
tokens-per-minute
token-quota
token-quota-period
token-quota-period
tokens-per-minute
token-quota
token-quota-period
token-quota
Hourly
Daily
Weekly
Monthly
Yearly
tokens-per-minute
token-quota
token-quota-period
true
false
false
counter-key
tokens-per-minute
token-quota
Retry-After
tokens-per-minute
token-quota
token-quota
token-quota-period
token-quota
token-quota-period
tokens-per-minute
tokens-per-minute
backend
estimate-prompt-tokens
true
outbound
Usage
Policy sections:inbound
Policy scopes:global, workspace, product, API, operation
Gateways:classic, v2, self-hosted, workspace
Usage notes
This policy can be used multiple times per policy definition.
Where available whenestimate-prompt-tokensis set tofalse, values in the usage section of the response from the LLM API are used to determine token usage.
estimate-prompt-tokens
false
Certain LLM endpoints support streaming of responses. Whenstreamis set totruein the API request to enable streaming, prompt tokens are always estimated, regardless of the value of theestimate-prompt-tokensattribute.
stream
true
estimate-prompt-tokens
For models that accept image input, image tokens are generally counted by the backend language model and included in limit and quota calculations. However, when streaming is used orestimate-prompt-tokensis set totrue, the policy currently over-counts each image as a maximum count of 1200 tokens.
estimate-prompt-tokens
true
API Management uses a single counter for eachcounter-keyvalue that you specify in the policy. The counter is updated at all scopes at which the policy is configured with that key value. If you want to configure separate counters at different scopes (for example, a specific API or product), specify different key values at the different scopes. For example, append a string that identifies the scope to the value of an expression.
counter-key
This policy tracks token usage independently at each gateway where it is applied, includingworkspace gatewaysand regional gateways in amulti-region deployment. It doesn't aggregate token counts across the entire instance.
Examples
Token rate limit
In the following example, the token rate limit of 5000 per minute is keyed by the caller IP address. The policy doesn't estimate the number of tokens required for a prompt. After each policy execution, the remaining tokens allowed for that caller IP address in the time period are stored in the variableremainingTokens.
remainingTokens
<policies>
    <inbound>
        <base />
        <llm-token-limit
            counter-key="@(context.Request.IpAddress)"
            tokens-per-minute="5000" estimate-prompt-tokens="false" remaining-tokens-variable-name="remainingTokens" />
    </inbound>
    <outbound>
        <base />
    </outbound>
</policies>
<policies>
    <inbound>
        <base />
        <llm-token-limit
            counter-key="@(context.Request.IpAddress)"
            tokens-per-minute="5000" estimate-prompt-tokens="false" remaining-tokens-variable-name="remainingTokens" />
    </inbound>
    <outbound>
        <base />
    </outbound>
</policies>
Token quota
In the following example, the token quota of 10000 is keyed by the subscription ID and resets monthly. After each policy execution, the number of remaining tokens allowed for that subscription ID in the time period is stored in the variableremainingQuotaTokens.
remainingQuotaTokens
<policies>
    <inbound>
        <base />
        <llm-token-limit
            counter-key="@(context.Subscription.Id)"
            token-quota="100000" token-quota-period="Monthly" remaining-quota-tokens-variable-name="remainingQuotaTokens" />
    </inbound>
    <outbound>
        <base />
    </outbound>
</policies>
<policies>
    <inbound>
        <base />
        <llm-token-limit
            counter-key="@(context.Subscription.Id)"
            token-quota="100000" token-quota-period="Monthly" remaining-quota-tokens-variable-name="remainingQuotaTokens" />
    </inbound>
    <outbound>
        <base />
    </outbound>
</policies>
Related policies
Rate limiting and quotas
azure-openai-token-limitpolicy
llm-emit-token-metricpolicy
Related content
For more information about working with policies, see:
Tutorial: Transform and protect your API
Policy referencefor a full list of policy statements and their settings
Policy expressions
Set or edit policies
Reuse policy configurations
Policy snippets repo
Azure API Management policy toolkit
Get Copilot assistance to create, explain, and troubleshoot policies
Feedback
Was this page helpful?
Additional resources