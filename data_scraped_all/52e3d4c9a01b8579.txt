Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
How to configure content filters
Article
2025-03-05
6 contributors
In this article
The content filtering system integrated into Azure AI Foundry runs alongside the core models, including image generation models. It uses an ensemble of multi-class classification models to detect four categories of harmful content (violence, hate, sexual, and self-harm) at four severity levels respectively (safe, low, medium, and high), and optional binary classifiers for detecting jailbreak risk, existing text, and code in public repositories.
The default content filtering configuration is set to filter at the medium severity threshold for all four content harms categories for both prompts and completions. That means that content that is detected at severity level medium or high is filtered, while content detected at severity level low or safe is not filtered by the content filters. Learn more about content categories, severity levels, and the behavior of the content filtering systemhere.
Jailbreak risk detection and protected text and code models are optional and on by default. For jailbreak and protected material text and code models, the configurability feature allows all customers to turn the models on and off. The models are by default on and can be turned off per your scenario. Some models are required to be on for certain scenarios to retain coverage under theCustomer Copyright Commitment.
Note
All customers have the ability to modify the content filters and configure the severity thresholds (low, medium, high). Approval is required for turning the content filters partially or fully off. Managed customers only may apply for full content filtering control via this form:Azure OpenAI Limited Access Review: Modified Content Filters. At this time, it is not possible to become a managed customer.
Content filters can be configured at the resource level. Once a new configuration is created, it can be associated with one or more deployments. For more information about model deployment, see theresource deployment guide.
Prerequisites
You must have an Azure OpenAI resource and a large language model (LLM) deployment to configure content filters. Follow aquickstartto get started.
Understand content filter configurability
Azure OpenAI Service includes default safety settings applied to all models, excluding audio API models such as Whisper. These configurations provide you with a responsible experience by default, including content filtering models, blocklists, prompt transformation,content credentials, and others.Read more about it here.
All customers can also configure content filters and create custom safety policies that are tailored to their use case requirements. The configurability feature allows customers to adjust the settings, separately for prompts and completions, to filter content for each content category at different severity levels as described in the table below. Content detected at the 'safe' severity level is labeled in annotations but is not subject to filtering and isn't configurable.
1For Azure OpenAI models, only customers who have been approved for modified content filtering have full content filtering control and can turn off content filters. Apply for modified content filters via this form:Azure OpenAI Limited Access Review: Modified Content Filters. For Azure Government customers, apply for modified content filters via this form:Azure Government - Request Modified Content Filtering for Azure OpenAI Service.
Configurable content filters for inputs (prompts) and outputs (completions) are available for all Azure OpenAI models.
Content filtering configurations are created within a Resource in Azure AI Foundry portal, and can be associated with Deployments.Learn more about configurability here.
Customers are responsible for ensuring that applications integrating Azure OpenAI comply with theCode of Conduct.
Understand other filters
You can configure the following filter categories in addition to the default harm category filters.
Create a content filter in Azure AI Foundry
For any model deployment inAzure AI Foundry, you can directly use the default content filter, but you might want to have more control. For example, you could make a filter stricter or more lenient, or enable more advanced capabilities like prompt shields and protected material detection.
Tip
For guidance with content filters in your Azure AI Foundry project, you can read more atAzure AI Foundry content filtering.
Follow these steps to create a content filter:
Go toAzure AI Foundryand navigate to your project. Then select theSafety + securitypage from the left menu and select theContent filterstab.
Go toAzure AI Foundryand navigate to your project. Then select theSafety + securitypage from the left menu and select theContent filterstab.

Select+ Create content filter.
Select+ Create content filter.
On theBasic informationpage, enter a name for your content filtering configuration. Select a connection to associate with the content filter. Then selectNext.Now you can configure the input filters (for user prompts) and output filters (for model completion).
On theBasic informationpage, enter a name for your content filtering configuration. Select a connection to associate with the content filter. Then selectNext.

Now you can configure the input filters (for user prompts) and output filters (for model completion).
On theInput filterspage, you can set the filter for the input prompt. For the first four content categories there are three severity levels that are configurable: Low, medium, and high. You can use the sliders to set the severity threshold if you determine that your application or usage scenario requires different filtering than the default values.
Some filters, such as Prompt Shields and Protected material detection, enable you to determine if the model should annotate and/or block content. SelectingAnnotate onlyruns the respective model and return annotations via API response, but it will not filter content. In addition to annotate, you can also choose to block content.If your use case was approved for modified content filters, you receive full control over content filtering configurations and can choose to turn filtering partially or fully off, or enable annotate only for the content harms categories (violence, hate, sexual and self-harm).Content will be annotated by category and blocked according to the threshold you set. For the violence, hate, sexual, and self-harm categories, adjust the slider to block content of high, medium, or low severity.
On theInput filterspage, you can set the filter for the input prompt. For the first four content categories there are three severity levels that are configurable: Low, medium, and high. You can use the sliders to set the severity threshold if you determine that your application or usage scenario requires different filtering than the default values.
Some filters, such as Prompt Shields and Protected material detection, enable you to determine if the model should annotate and/or block content. SelectingAnnotate onlyruns the respective model and return annotations via API response, but it will not filter content. In addition to annotate, you can also choose to block content.
If your use case was approved for modified content filters, you receive full control over content filtering configurations and can choose to turn filtering partially or fully off, or enable annotate only for the content harms categories (violence, hate, sexual and self-harm).
Content will be annotated by category and blocked according to the threshold you set. For the violence, hate, sexual, and self-harm categories, adjust the slider to block content of high, medium, or low severity.

On theOutput filterspage, you can configure the output filter, which will be applied to all output content generated by your model. Configure the individual filters as before. This page also provides the Streaming mode option, which lets you filter content in near-real-time as it's generated by the model, reducing latency. When you're finished selectNext.Content will be annotated by each category and blocked according to the threshold. For violent content, hate content, sexual content, and self-harm content category, adjust the threshold to block harmful content with equal or higher severity levels.
On theOutput filterspage, you can configure the output filter, which will be applied to all output content generated by your model. Configure the individual filters as before. This page also provides the Streaming mode option, which lets you filter content in near-real-time as it's generated by the model, reducing latency. When you're finished selectNext.
Content will be annotated by each category and blocked according to the threshold. For violent content, hate content, sexual content, and self-harm content category, adjust the threshold to block harmful content with equal or higher severity levels.

Optionally, on theDeploymentpage, you can associate the content filter with a deployment. If a selected deployment already has a filter attached, you must confirm that you want to replace it. You can also associate the content filter with a deployment later. SelectCreate.Content filtering configurations are created at the hub level in theAzure AI Foundry portal. Learn more about configurability in theAzure OpenAI Service documentation.
Optionally, on theDeploymentpage, you can associate the content filter with a deployment. If a selected deployment already has a filter attached, you must confirm that you want to replace it. You can also associate the content filter with a deployment later. SelectCreate.

Content filtering configurations are created at the hub level in theAzure AI Foundry portal. Learn more about configurability in theAzure OpenAI Service documentation.
On theReviewpage, review the settings and then selectCreate filter.
On theReviewpage, review the settings and then selectCreate filter.
Use a blocklist as a filter
You can apply a blocklist as either an input or output filter, or both. Enable theBlocklistoption on theInput filterand/orOutput filterpage. Select one or more blocklists from the dropdown, or use the built-in profanity blocklist. You can combine multiple blocklists into the same filter.
Apply a content filter
The filter creation process gives you the option to apply the filter to the deployments you want. You can also change or remove content filters from your deployments at any time.
Follow these steps to apply a content filter to a deployment:
Go toAzure AI Foundryand select a project.
Go toAzure AI Foundryand select a project.
SelectModels + endpointson the left pane and choose one of your deployments, then selectEdit.
SelectModels + endpointson the left pane and choose one of your deployments, then selectEdit.

In theUpdate deploymentwindow, select the content filter you want to apply to the deployment. Then selectSave and close.You can also edit and delete a content filter configuration if required. Before you delete a content filtering configuration, you will need to unassign and replace it from any deployment in theDeploymentstab.
In theUpdate deploymentwindow, select the content filter you want to apply to the deployment. Then selectSave and close.

You can also edit and delete a content filter configuration if required. Before you delete a content filtering configuration, you will need to unassign and replace it from any deployment in theDeploymentstab.
Now, you can go to the playground to test whether the content filter works as expected.
Tip
You can also create and update content filters using the REST APIs. For more information, see theAPI reference. Content filters can be configured at the resource level. Once a new configuration is created, it can be associated with one or more deployments. For more information about model deployment, see the resourcedeployment guide.
Specify a content filtering configuration at request time (preview)
In addition to the deployment-level content filtering configuration, we also provide a request header that allows you specify your custom configuration at request time for every API call.
curl --request POST \ 
    --url 'URL' \ 
    --header 'Content-Type: application/json' \ 
    --header 'api-key: API_KEY' \ 
    --header 'x-policy-id: CUSTOM_CONTENT_FILTER_NAME' \ 
    --data '{ 
        "messages": [ 
            { 
                "role": "system", 
                "content": "You are a creative assistant." 
            }, 
            { 
                "role": "user", 
                "content": "Write a poem about the beauty of nature." 
            } 
        ] 
    }'
curl --request POST \ 
    --url 'URL' \ 
    --header 'Content-Type: application/json' \ 
    --header 'api-key: API_KEY' \ 
    --header 'x-policy-id: CUSTOM_CONTENT_FILTER_NAME' \ 
    --data '{ 
        "messages": [ 
            { 
                "role": "system", 
                "content": "You are a creative assistant." 
            }, 
            { 
                "role": "user", 
                "content": "Write a poem about the beauty of nature." 
            } 
        ] 
    }'
The request-level content filtering configuration will override the deployment-level configuration, for the specific API call. If a configuration is specified that does not exist, the following error message will be returned.
{ 
    "error": 
        { 
            "code": "InvalidContentFilterPolicy", 
            "message": "Your request contains invalid content filter policy. Please provide a valid policy." 
        } 
}
{ 
    "error": 
        { 
            "code": "InvalidContentFilterPolicy", 
            "message": "Your request contains invalid content filter policy. Please provide a valid policy." 
        } 
}
Report content filtering feedback
If you are encountering a content filtering issue, select theFilters Feedbackbutton at the top of the playground. This is enabled in theImages, Chat, and Completionsplayground once you submit a prompt.
When the dialog appears, select the appropriate content filtering issue. Include as much detail as possible relating to your content filtering issue, such as the specific prompt and content filtering error you encountered. Do not include any private or sensitive information.
For support, pleasesubmit a support ticket.
Follow best practices
We recommend informing your content filtering configuration decisions through an iterative identification (for example, red team testing, stress-testing, and analysis) and measurement process to address the potential harms that are relevant for a specific model, application, and deployment scenario. After you implement mitigations such as content filtering, repeat measurement to test effectiveness. Recommendations and best practices for Responsible AI for Azure OpenAI, grounded in theMicrosoft Responsible AI Standardcan be found in theResponsible AI Overview for Azure OpenAI.
Related content
Learn more about Responsible AI practices for Azure OpenAI:Overview of Responsible AI practices for Azure OpenAI models.
Read more aboutcontent filtering categories and severity levelswith Azure AI Foundry.
Learn more about red teaming from our:Introduction to red teaming large language models (LLMs) article.
Learn how toconfigure content filters using the API
Feedback
Was this page helpful?
Additional resources