Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Deploy a flow for real-time inference
Article
2025-03-10
3 contributors
In this article
Important
Items marked (preview) in this article are currently in public preview. This preview is provided without a service-level agreement, and we don't recommend it for production workloads. Certain features might not be supported or might have constrained capabilities. For more information, seeSupplemental Terms of Use for Microsoft Azure Previews.
After you build a prompt flow and test it properly, you might want to deploy it as an online endpoint. Deployments are hosted within an endpoint, and they can receive data from clients and send responses back in real time.
You can invoke the endpoint for real-time inference for chat, a copilot, or another generative AI application. Prompt flow supports endpoint deployment from a flow or a bulk test run.
In this article, you learn how to deploy a flow as a managed online endpoint for real-time inference. The steps you take are:
Test your flow and get it ready for deployment.
Create an online deployment.
Grant permissions to the endpoint.
Test the endpoint.
Consume the endpoint.
Prerequisites
To deploy a prompt flow as an online endpoint, you need:
An Azure subscription. If you don't have an Azure subscription, create afree accountbefore you begin.
An Azure AI Foundry project.
AMicrosoft.PolicyInsightsresource provider registered in the selected subscription. For more information on how to register a resource provider, seeRegister a resource provider.
Microsoft.PolicyInsights
Create an online deployment
After you build a flow and test it properly, it's time to create your online endpoint for real-time inference.
To deploy a prompt flow as an online endpoint in the Azure AI Foundry portal:
Have a prompt flow ready for deployment. If you don't have one, seeDevelop a prompt flow.
Have a prompt flow ready for deployment. If you don't have one, seeDevelop a prompt flow.
Optional: SelectChatto test if the flow is working correctly. We recommend that you test your flow before deployment.
Optional: SelectChatto test if the flow is working correctly. We recommend that you test your flow before deployment.
SelectDeployon the flow editor.
SelectDeployon the flow editor.

Provide the requested information on theBasic Settingspage in the deployment wizard.
Provide the requested information on theBasic Settingspage in the deployment wizard.

SelectReview + Createto review the settings and create the deployment. Otherwise, selectNextto proceed to the advanced settings pages.
SelectReview + Createto review the settings and create the deployment. Otherwise, selectNextto proceed to the advanced settings pages.
SelectCreateto deploy the prompt flow.
SelectCreateto deploy the prompt flow.
To view the status of your deployment, selectModels + endpointson the left pane. After the deployment is created successfully, select the deployment to see more information.
To view the status of your deployment, selectModels + endpointson the left pane. After the deployment is created successfully, select the deployment to see more information.

Select theConsumetab to see code samples that you can use to consume the deployed model in your application.On this page, you can also see the endpoint URL that you can use to consume the endpoint.
Select theConsumetab to see code samples that you can use to consume the deployed model in your application.
On this page, you can also see the endpoint URL that you can use to consume the endpoint.

You can use the REST endpoint directly or get started with one of the samples shown here.
You can use the REST endpoint directly or get started with one of the samples shown here.

For information about how to deploy a base model, seeDeploy models with Azure AI Foundry.
Settings and configurations
Requirements text file
Optionally, you can specify extra packages that you need inrequirements.txt. You can findrequirements.txtin the root folder of your flow folder. When you deploy a prompt flow to a managed online endpoint in the UI, by default, the deployment uses the environment that was created based on the base image specified inflow.dag.yamland the dependencies specified inrequirements.txtof the flow.
requirements.txt
requirements.txt
flow.dag.yaml
requirements.txt
The base image specified inflow.dag.yamlmust be created based on the prompt flow base imagemcr.microsoft.com/azureml/promptflow/promptflow-runtime-stable:<newest_version>. You can find the latest version onthis website. If you don't specify the base image inflow.dag.yaml, the deployment uses the default base imagemcr.microsoft.com/azureml/promptflow/promptflow-runtime-stable:latest.
flow.dag.yaml
mcr.microsoft.com/azureml/promptflow/promptflow-runtime-stable:<newest_version>
flow.dag.yaml
mcr.microsoft.com/azureml/promptflow/promptflow-runtime-stable:latest

Basic settings
In this step, you configure the basic settings when you selectDeployon the flow editor.
3
After you finish the basic settings, selectReview + Createto finish the creation. You can also selectNextto configure advanced settings.
Advanced settings: Endpoint
You can specify the following settings for the endpoint.

In theAdvanced settingsworkflow, you can also specify deployment tags and select a custom environment.

This setting identifies the authentication method for the endpoint. Key-based authentication provides a primary and secondary key that doesn't expire. Azure Machine Learning token-based authentication provides a token that periodically refreshes automatically.
The endpoint needs to access Azure resources, such as Azure Container Registry or your Azure AI Foundry hub connections, for inferencing. You can allow the endpoint permission to access Azure resources by giving permission to its managed identity.
System-assigned identity is autocreated after your endpoint is created. The user creates the user-assigned identity. For more information, see themanaged identities overview.
Notice the optionEnforce access to connection secrets (preview). If your flow uses connections, the endpoint needs to access connections to perform inference. The option is enabled by default.
The endpoint is granted the Azure Machine Learning Workspace Connection Secrets Reader role to access connections automatically if you have connection secrets reader permission. If you disable this option, you need to grant this role to the system-assigned identity manually or ask your admin for help. For more information, seeGrant permission to the endpoint identity.
When you create the deployment, Azure tries to pull the user container image from the Azure AI Foundry hub's container registry and mounts the user model and code artifacts into the user container from the hub's storage account.
If you created the associated endpoint with theUser Assigned Identityoption, the user-assigned identity must be granted the following roles before the deployment creation. Otherwise, the deployment creation fails.
Microsoft.MachineLearningServices/workspaces/connections/listsecrets/action
For more information about how to grant permissions to the endpoint identity, seeGrant permissions to the endpoint.
Important
If your flow uses authentication connections based on Microsoft Entra ID, whether you use system-assigned identity or user-assigned identity, you always need to grant the managed identity appropriate roles of the corresponding resources so that it can make API calls to that resource. For example, if your Azure OpenAI connection uses Microsoft Entra ID-based authentication, you need to grant your endpoint managed identity the Cognitive Services OpenAI User or Cognitive Services OpenAI Contributor role of the corresponding Azure OpenAI resources.
Advanced settings: Outputs and connections
In this step, you can view all flow outputs and specify which outputs to include in the response of the endpoint you deploy. By default, all flow outputs are selected.
You can also specify the connections that are used by the endpoint when it performs inference. By default, they're inherited from the flow.
After you configure and review all the preceding steps, selectReview + Createto finish the creation.
Expect the endpoint creation to take more than 15 minutes. The stages include creating an endpoint, registering a model, and creating a deployment.
The deployment creation progress sends a notification that starts withPrompt flow deployment.
If you enable this capability, tracing data and system metrics during inference time (such as token count, flow latency, and flow request) are collected into workspace-linked Application Insights. To learn more, seePrompt flow serving tracing data and metrics.
Grant permissions to the endpoint
Important
Granting permissions (adding a role assignment) is enabled only to the owner of the specific Azure resources. You might need to ask your Azure subscription owner for help. This person might be your IT admin.
We recommend that you grant roles to the user-assigned identity as soon as the endpoint creation finishes. It might take more than 15 minutes for the granted permission to take effect.
To grant the required permissions in the Azure portal UI, follow these steps:
Go to the Azure AI Foundry project overview page in theAzure portal.
Go to the Azure AI Foundry project overview page in theAzure portal.
SelectAccess control (IAM), and then selectAdd role assignment.
SelectAccess control (IAM), and then selectAdd role assignment.

SelectAzure Machine Learning Workspace Connection Secrets Reader, and selectNext.TheAzure Machine Learning Workspace Connection Secrets Readerrole is a built-in role that has permission to get hub connections.If you want to use a customized role, make sure that the customized role has the permission ofMicrosoft.MachineLearningServices/workspaces/connections/listsecrets/action. Learn more abouthow to create custom roles.
SelectAzure Machine Learning Workspace Connection Secrets Reader, and selectNext.
TheAzure Machine Learning Workspace Connection Secrets Readerrole is a built-in role that has permission to get hub connections.
If you want to use a customized role, make sure that the customized role has the permission ofMicrosoft.MachineLearningServices/workspaces/connections/listsecrets/action. Learn more abouthow to create custom roles.
Microsoft.MachineLearningServices/workspaces/connections/listsecrets/action
SelectManaged identityand then select members:System-assigned identity: UnderSystem-assigned managed identity, selectMachine learning online endpointand search by endpoint name.User-assigned identity: SelectUser-assigned managed identity, and search by identity name.
SelectManaged identityand then select members:
System-assigned identity: UnderSystem-assigned managed identity, selectMachine learning online endpointand search by endpoint name.
User-assigned identity: SelectUser-assigned managed identity, and search by identity name.
For user-assigned identity, you need to grant permissions to the hub container registry and storage account. You can find the container registry and storage account on the hub overview page in the Azure portal.Go to the hub container registry overview page and selectAccess control>Add role assignment. AssignACR Pullto the endpoint identity.Go to the hub default storage overview page and selectAccess control>Add role assignment. AssignStorage Blob Data Readerto the endpoint identity.
For user-assigned identity, you need to grant permissions to the hub container registry and storage account. You can find the container registry and storage account on the hub overview page in the Azure portal.

Go to the hub container registry overview page and selectAccess control>Add role assignment. AssignACR Pullto the endpoint identity.
Go to the hub default storage overview page and selectAccess control>Add role assignment. AssignStorage Blob Data Readerto the endpoint identity.
Optional: For user-assigned identity, if you want to monitor the endpoint-related metrics like CPU/GPU/Disk/Memory utilization, you need to grant theWorkspace metrics writerrole of the hub to the identity.
Optional: For user-assigned identity, if you want to monitor the endpoint-related metrics like CPU/GPU/Disk/Memory utilization, you need to grant theWorkspace metrics writerrole of the hub to the identity.
Check the status of the endpoint
You receive notifications after you finish the deployment wizard. After the endpoint and deployment are created successfully, selectView detailsin the notification to deployment detail page.
You can also go directly to theModel + endpointspage on the left pane, select the deployment, and check the status.
Test the endpoint
On the deployment detail page, select theTesttab.
For endpoints deployed from standard flow, you can input values in the form editor or JSON editor to test the endpoint.
Test the endpoint deployed from a chat flow
For endpoints deployed from a chat flow, you can test it in an immersive chat window.
Thechat_inputmessage was set during the development of the chat flow. You can put thechat_inputmessage in the input box. If your flow has multiple inputs, you can specify the values for other inputs besides thechat_inputmessage on theInputspane on the right side.
chat_input
chat_input
chat_input
Consume the endpoint
On the deployment detail page, select theConsumetab. You can find the REST endpoint and key/token to consume your endpoint. Sample code is also available for you to consume the endpoint in different languages.

You need to input values forRequestBodyordataandapi_key. For example, if your flow has two inputs,locationandurl, you need to specify data as the following example:
RequestBody
data
api_key
location
url
{
"location": "LA",
"url": "<the_url_to_be_classified>"
}
{
"location": "LA",
"url": "<the_url_to_be_classified>"
}
Clean up resources
If you aren't going to use the endpoint after you finish this tutorial, delete the endpoint. The complete deletion might take approximately 20 minutes.
Related content
Learn more about what you can do inAzure AI Foundry.
Get answers to frequently asked questions in theAzure AI Foundry FAQ.
Enable trace and collect feedback for your deployment.
Feedback
Was this page helpful?
Additional resources