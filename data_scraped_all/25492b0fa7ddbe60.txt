Analytics end-to-end with Azure Synapse
The solution described in this article combines a range of Azure services that will ingest, store, process, enrich, and serve data and insights from different sources (structured, semi-structured, unstructured, and streaming).
Architecture

Download aVisio fileof this architecture.
Note
The services covered by this architecture are only a subset of a much larger family of Azure services. Similar outcomes can be achieved by using other services or features that are not covered by this design.
Specific business requirements for your analytics use case could require the use of different services or features that are not considered in this design.
Dataflow
The analytics use cases covered by the architecture are illustrated by the different data sources on the left-hand side of the diagram. Data flows through the solution from the bottom up as follows:
Note
In the following sections, Azure Data Lake is used as the home for data throughout the various stages of the data lifecycle. Azure Data Lake is organized by different layers and containers as follows:
The Raw layer is the landing area for data coming in from source systems. As the name implies, data in this layer is in raw, unfiltered, and unpurified form.
In the next stage of the lifecycle, data moves to the Enriched layer where data is cleaned, filtered, and possibly transformed.
Data then moves to the Curated layer, which is where consumer-ready data is maintained.
Please refer to theData lake zones and containersdocumentation for a full review of Azure Data Lake layers and containers and their uses.
Azure Synapse Link for Azure Cosmos DBandAzure Synapse Link for Dataverseenable you to run near real-time analytics over operational and business application data, by using the analytics engines that are available from your Azure Synapse workspace:SQL ServerlessandSpark Pools.
Azure Synapse Link for Azure Cosmos DBandAzure Synapse Link for Dataverseenable you to run near real-time analytics over operational and business application data, by using the analytics engines that are available from your Azure Synapse workspace:SQL ServerlessandSpark Pools.
When using Azure Synapse Link for Azure Cosmos DB, use either aSQL Serverless queryor aSpark Pool notebook. You can access theAzure Cosmos DB analytical storeand then combine datasets from your near real-time operational data with data from your data lake or from your data warehouse.
When using Azure Synapse Link for Azure Cosmos DB, use either aSQL Serverless queryor aSpark Pool notebook. You can access theAzure Cosmos DB analytical storeand then combine datasets from your near real-time operational data with data from your data lake or from your data warehouse.
When using Azure Synapse Link for Dataverse, use either aSQL Serverless queryor aSpark Pool notebook. You can access the selected Dataverse tables and then combine datasets from your near real-time business applications data with data from your data lake or from your data warehouse.
When using Azure Synapse Link for Dataverse, use either aSQL Serverless queryor aSpark Pool notebook. You can access the selected Dataverse tables and then combine datasets from your near real-time business applications data with data from your data lake or from your data warehouse.
The resulting datasets from yourSQL Serverless queriescan be persisted in your data lake. If you are usingSpark notebooks, the resulting datasets can be persisted either in your data lake or data warehouse (SQL pool).
Load relevant data from the Azure Synapse SQL pool or data lake intoPower BI datasetsfor data visualization and exploration.Power BI modelsimplement a semantic model to simplify the analysis of business data and relationships. Business analysts usePower BIreports and dashboards to analyze data and derive business insights.
Load relevant data from the Azure Synapse SQL pool or data lake intoPower BI datasetsfor data visualization and exploration.Power BI modelsimplement a semantic model to simplify the analysis of business data and relationships. Business analysts usePower BIreports and dashboards to analyze data and derive business insights.
Data can also be securely shared to other business units or external trusted partners usingAzure Data Share. Data consumers have the freedom to choose what data format they want to use and also what compute engine is best to process the shared datasets.
Data can also be securely shared to other business units or external trusted partners usingAzure Data Share. Data consumers have the freedom to choose what data format they want to use and also what compute engine is best to process the shared datasets.
Structured and unstructured data stored in your Synapse workspace can also be used to buildknowledge mining solutionsand use AI to uncover valuable business insights across different document types and formats including from Office documents, PDFs, images, audio, forms, and web pages.
Structured and unstructured data stored in your Synapse workspace can also be used to buildknowledge mining solutionsand use AI to uncover valuable business insights across different document types and formats including from Office documents, PDFs, images, audio, forms, and web pages.
UseAzure Synapse pipelinesto pull data from a wide variety of databases, both on-premises and in the cloud. Pipelines can be triggered based on a pre-defined schedule, in response to an event, or can be explicitly called via REST APIs.
Within the Raw data lake layer,organize your data lakefollowing the best practices around which layers to create, what folder structures to use in each layer and what files format to use for each analytics scenario.
Within the Raw data lake layer,organize your data lakefollowing the best practices around which layers to create, what folder structures to use in each layer and what files format to use for each analytics scenario.
From the Azure Synapse pipeline, use aCopy data activityto stage the data copied from the relational databases into theraw layerof yourAzure Data Lake Store Gen 2data lake. You can save the data in delimited text format or compressed as Parquet files.
From the Azure Synapse pipeline, use aCopy data activityto stage the data copied from the relational databases into theraw layerof yourAzure Data Lake Store Gen 2data lake. You can save the data in delimited text format or compressed as Parquet files.
Use eitherdata flows,SQL serverless queries, orSpark notebooksto validate, transform, and move the datasets from the Raw layer, through the Enriched layer and into your Curated layer in your data lake.As part of your data transformations, you can invoke machine-training models from yourSQL pools using standard T-SQLor Spark notebooks. These ML models can be used to enrich your datasets and generate further business insights. These machine-learning models can be consumed fromAzure AI servicesorcustom ML models from Azure ML.
Use eitherdata flows,SQL serverless queries, orSpark notebooksto validate, transform, and move the datasets from the Raw layer, through the Enriched layer and into your Curated layer in your data lake.
As part of your data transformations, you can invoke machine-training models from yourSQL pools using standard T-SQLor Spark notebooks. These ML models can be used to enrich your datasets and generate further business insights. These machine-learning models can be consumed fromAzure AI servicesorcustom ML models from Azure ML.
You can serve your final dataset directly from the data lake Curated layer or you can use Copy Data activity to ingest the final dataset into your SQL pool tables using theCOPY commandfor fast ingestion.
You can serve your final dataset directly from the data lake Curated layer or you can use Copy Data activity to ingest the final dataset into your SQL pool tables using theCOPY commandfor fast ingestion.
Load relevant data from the Azure Synapse SQL pool or data lake intoPower BI datasetsfor data visualization.Power BI modelsimplement a semantic model to simplify the analysis of business data and relationships. Business analysts usePower BIreports and dashboards to analyze data and derive business insights.
Load relevant data from the Azure Synapse SQL pool or data lake intoPower BI datasetsfor data visualization.Power BI modelsimplement a semantic model to simplify the analysis of business data and relationships. Business analysts usePower BIreports and dashboards to analyze data and derive business insights.
Data can also be securely shared to other business units or external trusted partners usingAzure Data Share. Data consumers have the freedom to choose what data format they want to use and also what compute engine is best to process the shared datasets.
Data can also be securely shared to other business units or external trusted partners usingAzure Data Share. Data consumers have the freedom to choose what data format they want to use and also what compute engine is best to process the shared datasets.
Structured and unstructured data stored in your Synapse workspace can also be used to buildknowledge mining solutionsand use AI to uncover valuable business insights across different document types and formats including from Office documents, PDFs, images, audio, forms, and web pages.
Structured and unstructured data stored in your Synapse workspace can also be used to buildknowledge mining solutionsand use AI to uncover valuable business insights across different document types and formats including from Office documents, PDFs, images, audio, forms, and web pages.
UseAzure Synapse pipelinesto pull data from a wide variety of semi-structured data sources, both on-premises and in the cloud. For example:Ingest data from file-based sources containing CSV or JSON files.Connect to No-SQL databases such as Azure Cosmos DB or MongoDB.Call REST APIs provided by SaaS applications that will function as your data source for the pipeline.
UseAzure Synapse pipelinesto pull data from a wide variety of semi-structured data sources, both on-premises and in the cloud. For example:
Ingest data from file-based sources containing CSV or JSON files.
Connect to No-SQL databases such as Azure Cosmos DB or MongoDB.
Call REST APIs provided by SaaS applications that will function as your data source for the pipeline.
Within the Raw data lake layer,organize your data lakefollowing the best practices around which layers to create, what folder structures to use in each layer and what files format to use for each analytics scenario.
Within the Raw data lake layer,organize your data lakefollowing the best practices around which layers to create, what folder structures to use in each layer and what files format to use for each analytics scenario.
From the Azure Synapse pipeline, use aCopy data activityto stage the data copied from the semi-structured data sources into theraw layerof yourAzure Data Lake Store Gen 2data lake. Save data to preserve the original format, as acquired from the data sources.
From the Azure Synapse pipeline, use aCopy data activityto stage the data copied from the semi-structured data sources into theraw layerof yourAzure Data Lake Store Gen 2data lake. Save data to preserve the original format, as acquired from the data sources.
For batch/micro-batch pipelines, use eitherdata flows,SQL serverless queriesorSpark notebooksto validate, transform, and move your datasets into your Curated layer in your data lake. SQL Serverless queries expose underlyingCSV,Parquet, orJSONfiles as external tables, so that they can be queried using T-SQL.As part of your data transformations, you can invoke machine-learning models from yourSQL pools using standard T-SQLor Spark notebooks. These ML models can be used to enrich your datasets and generate further business insights. These machine-learning models can be consumed fromAzure AI servicesorcustom ML models from Azure ML.
For batch/micro-batch pipelines, use eitherdata flows,SQL serverless queriesorSpark notebooksto validate, transform, and move your datasets into your Curated layer in your data lake. SQL Serverless queries expose underlyingCSV,Parquet, orJSONfiles as external tables, so that they can be queried using T-SQL.
As part of your data transformations, you can invoke machine-learning models from yourSQL pools using standard T-SQLor Spark notebooks. These ML models can be used to enrich your datasets and generate further business insights. These machine-learning models can be consumed fromAzure AI servicesorcustom ML models from Azure ML.
For near real-time telemetry and time-series analytics scenarios, useData Explorer poolsto easilyingest, consolidate, and correlate logs and IoT events data across multiple data sources. With Data Explorer pools, you can useKusto queries (KQL)to performtime-series analysis,geospatial clustering, and machine learning enrichment.
For near real-time telemetry and time-series analytics scenarios, useData Explorer poolsto easilyingest, consolidate, and correlate logs and IoT events data across multiple data sources. With Data Explorer pools, you can useKusto queries (KQL)to performtime-series analysis,geospatial clustering, and machine learning enrichment.
You can serve your final dataset directly from the data lake Curated layer or you can use Copy Data activity to ingest the final dataset into your SQL pool tables using theCOPY commandfor fast ingestion.
You can serve your final dataset directly from the data lake Curated layer or you can use Copy Data activity to ingest the final dataset into your SQL pool tables using theCOPY commandfor fast ingestion.
Load relevant data from the Azure SynapseSQL pools,Data Explorer pools, or adata lakeintoPower BI datasetsfor data visualization.Power BI modelsimplement a semantic model to simplify the analysis of business data and relationships. Business analysts usePower BIreports and dashboards to analyze data and derive business insights.
Load relevant data from the Azure SynapseSQL pools,Data Explorer pools, or adata lakeintoPower BI datasetsfor data visualization.Power BI modelsimplement a semantic model to simplify the analysis of business data and relationships. Business analysts usePower BIreports and dashboards to analyze data and derive business insights.
Data can also be securely shared to other business units or external trusted partners usingAzure Data Share. Data consumers have the freedom to choose what data format they want to use and also what compute engine is best to process the shared datasets.
Data can also be securely shared to other business units or external trusted partners usingAzure Data Share. Data consumers have the freedom to choose what data format they want to use and also what compute engine is best to process the shared datasets.
Structured and unstructured data stored in your Synapse workspace can also be used to buildknowledge mining solutionsand use AI to uncover valuable business insights across different document types and formats including from Office documents, PDFs, images, audio, forms, and web pages.
Structured and unstructured data stored in your Synapse workspace can also be used to buildknowledge mining solutionsand use AI to uncover valuable business insights across different document types and formats including from Office documents, PDFs, images, audio, forms, and web pages.
UseAzure Synapse pipelinesto pull data from a wide variety of non-structured  data sources, both on-premises and in the cloud. For example:Ingest video, image, audio, or free text from file-based sources that contain the source files.Call REST APIs provided by SaaS applications that will function as your data source for the pipeline.
UseAzure Synapse pipelinesto pull data from a wide variety of non-structured  data sources, both on-premises and in the cloud. For example:
Ingest video, image, audio, or free text from file-based sources that contain the source files.
Call REST APIs provided by SaaS applications that will function as your data source for the pipeline.
Within the Raw data lake layer,organize your data lakeby following the best practices about which layers to create, what folder structures to use in each layer, and what files format to use for each analytics scenario.
Within the Raw data lake layer,organize your data lakeby following the best practices about which layers to create, what folder structures to use in each layer, and what files format to use for each analytics scenario.
From the Azure Synapse pipeline, use aCopy data activityto stage the data copied from the non-structured data sources into theraw layerof yourAzure Data Lake Store Gen 2data lake. Save data by preserving the original format, as acquired from the data sources.
From the Azure Synapse pipeline, use aCopy data activityto stage the data copied from the non-structured data sources into theraw layerof yourAzure Data Lake Store Gen 2data lake. Save data by preserving the original format, as acquired from the data sources.
UseSpark notebooksto validate, transform, enrich, and move your datasets from the Raw layer, through the Enriched layer and  into your Curated layer in your data lake.As part of your data transformations, you can invoke machine-learning models from yourSQL pools using standard T-SQLor Spark notebooks. These ML models can be used to enrich your datasets and generate further business insights. These machine-learning models can be consumed fromAzure AI servicesorcustom ML models from Azure ML.
UseSpark notebooksto validate, transform, enrich, and move your datasets from the Raw layer, through the Enriched layer and  into your Curated layer in your data lake.
As part of your data transformations, you can invoke machine-learning models from yourSQL pools using standard T-SQLor Spark notebooks. These ML models can be used to enrich your datasets and generate further business insights. These machine-learning models can be consumed fromAzure AI servicesorcustom ML models from Azure ML.
You can serve your final dataset directly from the data lake Curated layer or you can use Copy Data activity to ingest the final dataset into your data warehouse tables using theCOPY commandfor fast ingestion.
You can serve your final dataset directly from the data lake Curated layer or you can use Copy Data activity to ingest the final dataset into your data warehouse tables using theCOPY commandfor fast ingestion.
Load relevant data from the Azure Synapse SQL pool or data lake intoPower BI datasetsfor data visualization.Power BI modelsimplement a semantic model to simplify the analysis of business data and relationships.
Load relevant data from the Azure Synapse SQL pool or data lake intoPower BI datasetsfor data visualization.Power BI modelsimplement a semantic model to simplify the analysis of business data and relationships.
Business analysts usePower BIreports and dashboards to analyze data and derive business insights.
Business analysts usePower BIreports and dashboards to analyze data and derive business insights.
Data can also be securely shared to other business units or external trusted partners usingAzure Data Share. Data consumers have the freedom to choose what data format they want to use and also what compute engine is best to process the shared datasets.
Data can also be securely shared to other business units or external trusted partners usingAzure Data Share. Data consumers have the freedom to choose what data format they want to use and also what compute engine is best to process the shared datasets.
Structured and unstructured data stored in your Synapse workspace can also be used to buildknowledge mining solutionsand use AI to uncover valuable business insights across different document types and formats including from Office documents, PDFs, images, audio, forms, and web pages.
Structured and unstructured data stored in your Synapse workspace can also be used to buildknowledge mining solutionsand use AI to uncover valuable business insights across different document types and formats including from Office documents, PDFs, images, audio, forms, and web pages.
UseAzure Event Hubs or Azure IoT Hubsto ingest data streams generated by client applications or IoT devices. Event Hubs or IoT Hub will then ingest and store streaming data preserving the sequence of events received. Consumers can then connect to Event Hubs or IoT Hub endpoints and retrieve messages for processing.
Within the Raw data lake layer,organize your data lakefollowing the best practices around which layers to create, what folder structures to use in each layer and what files format to use for each analytics scenario.
Within the Raw data lake layer,organize your data lakefollowing the best practices around which layers to create, what folder structures to use in each layer and what files format to use for each analytics scenario.
ConfigureEvent Hubs CaptureorIoT Hub Storage Endpointsto save a copy of the events into theRaw layerof yourAzure Data Lake Store Gen 2data lake. This feature implements the "Cold Path" of theLambda architecture patternand allows you to perform historical and trend analysis on the stream data saved in your data lake usingSQL Serverless queriesorSpark notebooksfollowing the pattern for semi-structured data sources described above.
ConfigureEvent Hubs CaptureorIoT Hub Storage Endpointsto save a copy of the events into theRaw layerof yourAzure Data Lake Store Gen 2data lake. This feature implements the "Cold Path" of theLambda architecture patternand allows you to perform historical and trend analysis on the stream data saved in your data lake usingSQL Serverless queriesorSpark notebooksfollowing the pattern for semi-structured data sources described above.
For real-time insights, use aStream Analytics jobto implement the "Hot Path" of theLambda architecture patternand derive insights from the stream data in transit. Define at least one input for the data stream coming from yourEvent HubsorIoT Hub, one query to process the input data stream and one Power BI output to where the query results will be sent to.As part of your data processing with Stream Analytics, you can invoke machine-learning models to enrich your stream datasets and drive business decisions based on the predictions generated. These machine-learning models can be consumed from Azure AI services or fromcustom ML models in Azure Machine learning.
For real-time insights, use aStream Analytics jobto implement the "Hot Path" of theLambda architecture patternand derive insights from the stream data in transit. Define at least one input for the data stream coming from yourEvent HubsorIoT Hub, one query to process the input data stream and one Power BI output to where the query results will be sent to.
As part of your data processing with Stream Analytics, you can invoke machine-learning models to enrich your stream datasets and drive business decisions based on the predictions generated. These machine-learning models can be consumed from Azure AI services or fromcustom ML models in Azure Machine learning.
Use other Stream Analytics job outputs to send processed events to Azure SynapseSQL poolsorData Explorer poolsfor further analytics use cases.
Use other Stream Analytics job outputs to send processed events to Azure SynapseSQL poolsorData Explorer poolsfor further analytics use cases.
For near real-time telemetry and time-series analytics scenarios, useData Explorer poolsto easily ingest IoT events directly fromEvent HubsorIoT Hubs. With Data Explorer pools, you can useKusto queries (KQL)to performtime-series analysis,geospatial clustering, and machine learning enrichment.
For near real-time telemetry and time-series analytics scenarios, useData Explorer poolsto easily ingest IoT events directly fromEvent HubsorIoT Hubs. With Data Explorer pools, you can useKusto queries (KQL)to performtime-series analysis,geospatial clustering, and machine learning enrichment.
Business analysts then usePower BI real-time datasets and dashboardcapabilities to visualize the fast changing insights generated by your Stream Analytics query.
Business analysts then usePower BI real-time datasets and dashboardcapabilities to visualize the fast changing insights generated by your Stream Analytics query.
Data can also be securely shared to other business units or external trusted partners usingAzure Data Share. Data consumers have the freedom to choose what data format they want to use and also what compute engine is best to process the shared datasets.
Data can also be securely shared to other business units or external trusted partners usingAzure Data Share. Data consumers have the freedom to choose what data format they want to use and also what compute engine is best to process the shared datasets.
Structured and unstructured data stored in your Synapse workspace can also be used to buildknowledge mining solutionsand use AI to uncover valuable business insights across different document types and formats including from Office documents, PDFs, images, audio, forms and web pages.
Structured and unstructured data stored in your Synapse workspace can also be used to buildknowledge mining solutionsand use AI to uncover valuable business insights across different document types and formats including from Office documents, PDFs, images, audio, forms and web pages.
Components
The following Azure services have been used in the architecture:
Azure Synapse Analytics
Azure Data Lake Gen2
Azure Cosmos DB
Azure AI services
Azure Machine Learning
Azure Event Hubs
Azure IoT Hub
Azure Stream Analytics
Microsoft Purview
Azure Data Share
Microsoft Power BI
Microsoft Entra ID
Microsoft Cost Management
Azure Key Vault
Azure Monitor
Microsoft Defender for Cloud
Azure DevOps
Azure Policy
GitHub
Alternatives
In the architecture above, Azure Synapse pipelines are responsible for data pipeline orchestration.Azure Data Factorypipelines also provide the same capabilities as described in this article.
In the architecture above, Azure Synapse pipelines are responsible for data pipeline orchestration.Azure Data Factorypipelines also provide the same capabilities as described in this article.
Azure Databrickscan also be used as the compute engine used to process structured and unstructured data directly on the data lake.
Azure Databrickscan also be used as the compute engine used to process structured and unstructured data directly on the data lake.
In the architecture above, Azure Stream Analytics is the service responsible for processing streaming data. Azure Synapse Spark pools and Azure Databricks can also be used to perform the same role through the execution of notebooks.
In the architecture above, Azure Stream Analytics is the service responsible for processing streaming data. Azure Synapse Spark pools and Azure Databricks can also be used to perform the same role through the execution of notebooks.
Azure HDInsight Kafkaclusters can also be used to ingest streaming data and provide the right level of performance and scalability required by large streaming workloads.
Azure HDInsight Kafkaclusters can also be used to ingest streaming data and provide the right level of performance and scalability required by large streaming workloads.
You also can make use ofAzure Functionsto invoke Azure AI services or Azure Machine Learning custom ML models from an Azure Synapse pipeline.
You also can make use ofAzure Functionsto invoke Azure AI services or Azure Machine Learning custom ML models from an Azure Synapse pipeline.
The technologies in this architecture were chosen because each of them provides the necessary functionality to handle the most common data challenges in an organization. These services meet the requirements for scalability and availability, while helping them control costs. The services covered by this architecture are only a subset of a much larger family of Azure services. Similar outcomes can be achieved by using other services or features not covered by this design.
The technologies in this architecture were chosen because each of them provides the necessary functionality to handle the most common data challenges in an organization. These services meet the requirements for scalability and availability, while helping them control costs. The services covered by this architecture are only a subset of a much larger family of Azure services. Similar outcomes can be achieved by using other services or features not covered by this design.
Specific business requirements for your analytics use cases may also ask for the use of different services or features not considered in this design.
Specific business requirements for your analytics use cases may also ask for the use of different services or features not considered in this design.
For comparisons of other alternatives, see:Choosing a data pipeline orchestration technology in AzureChoosing a batch processing technology in AzureChoosing an analytical data store in AzureChoosing a data analytics technology in AzureChoosing a stream processing technology in Azure
For comparisons of other alternatives, see:
Choosing a data pipeline orchestration technology in Azure
Choosing a batch processing technology in Azure
Choosing an analytical data store in Azure
Choosing a data analytics technology in Azure
Choosing a stream processing technology in Azure
Scenario details
This example scenario demonstrates how to use Azure Synapse Analytics with the extensive family of Azure Data Services to build a modern data platform that's capable of handling the most common data challenges in an organization.
Potential use cases
This approach can also be used to:
Establish adata productarchitecture, which consists of a data warehouse for structured data and a data lake for semi-structured and unstructured data. You can choose to deploy a single data product for centralized environments or multiple data products for distributed environments such as Data Mesh. See more information aboutData Management and Data Landing Zones.
Integrate relational data sources with other unstructured datasets, with the use of big data processing technologies.
Use semantic modeling and powerful visualization tools for simpler data analysis.
Share datasets within the organization or with trusted external partners.
Implement knowledge mining solutions to extract valuable business information hidden in images, PDFs, documents, and so on.
Recommendations
Discover and govern
Data governance is a common challenge in large enterprise environments. On one hand, business analysts need to be able to discover and understand data assets that can help them solve business problems. On the other hand, Chief Data Officers want insights on privacy and security of business data.
UseMicrosoft Purviewfordata discoveryand insights on yourdata assets,data classification, andsensitivity, which covers the entire organizational data landscape.
UseMicrosoft Purviewfordata discoveryand insights on yourdata assets,data classification, andsensitivity, which covers the entire organizational data landscape.
Microsoft Purview can help you maintain abusiness glossarywith the specific business terminology required for users to understand the semantics of what datasets mean and how they are meant to be used across the organization.
Microsoft Purview can help you maintain abusiness glossarywith the specific business terminology required for users to understand the semantics of what datasets mean and how they are meant to be used across the organization.
You canregister all your data sourcesand organize them intoCollections, which also serves as a security boundary for your metadata.
You canregister all your data sourcesand organize them intoCollections, which also serves as a security boundary for your metadata.
Setupregular scansto automatically catalog and update relevant metadata about data assets in the organization. Microsoft Purview can also automatically adddata lineageinformation based on information from Azure Data Factory or Azure Synapse pipelines.
Setupregular scansto automatically catalog and update relevant metadata about data assets in the organization. Microsoft Purview can also automatically adddata lineageinformation based on information from Azure Data Factory or Azure Synapse pipelines.
Data classificationanddata sensitivitylabels can be added automatically to your data assets based on pre-configured or customs rules applied during the regular scans.
Data classificationanddata sensitivitylabels can be added automatically to your data assets based on pre-configured or customs rules applied during the regular scans.
Data governance professionals can use the reports andinsightsgenerated by Microsoft Purview to keep control over the entire data landscape and protect the organization against any security and privacy issues.
Data governance professionals can use the reports andinsightsgenerated by Microsoft Purview to keep control over the entire data landscape and protect the organization against any security and privacy issues.
Platform services
In order to improve the quality of your Azure solutions, follow the recommendations and guidelines defined in theAzure Well-Architected Frameworkfive pillars of architecture excellence: Cost Optimization, Operational Excellence, Performance Efficiency, Reliability, and Security.
Following these recommendations, the services below should be considered as part of the design:
Microsoft Entra ID: identity services, single sign-on and multi-factor authentication across Azure workloads.
Microsoft Cost Management: financial governance over your Azure workloads.
Azure Key Vault: secure credential and certificate management. For example,Azure Synapse Pipelines,Azure Synapse Spark PoolsandAzure MLcan retrieve credentials and certificates from Azure Key Vault used to securely access data stores.
Azure Monitor: collect, analyze, and act on telemetry information of your Azure resources to proactively identify problems and maximize performance and reliability.
Microsoft Defender for Cloud: strengthen and monitor the security posture of your Azure workloads.
Azure DevOps&GitHub: implement DevOps practices to enforce automation and compliance to your workload development and deployment pipelines for Azure Synapse and Azure ML.
Azure Policy: implement organizational standards and governance for resource consistency, regulatory compliance, security, cost, and management.
Considerations
These considerations implement the pillars of the Azure Well-Architected Framework, which is a set of guiding tenets that you can use to improve the quality of a workload. For more information, seeWell-Architected Framework.
Cost Optimization
Cost Optimization focuses on ways to reduce unnecessary expenses and improve operational efficiencies. For more information, seeDesign review checklist for Cost Optimization.
In general, use theAzure pricing calculatorto estimate costs. The ideal individual pricing tier and the total overall cost of each service included in the architecture is dependent on the amount of data to be processed and stored and the acceptable performance level expected. Use the guide below to learn more about how each service is priced:
Azure Synapse Analyticsserverless architecture allows you to scale your compute and storage levels independently. Compute resources are charged based on usage, and you can scale or pause these resources on demand. Storage resources are billed per terabyte, so your costs will increase as you ingest more data.
Azure Synapse Analyticsserverless architecture allows you to scale your compute and storage levels independently. Compute resources are charged based on usage, and you can scale or pause these resources on demand. Storage resources are billed per terabyte, so your costs will increase as you ingest more data.
Azure Data Lake Gen 2is charged based on the amount of data stored and based on the number of transactions to read and write data.
Azure Data Lake Gen 2is charged based on the amount of data stored and based on the number of transactions to read and write data.
Azure Event HubsandAzure IoT Hubsare charged based on the amount of compute resources required to process your message streams.
Azure Event HubsandAzure IoT Hubsare charged based on the amount of compute resources required to process your message streams.
Azure Machine Learningcharges come from the amount of compute resources used to train and deploy your machine-learning models.
Azure Machine Learningcharges come from the amount of compute resources used to train and deploy your machine-learning models.
AI servicesis charged based on the number of call you make to the service APIs.
AI servicesis charged based on the number of call you make to the service APIs.
Microsoft Purviewis priced based on the number of data assets in the catalog and the amount of compute power required to scan them.
Microsoft Purviewis priced based on the number of data assets in the catalog and the amount of compute power required to scan them.
Azure Stream Analyticsis charged based on the amount of compute power required to process your stream queries.
Azure Stream Analyticsis charged based on the amount of compute power required to process your stream queries.
Power BIhas different product options for different requirements.Power BI Embeddedprovides an Azure-based option for embedding Power BI functionality inside your applications. A Power BI Embedded instance is included in the pricing sample above.
Power BIhas different product options for different requirements.Power BI Embeddedprovides an Azure-based option for embedding Power BI functionality inside your applications. A Power BI Embedded instance is included in the pricing sample above.
Azure Cosmos DBis priced based on the amount of storage and compute resources required by your databases.
Azure Cosmos DBis priced based on the amount of storage and compute resources required by your databases.
Similar architecture can also be implemented for pre-production environments where you can develop and test your workloads. Consider the specific requirements for your workloads and the capabilities of each service for a cost-effective pre-production environment.
Deploy this scenario
This article has a companion repository available in GitHub that shows how you automate the deployment of the services covered in this architecture. Follow theAzure analytics end-to-end with Azure Synapse deployment guideto deploy this architecture to your subscription. That deployment guide has detailed instructions and multiple deployment options.
Contributors
This article is being updated and maintained by Microsoft. It was originally written by the following contributors.
Principal author:
Fabio Braga| Principal MTC Technical Architect
To see non-public LinkedIn profiles, sign in to LinkedIn.
Next steps
Review the guidelines defined in theAzure data management and analytics scenariofor scalable analytics environment in Azure.
Review the guidelines defined in theAzure data management and analytics scenariofor scalable analytics environment in Azure.
Explore theData Engineer Learning Paths at Microsoft learnfor further training content and labs on the services involved in this reference architecture.
Explore theData Engineer Learning Paths at Microsoft learnfor further training content and labs on the services involved in this reference architecture.
Review the documentation and deploy the reference architecture using thedeployment guidance available on GitHub.
Review the documentation and deploy the reference architecture using thedeployment guidance available on GitHub.