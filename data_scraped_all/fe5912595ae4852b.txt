Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Azure Data Explorer data ingestion overview
Article
2025-04-21
26 contributors
In this article
Data ingestion involves loading data into a table in your cluster. Azure Data Explorer ensures data validity, converts formats as needed, and performs manipulations like schema matching, organization, indexing, encoding, and compression. Once ingested, data is available for query.
Azure Data Explorer offers one-time ingestion or the establishment of a continuous ingestion pipeline, using either streaming or queued ingestion. To determine which is right for you, seeOne-time data ingestionandContinuous data ingestion.
Note
Data is persisted in storage according to the setretention policy.
One-time data ingestion
One-time ingestion is helpful for the transfer of historical data, filling in missing data, and the initial stages of prototyping and data analysis. This approach facilitates fast data integration without the need for a continuous pipeline commitment.
There are multiple ways to perform one-time data ingestion. Use the following decision tree to determine the most suitable option for your use case:

"Diagram that acts as a decision tree for one-time ingestion. If you're ingesting historical data, you should follow the guidance in the Ingest historical data document. If you're not ingesting historical data, and the data is in a supported data format, we recommend using the Get data experience. If the data is in an unsupported format, you can integrate with Azure Data Factory or write your own custom code using the Kusto client libraries. Articles with guidance on each of these options are linked to directly following this flow chart."

For more information, see the relevant documentation:
Continuous data ingestion
Continuous ingestion excels in situations demanding immediate insights from live data. For example, continuous ingestion is useful for monitoring systems, log and event data, and real-time analytics.
Continuous data ingestion involves setting up an ingestion pipeline with either streaming or queued ingestion:
Streaming ingestion: This method ensures near-real-time latency for small sets of data per table. Data is ingested in micro batches from a streaming source, initially placed in the row store, and then transferred to column store extents. For more information, seeConfigure streaming ingestion.
Streaming ingestion: This method ensures near-real-time latency for small sets of data per table. Data is ingested in micro batches from a streaming source, initially placed in the row store, and then transferred to column store extents. For more information, seeConfigure streaming ingestion.
Queued ingestion: This method is optimized for high ingestion throughput. Data is batched based on ingestion properties, with small batches then merged and optimized for fast query results. By default, the maximum queued values are 5 minutes, 1000 items, or a total size of 1 GB. The data size limit for a queued ingestion command is 6 GB. This method uses retry mechanisms to mitigate transient failures and follows the 'at least once' messaging semantics to ensure no messages are lost in the process. For more information about queued ingestion, seeIngestion batching policy.
Queued ingestion: This method is optimized for high ingestion throughput. Data is batched based on ingestion properties, with small batches then merged and optimized for fast query results. By default, the maximum queued values are 5 minutes, 1000 items, or a total size of 1 GB. The data size limit for a queued ingestion command is 6 GB. This method uses retry mechanisms to mitigate transient failures and follows the 'at least once' messaging semantics to ensure no messages are lost in the process. For more information about queued ingestion, seeIngestion batching policy.
Note
For most scenarios, we recommend using queued ingestion as it is the more performant option.
There are multiple ways to configure continuous data ingestion. Use the following decision tree to determine the most suitable option for your use case:

"Flow chart for continuous ingestion decision making. First, determine the type and location of your data. For event data, you can create an Event Hubs data connection or ingest data with Apache Kafka. For IoT data, you can create an IoT Hubs data connection. For data in Azure Storage, you can create an Event Grid data connection. For data stored in other places, check the connectors overview to see if there's a dedicated connector that can fit your use case. If so, follow the guidance to use that connector. If not, write custom code using Kusto client libraries. Articles with guidance on each of these options are linked to directly following this flow chart."

For more information, see the relevant documentation:
Note
Streaming ingestion isn't supported for all ingestion methods. For support details, check the documentation for the specific ingestion method.
Direct ingestion with management commands
Azure Data Explorer offers the following ingestion management commands, which ingest data directly to your cluster instead of using the data management service. They should be used only for exploration and prototyping and not in production or high-volume scenarios.
Inline ingestion: The.ingest inline commandcontains the data to ingest being a part of the command text itself. This method is intended for improvised testing purposes.
Ingest from query: The.set, .append, .set-or-append, or .set-or-replace commandsindirectly specifies the data to ingest as the results of a query or a command.
Ingest from storage: The.ingest into commandgets the data to ingest from external storage, such as Azure Blob Storage, accessible by your cluster and pointed-to by the command.
Note
In the event of a failure, ingestion is performed again, and is retried for up to 48 hours using the exponential backoff method for wait time between tries.
Compare ingestion methods
The following table compares the main ingestion methods:
For information on other connectors, seeConnectors overview.
Permissions
The following list describes thepermissionsrequired for various ingestion scenarios:
To create a new table, you must have at least Database User permissions.
To ingest data into an existing table, without changing its schema, you must have at least Table Ingestor permissions.
To change the schema of an existing table, you must have at least Table Admin or Database Admin permissions.
The following table describes the permissions required for each ingestion method:
For more information, seeKusto role-based access control.
The ingestion process
The following steps outline the general ingestion process:
Set batching policy (optional): Data is batched based on theingestion batching policy. For guidance, seeOptimize for throughput.
Set batching policy (optional): Data is batched based on theingestion batching policy. For guidance, seeOptimize for throughput.
Set retention policy (optional): If the database retention policy isn't suitable for your needs, override it at the table level. For more information, seeRetention policy.
Set retention policy (optional): If the database retention policy isn't suitable for your needs, override it at the table level. For more information, seeRetention policy.
Create a table: If you're using the Get data experience, you can create a table as part of the ingestion flow. Otherwise, create a table prior to ingestion in theAzure Data Explorer web UIor with the.create table command.
Create a table: If you're using the Get data experience, you can create a table as part of the ingestion flow. Otherwise, create a table prior to ingestion in theAzure Data Explorer web UIor with the.create table command.
Create a schema mapping:Schema mappingshelp bind source data fields to destination table columns. Different types of mappings are supported, including row-oriented formats like CSV, JSON, and AVRO, and column-oriented formats like Parquet. In most methods, mappings can also beprecreated on the table.
Create a schema mapping:Schema mappingshelp bind source data fields to destination table columns. Different types of mappings are supported, including row-oriented formats like CSV, JSON, and AVRO, and column-oriented formats like Parquet. In most methods, mappings can also beprecreated on the table.
Set update policy (optional): Certain data formats like Parquet, JSON, and Avro enable straightforward ingest-time transformations. For more intricate processing during ingestion, use theupdate policy. This policy automatically executes extractions and transformations on ingested data within the original table, then ingests the modified data into one or more destination tables.
Set update policy (optional): Certain data formats like Parquet, JSON, and Avro enable straightforward ingest-time transformations. For more intricate processing during ingestion, use theupdate policy. This policy automatically executes extractions and transformations on ingested data within the original table, then ingests the modified data into one or more destination tables.
Ingest data: Use your preferred ingestion tool, connector, or method to bring in the data.
Ingest data: Use your preferred ingestion tool, connector, or method to bring in the data.
Related content
Connectors overview
Supported data formats
Supported ingestion properties
Policies overview
Feedback
Was this page helpful?
Additional resources