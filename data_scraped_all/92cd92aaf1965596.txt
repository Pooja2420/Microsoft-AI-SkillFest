Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Deploy and score a machine learning model by using an online endpoint
Article
2025-04-09
17 contributors
In this article
APPLIES TO:Azure CLI ml extension v2 (current)Python SDK azure-ai-ml v2 (current)
In this article, you learn to deploy your model to an online endpoint for use in real-time inferencing. You begin by deploying a model on your local machine to debug any errors. Then, you deploy and test the model in Azure, view the deployment logs, and monitor the service-level agreement (SLA). By the end of this article, you'll have a scalable HTTPS/REST endpoint that you can use for real-time inference.
Online endpoints are endpoints that are used for real-time inferencing. There are two types of online endpoints:managed online endpointsandKubernetes online endpoints. For more information about the differences, seeManaged online endpoints vs Kubernetes online endpoints.
Managed online endpoints help to deploy your machine learning models in a turnkey manner. Managed online endpoints work with powerful CPU and GPU machines in Azure in a scalable, fully managed way. Managed online endpoints take care of serving, scaling, securing, and monitoring your models, freeing you from the overhead of setting up and managing the underlying infrastructure.
The main example in this article uses managed online endpoints for deployment. To use Kubernetes instead, see the notes in this document that are inline with the managed online endpoint discussion.
Prerequisites
Azure CLI
Python SDK
Studio
ARM template
APPLIES TO:Azure CLI ml extensionv2 (current)
TheAzure CLIand themlextension to the Azure CLI, installed and configured. For more information, seeInstall and set up the CLI (v2).
TheAzure CLIand themlextension to the Azure CLI, installed and configured. For more information, seeInstall and set up the CLI (v2).
ml
A Bash shell or a compatible shell, for example, a shell on a Linux system orWindows Subsystem for Linux. The Azure CLI examples in this article assume that you use this type of shell.
A Bash shell or a compatible shell, for example, a shell on a Linux system orWindows Subsystem for Linux. The Azure CLI examples in this article assume that you use this type of shell.
An Azure Machine Learning workspace. For instructions to create a workspace, seeSet up.
An Azure Machine Learning workspace. For instructions to create a workspace, seeSet up.
Azure role-based access control (Azure RBAC) is used to grant access to operations in Azure Machine Learning. To perform the steps in this article, your user account must be assigned theownerorcontributorrole for the Azure Machine Learning workspace, or a custom role allowingMicrosoft.MachineLearningServices/workspaces/onlineEndpoints/*. If you use the studio to create and manage online endpoints or deployments, you need an extra permissionMicrosoft.Resources/deployments/writefrom the resource group owner. For more information, seeManage access to Azure Machine Learning workspaces.
Azure role-based access control (Azure RBAC) is used to grant access to operations in Azure Machine Learning. To perform the steps in this article, your user account must be assigned theownerorcontributorrole for the Azure Machine Learning workspace, or a custom role allowingMicrosoft.MachineLearningServices/workspaces/onlineEndpoints/*. If you use the studio to create and manage online endpoints or deployments, you need an extra permissionMicrosoft.Resources/deployments/writefrom the resource group owner. For more information, seeManage access to Azure Machine Learning workspaces.
Microsoft.MachineLearningServices/workspaces/onlineEndpoints/*
Microsoft.Resources/deployments/write
(Optional) To deploy locally, you mustinstall Docker Engineon your local computer. Wehighly recommendthis option, so it's easier to debug issues.
(Optional) To deploy locally, you mustinstall Docker Engineon your local computer. Wehighly recommendthis option, so it's easier to debug issues.
APPLIES TO:Python SDK azure-ai-mlv2 (current)
An Azure Machine Learning workspace. For steps for creating a workspace, seeCreate the workspace.
An Azure Machine Learning workspace. For steps for creating a workspace, seeCreate the workspace.
The Azure Machine Learning SDK for Python v2. To install the SDK, use the following command:pip install azure-ai-ml azure-identityTo update an existing installation of the SDK to the latest version, use the following command:pip install --upgrade azure-ai-ml azure-identityFor more information, seeAzure Machine Learning Package client library for Python.
The Azure Machine Learning SDK for Python v2. To install the SDK, use the following command:
pip install azure-ai-ml azure-identity
pip install azure-ai-ml azure-identity
To update an existing installation of the SDK to the latest version, use the following command:
pip install --upgrade azure-ai-ml azure-identity
pip install --upgrade azure-ai-ml azure-identity
For more information, seeAzure Machine Learning Package client library for Python.
Azure role-based access control (Azure RBAC) is used to grant access to operations in Azure Machine Learning. To perform the steps in this article, your user account must be assigned theownerorcontributorrole for the Azure Machine Learning workspace, or a custom role allowingMicrosoft.MachineLearningServices/workspaces/onlineEndpoints/*. For more information, seeManage access to Azure Machine Learning workspaces.
Azure role-based access control (Azure RBAC) is used to grant access to operations in Azure Machine Learning. To perform the steps in this article, your user account must be assigned theownerorcontributorrole for the Azure Machine Learning workspace, or a custom role allowingMicrosoft.MachineLearningServices/workspaces/onlineEndpoints/*. For more information, seeManage access to Azure Machine Learning workspaces.
Microsoft.MachineLearningServices/workspaces/onlineEndpoints/*
(Optional) To deploy locally, you mustinstall Docker Engineon your local computer. Wehighly recommendthis option, so it's easier to debug issues.
(Optional) To deploy locally, you mustinstall Docker Engineon your local computer. Wehighly recommendthis option, so it's easier to debug issues.
Before following the steps in this article, make sure you have the following prerequisites:
An Azure subscription. If you don't have an Azure subscription, create a free account before you begin. Try thefree or paid version of Azure Machine Learning.
An Azure subscription. If you don't have an Azure subscription, create a free account before you begin. Try thefree or paid version of Azure Machine Learning.
An Azure Machine Learning workspace and a compute instance. If you don't have these resources, seeCreate resources you need to get started.
An Azure Machine Learning workspace and a compute instance. If you don't have these resources, seeCreate resources you need to get started.
Azure role-based access control (Azure RBAC) is used to grant access to operations in Azure Machine Learning. To perform the steps in this article, your user account must be assigned theownerorcontributorrole for the Azure Machine Learning workspace, or a custom role allowingMicrosoft.MachineLearningServices/workspaces/onlineEndpoints/*. For more information, seeManage access to an Azure Machine Learning workspace.
Azure role-based access control (Azure RBAC) is used to grant access to operations in Azure Machine Learning. To perform the steps in this article, your user account must be assigned theownerorcontributorrole for the Azure Machine Learning workspace, or a custom role allowingMicrosoft.MachineLearningServices/workspaces/onlineEndpoints/*. For more information, seeManage access to an Azure Machine Learning workspace.
Microsoft.MachineLearningServices/workspaces/onlineEndpoints/*
Note
While the Azure CLI and CLI extension for machine learning are used in these steps, they're not the main focus. They're used more as utilities, passing templates to Azure and checking the status of template deployments.
TheAzure CLIand themlextension to the Azure CLI, installed and configured. For more information, seeInstall and set up the CLI (v2).
TheAzure CLIand themlextension to the Azure CLI, installed and configured. For more information, seeInstall and set up the CLI (v2).
ml
A Bash shell or a compatible shell, for example, a shell on a Linux system orWindows Subsystem for Linux. The Azure CLI examples in this article assume that you use this type of shell.
A Bash shell or a compatible shell, for example, a shell on a Linux system orWindows Subsystem for Linux. The Azure CLI examples in this article assume that you use this type of shell.
An Azure Machine Learning workspace. For instructions to create a workspace, seeSet up.
An Azure Machine Learning workspace. For instructions to create a workspace, seeSet up.
Azure role-based access control (Azure RBAC) is used to grant access to operations in Azure Machine Learning. To perform the steps in this article, your user account must be assigned theownerorcontributorrole for the Azure Machine Learning workspace, or a custom role allowingMicrosoft.MachineLearningServices/workspaces/onlineEndpoints/*. For more information, seeManage access to an Azure Machine Learning workspace.
Microsoft.MachineLearningServices/workspaces/onlineEndpoints/*
Ensure that you have enough virtual machine (VM) quota allocated for deployment. Azure Machine Learning reserves 20% of your compute resources for performing upgrades on some VM SKUs. For example, if you request 10 instances in a deployment, you must have a quota of 12 for each number of cores for the VM SKU. Failure to account for the extra compute resources results in an error. There are some VM SKUs that are exempt from the extra quota reservation. For more information on quota allocation, seevirtual machine quota allocation for deployment.
Ensure that you have enough virtual machine (VM) quota allocated for deployment. Azure Machine Learning reserves 20% of your compute resources for performing upgrades on some VM SKUs. For example, if you request 10 instances in a deployment, you must have a quota of 12 for each number of cores for the VM SKU. Failure to account for the extra compute resources results in an error. There are some VM SKUs that are exempt from the extra quota reservation. For more information on quota allocation, seevirtual machine quota allocation for deployment.
Alternatively, you could use quota from Azure Machine Learning's shared quota pool for a limited time. Azure Machine Learning provides a shared quota pool from which users across various regions can access quota to perform testing for a limited time, depending upon availability.
When you use the studio to deploy Llama-2, Phi, Nemotron, Mistral, Dolly, and Deci-DeciLM models from the model catalog to a managed online endpoint, Azure Machine Learning allows you to access its shared quota pool for a short time so that you can perform testing. For more information on the shared quota pool, seeAzure Machine Learning shared quota.
Alternatively, you could use quota from Azure Machine Learning's shared quota pool for a limited time. Azure Machine Learning provides a shared quota pool from which users across various regions can access quota to perform testing for a limited time, depending upon availability.
When you use the studio to deploy Llama-2, Phi, Nemotron, Mistral, Dolly, and Deci-DeciLM models from the model catalog to a managed online endpoint, Azure Machine Learning allows you to access its shared quota pool for a short time so that you can perform testing. For more information on the shared quota pool, seeAzure Machine Learning shared quota.
Prepare your system
Azure CLI
Python SDK
Studio
ARM template
Set environment variables
If you haven't already set the defaults for the Azure CLI, save your default settings. To avoid passing in the values for your subscription, workspace, and resource group multiple times, run this code:
az account set --subscription <subscription ID>
az configure --defaults workspace=<Azure Machine Learning workspace name> group=<resource group>
az account set --subscription <subscription ID>
az configure --defaults workspace=<Azure Machine Learning workspace name> group=<resource group>
Clone the examples repository
To follow along with this article, first clone theazureml-examples repository, then change into the repository'sazureml-examples/clidirectory:
git clone --depth 1 https://github.com/Azure/azureml-examples
cd azureml-examples/cli
git clone --depth 1 https://github.com/Azure/azureml-examples
cd azureml-examples/cli
Tip
Use--depth 1to clone only the latest commit to the repository, which reduces time to complete the operation.
--depth 1
The commands in this tutorial are in the filesdeploy-local-endpoint.shanddeploy-managed-online-endpoint.shin theclidirectory, and the YAML configuration files are in theendpoints/online/managed/sample/subdirectory.
Note
The YAML configuration files for Kubernetes online endpoints are in theendpoints/online/kubernetes/subdirectory.
Clone the examples repository
To run the training examples, first clone theazureml-examples repository, then change into theazureml-examples/sdk/python/endpoints/online/manageddirectory:
git clone --depth 1 https://github.com/Azure/azureml-examples
cd azureml-examples/sdk/python/endpoints/online/managed
git clone --depth 1 https://github.com/Azure/azureml-examples
cd azureml-examples/sdk/python/endpoints/online/managed
Tip
Use--depth 1to clone only the latest commit to the repository, which reduces time to complete the operation.
--depth 1
The information in this article is based on theonline-endpoints-simple-deployment.ipynbnotebook. It contains the same content as this article, although the order of the codes is slightly different.
Connect to Azure Machine Learning workspace
Theworkspaceis the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section, you connect to the workspace in which you'll perform deployment tasks. To follow along, open youronline-endpoints-simple-deployment.ipynbnotebook.
Import the required libraries:# import required libraries
from azure.ai.ml import MLClient
from azure.ai.ml.entities import (
    ManagedOnlineEndpoint,
    ManagedOnlineDeployment,
    Model,
    Environment,
    CodeConfiguration
)
from azure.identity import DefaultAzureCredentialNoteIf you're using the Kubernetes online endpoint, import theKubernetesOnlineEndpointandKubernetesOnlineDeploymentclass from theazure.ai.ml.entitieslibrary.
Import the required libraries:
# import required libraries
from azure.ai.ml import MLClient
from azure.ai.ml.entities import (
    ManagedOnlineEndpoint,
    ManagedOnlineDeployment,
    Model,
    Environment,
    CodeConfiguration
)
from azure.identity import DefaultAzureCredential
# import required libraries
from azure.ai.ml import MLClient
from azure.ai.ml.entities import (
    ManagedOnlineEndpoint,
    ManagedOnlineDeployment,
    Model,
    Environment,
    CodeConfiguration
)
from azure.identity import DefaultAzureCredential
Note
If you're using the Kubernetes online endpoint, import theKubernetesOnlineEndpointandKubernetesOnlineDeploymentclass from theazure.ai.ml.entitieslibrary.
KubernetesOnlineEndpoint
KubernetesOnlineDeployment
azure.ai.ml.entities
Configure workspace details and get a handle to the workspace:To connect to a workspace, you need identifier parameters: a subscription, resource group, and workspace name. You use these details in theMLClientfromazure.ai.mlto get a handle to the required Azure Machine Learning workspace. This example uses thedefault Azure authentication.# enter details of your Azure Machine Learning workspace
subscription_id = "<subscription ID>"
resource_group = "<resource group>"
workspace = "<workspace name>"# get a handle to the workspace
ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)
Configure workspace details and get a handle to the workspace:
To connect to a workspace, you need identifier parameters: a subscription, resource group, and workspace name. You use these details in theMLClientfromazure.ai.mlto get a handle to the required Azure Machine Learning workspace. This example uses thedefault Azure authentication.
MLClient
azure.ai.ml
# enter details of your Azure Machine Learning workspace
subscription_id = "<subscription ID>"
resource_group = "<resource group>"
workspace = "<workspace name>"
# enter details of your Azure Machine Learning workspace
subscription_id = "<subscription ID>"
resource_group = "<resource group>"
workspace = "<workspace name>"
# get a handle to the workspace
ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)
# get a handle to the workspace
ml_client = MLClient(
    DefaultAzureCredential(), subscription_id, resource_group, workspace
)
If you have Git installed on your local machine, you can follow the instructions to clone the examples repository. Otherwise, follow the instructions to download files from the examples repository.
Clone the examples repository
To follow along with this article, first clone theazureml-examples repository, then change into theazureml-examples/cli/endpoints/online/model-1directory.
git clone --depth 1 https://github.com/Azure/azureml-examples
cd azureml-examples/cli/endpoints/online/model-1
git clone --depth 1 https://github.com/Azure/azureml-examples
cd azureml-examples/cli/endpoints/online/model-1
Tip
Use--depth 1to clone only the latest commit to the repository, which reduces time to complete the operation.
--depth 1
Download files from the examples repository
If you cloned the examples repo, your local machine already has copies of the files for this example, and you can skip to the next section. If you didn't clone the repo, you can download it to your local machine.
Go tohttps://github.com/Azure/azureml-examples/.
Go tohttps://github.com/Azure/azureml-examples/.
Go to the<> Codebutton on the page, and then selectDownload ZIPfrom theLocaltab.
Go to the<> Codebutton on the page, and then selectDownload ZIPfrom theLocaltab.
Locate the folder/cli/endpoints/online/model-1/modeland the file/cli/endpoints/online/model-1/onlinescoring/score.py.
Locate the folder/cli/endpoints/online/model-1/modeland the file/cli/endpoints/online/model-1/onlinescoring/score.py.
Set environment variables
Set the following environment variables, as they're used in the examples in this article. Replace the values with your Azure subscription ID, the Azure region where your workspace is located, the resource group that contains the workspace, and the workspace name:
export SUBSCRIPTION_ID="<subscription ID>"
export LOCATION="<your region>"
export RESOURCE_GROUP="<resource group>"
export WORKSPACE="<workspace name>"
export SUBSCRIPTION_ID="<subscription ID>"
export LOCATION="<your region>"
export RESOURCE_GROUP="<resource group>"
export WORKSPACE="<workspace name>"
A couple of the template examples require you to upload files to the Azure Blob store for your workspace. The following steps query the workspace and store this information in environment variables used in the examples:
Get an access token:TOKEN=$(az account get-access-token --query accessToken -o tsv)
Get an access token:
TOKEN=$(az account get-access-token --query accessToken -o tsv)
TOKEN=$(az account get-access-token --query accessToken -o tsv)
Set the REST API version:API_VERSION="2022-05-01"
Set the REST API version:
API_VERSION="2022-05-01"
API_VERSION="2022-05-01"
Get the storage information:# Get values for storage account
response=$(curl --location --request GET "https://management.azure.com/subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.MachineLearningServices/workspaces/$WORKSPACE/datastores?api-version=$API_VERSION&isDefault=true" \
--header "Authorization: Bearer $TOKEN")
AZUREML_DEFAULT_DATASTORE=$(echo $response | jq -r '.value[0].name')
AZUREML_DEFAULT_CONTAINER=$(echo $response | jq -r '.value[0].properties.containerName')
export AZURE_STORAGE_ACCOUNT=$(echo $response | jq -r '.value[0].properties.accountName')
Get the storage information:
# Get values for storage account
response=$(curl --location --request GET "https://management.azure.com/subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.MachineLearningServices/workspaces/$WORKSPACE/datastores?api-version=$API_VERSION&isDefault=true" \
--header "Authorization: Bearer $TOKEN")
AZUREML_DEFAULT_DATASTORE=$(echo $response | jq -r '.value[0].name')
AZUREML_DEFAULT_CONTAINER=$(echo $response | jq -r '.value[0].properties.containerName')
export AZURE_STORAGE_ACCOUNT=$(echo $response | jq -r '.value[0].properties.accountName')
# Get values for storage account
response=$(curl --location --request GET "https://management.azure.com/subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.MachineLearningServices/workspaces/$WORKSPACE/datastores?api-version=$API_VERSION&isDefault=true" \
--header "Authorization: Bearer $TOKEN")
AZUREML_DEFAULT_DATASTORE=$(echo $response | jq -r '.value[0].name')
AZUREML_DEFAULT_CONTAINER=$(echo $response | jq -r '.value[0].properties.containerName')
export AZURE_STORAGE_ACCOUNT=$(echo $response | jq -r '.value[0].properties.accountName')
Clone the examples repository
To follow along with this article, first clone theazureml-examples repository, then change into theazureml-examplesdirectory:
git clone --depth 1 https://github.com/Azure/azureml-examples
cd azureml-examples
git clone --depth 1 https://github.com/Azure/azureml-examples
cd azureml-examples
Tip
Use--depth 1to clone only the latest commit to the repository, which reduces time to complete the operation.
--depth 1
Define the endpoint
To define an online endpoint, specify theendpoint nameandauthentication mode. For more information on managed online endpoints, seeOnline endpoints.
Azure CLI
Python SDK
Studio
ARM template
Set an endpoint name
To set your endpoint name, run the following command. Replace<YOUR_ENDPOINT_NAME>with a name that's unique in the Azure region. For more information on the naming rules, seeendpoint limits.
<YOUR_ENDPOINT_NAME>
export ENDPOINT_NAME="<YOUR_ENDPOINT_NAME>"
export ENDPOINT_NAME="<YOUR_ENDPOINT_NAME>"
Configure the endpoint
The following snippet shows theendpoints/online/managed/sample/endpoint.ymlfile:
$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineEndpoint.schema.json
name: my-endpoint
auth_mode: key
$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineEndpoint.schema.json
name: my-endpoint
auth_mode: key
The reference for the endpoint YAML format is described in the following table. To learn how to specify these attributes, see theonline endpoint YAML reference. For information about limits related to managed endpoints, seeAzure Machine Learning online endpoints and batch endpoints.
$schema
name
auth_mode
key
aml_token
aad_token
Configure an endpoint
First define the name of the online endpoint, then configure the endpoint.
Replace<YOUR_ENDPOINT_NAME>with a name that's unique in the Azure region, or use the example method to define a random name. Be sure to delete the method that you don't use. For more information on the naming rules, seeendpoint limits.
<YOUR_ENDPOINT_NAME>
# method 1: define an endpoint name
endpoint_name = "<YOUR_ENDPOINT_NAME>"

# method 2: example way to define a random name
import datetime

endpoint_name = "endpt-" + datetime.datetime.now().strftime("%m%d%H%M%f")

# create an online endpoint
endpoint = ManagedOnlineEndpoint(
    name = endpoint_name, 
    description="this is a sample endpoint",
    auth_mode="key"
)
# method 1: define an endpoint name
endpoint_name = "<YOUR_ENDPOINT_NAME>"

# method 2: example way to define a random name
import datetime

endpoint_name = "endpt-" + datetime.datetime.now().strftime("%m%d%H%M%f")

# create an online endpoint
endpoint = ManagedOnlineEndpoint(
    name = endpoint_name, 
    description="this is a sample endpoint",
    auth_mode="key"
)
The previous code useskeyfor key-based authentication. To use Azure Machine Learning token-based authentication, useaml_token. To use Microsoft Entra token-based authentication (preview), useaad_token. For more information on authenticating, seeAuthenticate clients for online endpoints.
key
aml_token
aad_token
Configure an endpoint
When you deploy to Azure from the studio, you'll create an endpoint and a deployment to add to it. At that time, you'll be prompted to provide names for the endpoint and deployment.
Set an endpoint name
To set your endpoint name, run the following command to generate a random name. It must be unique in the Azure region. For more information on the naming rules, seeendpoint limits.
export ENDPOINT_NAME=endpoint-`echo $RANDOM`
export ENDPOINT_NAME=endpoint-`echo $RANDOM`
Configure the endpoint
To define the endpoint and deployment, this article uses the Azure Resource Manager templatesonline-endpoint.jsonandonline-endpoint-deployment.json. To use the templates for defining an online endpoint and deployment, see theDeploy to Azuresection.
Define the deployment
A deployment is a set of resources required for hosting the model that does the actual inferencing. For this example, you deploy a scikit-learn model that does regression and use a scoring scriptscore.pyto execute the model upon a given input request.
To learn about the key attributes of a deployment, seeOnline deployments.
Configure a deployment
Your deployment configuration uses the location of the model that you wish to deploy.
Azure CLI
Python SDK
Studio
ARM template
The following snippet shows theendpoints/online/managed/sample/blue-deployment.ymlfile, with all the required inputs to configure a deployment:
blue-deployment.yml
$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json
name: blue
endpoint_name: my-endpoint
model:
  path: ../../model-1/model/
code_configuration:
  code: ../../model-1/onlinescoring/
  scoring_script: score.py
environment: 
  conda_file: ../../model-1/environment/conda.yaml
  image: mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu22.04:latest
instance_type: Standard_DS3_v2
instance_count: 1
$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json
name: blue
endpoint_name: my-endpoint
model:
  path: ../../model-1/model/
code_configuration:
  code: ../../model-1/onlinescoring/
  scoring_script: score.py
environment: 
  conda_file: ../../model-1/environment/conda.yaml
  image: mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu22.04:latest
instance_type: Standard_DS3_v2
instance_count: 1
Theblue-deployment.ymlfile specifies the following deployment attributes:
model- specifies the model properties inline, using thepath(where to upload files from). The CLI automatically uploads the model files and registers the model with an autogenerated name.
model
path
environment- using inline definitions that include where to upload files from, the CLI automatically uploads theconda.yamlfile and registers the environment. Later, to build the environment, the deployment uses theimage(in this example, it'smcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest) for the base image, and theconda_filedependencies are installed on top of the base image.
environment
image
mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest
conda_file
code_configuration- during deployment, the local files such as the Python source for the scoring model, are uploaded from the development environment.
code_configuration
For more information about the YAML schema, see theonline endpoint YAML reference.
Note
To use Kubernetes endpoints instead of managed online endpoints as a compute target:
Create and attach your Kubernetes cluster as a compute target to your Azure Machine Learning workspace by usingAzure Machine Learning studio.
Use theendpoint YAMLto target Kubernetes, instead of the managed endpoint YAML. You need to edit the YAML to change the value ofcomputeto the name of your registered compute target. You can use thisdeployment.yamlthat has additional properties applicable to a Kubernetes deployment.
compute
All the commands that are used in this article for managed online endpoints also apply to Kubernetes endpoints, except for the following capabilities that don't apply to Kubernetes endpoints:
The optionalSLA monitoring and Azure Log Analytics integration, using Azure Monitor
Use of Microsoft Entra token
Autoscaling as described in the optionalConfigure autoscalingsection
Use the following code to configure a deployment:
model = Model(path="../model-1/model/sklearn_regression_model.pkl")
env = Environment(
    conda_file="../model-1/environment/conda.yaml",
    image="mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest",
)

blue_deployment = ManagedOnlineDeployment(
    name="blue",
    endpoint_name=endpoint_name,
    model=model,
    environment=env,
    code_configuration=CodeConfiguration(
        code="../model-1/onlinescoring", scoring_script="score.py"
    ),
    instance_type="Standard_DS3_v2",
    instance_count=1,
)
model = Model(path="../model-1/model/sklearn_regression_model.pkl")
env = Environment(
    conda_file="../model-1/environment/conda.yaml",
    image="mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest",
)

blue_deployment = ManagedOnlineDeployment(
    name="blue",
    endpoint_name=endpoint_name,
    model=model,
    environment=env,
    code_configuration=CodeConfiguration(
        code="../model-1/onlinescoring", scoring_script="score.py"
    ),
    instance_type="Standard_DS3_v2",
    instance_count=1,
)
Model- specifies the model properties inline, using thepath(where to upload files from). The SDK automatically uploads the model files and registers the model with an autogenerated name.
Model
path
Environment- using inline definitions that include where to upload files from, the SDK automatically uploads theconda.yamlfile and registers the environment. Later, to build the environment, the deployment uses theimage(in this example, it'smcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest) for the base image, and theconda_filedependencies are installed on top of the base image.
Environment
image
mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest
conda_file
CodeConfiguration- during deployment, the local files such as the Python source for the scoring model, are uploaded from the development environment.
CodeConfiguration
For more information about online deployment definition, seeOnlineDeployment Class.
When you deploy to Azure, you'll create an endpoint and a deployment to add to it. At that time, you'll be prompted to provide names for the endpoint and deployment.
To define the endpoint and deployment, this article uses the Azure Resource Manager templatesonline-endpoint.jsonandonline-endpoint-deployment.json. To use the templates for defining an online endpoint and deployment, see theDeploy to Azuresection.
Understand the scoring script
Tip
The format of the scoring script for online endpoints is the same format that's used in the preceding version of the CLI and in the Python SDK.
Azure CLI
Python SDK
Studio
ARM template
The scoring script specified incode_configuration.scoring_scriptmust have aninit()function and arun()function.
code_configuration.scoring_script
init()
run()
The scoring script must have aninit()function and arun()function.
init()
run()
The scoring script must have aninit()function and arun()function.
init()
run()
The scoring script must have aninit()function and arun()function. This article uses thescore.py file.
init()
run()
When using a template for deployment, you must first upload the scoring file to an Azure Blob store, and then register it:
The following code uses the Azure CLI commandaz storage blob upload-batchto upload the scoring file:az storage blob upload-batch -d $AZUREML_DEFAULT_CONTAINER/score -s cli/endpoints/online/model-1/onlinescoring --account-name $AZURE_STORAGE_ACCOUNT
The following code uses the Azure CLI commandaz storage blob upload-batchto upload the scoring file:
az storage blob upload-batch
az storage blob upload-batch -d $AZUREML_DEFAULT_CONTAINER/score -s cli/endpoints/online/model-1/onlinescoring --account-name $AZURE_STORAGE_ACCOUNT
az storage blob upload-batch -d $AZUREML_DEFAULT_CONTAINER/score -s cli/endpoints/online/model-1/onlinescoring --account-name $AZURE_STORAGE_ACCOUNT
The following code registers the code, using a template:az deployment group create -g $RESOURCE_GROUP \
--template-file arm-templates/code-version.json \
--parameters \
workspaceName=$WORKSPACE \
codeAssetName="score-sklearn" \
codeUri="https://$AZURE_STORAGE_ACCOUNT.blob.core.windows.net/$AZUREML_DEFAULT_CONTAINER/score"
The following code registers the code, using a template:
az deployment group create -g $RESOURCE_GROUP \
--template-file arm-templates/code-version.json \
--parameters \
workspaceName=$WORKSPACE \
codeAssetName="score-sklearn" \
codeUri="https://$AZURE_STORAGE_ACCOUNT.blob.core.windows.net/$AZUREML_DEFAULT_CONTAINER/score"
az deployment group create -g $RESOURCE_GROUP \
--template-file arm-templates/code-version.json \
--parameters \
workspaceName=$WORKSPACE \
codeAssetName="score-sklearn" \
codeUri="https://$AZURE_STORAGE_ACCOUNT.blob.core.windows.net/$AZUREML_DEFAULT_CONTAINER/score"
This example uses thescore.py filefrom the repo you cloned or downloaded earlier:
import os
import logging
import json
import numpy
import joblib


def init():
    """
    This function is called when the container is initialized/started, typically after create/update of the deployment.
    You can write the logic here to perform init operations like caching the model in memory
    """
    global model
    # AZUREML_MODEL_DIR is an environment variable created during deployment.
    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)
    # Please provide your model's folder name if there is one
    model_path = os.path.join(
        os.getenv("AZUREML_MODEL_DIR"), "model/sklearn_regression_model.pkl"
    )
    # deserialize the model file back into a sklearn model
    model = joblib.load(model_path)
    logging.info("Init complete")


def run(raw_data):
    """
    This function is called for every invocation of the endpoint to perform the actual scoring/prediction.
    In the example we extract the data from the json input and call the scikit-learn model's predict()
    method and return the result back
    """
    logging.info("model 1: request received")
    data = json.loads(raw_data)["data"]
    data = numpy.array(data)
    result = model.predict(data)
    logging.info("Request processed")
    return result.tolist()
import os
import logging
import json
import numpy
import joblib


def init():
    """
    This function is called when the container is initialized/started, typically after create/update of the deployment.
    You can write the logic here to perform init operations like caching the model in memory
    """
    global model
    # AZUREML_MODEL_DIR is an environment variable created during deployment.
    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)
    # Please provide your model's folder name if there is one
    model_path = os.path.join(
        os.getenv("AZUREML_MODEL_DIR"), "model/sklearn_regression_model.pkl"
    )
    # deserialize the model file back into a sklearn model
    model = joblib.load(model_path)
    logging.info("Init complete")


def run(raw_data):
    """
    This function is called for every invocation of the endpoint to perform the actual scoring/prediction.
    In the example we extract the data from the json input and call the scikit-learn model's predict()
    method and return the result back
    """
    logging.info("model 1: request received")
    data = json.loads(raw_data)["data"]
    data = numpy.array(data)
    result = model.predict(data)
    logging.info("Request processed")
    return result.tolist()
Theinit()function is called when the container is initialized or started. Initialization typically occurs shortly after the deployment is created or updated. Theinitfunction is the place to write logic for global initialization operations like caching the model in memory (as shown in thisscore.pyfile).
init()
init
Therun()function is called every time the endpoint is invoked, and it does the actual scoring and prediction. In thisscore.pyfile, therun()function extracts data from a JSON input, calls the scikit-learn model'spredict()method, and then returns the prediction result.
run()
run()
predict()
Deploy and debug locally by using a local endpoint
Wehighly recommendthat you test-run your endpoint locally to validate and debug your code and configuration before you deploy to Azure. Azure CLI and Python SDK support local endpoints and deployments, while Azure Machine Learning studio and ARM template don't.
To deploy locally,Docker Enginemust be installed and running. Docker Engine typically starts when the computer starts. If it doesn't, you cantroubleshoot Docker Engine.
Tip
You can useAzure Machine Learning inference HTTP server Python packageto debug your scoring script locallywithout Docker Engine. Debugging with the inference server helps you to debug the scoring script before deploying to local endpoints so that you can debug without being affected by the deployment container configurations.
For more information on debugging online endpoints locally before deploying to Azure, seeOnline endpoint debugging.
Deploy the model locally
First create an endpoint. Optionally, for a local endpoint, you can skip this step and directly create the deployment in the next step, which creates the required metadata. Deploying models locally is useful for development and testing purposes.
Azure CLI
Python SDK
Studio
ARM template
az ml online-endpoint create --local -n $ENDPOINT_NAME -f endpoints/online/managed/sample/endpoint.yml
az ml online-endpoint create --local -n $ENDPOINT_NAME -f endpoints/online/managed/sample/endpoint.yml
ml_client.online_endpoints.begin_create_or_update(endpoint, local=True)
ml_client.online_endpoints.begin_create_or_update(endpoint, local=True)
The studio doesn't support local endpoints. See the Azure CLI or Python tabs for steps to test the endpoint locally.
The template doesn't support local endpoints. See the Azure CLI or Python tabs for steps to test the endpoint locally.
Now, create a deployment namedblueunder the endpoint.
blue
Azure CLI
Python SDK
Studio
ARM template
az ml online-deployment create --local -n blue --endpoint $ENDPOINT_NAME -f endpoints/online/managed/sample/blue-deployment.yml
az ml online-deployment create --local -n blue --endpoint $ENDPOINT_NAME -f endpoints/online/managed/sample/blue-deployment.yml
The--localflag directs the CLI to deploy the endpoint in the Docker environment.
--local
ml_client.online_deployments.begin_create_or_update(
    deployment=blue_deployment, local=True
)
ml_client.online_deployments.begin_create_or_update(
    deployment=blue_deployment, local=True
)
Thelocal=Trueflag directs the SDK to deploy the endpoint in the Docker environment.
local=True
The studio doesn't support local endpoints. See the Azure CLI or Python tabs for steps to test the endpoint locally.
The template doesn't support local endpoints. See the Azure CLI or Python tabs for steps to test the endpoint locally.
Tip
Use Visual Studio Code to test and debug your endpoints locally. For more information, seedebug online endpoints locally in Visual Studio Code.
Verify that the local deployment succeeded
Check the deployment status to see whether the model was deployed without error:
Azure CLI
Python SDK
Studio
ARM template
az ml online-endpoint show -n $ENDPOINT_NAME --local
az ml online-endpoint show -n $ENDPOINT_NAME --local
The output should appear similar to the following JSON. Theprovisioning_stateisSucceeded.
provisioning_state
Succeeded
{
  "auth_mode": "key",
  "location": "local",
  "name": "docs-endpoint",
  "properties": {},
  "provisioning_state": "Succeeded",
  "scoring_uri": "http://localhost:49158/score",
  "tags": {},
  "traffic": {}
}
{
  "auth_mode": "key",
  "location": "local",
  "name": "docs-endpoint",
  "properties": {},
  "provisioning_state": "Succeeded",
  "scoring_uri": "http://localhost:49158/score",
  "tags": {},
  "traffic": {}
}
ml_client.online_endpoints.get(name=endpoint_name, local=True)
ml_client.online_endpoints.get(name=endpoint_name, local=True)
The method returnsManagedOnlineEndpointentity. Theprovisioning_stateisSucceeded.
ManagedOnlineEndpoint
provisioning_state
Succeeded
ManagedOnlineEndpoint({'public_network_access': None, 'provisioning_state': 'Succeeded', 'scoring_uri': 'http://localhost:49158/score', 'swagger_uri': None, 'name': 'endpt-10061534497697', 'description': 'this is a sample endpoint', 'tags': {}, 'properties': {}, 'id': None, 'Resource__source_path': None, 'base_path': '/path/to/your/working/directory', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7ffb781bccd0>, 'auth_mode': 'key', 'location': 'local', 'identity': None, 'traffic': {}, 'mirror_traffic': {}, 'kind': None})
ManagedOnlineEndpoint({'public_network_access': None, 'provisioning_state': 'Succeeded', 'scoring_uri': 'http://localhost:49158/score', 'swagger_uri': None, 'name': 'endpt-10061534497697', 'description': 'this is a sample endpoint', 'tags': {}, 'properties': {}, 'id': None, 'Resource__source_path': None, 'base_path': '/path/to/your/working/directory', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7ffb781bccd0>, 'auth_mode': 'key', 'location': 'local', 'identity': None, 'traffic': {}, 'mirror_traffic': {}, 'kind': None})
The studio doesn't support local endpoints. See the Azure CLI or Python tabs for steps to test the endpoint locally.
The template doesn't support local endpoints. See the Azure CLI or Python tabs for steps to test the endpoint locally.
The following table contains the possible values forprovisioning_state:
provisioning_state
Invoke the local endpoint to score data by using your model
Azure CLI
Python SDK
Studio
ARM template
Invoke the endpoint to score the model by using theinvokecommand and passing query parameters that are stored in a JSON file:
invoke
az ml online-endpoint invoke --local --name $ENDPOINT_NAME --request-file endpoints/online/model-1/sample-request.json
az ml online-endpoint invoke --local --name $ENDPOINT_NAME --request-file endpoints/online/model-1/sample-request.json
If you want to use a REST client (like curl), you must have the scoring URI. To get the scoring URI, runaz ml online-endpoint show --local -n $ENDPOINT_NAME. In the returned data, find thescoring_uriattribute.
az ml online-endpoint show --local -n $ENDPOINT_NAME
scoring_uri
Invoke the endpoint to score the model by using theinvokecommand and passing query parameters that are stored in a JSON file.
invoke
ml_client.online_endpoints.invoke(
    endpoint_name=endpoint_name,
    request_file="../model-1/sample-request.json",
    local=True,
)
ml_client.online_endpoints.invoke(
    endpoint_name=endpoint_name,
    request_file="../model-1/sample-request.json",
    local=True,
)
If you want to use a REST client (like curl), you must have the scoring URI. To get the scoring URI, run the following code. In the returned data, find thescoring_uriattribute.
scoring_uri
endpoint = ml_client.online_endpoints.get(endpoint_name, local=True)
scoring_uri = endpoint.scoring_uri
endpoint = ml_client.online_endpoints.get(endpoint_name, local=True)
scoring_uri = endpoint.scoring_uri
The studio doesn't support local endpoints. See the Azure CLI or Python tabs for steps to test the endpoint locally.
The template doesn't support local endpoints. See the Azure CLI or Python tabs for steps to test the endpoint locally.
Review the logs for output from the invoke operation
In the examplescore.pyfile, therun()method logs some output to the console.
run()
Azure CLI
Python SDK
Studio
ARM template
You can view this output by using theget-logscommand:
get-logs
az ml online-deployment get-logs --local -n blue --endpoint $ENDPOINT_NAME
az ml online-deployment get-logs --local -n blue --endpoint $ENDPOINT_NAME
You can view this output by using theget_logsmethod:
get_logs
ml_client.online_deployments.get_logs(
    name="blue", endpoint_name=endpoint_name, local=True, lines=50
)
ml_client.online_deployments.get_logs(
    name="blue", endpoint_name=endpoint_name, local=True, lines=50
)
The studio doesn't support local endpoints. See the Azure CLI or Python tabs for steps to test the endpoint locally.
The template doesn't support local endpoints. See the Azure CLI or Python tabs for steps to test the endpoint locally.
Deploy your online endpoint to Azure
Next, deploy your online endpoint to Azure. As a best practice for production, we recommend that you register the model and environment that you'll use in your deployment.
Register your model and environment
We recommend that you register your model and environment before deployment to Azure so that you can specify their registered names and versions during deployment. Registering your assets allows you to reuse them without the need to upload them every time you create deployments, thereby increasing reproducibility and traceability.
Note
Unlike deployment to Azure, local deployment doesn't support using registered models and environments. Rather, local deployment uses local model files and uses environments with local files only.
For deployment to Azure, you can use either local or registered assets (models and environments). In this section of the article, the deployment to Azure uses registered assets, but you have the option of using local assets instead. For an example of a deployment configuration that uploads local files to use for local deployment, seeConfigure a deployment.
Azure CLI
Python SDK
Studio
ARM template
To register the model and environment, use the formmodel: azureml:my-model:1orenvironment: azureml:my-env:1.
model: azureml:my-model:1
environment: azureml:my-env:1
For registration, you can extract the YAML definitions ofmodelandenvironmentinto separate YAML files in theendpoints/online/managed/samplefolder, and use the commandsaz ml model createandaz ml environment create. To learn more about these commands, runaz ml model create -handaz ml environment create -h.
model
environment
az ml model create
az ml environment create
az ml model create -h
az ml environment create -h
Create a YAML definition for the model. Name the filemodel.yml:$schema: https://azuremlschemas.azureedge.net/latest/model.schema.json
name: my-model
path: ../../model-1/model/
Create a YAML definition for the model. Name the filemodel.yml:
$schema: https://azuremlschemas.azureedge.net/latest/model.schema.json
name: my-model
path: ../../model-1/model/
$schema: https://azuremlschemas.azureedge.net/latest/model.schema.json
name: my-model
path: ../../model-1/model/
Register the model:az ml model create -n my-model -v 1 -f endpoints/online/managed/sample/model.yml
Register the model:
az ml model create -n my-model -v 1 -f endpoints/online/managed/sample/model.yml
az ml model create -n my-model -v 1 -f endpoints/online/managed/sample/model.yml
Create a YAML definition for the environment. Name the fileenvironment.yml:$schema: https://azuremlschemas.azureedge.net/latest/environment.schema.json
name: my-env
image: mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest
conda_file: ../../model-1/environment/conda.yaml
Create a YAML definition for the environment. Name the fileenvironment.yml:
$schema: https://azuremlschemas.azureedge.net/latest/environment.schema.json
name: my-env
image: mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest
conda_file: ../../model-1/environment/conda.yaml
$schema: https://azuremlschemas.azureedge.net/latest/environment.schema.json
name: my-env
image: mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest
conda_file: ../../model-1/environment/conda.yaml
Register the environment:az ml environment create -n my-env -v 1 -f endpoints/online/managed/sample/environment.yml
Register the environment:
az ml environment create -n my-env -v 1 -f endpoints/online/managed/sample/environment.yml
az ml environment create -n my-env -v 1 -f endpoints/online/managed/sample/environment.yml
For more information on registering your model as an asset, seeRegister a model by using the Azure CLI or Python SDK. For more information on creating an environment, seeCreate a custom environment.
Register a model:from azure.ai.ml.entities import Model
from azure.ai.ml.constants import AssetTypes

file_model = Model(
    path="../model-1/model/",
    type=AssetTypes.CUSTOM_MODEL,
    name="my-model",
    description="Model created from local file.",
)
ml_client.models.create_or_update(file_model)
Register a model:
from azure.ai.ml.entities import Model
from azure.ai.ml.constants import AssetTypes

file_model = Model(
    path="../model-1/model/",
    type=AssetTypes.CUSTOM_MODEL,
    name="my-model",
    description="Model created from local file.",
)
ml_client.models.create_or_update(file_model)
from azure.ai.ml.entities import Model
from azure.ai.ml.constants import AssetTypes

file_model = Model(
    path="../model-1/model/",
    type=AssetTypes.CUSTOM_MODEL,
    name="my-model",
    description="Model created from local file.",
)
ml_client.models.create_or_update(file_model)
Register the environment:from azure.ai.ml.entities import Environment

env_docker_conda = Environment(
    image="mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04",
    conda_file="../model-1/environment/conda.yaml",
    name="my-env",
    description="Environment created from a Docker image plus Conda environment.",
)
ml_client.environments.create_or_update(env_docker_conda)
Register the environment:
from azure.ai.ml.entities import Environment

env_docker_conda = Environment(
    image="mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04",
    conda_file="../model-1/environment/conda.yaml",
    name="my-env",
    description="Environment created from a Docker image plus Conda environment.",
)
ml_client.environments.create_or_update(env_docker_conda)
from azure.ai.ml.entities import Environment

env_docker_conda = Environment(
    image="mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04",
    conda_file="../model-1/environment/conda.yaml",
    name="my-env",
    description="Environment created from a Docker image plus Conda environment.",
)
ml_client.environments.create_or_update(env_docker_conda)
To learn how to register your model as an asset so that you can specify its registered name and version during deployment, seeRegister a model by using the Azure CLI or Python SDK.
For more information on creating an environment, seeCreate a custom environment.
Register the model
A model registration is a logical entity in the workspace that can contain a single model file or a directory of multiple files. As a best practice for production, you should register the model and environment. Before creating the endpoint and deployment in this article, you should register themodel folderthat contains the model.
To register the example model, follow these steps:
Go to theAzure Machine Learning studio.
Go to theAzure Machine Learning studio.
In the side pane, select theModelspage.
In the side pane, select theModelspage.
SelectRegister, and then chooseFrom local files.
SelectRegister, and then chooseFrom local files.
SelectUnspecified typefor theModel type.
SelectUnspecified typefor theModel type.
SelectBrowse, and chooseBrowse folder.
SelectBrowse, and chooseBrowse folder.

Select the\azureml-examples\cli\endpoints\online\model-1\modelfolder from the local copy of the repo you cloned or downloaded earlier. When prompted, selectUploadand wait for the upload to complete.
Select the\azureml-examples\cli\endpoints\online\model-1\modelfolder from the local copy of the repo you cloned or downloaded earlier. When prompted, selectUploadand wait for the upload to complete.
\azureml-examples\cli\endpoints\online\model-1\model
SelectNextafter the folder upload is completed.
SelectNextafter the folder upload is completed.
Enter a friendlyNamefor the model. The steps in this article assume the model is namedmodel-1.
Enter a friendlyNamefor the model. The steps in this article assume the model is namedmodel-1.
model-1
SelectNext, and thenRegisterto complete registration.
SelectNext, and thenRegisterto complete registration.
For more information on working with registered models, seeWork with registered models.
Create and register the environment
In the side navigation bar, select theEnvironmentspage.
In the side navigation bar, select theEnvironmentspage.
Select theCustom environmentstab, then chooseCreate.
Select theCustom environmentstab, then chooseCreate.
On theSettingspage, provide a name, such asmy-envfor the environment.
On theSettingspage, provide a name, such asmy-envfor the environment.
ForSelect environment source, chooseUse existing docker image with optional conda source.
ForSelect environment source, chooseUse existing docker image with optional conda source.

SelectNextto go to theCustomizepage.
SelectNextto go to theCustomizepage.
Copy the contents of the\azureml-examples\cli\endpoints\online\model-1\environment\conda.yamlfile from the repo you cloned or downloaded earlier.
Copy the contents of the\azureml-examples\cli\endpoints\online\model-1\environment\conda.yamlfile from the repo you cloned or downloaded earlier.
Paste the contents into the text box.
Paste the contents into the text box.

SelectNextuntil you get to theCreatepage, then selectCreate.
SelectNextuntil you get to theCreatepage, then selectCreate.
For more information on creating an environment in the studio, seeCreate an environment.
To register the model using a template, you must first upload the model file to an Azure Blob store. The following example uses theaz storage blob upload-batchcommand to upload a file to the default storage for your workspace:az storage blob upload-batch -d $AZUREML_DEFAULT_CONTAINER/model -s cli/endpoints/online/model-1/model --account-name $AZURE_STORAGE_ACCOUNT
To register the model using a template, you must first upload the model file to an Azure Blob store. The following example uses theaz storage blob upload-batchcommand to upload a file to the default storage for your workspace:
az storage blob upload-batch
az storage blob upload-batch -d $AZUREML_DEFAULT_CONTAINER/model -s cli/endpoints/online/model-1/model --account-name $AZURE_STORAGE_ACCOUNT
az storage blob upload-batch -d $AZUREML_DEFAULT_CONTAINER/model -s cli/endpoints/online/model-1/model --account-name $AZURE_STORAGE_ACCOUNT
After uploading the file, use the template to create a model registration. In the following example, themodelUriparameter contains the path to the model:az deployment group create -g $RESOURCE_GROUP \
--template-file arm-templates/model-version.json \
--parameters \
workspaceName=$WORKSPACE \
modelAssetName="sklearn" \
modelUri="azureml://subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/workspaces/$WORKSPACE/datastores/$AZUREML_DEFAULT_DATASTORE/paths/model/sklearn_regression_model.pkl"
After uploading the file, use the template to create a model registration. In the following example, themodelUriparameter contains the path to the model:
modelUri
az deployment group create -g $RESOURCE_GROUP \
--template-file arm-templates/model-version.json \
--parameters \
workspaceName=$WORKSPACE \
modelAssetName="sklearn" \
modelUri="azureml://subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/workspaces/$WORKSPACE/datastores/$AZUREML_DEFAULT_DATASTORE/paths/model/sklearn_regression_model.pkl"
az deployment group create -g $RESOURCE_GROUP \
--template-file arm-templates/model-version.json \
--parameters \
workspaceName=$WORKSPACE \
modelAssetName="sklearn" \
modelUri="azureml://subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/workspaces/$WORKSPACE/datastores/$AZUREML_DEFAULT_DATASTORE/paths/model/sklearn_regression_model.pkl"
Part of the environment is a conda file that specifies the model dependencies needed to host the model. The following example demonstrates how to read the contents of the conda file into environment variables:CONDA_FILE=$(cat cli/endpoints/online/model-1/environment/conda.yaml)
Part of the environment is a conda file that specifies the model dependencies needed to host the model. The following example demonstrates how to read the contents of the conda file into environment variables:
CONDA_FILE=$(cat cli/endpoints/online/model-1/environment/conda.yaml)
CONDA_FILE=$(cat cli/endpoints/online/model-1/environment/conda.yaml)
The following example demonstrates how to use the template to register the environment. The contents of the conda file from the previous step are passed to the template using thecondaFileparameter:ENV_VERSION=$RANDOM
az deployment group create -g $RESOURCE_GROUP \
--template-file arm-templates/environment-version.json \
--parameters \
workspaceName=$WORKSPACE \
environmentAssetName=sklearn-env \
environmentAssetVersion=$ENV_VERSION \
dockerImage=mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210727.v1 \
condaFile="$CONDA_FILE"
The following example demonstrates how to use the template to register the environment. The contents of the conda file from the previous step are passed to the template using thecondaFileparameter:
condaFile
ENV_VERSION=$RANDOM
az deployment group create -g $RESOURCE_GROUP \
--template-file arm-templates/environment-version.json \
--parameters \
workspaceName=$WORKSPACE \
environmentAssetName=sklearn-env \
environmentAssetVersion=$ENV_VERSION \
dockerImage=mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210727.v1 \
condaFile="$CONDA_FILE"
ENV_VERSION=$RANDOM
az deployment group create -g $RESOURCE_GROUP \
--template-file arm-templates/environment-version.json \
--parameters \
workspaceName=$WORKSPACE \
environmentAssetName=sklearn-env \
environmentAssetVersion=$ENV_VERSION \
dockerImage=mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20210727.v1 \
condaFile="$CONDA_FILE"
Important
When defining a custom environment for your deployment, ensure theazureml-inference-server-httppackage is included in the conda file. This package is essential for the inference server to function properly. If you're unfamiliar with creating your own custom environment, it's advisable to instead use one of our curated environments such asminimal-py-inference(for custom models that don't use mlflow) ormlflow-py-inference(for models that use mlflow). These curated environments can be found in theEnvironmentstab of your Machine Learning Studio.
azureml-inference-server-http
minimal-py-inference
mlflow-py-inference
Configure a deployment that uses registered assets
Your deployment configuration uses the registered model that you wish to deploy and your registered environment.
Azure CLI
Python SDK
Studio
ARM template
Use the registered assets (model and environment) in your deployment definition. The following snippet shows theendpoints/online/managed/sample/blue-deployment-with-registered-assets.ymlfile, with all the required inputs to configure a deployment:
blue-deployment-with-registered-assets.yml
$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json
name: blue
endpoint_name: my-endpoint
model: azureml:my-model:1
code_configuration:
  code: ../../model-1/onlinescoring/
  scoring_script: score.py
environment: azureml:my-env:1
instance_type: Standard_DS3_v2
instance_count: 1
$schema: https://azuremlschemas.azureedge.net/latest/managedOnlineDeployment.schema.json
name: blue
endpoint_name: my-endpoint
model: azureml:my-model:1
code_configuration:
  code: ../../model-1/onlinescoring/
  scoring_script: score.py
environment: azureml:my-env:1
instance_type: Standard_DS3_v2
instance_count: 1
To configure a deployment, use the registered model and environment:
model = "azureml:my-model:1"
env = "azureml:my-env:1"
 
blue_deployment_with_registered_assets = ManagedOnlineDeployment(
    name="blue",
    endpoint_name=endpoint_name,
    model=model,
    environment=env,
    code_configuration=CodeConfiguration(
        code="../model-1/onlinescoring", scoring_script="score.py"
    ),
    instance_type="Standard_DS3_v2",
    instance_count=1,
)
model = "azureml:my-model:1"
env = "azureml:my-env:1"
 
blue_deployment_with_registered_assets = ManagedOnlineDeployment(
    name="blue",
    endpoint_name=endpoint_name,
    model=model,
    environment=env,
    code_configuration=CodeConfiguration(
        code="../model-1/onlinescoring", scoring_script="score.py"
    ),
    instance_type="Standard_DS3_v2",
    instance_count=1,
)
When you deploy from the studio, you'll create an endpoint and a deployment to add to it. At that time, you'll be prompted to provide names for the endpoint and deployment.
To define the endpoint and deployment, this article uses the Azure Resource Manager templatesonline-endpoint.jsonandonline-endpoint-deployment.json. To use the templates for defining an online endpoint and deployment, see theDeploy to Azuresection.
Use different CPU and GPU instance types and images
Azure CLI
Python SDK
Studio
ARM template
You can specify the CPU or GPU instance types and images in your deployment definition for both local deployment and deployment to Azure.
Your deployment definition in theblue-deployment-with-registered-assets.ymlfile used a general-purpose typeStandard_DS3_v2instance and a non-GPU Docker imagemcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest. For GPU compute, choose a GPU compute type SKU and a GPU Docker image.
Standard_DS3_v2
mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest
For supported general-purpose and GPU instance types, seeManaged online endpoints SKU list. For a list of Azure Machine Learning CPU and GPU base images, seeAzure Machine Learning base images.
Note
To use Kubernetes, instead of managed endpoints, as a compute target, seeIntroduction to Kubernetes compute target.
You can specify the CPU or GPU instance types and images in your deployment configuration for both local deployment and deployment to Azure.
Earlier, you configured a deployment that used a general-purpose typeStandard_DS3_v2instance and a non-GPU Docker imagemcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest. For GPU compute, choose a GPU compute type SKU and a GPU Docker image.
Standard_DS3_v2
mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04:latest
For supported general-purpose and GPU instance types, seeManaged online endpoints SKU list. For a list of Azure Machine Learning CPU and GPU base images, seeAzure Machine Learning base images.
Note
To use Kubernetes, instead of managed endpoints, as a compute target, seeIntroduction to Kubernetes compute target.
When using the studio todeploy to Azure, you'll be prompted to specify the compute properties (instance type and instance count) and environment to use for your deployment.
For supported general-purpose and GPU instance types, seeManaged online endpoints SKU list. For more information on environments, seeManage software environments in Azure Machine Learning studio.
The preceding registration of the environment specifies a non-GPU docker imagemcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04by passing the value to theenvironment-version.jsontemplate using thedockerImageparameter. For a GPU compute, provide a value for a GPU docker image to the template (using thedockerImageparameter) and provide a GPU compute type SKU to theonline-endpoint-deployment.jsontemplate (using theskuNameparameter).
mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04
dockerImage
dockerImage
online-endpoint-deployment.json
skuName
For supported general-purpose and GPU instance types, seeManaged online endpoints SKU list. For a list of Azure Machine Learning CPU and GPU base images, seeAzure Machine Learning base images.
Next, deploy your online endpoint to Azure.
Deploy to Azure
Azure CLI
Python SDK
Studio
ARM template
Create the endpoint in the Azure cloud.az ml online-endpoint create --name $ENDPOINT_NAME -f endpoints/online/managed/sample/endpoint.yml
Create the endpoint in the Azure cloud.
az ml online-endpoint create --name $ENDPOINT_NAME -f endpoints/online/managed/sample/endpoint.yml
az ml online-endpoint create --name $ENDPOINT_NAME -f endpoints/online/managed/sample/endpoint.yml
Create the deployment namedblueunder the endpoint.az ml online-deployment create --name blue --endpoint $ENDPOINT_NAME -f endpoints/online/managed/sample/blue-deployment-with-registered-assets.yml --all-trafficThe deployment creation can take up to 15 minutes, depending on whether the underlying environment or image is being built for the first time. Subsequent deployments that use the same environment are processed faster.TipIf you prefer not to block your CLI console, you can add the flag--no-waitto the command. However, this option will stop the interactive display of the deployment status.ImportantThe--all-trafficflag in the codeaz ml online-deployment createthat's used to create the deployment allocates 100% of the endpoint traffic to the newly created blue deployment. Though this is helpful for development and testing purposes, for production, you might want to route traffic to the new deployment through an explicit command. For example,az ml online-endpoint update -n $ENDPOINT_NAME --traffic "blue=100".
Create the deployment namedblueunder the endpoint.
blue
az ml online-deployment create --name blue --endpoint $ENDPOINT_NAME -f endpoints/online/managed/sample/blue-deployment-with-registered-assets.yml --all-traffic
az ml online-deployment create --name blue --endpoint $ENDPOINT_NAME -f endpoints/online/managed/sample/blue-deployment-with-registered-assets.yml --all-traffic
The deployment creation can take up to 15 minutes, depending on whether the underlying environment or image is being built for the first time. Subsequent deployments that use the same environment are processed faster.
Tip
If you prefer not to block your CLI console, you can add the flag--no-waitto the command. However, this option will stop the interactive display of the deployment status.
--no-wait
Important
The--all-trafficflag in the codeaz ml online-deployment createthat's used to create the deployment allocates 100% of the endpoint traffic to the newly created blue deployment. Though this is helpful for development and testing purposes, for production, you might want to route traffic to the new deployment through an explicit command. For example,az ml online-endpoint update -n $ENDPOINT_NAME --traffic "blue=100".
--all-traffic
az ml online-deployment create
az ml online-endpoint update -n $ENDPOINT_NAME --traffic "blue=100"
Create the endpoint:Using theendpointyou defined earlier and theMLClientyou created earlier, you can now create the endpoint in the workspace. This command starts the endpoint creation and returns a confirmation response while the endpoint creation continues.ml_client.online_endpoints.begin_create_or_update(endpoint)
Create the endpoint:
Using theendpointyou defined earlier and theMLClientyou created earlier, you can now create the endpoint in the workspace. This command starts the endpoint creation and returns a confirmation response while the endpoint creation continues.
endpoint
MLClient
ml_client.online_endpoints.begin_create_or_update(endpoint)
ml_client.online_endpoints.begin_create_or_update(endpoint)
Create the deployment:Using theblue_deployment_with_registered_assetsthat you defined earlier and theMLClientyou created earlier, you can now create the deployment in the workspace. This command starts the deployment creation and returns a confirmation response while the deployment creation continues.ml_client.online_deployments.begin_create_or_update(blue_deployment_with_registered_assets)TipIf you prefer not to block your Python console, you can add the flagno_wait=Trueto the parameters. However, this option stops the interactive display of the deployment status.# blue deployment takes 100 traffic
endpoint.traffic = {"blue": 100}
ml_client.online_endpoints.begin_create_or_update(endpoint)
Create the deployment:
Using theblue_deployment_with_registered_assetsthat you defined earlier and theMLClientyou created earlier, you can now create the deployment in the workspace. This command starts the deployment creation and returns a confirmation response while the deployment creation continues.
blue_deployment_with_registered_assets
MLClient
ml_client.online_deployments.begin_create_or_update(blue_deployment_with_registered_assets)
ml_client.online_deployments.begin_create_or_update(blue_deployment_with_registered_assets)
Tip
If you prefer not to block your Python console, you can add the flagno_wait=Trueto the parameters. However, this option stops the interactive display of the deployment status.
no_wait=True
# blue deployment takes 100 traffic
endpoint.traffic = {"blue": 100}
ml_client.online_endpoints.begin_create_or_update(endpoint)
# blue deployment takes 100 traffic
endpoint.traffic = {"blue": 100}
ml_client.online_endpoints.begin_create_or_update(endpoint)
Create a managed online endpoint and deployment
Use the studio to create a managed online endpoint directly in your browser. When you create a managed online endpoint in the studio, you must define an initial deployment. You can't create an empty managed online endpoint.
One way to create a managed online endpoint in the studio is from theModelspage. This method also provides an easy way to add a model to an existing managed online deployment. To deploy the model namedmodel-1that you registered previously in theRegister your model and environmentsection:
model-1
Go to theAzure Machine Learning studio.
Go to theAzure Machine Learning studio.
In the side navigation bar, select theModelspage.
In the side navigation bar, select theModelspage.
Select the model namedmodel-1by checking the circle next to its name.
Select the model namedmodel-1by checking the circle next to its name.
model-1
SelectDeploy>Real-time endpoint.This action opens up a window where you can specify details about your endpoint.
SelectDeploy>Real-time endpoint.

This action opens up a window where you can specify details about your endpoint.

Enter anEndpoint namethat's unique in the Azure region. For more information on the naming rules, seeendpoint limits.
Enter anEndpoint namethat's unique in the Azure region. For more information on the naming rules, seeendpoint limits.
Keep the default selection:Managedfor the compute type.
Keep the default selection:Managedfor the compute type.
Keep the default selection:key-based authenticationfor the authentication type. For more information on authenticating, seeAuthenticate clients for online endpoints.
Keep the default selection:key-based authenticationfor the authentication type. For more information on authenticating, seeAuthenticate clients for online endpoints.
SelectNext, until you get to theDeploymentpage. Here, toggleApplication Insights diagnosticstoEnabledto allow you to view graphs of your endpoint's activities in the studio later and analyze metrics and logs using Application Insights.
SelectNext, until you get to theDeploymentpage. Here, toggleApplication Insights diagnosticstoEnabledto allow you to view graphs of your endpoint's activities in the studio later and analyze metrics and logs using Application Insights.
SelectNextto go to theCode + environmentpage. Here, select the following options:Select a scoring script for inferencing: Browse and select the\azureml-examples\cli\endpoints\online\model-1\onlinescoring\score.pyfile from the repo you cloned or downloaded earlier.Select environmentsection: SelectCustom environmentsand then select themy-env:1environment that you created earlier.
SelectNextto go to theCode + environmentpage. Here, select the following options:
Select a scoring script for inferencing: Browse and select the\azureml-examples\cli\endpoints\online\model-1\onlinescoring\score.pyfile from the repo you cloned or downloaded earlier.
Select environmentsection: SelectCustom environmentsand then select themy-env:1environment that you created earlier.

SelectNext, accepting defaults, until you're prompted to create the deployment.
SelectNext, accepting defaults, until you're prompted to create the deployment.
Review your deployment settings and select theCreatebutton.
Review your deployment settings and select theCreatebutton.
Alternatively, you can create a managed online endpoint from theEndpointspage in the studio.
Go to theAzure Machine Learning studio.
Go to theAzure Machine Learning studio.
In the side pane, select theEndpointspage.
In the side pane, select theEndpointspage.
Select+ Create.
Select+ Create.

This action opens up a window for you to select your model and specify details about your endpoint and deployment. Enter settings for your endpoint and deployment as described previously, thenCreatethe deployment.
Use the template to create an online endpoint:az deployment group create -g $RESOURCE_GROUP \
--template-file arm-templates/online-endpoint.json \
--parameters \
workspaceName=$WORKSPACE \
onlineEndpointName=$ENDPOINT_NAME \
identityType=SystemAssigned \
authMode=AMLToken \
location=$LOCATION
Use the template to create an online endpoint:
az deployment group create -g $RESOURCE_GROUP \
--template-file arm-templates/online-endpoint.json \
--parameters \
workspaceName=$WORKSPACE \
onlineEndpointName=$ENDPOINT_NAME \
identityType=SystemAssigned \
authMode=AMLToken \
location=$LOCATION
az deployment group create -g $RESOURCE_GROUP \
--template-file arm-templates/online-endpoint.json \
--parameters \
workspaceName=$WORKSPACE \
onlineEndpointName=$ENDPOINT_NAME \
identityType=SystemAssigned \
authMode=AMLToken \
location=$LOCATION
Deploy the model to the endpoint after the endpoint is created:resourceScope="/subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.MachineLearningServices"
az deployment group create -g $RESOURCE_GROUP \
 --template-file arm-templates/online-endpoint-deployment.json \
 --parameters \
 workspaceName=$WORKSPACE \
 location=$LOCATION \
 onlineEndpointName=$ENDPOINT_NAME \
 onlineDeploymentName=blue \
 codeId="$resourceScope/workspaces/$WORKSPACE/codes/score-sklearn/versions/1" \
 scoringScript=score.py \
 environmentId="$resourceScope/workspaces/$WORKSPACE/environments/sklearn-env/versions/$ENV_VERSION" \
 model="$resourceScope/workspaces/$WORKSPACE/models/sklearn/versions/1" \
 endpointComputeType=Managed \
 skuName=Standard_F2s_v2 \
 skuCapacity=1
Deploy the model to the endpoint after the endpoint is created:
resourceScope="/subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.MachineLearningServices"
az deployment group create -g $RESOURCE_GROUP \
 --template-file arm-templates/online-endpoint-deployment.json \
 --parameters \
 workspaceName=$WORKSPACE \
 location=$LOCATION \
 onlineEndpointName=$ENDPOINT_NAME \
 onlineDeploymentName=blue \
 codeId="$resourceScope/workspaces/$WORKSPACE/codes/score-sklearn/versions/1" \
 scoringScript=score.py \
 environmentId="$resourceScope/workspaces/$WORKSPACE/environments/sklearn-env/versions/$ENV_VERSION" \
 model="$resourceScope/workspaces/$WORKSPACE/models/sklearn/versions/1" \
 endpointComputeType=Managed \
 skuName=Standard_F2s_v2 \
 skuCapacity=1
resourceScope="/subscriptions/$SUBSCRIPTION_ID/resourceGroups/$RESOURCE_GROUP/providers/Microsoft.MachineLearningServices"
az deployment group create -g $RESOURCE_GROUP \
 --template-file arm-templates/online-endpoint-deployment.json \
 --parameters \
 workspaceName=$WORKSPACE \
 location=$LOCATION \
 onlineEndpointName=$ENDPOINT_NAME \
 onlineDeploymentName=blue \
 codeId="$resourceScope/workspaces/$WORKSPACE/codes/score-sklearn/versions/1" \
 scoringScript=score.py \
 environmentId="$resourceScope/workspaces/$WORKSPACE/environments/sklearn-env/versions/$ENV_VERSION" \
 model="$resourceScope/workspaces/$WORKSPACE/models/sklearn/versions/1" \
 endpointComputeType=Managed \
 skuName=Standard_F2s_v2 \
 skuCapacity=1
To debug errors in your deployment, seeTroubleshooting online endpoint deployments.
Check the status of the online endpoint
Azure CLI
Python SDK
Studio
ARM template
Use theshowcommand to display information in theprovisioning_statefor the endpoint and deployment:az ml online-endpoint show -n $ENDPOINT_NAME
Use theshowcommand to display information in theprovisioning_statefor the endpoint and deployment:
show
provisioning_state
az ml online-endpoint show -n $ENDPOINT_NAME
az ml online-endpoint show -n $ENDPOINT_NAME
List all the endpoints in the workspace in a table format by using thelistcommand:az ml online-endpoint list --output table
List all the endpoints in the workspace in a table format by using thelistcommand:
list
az ml online-endpoint list --output table
az ml online-endpoint list --output table
Check the endpoint's status to see whether the model was deployed without error:ml_client.online_endpoints.get(name=endpoint_name)
Check the endpoint's status to see whether the model was deployed without error:
ml_client.online_endpoints.get(name=endpoint_name)
ml_client.online_endpoints.get(name=endpoint_name)
List all the endpoints in the workspace in a table format by using thelistmethod:for endpoint in ml_client.online_endpoints.list():
    print(endpoint.name)The method returns a list (iterator) ofManagedOnlineEndpointentities.
List all the endpoints in the workspace in a table format by using thelistmethod:
list
for endpoint in ml_client.online_endpoints.list():
    print(endpoint.name)
for endpoint in ml_client.online_endpoints.list():
    print(endpoint.name)
The method returns a list (iterator) ofManagedOnlineEndpointentities.
ManagedOnlineEndpoint
You can get more information by specifyingmore parameters. For example, output the list of endpoints like a table:print("Kind\tLocation\tName")
print("-------\t----------\t------------------------")
for endpoint in ml_client.online_endpoints.list():
    print(f"{endpoint.kind}\t{endpoint.location}\t{endpoint.name}")
You can get more information by specifyingmore parameters. For example, output the list of endpoints like a table:
print("Kind\tLocation\tName")
print("-------\t----------\t------------------------")
for endpoint in ml_client.online_endpoints.list():
    print(f"{endpoint.kind}\t{endpoint.location}\t{endpoint.name}")
print("Kind\tLocation\tName")
print("-------\t----------\t------------------------")
for endpoint in ml_client.online_endpoints.list():
    print(f"{endpoint.kind}\t{endpoint.location}\t{endpoint.name}")
You can view all your managed online endpoints in theEndpointspage. Go to the endpoint'sDetailspage to find critical information including the endpoint URI, status, testing tools, activity monitors, deployment logs, and sample consumption code:
In the side navigation bar, selectEndpoints. Here, you can see a list of all the endpoints in the workspace.
In the side navigation bar, selectEndpoints. Here, you can see a list of all the endpoints in the workspace.
(Optional) Create aFilteronCompute typeto show onlyManagedcompute types.
(Optional) Create aFilteronCompute typeto show onlyManagedcompute types.
Select an endpoint name to view the endpoint'sDetailspage.
Select an endpoint name to view the endpoint'sDetailspage.

Tip
While templates are useful for deploying resources, they can't be used to list, show, or invoke resources. Use the Azure CLI, Python SDK, or the studio to perform these operations. The following code uses the Azure CLI.
Use theshowcommand to display information in theprovisioning_statefor the endpoint and deployment:az ml online-endpoint show -n $ENDPOINT_NAME
Use theshowcommand to display information in theprovisioning_statefor the endpoint and deployment:
show
provisioning_state
az ml online-endpoint show -n $ENDPOINT_NAME
az ml online-endpoint show -n $ENDPOINT_NAME
List all the endpoints in the workspace in a table format by using thelistcommand:az ml online-endpoint list --output table
List all the endpoints in the workspace in a table format by using thelistcommand:
list
az ml online-endpoint list --output table
az ml online-endpoint list --output table
Check the status of the online deployment
Check the logs to see whether the model was deployed without error.
Azure CLI
Python SDK
Studio
ARM template
To see log output from a container, use the following CLI command:
az ml online-deployment get-logs --name blue --endpoint $ENDPOINT_NAME
az ml online-deployment get-logs --name blue --endpoint $ENDPOINT_NAME
By default, logs are pulled from the inference server container. To see logs from the storage initializer container, add the--container storage-initializerflag. For more information on deployment logs, seeGet container logs.
--container storage-initializer
You can view log output by using theget_logsmethod:ml_client.online_deployments.get_logs(
    name="blue", endpoint_name=endpoint_name, lines=50
)
You can view log output by using theget_logsmethod:
get_logs
ml_client.online_deployments.get_logs(
    name="blue", endpoint_name=endpoint_name, lines=50
)
ml_client.online_deployments.get_logs(
    name="blue", endpoint_name=endpoint_name, lines=50
)
By default, logs are pulled from the inference server container. To see logs from the storage initializer container, add thecontainer_type="storage-initializer"option. For more information on deployment logs, seeGet container logs.ml_client.online_deployments.get_logs(
    name="blue", endpoint_name=endpoint_name, lines=50, container_type="storage-initializer"
)
By default, logs are pulled from the inference server container. To see logs from the storage initializer container, add thecontainer_type="storage-initializer"option. For more information on deployment logs, seeGet container logs.
container_type="storage-initializer"
ml_client.online_deployments.get_logs(
    name="blue", endpoint_name=endpoint_name, lines=50, container_type="storage-initializer"
)
ml_client.online_deployments.get_logs(
    name="blue", endpoint_name=endpoint_name, lines=50, container_type="storage-initializer"
)
To view log output, select theLogstab from the endpoint's page. If you have multiple deployments in your endpoint, use the dropdown to select the deployment whose log you want to see.

By default, logs are pulled from the inference server. To see logs from the storage initializer container, use the Azure CLI or Python SDK (see each tab for details). Logs from the storage initializer container provide information on whether code and model data were successfully downloaded to the container. For more information on deployment logs, seeGet container logs.
Tip
While templates are useful for deploying resources, they can't be used to list, show, or invoke resources. Use the Azure CLI, Python SDK, or the studio to perform these operations. The following code uses the Azure CLI.
To see log output from a container, use the following CLI command:
az ml online-deployment get-logs --name blue --endpoint $ENDPOINT_NAME
az ml online-deployment get-logs --name blue --endpoint $ENDPOINT_NAME
By default, logs are pulled from the inference server container. To see logs from the storage initializer container, add the--container storage-initializerflag. For more information on deployment logs, seeGet container logs.
--container storage-initializer
Invoke the endpoint to score data by using your model
Azure CLI
Python SDK
Studio
ARM template
Use either theinvokecommand or a REST client of your choice to invoke the endpoint and score some data:az ml online-endpoint invoke --name $ENDPOINT_NAME --request-file endpoints/online/model-1/sample-request.json
Use either theinvokecommand or a REST client of your choice to invoke the endpoint and score some data:
invoke
az ml online-endpoint invoke --name $ENDPOINT_NAME --request-file endpoints/online/model-1/sample-request.json
az ml online-endpoint invoke --name $ENDPOINT_NAME --request-file endpoints/online/model-1/sample-request.json
Get the key used to authenticate to the endpoint:TipYou can control which Microsoft Entra security principals can get the authentication key by assigning them to a custom role that allowsMicrosoft.MachineLearningServices/workspaces/onlineEndpoints/token/actionandMicrosoft.MachineLearningServices/workspaces/onlineEndpoints/listkeys/action. For more information on managing authorization to workspaces, seeManage access to an Azure Machine Learning workspace.ENDPOINT_KEY=$(az ml online-endpoint get-credentials -n $ENDPOINT_NAME -o tsv --query primaryKey)
Get the key used to authenticate to the endpoint:
Tip
You can control which Microsoft Entra security principals can get the authentication key by assigning them to a custom role that allowsMicrosoft.MachineLearningServices/workspaces/onlineEndpoints/token/actionandMicrosoft.MachineLearningServices/workspaces/onlineEndpoints/listkeys/action. For more information on managing authorization to workspaces, seeManage access to an Azure Machine Learning workspace.
Microsoft.MachineLearningServices/workspaces/onlineEndpoints/token/action
Microsoft.MachineLearningServices/workspaces/onlineEndpoints/listkeys/action
ENDPOINT_KEY=$(az ml online-endpoint get-credentials -n $ENDPOINT_NAME -o tsv --query primaryKey)
ENDPOINT_KEY=$(az ml online-endpoint get-credentials -n $ENDPOINT_NAME -o tsv --query primaryKey)
Use curl to score data.SCORING_URI=$(az ml online-endpoint show -n $ENDPOINT_NAME -o tsv --query scoring_uri)

curl --request POST "$SCORING_URI" --header "Authorization: Bearer $ENDPOINT_KEY" --header 'Content-Type: application/json' --data @endpoints/online/model-1/sample-request.jsonNotice that theshowandget-credentialscommands are used to get the authentication credentials. Also notice that you're using the--queryflag to filter only the attributes that are needed. To learn more about the--queryflag, seeQuery Azure CLI command output.
Use curl to score data.
SCORING_URI=$(az ml online-endpoint show -n $ENDPOINT_NAME -o tsv --query scoring_uri)

curl --request POST "$SCORING_URI" --header "Authorization: Bearer $ENDPOINT_KEY" --header 'Content-Type: application/json' --data @endpoints/online/model-1/sample-request.json
SCORING_URI=$(az ml online-endpoint show -n $ENDPOINT_NAME -o tsv --query scoring_uri)

curl --request POST "$SCORING_URI" --header "Authorization: Bearer $ENDPOINT_KEY" --header 'Content-Type: application/json' --data @endpoints/online/model-1/sample-request.json
Notice that theshowandget-credentialscommands are used to get the authentication credentials. Also notice that you're using the--queryflag to filter only the attributes that are needed. To learn more about the--queryflag, seeQuery Azure CLI command output.
show
get-credentials
--query
--query
To see the invocation logs, runget-logsagain.
To see the invocation logs, runget-logsagain.
get-logs
Using theMLClientcreated earlier, get a handle to the endpoint. The endpoint can then be invoked using theinvokecommand with the following parameters:
MLClient
invoke
endpoint_name- Name of the endpoint
endpoint_name
request_file- File with request data
request_file
deployment_name- Name of the specific deployment to test in an endpoint
deployment_name
Send a sample request using ajsonfile.
# test the blue deployment with some sample data
ml_client.online_endpoints.invoke(
    endpoint_name=endpoint_name,
    deployment_name="blue",
    request_file="../model-1/sample-request.json",
)
# test the blue deployment with some sample data
ml_client.online_endpoints.invoke(
    endpoint_name=endpoint_name,
    deployment_name="blue",
    request_file="../model-1/sample-request.json",
)
Use theTesttab in the endpoint's details page to test your managed online deployment. Enter sample input and view the results.
Select theTesttab in the endpoint's detail page.
Select theTesttab in the endpoint's detail page.
Use the dropdown to select the deployment you want to test.
Use the dropdown to select the deployment you want to test.
Enter thesample input.
Enter thesample input.
SelectTest.
SelectTest.

Tip
While templates are useful for deploying resources, they can't be used to list, show, or invoke resources. Use the Azure CLI, Python SDK, or the studio to perform these operations. The following code uses the Azure CLI.
Use either theinvokecommand or a REST client of your choice to invoke the endpoint and score some data:
invoke
az ml online-endpoint invoke --name $ENDPOINT_NAME --request-file cli/endpoints/online/model-1/sample-request.json
az ml online-endpoint invoke --name $ENDPOINT_NAME --request-file cli/endpoints/online/model-1/sample-request.json
(Optional) Update the deployment
Azure CLI
Python SDK
Studio
ARM template
If you want to update the code, model, or environment, update the YAML file, and then run theaz ml online-endpoint updatecommand.
az ml online-endpoint update
Note
If you update instance count (to scale your deployment) along with other model settings (such as code, model, or environment) in a singleupdatecommand, the scaling operation is performed first, then the other updates are applied. It's a good practice to perform these operations separately in a production environment.
update
To understand howupdateworks:
update
Open the fileonline/model-1/onlinescoring/score.py.
Open the fileonline/model-1/onlinescoring/score.py.
Change the last line of theinit()function: Afterlogging.info("Init complete"), addlogging.info("Updated successfully").
Change the last line of theinit()function: Afterlogging.info("Init complete"), addlogging.info("Updated successfully").
init()
logging.info("Init complete")
logging.info("Updated successfully")
Save the file.
Save the file.
Run this command:az ml online-deployment update -n blue --endpoint $ENDPOINT_NAME -f endpoints/online/managed/sample/blue-deployment-with-registered-assets.ymlNoteUpdating by using YAML is declarative. That is, changes in the YAML are reflected in the underlying Azure Resource Manager resources (endpoints and deployments). A declarative approach facilitatesGitOps:Allchanges to endpoints and deployments (eveninstance_count) go through the YAML.TipYou can usegeneric update parameters, such as the--setparameter, with the CLIupdatecommand to override attributes in your YAMLorto set specific attributes without passing them in the YAML file. Using--setfor single attributes is especially valuable in development and test scenarios. For example, to scale up theinstance_countvalue for the first deployment, you could use the--set instance_count=2flag. However, because the YAML isn't updated, this technique doesn't facilitateGitOps.Specifying the YAML file is NOT mandatory. For example, if you wanted to test different concurrency setting for a given deployment, you can try something likeaz ml online-deployment update -n blue -e my-endpoint --set request_settings.max_concurrent_requests_per_instance=4 environment_variables.WORKER_COUNT=4. This keeps all existing configuration but updates only the specified parameters.
Run this command:
az ml online-deployment update -n blue --endpoint $ENDPOINT_NAME -f endpoints/online/managed/sample/blue-deployment-with-registered-assets.yml
az ml online-deployment update -n blue --endpoint $ENDPOINT_NAME -f endpoints/online/managed/sample/blue-deployment-with-registered-assets.yml
Note
Updating by using YAML is declarative. That is, changes in the YAML are reflected in the underlying Azure Resource Manager resources (endpoints and deployments). A declarative approach facilitatesGitOps:Allchanges to endpoints and deployments (eveninstance_count) go through the YAML.
instance_count
Tip
You can usegeneric update parameters, such as the--setparameter, with the CLIupdatecommand to override attributes in your YAMLorto set specific attributes without passing them in the YAML file. Using--setfor single attributes is especially valuable in development and test scenarios. For example, to scale up theinstance_countvalue for the first deployment, you could use the--set instance_count=2flag. However, because the YAML isn't updated, this technique doesn't facilitateGitOps.
--set
update
--set
instance_count
--set instance_count=2
Specifying the YAML file is NOT mandatory. For example, if you wanted to test different concurrency setting for a given deployment, you can try something likeaz ml online-deployment update -n blue -e my-endpoint --set request_settings.max_concurrent_requests_per_instance=4 environment_variables.WORKER_COUNT=4. This keeps all existing configuration but updates only the specified parameters.
az ml online-deployment update -n blue -e my-endpoint --set request_settings.max_concurrent_requests_per_instance=4 environment_variables.WORKER_COUNT=4
Because you modified theinit()function, which runs when the endpoint is created or updated, the messageUpdated successfullywill be in the logs. Retrieve the logs by running:az ml online-deployment get-logs --name blue --endpoint $ENDPOINT_NAME
Because you modified theinit()function, which runs when the endpoint is created or updated, the messageUpdated successfullywill be in the logs. Retrieve the logs by running:
init()
Updated successfully
az ml online-deployment get-logs --name blue --endpoint $ENDPOINT_NAME
az ml online-deployment get-logs --name blue --endpoint $ENDPOINT_NAME
Theupdatecommand also works with local deployments. Use the sameaz ml online-deployment updatecommand with the--localflag.
update
az ml online-deployment update
--local
If you want to update the code, model, or environment, update the configuration, and then run theMLClient'sonline_deployments.begin_create_or_updatemethod tocreate or update a deployment.
MLClient
online_deployments.begin_create_or_update
Note
If you update instance count (to scale your deployment) along with other model settings (such as code, model, or environment) in a singlebegin_create_or_updatemethod, the scaling operation is performed first, then the other updates are applied. It's a good practice to perform these operations separately in a production environment.
begin_create_or_update
To understand howbegin_create_or_updateworks:
begin_create_or_update
Open the fileonline/model-1/onlinescoring/score.py.
Open the fileonline/model-1/onlinescoring/score.py.
Change the last line of theinit()function: Afterlogging.info("Init complete"), addlogging.info("Updated successfully").
Change the last line of theinit()function: Afterlogging.info("Init complete"), addlogging.info("Updated successfully").
init()
logging.info("Init complete")
logging.info("Updated successfully")
Save the file.
Save the file.
Run the method:ml_client.online_deployments.begin_create_or_update(blue_deployment_with_registered_assets)
Run the method:
ml_client.online_deployments.begin_create_or_update(blue_deployment_with_registered_assets)
ml_client.online_deployments.begin_create_or_update(blue_deployment_with_registered_assets)
Because you modified theinit()function, which runs when the endpoint is created or updated, the messageUpdated successfullywill be in the logs. Retrieve the logs by running:ml_client.online_deployments.get_logs(
    name="blue", endpoint_name=endpoint_name, lines=50
)
Because you modified theinit()function, which runs when the endpoint is created or updated, the messageUpdated successfullywill be in the logs. Retrieve the logs by running:
init()
Updated successfully
ml_client.online_deployments.get_logs(
    name="blue", endpoint_name=endpoint_name, lines=50
)
ml_client.online_deployments.get_logs(
    name="blue", endpoint_name=endpoint_name, lines=50
)
Thebegin_create_or_updatemethod also works with local deployments. Use the same method with thelocal=Trueflag.
begin_create_or_update
local=True
Currently, the studio allows you to make updates only to the instance count of a deployment. Use the following instructions to scale an individual deployment up or down by adjusting the number of instances:
Open the endpoint'sDetailspage and find the card for the deployment you want to update.
Open the endpoint'sDetailspage and find the card for the deployment you want to update.
Select the edit icon (pencil icon) next to the deployment's name.
Select the edit icon (pencil icon) next to the deployment's name.
Update the instance count associated with the deployment. You can choose betweenDefaultorTarget UtilizationforDeployment scale type.If you selectDefault, you can also specify a numerical value for theInstance count.If you selectTarget Utilization, you can specify values to use for parameters when autoscaling the deployment.
Update the instance count associated with the deployment. You can choose betweenDefaultorTarget UtilizationforDeployment scale type.
If you selectDefault, you can also specify a numerical value for theInstance count.
If you selectTarget Utilization, you can specify values to use for parameters when autoscaling the deployment.
SelectUpdateto finish updating the instance counts for your deployment.
SelectUpdateto finish updating the instance counts for your deployment.
There currently isn't an option to update the deployment using an ARM template.
Note
The update to the deployment in this section is an example of an in-place rolling update.
For a managed online endpoint, the deployment is updated to the new configuration with 20% nodes at a time. That is, if the deployment has 10 nodes, 2 nodes at a time are updated.
For a Kubernetes online endpoint, the system iteratively creates a new deployment instance with the new configuration and deletes the old one.
For production usage, you should considerblue-green deployment, which offers a safer alternative for updating a web service.
(Optional) Configure autoscaling
Autoscale automatically runs the right amount of resources to handle the load on your application. Managed online endpoints support autoscaling through integration with the Azure monitor autoscale feature. To configure autoscaling, seeAutoscale online endpoints in Azure Machine Learning.
(Optional) Monitor SLA by using Azure Monitor
To view metrics and set alerts based on your SLA, complete the steps that are described inMonitor online endpoints.
(Optional) Integrate with Log Analytics
Theget-logscommand for CLI or theget_logsmethod for SDK provides only the last few hundred lines of logs from an automatically selected instance. However, Log Analytics provides a way to durably store and analyze logs. For more information on using logging, seeUse logs.
get-logs
get_logs
Delete the endpoint and the deployment
Azure CLI
Python SDK
Studio
ARM template
Use the following command to delete the endpoint and all its underlying deployments:
az ml online-endpoint delete --name $ENDPOINT_NAME --yes --no-wait
az ml online-endpoint delete --name $ENDPOINT_NAME --yes --no-wait
Use the following command to delete the endpoint and all its underlying deployments:
ml_client.online_endpoints.begin_delete(name=endpoint_name)
ml_client.online_endpoints.begin_delete(name=endpoint_name)
If you aren't going use the endpoint and deployment, you should delete them. By deleting the endpoint, you also delete all its underlying deployments.
Go to theAzure Machine Learning studio.
Go to theAzure Machine Learning studio.
In the side pane, select theEndpointspage.
In the side pane, select theEndpointspage.
Select an endpoint by checking the circle next to the model name.
Select an endpoint by checking the circle next to the model name.
SelectDelete.
SelectDelete.
Alternatively, you can delete a managed online endpoint directly by selecting theDeleteicon in theendpoint details page.
Use the following command to delete the endpoint and all its underlying deployments:
az ml online-endpoint delete --name $ENDPOINT_NAME --yes --no-wait
az ml online-endpoint delete --name $ENDPOINT_NAME --yes --no-wait
Related content
Perform safe rollout of new deployments for real-time inference
Deploy models with REST
Autoscale online endpoints in Azure Machine Learning
Monitor online endpoints
Feedback
Was this page helpful?
Additional resources