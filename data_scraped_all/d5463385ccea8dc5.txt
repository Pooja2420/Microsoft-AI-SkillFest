Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Log, load, and register MLflow models
Article
2025-01-28
5 contributors
In this article
An MLflowModelis a standard format for packaging machine learning models that can be used in a variety of downstream toolsâfor example, batch inference on Apache Spark or real-time serving through a REST API. The format defines a convention that lets you save a model in differentflavors(python-function, pytorch, sklearn, and so on), that can be understood by different modelserving and inference platforms.
To learn how to log and score a streaming model, seeHow to save and load a streaming model.
Log and load models
When you log a model, MLflow automatically logsrequirements.txtandconda.yamlfiles. You can use these files to recreate the model development environment and reinstall dependencies usingvirtualenv(recommended) orconda.
requirements.txt
conda.yaml
virtualenv
conda
Important
Anaconda Inc. updated theirterms of servicefor anaconda.org channels. Based on the new terms of service you may require a commercial license if you rely on Anacondaâs packaging and distribution. SeeAnaconda Commercial Edition FAQfor more information. Your use of any Anaconda channels is governed by their terms of service.
MLflow models logged beforev1.18(Databricks Runtime 8.3 ML or earlier) were by default logged with the condadefaultschannel (https://repo.anaconda.com/pkgs/) as a dependency. Because of this license change, Databricks has stopped the use of thedefaultschannel for models logged using MLflow v1.18 and above. The default channel logged is nowconda-forge, which points at the community managedhttps://conda-forge.org/.
defaults
defaults
conda-forge
If you logged a model before MLflow v1.18 without excluding thedefaultschannel from the conda environment for the model, that model may have a dependency on thedefaultschannel that you may not have intended.
To manually confirm whether a model has this dependency, you can examinechannelvalue in theconda.yamlfile that is packaged with the logged model. For example, a modelâsconda.yamlwith adefaultschannel dependency may look like this:
defaults
defaults
channel
conda.yaml
conda.yaml
defaults
channels:
- defaults
dependencies:
- python=3.8.8
- pip
- pip:
    - mlflow
    - scikit-learn==0.23.2
    - cloudpickle==1.6.0
      name: mlflow-env
channels:
- defaults
dependencies:
- python=3.8.8
- pip
- pip:
    - mlflow
    - scikit-learn==0.23.2
    - cloudpickle==1.6.0
      name: mlflow-env
Because Databricks can not determine whether your use of the Anaconda repository to interact with your models is permitted under your relationship with Anaconda, Databricks is not forcing its customers to make any changes. If your use of the Anaconda.com repo through the use of Databricks is permitted under Anacondaâs terms, you do not need to take any action.
If you would like to change the channel used in a modelâs environment, you can re-register the model to the model registry with a newconda.yaml. You can do this by specifying the channel in theconda_envparameter oflog_model().
conda.yaml
conda_env
log_model()
For more information on thelog_model()API, see the MLflow documentation for the model flavor you are working with, for example,log_model for scikit-learn.
log_model()
For more information onconda.yamlfiles, see theMLflow documentation.
conda.yaml
API commands
To log a model to the MLflowtracking server, usemlflow.<model-type>.log_model(model, ...).
mlflow.<model-type>.log_model(model, ...)
To load a previously logged model for inference or further development, usemlflow.<model-type>.load_model(modelpath), wheremodelpathis one of the following:
mlflow.<model-type>.load_model(modelpath)
modelpath
a run-relative path (such asruns:/{run_id}/{model-path})
runs:/{run_id}/{model-path}
a Unity Catalog volumes path (such asdbfs:/Volumes/catalog_name/schema_name/volume_name/{path_to_artifact_root}/{model_path})
dbfs:/Volumes/catalog_name/schema_name/volume_name/{path_to_artifact_root}/{model_path}
an MLflow-managed artifact storage path beginning withdbfs:/databricks/mlflow-tracking/
dbfs:/databricks/mlflow-tracking/
aregistered modelpath (such asmodels:/{model_name}/{model_stage}).
models:/{model_name}/{model_stage}
For a complete list of options for loading MLflow models, seeReferencing Artifacts in the MLflow documentation.
For Python MLflow models, an additional option is to usemlflow.pyfunc.load_model()to load the model as a generic Python function.
mlflow.pyfunc.load_model()
You can use the following code snippet to load the model and score data points.
model = mlflow.pyfunc.load_model(model_path)
model.predict(model_input)
model = mlflow.pyfunc.load_model(model_path)
model.predict(model_input)
As an alternative, you can export the model as an Apache Spark UDF to use for scoring on a Spark cluster,
either as a batch job or as a real-timeSpark Streamingjob.
# load input data table as a Spark DataFrame
input_data = spark.table(input_table_name)
model_udf = mlflow.pyfunc.spark_udf(spark, model_path)
df = input_data.withColumn("prediction", model_udf())
# load input data table as a Spark DataFrame
input_data = spark.table(input_table_name)
model_udf = mlflow.pyfunc.spark_udf(spark, model_path)
df = input_data.withColumn("prediction", model_udf())
Log model dependencies
To accurately load a model, you should make sure the model dependencies are loaded with the correct versions into the notebook environment. In Databricks Runtime 10.5 ML and above, MLflow warns you if a mismatch is detected between the current environment and the modelâs dependencies.
Additional functionality to simplify restoring model dependencies is included in Databricks Runtime 11.0 ML and above. In Databricks Runtime 11.0 ML and above, forpyfuncflavor models, you can callmlflow.pyfunc.get_model_dependenciesto retrieve and download the model dependencies. This function returns a path to the dependencies file which you can then install by using%pip install <file-path>. When you load a model as a PySpark UDF, specifyenv_manager="virtualenv"in themlflow.pyfunc.spark_udfcall. This restores model dependencies in the context of the PySpark UDF and does not affect the outside environment.
pyfunc
mlflow.pyfunc.get_model_dependencies
%pip install <file-path>
env_manager="virtualenv"
mlflow.pyfunc.spark_udf
You can also use this functionality in Databricks Runtime 10.5 or below by manually installingMLflow version 1.25.0 or above:
%pip install "mlflow>=1.25.0"
%pip install "mlflow>=1.25.0"
For additional information on how to log model dependencies (Python and non-Python) and artifacts, seeLog model dependencies.
Learn how to log model dependencies and custom artifacts for model serving:
Deploy models with dependencies
Use custom Python libraries with Model Serving
Package custom artifacts for Model Serving
Log model dependencies
Databricks Autologging
Automatically generated code snippets in the MLflow UI
When you log a model in an Azure Databricks notebook, Azure Databricks automatically generates code snippets that you can copy and use to load and run the model. To view these code snippets:
Navigate to the Runs screen for the run that generated the model. (SeeView notebook experimentfor how to display the Runs screen.)
Scroll to theArtifactssection.
Click the name of the logged model. A panel opens to the right showing code you can use to load the logged model and make predictions on Spark or pandas DataFrames.

Examples
For examples of logging models, see the examples inTrack machine learning training runs examples.
Register models in the Model Registry
You can register models in the MLflow Model Registry, a centralized model store that provides a UI and set of APIs to manage the full lifecycle of MLflow Models. For instructions on how to use the Model Registry to manage models in Databricks Unity Catalog, seeManage model lifecycle in Unity Catalog. To use the Workspace Model Registry, seeManage model lifecycle using the Workspace Model Registry (legacy).
To register a model using the API, usemlflow.register_model("runs:/{run_id}/{model-path}", "{registered-model-name}").
mlflow.register_model("runs:/{run_id}/{model-path}", "{registered-model-name}")
Save models to Unity Catalog volumes
To save a model locally, usemlflow.<model-type>.save_model(model, modelpath).modelpathmust be aUnity Catalog volumespath. For example, if you use a Unity Catalog volumes locationdbfs:/Volumes/catalog_name/schema_name/volume_name/my_project_modelsto store your project work, you must use the model path/dbfs/Volumes/catalog_name/schema_name/volume_name/my_project_models:
mlflow.<model-type>.save_model(model, modelpath)
modelpath
dbfs:/Volumes/catalog_name/schema_name/volume_name/my_project_models
/dbfs/Volumes/catalog_name/schema_name/volume_name/my_project_models
modelpath = "/dbfs/Volumes/catalog_name/schema_name/volume_name/my_project_models/model-%f-%f" % (alpha, l1_ratio)
mlflow.sklearn.save_model(lr, modelpath)
modelpath = "/dbfs/Volumes/catalog_name/schema_name/volume_name/my_project_models/model-%f-%f" % (alpha, l1_ratio)
mlflow.sklearn.save_model(lr, modelpath)
For MLlib models, useML Pipelines.
Download model artifacts
You can download the logged model artifacts (such as model files, plots, and metrics) for a registered model with various APIs.
Python APIexample:
mlflow.set_registry_uri("databricks-uc")
mlflow.artifacts.download_artifacts(f"models:/{model_name}/{model_version}")
mlflow.set_registry_uri("databricks-uc")
mlflow.artifacts.download_artifacts(f"models:/{model_name}/{model_version}")
Java APIexample:
MlflowClient mlflowClient = new MlflowClient();
// Get the model URI for a registered model version.
String modelURI = mlflowClient.getModelVersionDownloadUri(modelName, modelVersion);

// Or download the model artifacts directly.
File modelFile = mlflowClient.downloadModelVersion(modelName, modelVersion);
MlflowClient mlflowClient = new MlflowClient();
// Get the model URI for a registered model version.
String modelURI = mlflowClient.getModelVersionDownloadUri(modelName, modelVersion);

// Or download the model artifacts directly.
File modelFile = mlflowClient.downloadModelVersion(modelName, modelVersion);
CLI commandexample:
mlflow artifacts download --artifact-uri models:/<name>/<version|stage>
mlflow artifacts download --artifact-uri models:/<name>/<version|stage>
Deploy models for online serving
Note
Prior to deploying your model, it is beneficial to verify that the model is capable of being served. See the MLflow documentation for how you can usemlflow.models.predicttovalidate models before deployment.
mlflow.models.predict
UseMosaic AI Model Servingto host machine learning models registered in Unity Catalog model registry as REST endpoints. These endpoints are updated automatically based on the availability of model versions.
Feedback
Was this page helpful?
Additional resources