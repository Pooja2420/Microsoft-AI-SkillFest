Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Manage Azure Data Lake Analytics using the Azure portal
Article
2023-12-20
2 contributors
In this article
Azure portal
Azure CLI
PowerShell
Java
.NET SDK
Node.js
Python
Important
New Azure Data Lake Analytics accounts can no longer be created unless your subscription has been enabled.
If you need your subscription to be enabledcontact supportand provide your business scenario.
If you are already using Azure Data Lake Analytics, you'll need to create amigration plan to Azure Synapse Analyticsfor your organization by February 29th, 2024.
This article describes how to manage Azure Data Lake Analytics accounts, data sources, users, and jobs by using the Azure portal.
Manage Data Lake Analytics accounts
Create an account
Sign in to theAzure portal.
SelectCreate a resourceand search forData Lake Analytics.
Select values for the following items:Name: The name of the Data Lake Analytics account.Subscription: The Azure subscription used for the account.Resource Group: The Azure resource group in which to create the account.Location: The Azure datacenter for the Data Lake Analytics account.Data Lake Store: The default store to be used for the Data Lake Analytics account. The Azure Data Lake Store account and the Data Lake Analytics account must be in the same location.
Name: The name of the Data Lake Analytics account.
Subscription: The Azure subscription used for the account.
Resource Group: The Azure resource group in which to create the account.
Location: The Azure datacenter for the Data Lake Analytics account.
Data Lake Store: The default store to be used for the Data Lake Analytics account. The Azure Data Lake Store account and the Data Lake Analytics account must be in the same location.
SelectCreate.
Delete a Data Lake Analytics account
Before you delete a Data Lake Analytics account, delete its default Data Lake Store account.
In the Azure portal, go to your Data Lake Analytics account.
SelectDelete.
Type the account name.
SelectDelete.
Manage data sources
Data Lake Analytics supports the following data sources:
Data Lake Store
Azure Storage
You can use Data Explorer to browse data sources and perform basic file management operations.
Add a data source
In the Azure portal, go to your Data Lake Analytics account.
In the Azure portal, go to your Data Lake Analytics account.
SelectData explorer.
SelectData explorer.
SelectAdd Data Source.To add a Data Lake Store account, you need the account name and access to the account to be able to query it.To add Azure Blob storage, you need the storage account and the account key. To find them, go to the storage account in the portal and selectAccess keys.
SelectAdd Data Source.
To add a Data Lake Store account, you need the account name and access to the account to be able to query it.
To add Azure Blob storage, you need the storage account and the account key. To find them, go to the storage account in the portal and selectAccess keys.
Set up firewall rules
You can use Data Lake Analytics to further lock down access to your Data Lake Analytics account at the network level. You can enable a firewall, specify an IP address, or define an IP address range for your trusted clients. After you enable these measures, only clients that have the IP addresses within the defined range can connect to the store.
If other Azure services, like Azure Data Factory or VMs, connect to the Data Lake Analytics account, make sure thatAllow Azure Servicesis turnedOn.
Set up a firewall rule
In the Azure portal, go to your Data Lake Analytics account.
On the menu on the left, selectFirewall.
Add a new user
You can use theAdd User Wizardto easily create new Data Lake users.
In the Azure portal, go to your Data Lake Analytics account.
On the left, underGetting Started, selectAdd User Wizard.
Select a user, and then selectSelect.
Select a role, and then selectSelect. To set up a new developer to use Azure Data Lake, select theData Lake Analytics Developerrole.
Select the access control lists (ACLs) for the U-SQL databases. When you're satisfied with your choices, selectSelect.
Select the ACLs for files. For the default store, don't change the ACLs for the root folder "/" and for the /system folder. selectSelect.
Review all your selected changes, and then selectRun.
When the wizard is finished, selectDone.
Manage Azure role-based access control
Like other Azure services, you can use Azure role-based access control (Azure RBAC) to control how users interact with the service.
The standard Azure roles have the following capabilities:
Owner: Can submit jobs, monitor jobs, cancel jobs from any user, and configure the account.
Contributor: Can submit jobs, monitor jobs, cancel jobs from any user, and configure the account.
Reader: Can monitor jobs.
Use the Data Lake Analytics Developer role to enable U-SQL developers to use the Data Lake Analytics service. You can use the Data Lake Analytics Developer role to:
Submit jobs.
Monitor job status and the progress of jobs submitted by any user.
See the U-SQL scripts from jobs submitted by any user.
Cancel only your own jobs.
Add users or security groups to a Data Lake Analytics account
In the Azure portal, go to your Data Lake Analytics account.
In the Azure portal, go to your Data Lake Analytics account.
SelectAccess control (IAM).
SelectAccess control (IAM).
SelectAdd>Add role assignmentto open theAdd role assignmentpage.
SelectAdd>Add role assignmentto open theAdd role assignmentpage.
Assign a role to a user. For detailed steps, seeAssign Azure roles using the Azure portal.
Assign a role to a user. For detailed steps, seeAssign Azure roles using the Azure portal.

Note
If a user or a security group needs to submit jobs, they also need permission on the store account. For more information, seeSecure data stored in Data Lake Store.
Manage jobs
Submit a job
In the Azure portal, go to your Data Lake Analytics account.
In the Azure portal, go to your Data Lake Analytics account.
SelectNew Job. For each job,  configure:Job Name: The name of the job.Priority: This is underMore options. Lower numbers have higher priority. If two jobs are queued, the one with lower priority value runs first.AUs: The maximum number of Analytics Units, or compute processes to reserve for this job.Runtime: Also underMore options. Select the Default runtime unless you've received a custom runtime.
SelectNew Job. For each job,  configure:
Job Name: The name of the job.
Priority: This is underMore options. Lower numbers have higher priority. If two jobs are queued, the one with lower priority value runs first.
AUs: The maximum number of Analytics Units, or compute processes to reserve for this job.
Runtime: Also underMore options. Select the Default runtime unless you've received a custom runtime.
Add your script.
Add your script.
SelectSubmit Job.
SelectSubmit Job.
Monitor jobs
In the Azure portal, go to your Data Lake Analytics account.
SelectView All Jobsat the top of the page. A list of all the active and recently finished jobs in the account is shown.
Optionally, selectFilterto help you find the jobs byTime Range,Status,Job Name,Job ID,Pipeline nameorPipeline ID,Recurrence nameorRecurrence ID, andAuthorvalues.
Monitoring pipeline jobs
Jobs that are part of a pipeline work together, usually sequentially, to accomplish a specific scenario. For example, you can have a pipeline that cleans, extracts, transforms, aggregates usage for customer insights. Pipeline jobs are identified using the "Pipeline" property when the job was submitted. Jobs scheduled using ADF V2 will automatically have this property populated.
To view a list of U-SQL jobs that are part of pipelines:
In the Azure portal, go to your Data Lake Analytics accounts.
SelectJob Insights. The "All Jobs" tab will be defaulted, showing a list of running, queued, and ended jobs.
Select thePipeline Jobstab. A list of pipeline jobs will be shown along with aggregated statistics for each pipeline.
Monitoring recurring jobs
A recurring job is one that has the same business logic but uses different input data every time it runs. Ideally, recurring jobs should always succeed, and have relatively stable execution time; monitoring these behaviors will help ensure the job is healthy. Recurring jobs are identified using the "Recurrence" property. Jobs scheduled using ADF V2 will automatically have this property populated.
To view a list of U-SQL jobs that are recurring:
In the Azure portal, go to your Data Lake Analytics accounts.
SelectJob Insights. The "All Jobs" tab will be defaulted, showing a list of running, queued, and ended jobs.
Select theRecurring Jobstab. A list of recurring jobs will be shown along with aggregated statistics for each recurring job.
Next steps
Overview of Azure Data Lake Analytics
Manage Azure Data Lake Analytics by using Azure PowerShell
Manage Azure Data Lake Analytics using policies
Additional resources