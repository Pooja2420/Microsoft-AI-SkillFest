Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Copy data from SAP HANA using Azure Data Factory or Synapse Analytics
Article
2024-09-25
13 contributors
In this article
APPLIES TO:Azure Data FactoryAzure Synapse Analytics
Tip
Try outData Factory in Microsoft Fabric, an all-in-one analytics solution for enterprises.Microsoft Fabriccovers everything from data movement to data science, real-time analytics, business intelligence, and reporting. Learn how tostart a new trialfor free!
This article outlines how to use the Copy Activity in Azure Data Factory and Synapse Analytics pipelines to copy data from an SAP HANA database. It builds on thecopy activity overviewarticle that presents a general overview of copy activity.
Tip
To learn about overall support for the SAP data integration scenario, seeSAP data integration whitepaperwith detailed introduction on each SAP connector, comparison and guidance.
Supported capabilities
This SAP HANA connector is supported for the following capabilities:
â  Azure integration runtime â¡ Self-hosted integration runtime
For a list of data stores supported as sources/sinks by the copy activity, see theSupported data storestable.
Specifically, this SAP HANA connector supports:
Copying data from any version of SAP HANA database.
Copying data fromHANA information models(such as Analytic and Calculation views) andRow/Column tables.
Copying data usingBasicorWindowsauthentication.
Parallel copying from an SAP HANA source. See theParallel copy from SAP HANAsection for details.
Tip
To copy dataintoSAP HANA data store, use generic ODBC connector. SeeSAP HANA sinksection with details. Note the linked services for SAP HANA connector and ODBC connector are with different type thus cannot be reused.
Prerequisites
To use this SAP HANA connector, you need to:
Set up a Self-hosted Integration Runtime. SeeSelf-hosted Integration Runtimearticle for details.
Install the SAP HANA ODBC driver on the Integration Runtime machine. You can download the SAP HANA ODBC driver from theSAP Software Download Center. Search with the keywordSAP HANA CLIENT for Windows.
Getting started
To perform the Copy activity with a pipeline, you can use one of the following tools or SDKs:
The Copy Data tool
The Azure portal
The .NET SDK
The Python SDK
Azure PowerShell
The REST API
The Azure Resource Manager template
Create a linked service to SAP HANA using UI
Use the following steps to create a linked service to SAP HANA in the Azure portal UI.
Browse to the Manage tab in your Azure Data Factory or Synapse workspace and select Linked Services, then click New:Azure Data FactoryAzure Synapse
Browse to the Manage tab in your Azure Data Factory or Synapse workspace and select Linked Services, then click New:
Azure Data Factory
Azure Synapse


Search for SAP and select the SAP HANA connector.
Search for SAP and select the SAP HANA connector.

Configure the service details, test the connection, and create the new linked service.
Configure the service details, test the connection, and create the new linked service.

Connector configuration details
The following sections provide details about properties that are used to define Data Factory entities specific to SAP HANA connector.
Linked service properties
The following properties are supported for SAP HANA linked service:
user@domain.com
Example: use basic authentication
{
    "name": "SapHanaLinkedService",
    "properties": {
        "type": "SapHana",
        "typeProperties": {
            "connectionString": "SERVERNODE=<server>:<port (optional)>;UID=<userName>;PWD=<Password>"
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
{
    "name": "SapHanaLinkedService",
    "properties": {
        "type": "SapHana",
        "typeProperties": {
            "connectionString": "SERVERNODE=<server>:<port (optional)>;UID=<userName>;PWD=<Password>"
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
Example: use Windows authentication
{
    "name": "SapHanaLinkedService",
    "properties": {
        "type": "SapHana",
        "typeProperties": {
            "connectionString": "SERVERNODE=<server>:<port (optional)>;",
            "userName": "<username>", 
            "password": { 
                "type": "SecureString", 
                "value": "<password>" 
            } 
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
{
    "name": "SapHanaLinkedService",
    "properties": {
        "type": "SapHana",
        "typeProperties": {
            "connectionString": "SERVERNODE=<server>:<port (optional)>;",
            "userName": "<username>", 
            "password": { 
                "type": "SecureString", 
                "value": "<password>" 
            } 
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
If you were using SAP HANA linked service with the following payload, it is still supported as-is, while you are suggested to use the new one going forward.
Example:
{
    "name": "SapHanaLinkedService",
    "properties": {
        "type": "SapHana",
        "typeProperties": {
            "server": "<server>:<port (optional)>",
            "authenticationType": "Basic",
            "userName": "<username>",
            "password": {
                "type": "SecureString",
                "value": "<password>"
            }
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
{
    "name": "SapHanaLinkedService",
    "properties": {
        "type": "SapHana",
        "typeProperties": {
            "server": "<server>:<port (optional)>",
            "authenticationType": "Basic",
            "userName": "<username>",
            "password": {
                "type": "SecureString",
                "value": "<password>"
            }
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
Dataset properties
For a full list of sections and properties available for defining datasets, see thedatasetsarticle. This section provides a list of properties supported by SAP HANA dataset.
To copy data from SAP HANA, the following properties are supported:
Example:
{
    "name": "SAPHANADataset",
    "properties": {
        "type": "SapHanaTable",
        "typeProperties": {
            "schema": "<schema name>",
            "table": "<table name>"
        },
        "schema": [],
        "linkedServiceName": {
            "referenceName": "<SAP HANA linked service name>",
            "type": "LinkedServiceReference"
        }
    }
}
{
    "name": "SAPHANADataset",
    "properties": {
        "type": "SapHanaTable",
        "typeProperties": {
            "schema": "<schema name>",
            "table": "<table name>"
        },
        "schema": [],
        "linkedServiceName": {
            "referenceName": "<SAP HANA linked service name>",
            "type": "LinkedServiceReference"
        }
    }
}
If you were usingRelationalTabletyped dataset, it is still supported as-is, while you are suggested to use the new one going forward.
RelationalTable
Copy activity properties
For a full list of sections and properties available for defining activities, see thePipelinesarticle. This section provides a list of properties supported by SAP HANA source.
SAP HANA as source
Tip
To ingest data from SAP HANA efficiently by using data partitioning, learn more fromParallel copy from SAP HANAsection.
To copy data from SAP HANA, the following properties are supported in the copy activitysourcesection:
PhysicalPartitionsOfTable
None
parallelCopies
SapHanaDynamicRange
SapHanaDynamicRange
?AdfHanaDynamicRangePartitionCondition
SapHanaDynamicRange
Example:
"activities":[
    {
        "name": "CopyFromSAPHANA",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "<SAP HANA input dataset name>",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "<output dataset name>",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "SapHanaSource",
                "query": "<SQL query for SAP HANA>"
            },
            "sink": {
                "type": "<sink type>"
            }
        }
    }
]
"activities":[
    {
        "name": "CopyFromSAPHANA",
        "type": "Copy",
        "inputs": [
            {
                "referenceName": "<SAP HANA input dataset name>",
                "type": "DatasetReference"
            }
        ],
        "outputs": [
            {
                "referenceName": "<output dataset name>",
                "type": "DatasetReference"
            }
        ],
        "typeProperties": {
            "source": {
                "type": "SapHanaSource",
                "query": "<SQL query for SAP HANA>"
            },
            "sink": {
                "type": "<sink type>"
            }
        }
    }
]
If you were usingRelationalSourcetyped copy source, it is still supported as-is, while you are suggested to use the new one going forward.
RelationalSource
Parallel copy from SAP HANA
The SAP HANA connector provides built-in data partitioning to copy data from SAP HANA in parallel. You can find data partitioning options on theSourcetable of the copy activity.

When you enable partitioned copy, the service runs parallel queries against your SAP HANA source to retrieve data by partitions. The parallel degree is controlled by theparallelCopiessetting on the copy activity. For example, if you setparallelCopiesto four, the service concurrently generates and runs four queries based on your specified partition option and settings, and each query retrieves a portion of data from your SAP HANA.
parallelCopies
parallelCopies
You are suggested to enable parallel copy with data partitioning especially when you ingest large amount of data from your SAP HANA. The following are suggested configurations for different scenarios. When copying data into file-based data store, it's recommended to write to a folder as multiple files (only specify folder name), in which case the performance is better than writing to a single file.
SELECT * FROM <TABLENAME> WHERE (?AdfHanaDynamicRangePartitionCondition) AND <your_additional_where_clause>
?AdfHanaDynamicRangePartitionCondition
SELECT * FROM (SELECT *, CONCAT(<KeyColumn1>, <KeyColumn2>) AS PARTITIONCOLUMN FROM <TABLENAME>) WHERE (?AdfHanaDynamicRangePartitionCondition)
Example: query with physical partitions of a table
"source": {
    "type": "SapHanaSource",
    "partitionOption": "PhysicalPartitionsOfTable"
}
"source": {
    "type": "SapHanaSource",
    "partitionOption": "PhysicalPartitionsOfTable"
}
Example: query with dynamic range partition
"source": {
    "type": "SapHanaSource",
    "query":â¯"SELECT * FROM <TABLENAME> WHERE (?AdfHanaDynamicRangePartitionCondition) AND <your_additional_where_clause>",
    "partitionOption": "SapHanaDynamicRange",
    "partitionSettings": {
        "partitionColumnName": "<Partition_column_name>"
    }
}
"source": {
    "type": "SapHanaSource",
    "query":â¯"SELECT * FROM <TABLENAME> WHERE (?AdfHanaDynamicRangePartitionCondition) AND <your_additional_where_clause>",
    "partitionOption": "SapHanaDynamicRange",
    "partitionSettings": {
        "partitionColumnName": "<Partition_column_name>"
    }
}
Data type mapping for SAP HANA
When copying data from SAP HANA, the following mappings are used from SAP HANA data types to interim data types used internally within the service. SeeSchema and data type mappingsto learn about how copy activity maps the source schema and data type to the sink.
SAP HANA sink
Currently, the SAP HANA connector is not supported as sink, while you can use generic ODBC connector with SAP HANA driver to write data into SAP HANA.
Follow thePrerequisitesto set up Self-hosted Integration Runtime and install SAP HANA ODBC driver first. Create an ODBC linked service to connect to your SAP HANA data store as shown in the following example, then create dataset and copy activity sink with ODBC type accordingly. Learn more fromODBC connectorarticle.
{
    "name": "SAPHANAViaODBCLinkedService",
    "properties": {
        "type": "Odbc",
        "typeProperties": {
            "connectionString": "Driver={HDBODBC};servernode=<HANA server>.clouddatahub-int.net:30015",
            "authenticationType": "Basic",
            "userName": "<username>",
            "password": {
                "type": "SecureString",
                "value": "<password>"
            }
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
{
    "name": "SAPHANAViaODBCLinkedService",
    "properties": {
        "type": "Odbc",
        "typeProperties": {
            "connectionString": "Driver={HDBODBC};servernode=<HANA server>.clouddatahub-int.net:30015",
            "authenticationType": "Basic",
            "userName": "<username>",
            "password": {
                "type": "SecureString",
                "value": "<password>"
            }
        },
        "connectVia": {
            "referenceName": "<name of Integration Runtime>",
            "type": "IntegrationRuntimeReference"
        }
    }
}
Lookup activity properties
To learn details about the properties, checkLookup activity.
Related content
For a list of data stores supported as sources and sinks by the copy activity, seesupported data stores.
Feedback
Was this page helpful?
Additional resources