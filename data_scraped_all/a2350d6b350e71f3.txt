Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Use external tables with Synapse SQL
Article
2025-02-19
20 contributors
In this article
An external table points to data located in Hadoop, Azure Storage blob, or Azure Data Lake Storage (ADLS).
You can use external tables to read data from files or write data to files in Azure Storage. With Azure Synapse SQL, you can use external tables to read external data using dedicated SQL pool or serverless SQL pool.
Depending on the type of the external data source, you can use two types of external tables:
Hadoop external tablesthat you can use to read and export data in various data formats such as CSV, Parquet, and ORC. Hadoop external tables are available in dedicated SQL pools, but they aren't available in serverless SQL pools.
Native external tablesthat you can use to read and export data in various data formats such as CSV and Parquet. Native external tables are available in serverless SQL pools and in dedicated SQL pools. Writing/exporting data using CETAS and the native external tables is available only in the serverless SQL pool, but not in the dedicated SQL pools.
The key differences between Hadoop and native external tables:
Latin1_General_100_BIN2_UTF8
VARCHAR
/year=*/month=*/day=*
/logs/**
/**
Note
The native external tables are the recommended solution in the pools where they're generally available. If you need to access external data, always use the native tables in serverless or dedicated pools. Use the Hadoop tables only if you need to access some types that aren't supported in native external tables (for example - ORC, RC), or if the native version isn't available.
External tables in dedicated SQL pool and serverless SQL pool
You can use external tables to:
Query Azure Blob Storage and ADLS Gen2 with Transact-SQL statements.
Store query results to files in Azure Blob Storage or Azure Data Lake Storage usingCETAS with Synapse SQL.
Import data from Azure Blob Storage and Azure Data Lake Storage and store it in a dedicated SQL pool (only Hadoop tables in dedicated pool).
Note
When used with theCREATE TABLE AS SELECTstatement, selecting from an external table imports data into a table within thededicatedSQL pool.
If performance of Hadoop external tables in the dedicated pools does not satisfy your performance goals, consider loading external data into the Data warehouse tables using theCOPY statement.
For a loading tutorial, seeUse PolyBase to load data from Azure Blob Storage.
You can create external tables in Synapse SQL pools via the following steps:
CREATE EXTERNAL DATA SOURCEto reference an external Azure storage and specify the credential that should be used to access the storage.
CREATE EXTERNAL FILE FORMATto describe format of CSV or Parquet files.
CREATE EXTERNAL TABLEon top of the files placed on the data source with the same file format.
Folder partition elimination
The native external tables in Synapse pools are able to ignore the files placed in the folders that aren't relevant for the queries. If your files are stored in a folder hierarchy (for example -/year=2020/month=03/day=16) and the values foryear,month, anddayare exposed as the columns, the queries that contain filters likeyear=2020will read the files only from the subfolders placed within theyear=2020folder. The files and folders placed in other folders (year=2021oryear=2022) will be ignored in this query. This elimination is knownÂ aspartition elimination.
/year=2020/month=03/day=16
year
month
day
year=2020
year=2020
year=2021
year=2022
The folder partition elimination is available in the native external tables that are synchronized from the Synapse Spark pools. If you have partitioned data set and you would like to use the partition elimination with the external tables that you create, usethe partitioned viewsinstead of the external tables.
File elimination
Some data formats such as Parquet and Delta contain file statistics for each column (for example, min/max values for each column). The queries that filter data won't read the files where the required column values don't exist. The query will first explore min/max values for the columns used in the query predicate to find the files that don't contain the required data. These files are ignored and eliminated from the query plan.
This technique is also known as filter predicate pushdown and it can improve the performance of your queries. Filter pushdown is available in the serverless SQL pools on Parquet and Delta formats. To apply filter pushdown for the string types, use the VARCHAR type with theLatin1_General_100_BIN2_UTF8collation. For more information on collations, seeDatabase collation support for Synapse SQL in Azure Synapse Analytics.
Latin1_General_100_BIN2_UTF8
Security
User must haveSELECTpermission on an external table to read the data.
External tables access underlying Azure storage using the database scoped credential defined in data source using the following rules:
SELECT
Data source without credential enables external tables to access publicly available files on Azure storage.
Data source can have a credential that enables external tables to access only the files on Azure storage using SAS token or workspace Managed Identity - For examples, seethe Develop storage files storage access controlarticle.
Example for CREATE EXTERNAL DATA SOURCE
Hadoop
Native
The following example creates a Hadoop external data source in dedicated SQL pool for ADLS Gen2 pointing to the public New York data set:
CREATE DATABASE SCOPED CREDENTIAL [ADLS_credential]
WITH IDENTITY='SHARED ACCESS SIGNATURE',  
SECRET = 'sv=2022-11-02&ss=b&srt=co&sp=rl&se=2042-11-26T17:40:55Z&st=2024-11-24T09:40:55Z&spr=https&sig=DKZDuSeZhuCWP9IytWLQwu9shcI5pTJ%2Fw5Crw6fD%2BC8%3D'
GO
CREATE EXTERNAL DATA SOURCE AzureDataLakeStore
WITH
  -- Please note the abfss endpoint when your account has secure transfer enabled
  ( LOCATION = 'abfss://data@newyorktaxidataset.dfs.core.windows.net' ,
    CREDENTIAL = ADLS_credential ,
    TYPE = HADOOP
  ) ;
CREATE DATABASE SCOPED CREDENTIAL [ADLS_credential]
WITH IDENTITY='SHARED ACCESS SIGNATURE',  
SECRET = 'sv=2022-11-02&ss=b&srt=co&sp=rl&se=2042-11-26T17:40:55Z&st=2024-11-24T09:40:55Z&spr=https&sig=DKZDuSeZhuCWP9IytWLQwu9shcI5pTJ%2Fw5Crw6fD%2BC8%3D'
GO
CREATE EXTERNAL DATA SOURCE AzureDataLakeStore
WITH
  -- Please note the abfss endpoint when your account has secure transfer enabled
  ( LOCATION = 'abfss://data@newyorktaxidataset.dfs.core.windows.net' ,
    CREDENTIAL = ADLS_credential ,
    TYPE = HADOOP
  ) ;
The following example creates an external data source for ADLS Gen2 pointing to the publicly available New York data set:
CREATE EXTERNAL DATA SOURCE YellowTaxi
WITH ( LOCATION = 'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/',
       TYPE = HADOOP)
CREATE EXTERNAL DATA SOURCE YellowTaxi
WITH ( LOCATION = 'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/',
       TYPE = HADOOP)
The following example creates an external data source in serverless or dedicated SQL pool for ADLS Gen2 that can be accessed using SAS credential:
CREATE DATABASE SCOPED CREDENTIAL [sqlondemand]
WITH IDENTITY='SHARED ACCESS SIGNATURE',  
SECRET = 'sv=2022-11-02&ss=b&srt=co&sp=rl&se=2042-11-26T17:40:55Z&st=2024-11-24T09:40:55Z&spr=https&sig=DKZDuSeZhuCWP9IytWLQwu9shcI5pTJ%2Fw5Crw6fD%2BC8%3D'
GO
CREATE EXTERNAL DATA SOURCE SqlOnDemandDemo WITH (
    LOCATION = 'https://sqlondemandstorage.blob.core.windows.net',
    CREDENTIAL = sqlondemand
);
CREATE DATABASE SCOPED CREDENTIAL [sqlondemand]
WITH IDENTITY='SHARED ACCESS SIGNATURE',  
SECRET = 'sv=2022-11-02&ss=b&srt=co&sp=rl&se=2042-11-26T17:40:55Z&st=2024-11-24T09:40:55Z&spr=https&sig=DKZDuSeZhuCWP9IytWLQwu9shcI5pTJ%2Fw5Crw6fD%2BC8%3D'
GO
CREATE EXTERNAL DATA SOURCE SqlOnDemandDemo WITH (
    LOCATION = 'https://sqlondemandstorage.blob.core.windows.net',
    CREDENTIAL = sqlondemand
);
Note
The SQL users need to have proper permissions on database scoped credentials to access the data source in Azure Synapse Analytics Serverless SQL Pool.Access external storage using serverless SQL pool in Azure Synapse Analytics.
The following example creates an external data source for ADLS Gen2 pointing to the publicly available New York data set:
CREATE EXTERNAL DATA SOURCE YellowTaxi
WITH ( LOCATION = 'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/')
CREATE EXTERNAL DATA SOURCE YellowTaxi
WITH ( LOCATION = 'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/')
Example for CREATE EXTERNAL FILE FORMAT
The following example creates an external file format for census files:
CREATE EXTERNAL FILE FORMAT census_file_format
WITH
(  
    FORMAT_TYPE = PARQUET,
    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'
)
CREATE EXTERNAL FILE FORMAT census_file_format
WITH
(  
    FORMAT_TYPE = PARQUET,
    DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'
)
Example CREATE EXTERNAL TABLE
The following example creates an external table. It returns the first row:
CREATE EXTERNAL TABLE census_external_table
(
    decennialTime varchar(20),
    stateName varchar(100),
    countyName varchar(100),
    population int,
    race varchar(50),
    sex    varchar(10),
    minAge int,
    maxAge int
)  
WITH (
    LOCATION = '/parquet/',
    DATA_SOURCE = population_ds,  
    FILE_FORMAT = census_file_format
)
GO

SELECT TOP 1 * FROM census_external_table
CREATE EXTERNAL TABLE census_external_table
(
    decennialTime varchar(20),
    stateName varchar(100),
    countyName varchar(100),
    population int,
    race varchar(50),
    sex    varchar(10),
    minAge int,
    maxAge int
)  
WITH (
    LOCATION = '/parquet/',
    DATA_SOURCE = population_ds,  
    FILE_FORMAT = census_file_format
)
GO

SELECT TOP 1 * FROM census_external_table
Create and query external tables from a file in Azure Data Lake
Using Data Lake exploration capabilities of Synapse Studio you can now create and query an external table using Synapse SQL pool with a right-click on the file. The one-click gesture to create external tables from the ADLS Gen2 storage account is only supported for Parquet files.
Prerequisites
You must have access to the workspace with at least theStorage Blob Data Contributoraccess role to the ADLS Gen2 account or Access Control Lists (ACL) that enable you to query the files.
You must have access to the workspace with at least theStorage Blob Data Contributoraccess role to the ADLS Gen2 account or Access Control Lists (ACL) that enable you to query the files.
Storage Blob Data Contributor
You must have at leastpermissions to create an external tableand query external tables on the Synapse SQL pool (dedicated or serverless).
You must have at leastpermissions to create an external tableand query external tables on the Synapse SQL pool (dedicated or serverless).
From the Data panel, select the file that you would like to create the external table from:

A dialog window will open. Select dedicated SQL pool or serverless SQL pool, give a name to the table and select open script:

The SQL Script is autogenerated inferring the schema from the file:

Run the script. The script will automatically run aSELECT TOP 100 *:
SELECT TOP 100 *

The external table is now created. You can now query the external table directly from the Data pane.
Related content
See theCETASarticle for how to save query results to an external table in Azure Storage. Or you can start queryingApache Spark for Azure Synapse external tables.
Feedback
Was this page helpful?
Additional resources