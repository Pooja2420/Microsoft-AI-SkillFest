Note
Access to this page requires authorization. You can trysigning inorchanging directories.
Access to this page requires authorization. You can trychanging directories.
Tutorial: Build an ETL pipeline with Apache Spark on the Databricks platform
Article
2025-04-04
5 contributors
In this article
This tutorial shows you how to develop and deploy your first ETL (extract, transform, and load) pipeline for data orchestration with Apache Spark. Although this tutorial uses Databricks all-purpose compute, you can also use serverless compute if it's enabled for your workspace.
You can also use DLT to build ETL pipelines. Databricks DLT reduces the complexity of building, deploying, and maintaining production ETL pipelines. SeeTutorial: Build an ETL pipeline with DLT.
By the end of this article, you will know how to:
Launching a Databricks all-purpose compute cluster.
Creating a Databricks notebook.
Configuring incremental data ingestion to Delta Lake with Auto Loader.
Executing notebook cells to process, query, and preview data.
Scheduling a notebook as a Databricks job.
This tutorial uses interactive notebooks to complete common ETL tasks in Python or Scala.
You can also use theDatabricks Terraform providerto create this articleâs resources. SeeCreate clusters, notebooks, and jobs with Terraform.
Requirements
You are logged into a Azure Databricks workspace.
You havepermission to create a cluster.
Note
If you do not have cluster control privileges, you can still complete most of the steps below as long as you haveaccess to a cluster.
Step 1: Create a cluster
To do exploratory data analysis and data engineering, create a cluster to provide the compute resources needed to execute commands.
ClickComputein the sidebar.
On the Compute page, clickCreate Cluster. This opens the New Cluster page.
Specify a unique name for the cluster, leave the remaining values in their default state, and clickCreate Cluster.
To learn more about Databricks clusters, seeCompute.
Step 2: Create a Databricks notebook
To create a notebook in your workspace, clickNewin the sidebar, and then clickNotebook. A blank notebook opens in the workspace.
To learn more about creating and managing notebooks, seeManage notebooks.
Step 3: Configure Auto Loader to ingest data to Delta Lake
Databricks recommends usingAuto Loaderfor incremental data ingestion. Auto Loader automatically detects and processes new files as they arrive in cloud object storage.
Databricks recommends storing data withDelta Lake. Delta Lake is an open source storage layer that provides ACID transactions and enables the data lakehouse. Delta Lake is the default format for tables created in Databricks.
To configure Auto Loader to ingest data to a Delta Lake table, copy and paste the following code into the empty cell in your notebook:
Python
# Import functions
from pyspark.sql.functions import col, current_timestamp

# Define variables used in code below
file_path = "/databricks-datasets/structured-streaming/events"
username = spark.sql("SELECT regexp_replace(current_user(), '[^a-zA-Z0-9]', '_')").first()[0]
table_name = f"{username}_etl_quickstart"
checkpoint_path = f"/tmp/{username}/_checkpoint/etl_quickstart"

# Clear out data from previous demo execution
spark.sql(f"DROP TABLE IF EXISTS {table_name}")
dbutils.fs.rm(checkpoint_path, True)

# Configure Auto Loader to ingest JSON data to a Delta table
(spark.readStream
  .format("cloudFiles")
  .option("cloudFiles.format", "json")
  .option("cloudFiles.schemaLocation", checkpoint_path)
  .load(file_path)
  .select("*", col("_metadata.file_path").alias("source_file"), current_timestamp().alias("processing_time"))
  .writeStream
  .option("checkpointLocation", checkpoint_path)
  .trigger(availableNow=True)
  .toTable(table_name))
# Import functions
from pyspark.sql.functions import col, current_timestamp

# Define variables used in code below
file_path = "/databricks-datasets/structured-streaming/events"
username = spark.sql("SELECT regexp_replace(current_user(), '[^a-zA-Z0-9]', '_')").first()[0]
table_name = f"{username}_etl_quickstart"
checkpoint_path = f"/tmp/{username}/_checkpoint/etl_quickstart"

# Clear out data from previous demo execution
spark.sql(f"DROP TABLE IF EXISTS {table_name}")
dbutils.fs.rm(checkpoint_path, True)

# Configure Auto Loader to ingest JSON data to a Delta table
(spark.readStream
  .format("cloudFiles")
  .option("cloudFiles.format", "json")
  .option("cloudFiles.schemaLocation", checkpoint_path)
  .load(file_path)
  .select("*", col("_metadata.file_path").alias("source_file"), current_timestamp().alias("processing_time"))
  .writeStream
  .option("checkpointLocation", checkpoint_path)
  .trigger(availableNow=True)
  .toTable(table_name))
Scala
// Imports
import org.apache.spark.sql.functions.current_timestamp
import org.apache.spark.sql.streaming.Trigger
import spark.implicits._

// Define variables used in code below
val file_path = "/databricks-datasets/structured-streaming/events"
val username = spark.sql("SELECT regexp_replace(current_user(), '[^a-zA-Z0-9]', '_')").first.get(0)
val table_name = s"${username}_etl_quickstart"
val checkpoint_path = s"/tmp/${username}/_checkpoint"

// Clear out data from previous demo execution
spark.sql(s"DROP TABLE IF EXISTS ${table_name}")
dbutils.fs.rm(checkpoint_path, true)

// Configure Auto Loader to ingest JSON data to a Delta table
spark.readStream
  .format("cloudFiles")
  .option("cloudFiles.format", "json")
  .option("cloudFiles.schemaLocation", checkpoint_path)
  .load(file_path)
  .select($"*", $"_metadata.file_path".as("source_file"), current_timestamp.as("processing_time"))
  .writeStream
  .option("checkpointLocation", checkpoint_path)
  .trigger(Trigger.AvailableNow)
  .toTable(table_name)
// Imports
import org.apache.spark.sql.functions.current_timestamp
import org.apache.spark.sql.streaming.Trigger
import spark.implicits._

// Define variables used in code below
val file_path = "/databricks-datasets/structured-streaming/events"
val username = spark.sql("SELECT regexp_replace(current_user(), '[^a-zA-Z0-9]', '_')").first.get(0)
val table_name = s"${username}_etl_quickstart"
val checkpoint_path = s"/tmp/${username}/_checkpoint"

// Clear out data from previous demo execution
spark.sql(s"DROP TABLE IF EXISTS ${table_name}")
dbutils.fs.rm(checkpoint_path, true)

// Configure Auto Loader to ingest JSON data to a Delta table
spark.readStream
  .format("cloudFiles")
  .option("cloudFiles.format", "json")
  .option("cloudFiles.schemaLocation", checkpoint_path)
  .load(file_path)
  .select($"*", $"_metadata.file_path".as("source_file"), current_timestamp.as("processing_time"))
  .writeStream
  .option("checkpointLocation", checkpoint_path)
  .trigger(Trigger.AvailableNow)
  .toTable(table_name)
Note
The variables defined in this code should allow you to safely execute it without risk of conflicting with existing workspace assets or other users. Restricted network or storage permissions will raise errors when executing this code; contact your workspace administrator to troubleshoot these restrictions.
To learn more about Auto Loader, seeWhat is Auto Loader?.
Step 4: Process and interact with data
Notebooks execute logic cell-by-cell. To execute the logic in your cell:
To run the cell you completed in the previous step, select the cell and pressSHIFT+ENTER.
To run the cell you completed in the previous step, select the cell and pressSHIFT+ENTER.
To query the table youâve just created, copy and paste the following code into an empty cell, then pressSHIFT+ENTERto run the cell.Pythondf = spark.read.table(table_name)Scalaval df = spark.read.table(table_name)
To query the table youâve just created, copy and paste the following code into an empty cell, then pressSHIFT+ENTERto run the cell.
Python
df = spark.read.table(table_name)
df = spark.read.table(table_name)
Scala
val df = spark.read.table(table_name)
val df = spark.read.table(table_name)
To preview the data in your DataFrame, copy and paste the following code into an empty cell, then pressSHIFT+ENTERto run the cell.Pythondisplay(df)Scaladisplay(df)
To preview the data in your DataFrame, copy and paste the following code into an empty cell, then pressSHIFT+ENTERto run the cell.
Python
display(df)
display(df)
Scala
display(df)
display(df)
To learn more about interactive options for visualizing data, seeVisualizations in Databricks notebooks.
Step 5: Schedule a job
You can run Databricks notebooks as production scripts by adding them as a task in a Databricks job. In this step, you will create a new job that you can trigger manually.
To schedule your notebook as a task:
ClickScheduleon the right side of the header bar.
Enter a unique name for theJob name.
ClickManual.
In theClusterdrop-down, select the cluster you created in step 1.
ClickCreate.
In the window that appears, clickRun now.
To see the job run results, click theicon next to theLast runtimestamp.
For more information on jobs, seeWhat are jobs?.
Additional integrations
Learn more about integrations and tools for data engineering with Azure Databricks:
Tutorial: Build an ETL pipeline with DLT
Local development tools
Connect to dbt Core
What is the Databricks CLI?
Databricks Terraform provider
Feedback
Was this page helpful?
Additional resources